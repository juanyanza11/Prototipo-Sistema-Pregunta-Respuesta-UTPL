Pagina|Topic|Contenido|Fecha_Modificacion
Pollution|General| Lists Categories Pollution is the introduction of contaminants into the natural environment that cause adverse change.  Pollution can take the form of any substance (solid, liquid, or gas) or energy (such as radioactivity, heat, sound, or light). Pollutants, the components of pollution, can be either foreign substances/energies or naturally occurring contaminants. Although environmental pollution can be caused by natural events, the word pollution generally implies that the contaminants have an anthropogenic source – that is, a source created by human activities, such as manufacturing, extractive industries, poor waste management, transportation or agriculture. Pollution is often classed as point source (coming from a highly concentrated specific site, such as a factory or mine) or nonpoint source pollution (coming from a widespread distributed sources, such as microplastics or agricultural runoff). Many sources of pollution were unregulated parts of industrialization during the 19th and 20th centuries until the emergence of environmental regulation and pollution policy in the later half of the 20th century. Sites where historically polluting industries released persistent pollutants may have legacy pollution long after the source of the pollution is stopped.  Major forms of pollution include air pollution, light pollution, litter, noise pollution, plastic pollution, soil contamination, radioactive contamination, thermal pollution, visual pollution, and water pollution. Pollution has widespread consequences on human and environmental health, having systematic impact on social and economic systems. In 2019, pollution killed nine million people worldwide (one in six deaths), a number unchanged since 2015.    Air pollution accounted for 3⁄4 of these earlier deaths.   A 2022 literature review found that levels of anthropogenic chemical pollution have exceeded planetary boundaries and now threaten entire ecosystems around the world.   Pollutants frequently have outsized impacts on vulnerable populations, such as children and the elderly, and   marginalized communities, because polluting industries and toxic waste sites tend to be collocated with populations with less economic and political power.  This outsized impact is a core reason for the formation of the environmental justice movement,   and continues to be a core element of environmental conflicts, particularly in the Global South. Because of the impacts of these chemicals, local, country and international policy have increasingly sought to regulate pollutants, resulting in increasing air and water quality standards, alongside regulation of specific waste streams. Regional and national policy is typically supervised by environmental agencies or ministries, while international efforts are coordinated by the UN Environmental Program and other treaty bodies. Pollution mitigation is an important part of all of the Sustainable Development Goals.|2023-08-27-09-39-07
Pollution|Definitions and types|" Various definitions of pollution exist, which may or may not recognize certain types, such as noise pollution or greenhouse gases. The United States Environmental Protection Administration defines pollution as ""Any substances in water, soil, or air that degrade the natural quality of the environment, offend the senses of sight, taste, or smell, or cause a health hazard. The usefulness of the natural resource is usually impaired by the presence of pollutants and contaminants.""  In contrast, the United Nations considers pollution to be the ""presence of substances and heat in environmental media (air, water, land) whose nature, location, or quantity produces undesirable environmental effects."" The major forms of pollution are listed below along with the particular contaminants relevant to each of them:"|2023-08-27-09-39-07
Pollution|Pollutants| A pollutant or novel entity  is a substance or energy introduced into the environment that has undesired effects, or adversely affects the usefulness of a resource. These can be both naturally forming (i.e. minerals or extracted compounds like oil) or anthropogenic in origin  (i.e. manufactured materials or byproducts). Pollutants result in environmental pollution or become public health concerns when they reach a concentration high enough to have significant negative impacts. A pollutant may cause long- or short-term damage by changing the growth rate of plant or animal species, or by interfering with resources used by humans, human health or wellbeing, or property values. Some pollutants are biodegradable and therefore will not persist in the environment in the long term. However, the degradation products of some pollutants are themselves polluting such as the products DDE and DDD produced from the degradation of DDT.|2023-08-27-09-39-07
Pollution|Natural causes| One of the most significant natural sources of pollution are volcanoes, which during eruptions release large quantities of harmful gases into the atmosphere. Volcanic gases include carbon dioxide, which can be fatal in large concentrations and contributes to climate change, hydrogen halides which can cause acid rain, sulfur dioxides, which are harmful to animals and damage the ozone layer, and hydrogen sulfides, which are capable of killing humans at concentrations of less than 1 part per thousand.  Volcanic emissions also include fine and ultrafine particles which may contain toxic chemicals and substances such as arsenic, lead, and mercury. Wildfires, which can be caused naturally by lightning strikes, are also a significant source of air pollution. Wildfire smoke contains significant quantities of both carbon dioxide and carbon monoxide, which can cause suffocation. Large quantities of fine particulates are found within wildfire smoke as well, which pose a health risk to animals.|2023-08-27-09-39-07
Pollution|Human generation| Motor vehicle emissions are one of the leading causes of air pollution.    China, United States, Russia, India  Mexico, and Japan are the world leaders in air pollution emissions. Principal stationary pollution sources include chemical plants, coal-fired power plants, oil refineries,  petrochemical plants, nuclear waste disposal activity, incinerators, large livestock farms (dairy cows, pigs, poultry, etc.), PVC factories, metals production factories, plastics factories, and other heavy industry. Agricultural air pollution comes from contemporary practices which include clear felling and burning of natural vegetation as well as spraying of pesticides and herbicides. About 400 million metric tons of hazardous wastes are generated each year.  The United States alone produces about 250 million metric tons.  Americans constitute less than 5% of the world's population, but produce roughly 25% of the world's CO2,  and generate approximately 30% of world's waste.   In 2007, China overtook the United States as the world's biggest producer of CO2,  while still far behind based on per capita pollution (ranked 78th among the world's nations). Chlorinated hydrocarbons (CFH), heavy metals (such as chromium, cadmium – found in rechargeable batteries, and lead – found in lead paint, aviation fuel, and even in certain countries, gasoline), MTBE, zinc, arsenic, and benzene are some of the most frequent soil contaminants. A series of press reports published in 2001, culminating in the publication of the book Fateful Harvest, revealed a widespread practise of recycling industrial leftovers into fertilizer, resulting in metal poisoning of the soil.  Ordinary municipal landfills are the source of many chemical substances entering the soil environment (and often groundwater), emanating from the wide variety of refuse accepted, especially substances illegally discarded there, or from pre-1970 landfills that may have been subject to little control in the U.S. or EU. There have also been some unusual releases of polychlorinated dibenzodioxins, commonly called dioxins for simplicity, such as TCDD. Pollution can also occur as a result of natural disasters. Hurricanes, for example, frequently result in sewage contamination and petrochemical spills from burst boats or automobiles. When coastal oil rigs or refineries are involved, larger-scale and environmental damage is not unusual. When accidents occur, some pollution sources, such as nuclear power stations or oil ships, can create extensive and potentially catastrophic emissions. The motor vehicle is the most common cause of noise pollution, accounting for over 90% of all undesirable noise globally. Plastic pollution is choking our oceans by making plastic gyres, entangling marine animals, poisoning our food and water supply, and ultimately inflicting havoc on the health and well-being of humans and wildlife globally. With the exception of a small amount that has been incinerating, virtually every piece of plastic that was ever made in the past still exists in one form or another.  And since most of the plastics do not biodegrade in any meaningful sense, all that plastic waste could exist for hundreds or even thousands of years. If plastic production is not circumscribed, plastic pollution will be disastrous and will eventually outweigh fish in oceans.|2023-08-27-09-39-07
Pollution|Greenhouse gas emissions| Carbon dioxide, while vital for photosynthesis, is sometimes referred to as pollution, because raised levels of the gas in the atmosphere are affecting the Earth's climate. Disruption of the environment can also highlight the connection between areas of pollution that would normally be classified separately, such as those of water and air. Recent studies have investigated the potential for long-term rising levels of atmospheric carbon dioxide to cause slight but critical increases in the acidity of ocean waters, and the possible effects of this on marine ecosystems. In February 2007, a report by the Intergovernmental Panel on Climate Change (IPCC), representing the work of 2,500 scientists, economists, and policymakers from more than 120 countries, confirmed that humans have been the primary cause of global warming since 1950. Humans have ways to cut greenhouse gas emissions and avoid the consequences of global warming, a major climate report concluded. But to change the climate, the transition from fossil fuels like coal and oil needs to occur within decades, according to the final report this year from the UN's Intergovernmental Panel on Climate Change (IPCC).|2023-08-27-09-39-07
Pollution|Human health|" Pollution affects humans in every part of the world. An October 2017 study by the Lancet Commission on Pollution and Health found that global pollution, specifically toxic air, water, soil and workplaces, kills nine million people annually, which is triple the number of deaths caused by AIDS, tuberculosis and malaria combined, and 15 times higher than deaths caused by wars and other forms of human violence.  The study concluded that ""pollution is one of the great existential challenges of the Anthropocene era. Pollution endangers the stability of the Earth's support systems and threatens the continuing survival of human societies."" Adverse air quality can kill many organisms, including humans. Ozone pollution can cause respiratory disease, cardiovascular disease, throat inflammation, chest pain, and congestion. A 2010 analysis estimated that 1.2 million people died prematurely each year in China alone because of air pollution.  China's high smog levels can damage the human body and cause various diseases.  In 2019, air pollution caused 1.67 million deaths in India (17.8% of total deaths nationally).  Studies have estimated that the number of people killed annually in the United States could be over 50,000.  A study published in 2022 in GeoHealth concluded that energy-related fossil fuel emissions in the United States cause 46,900–59,400 premature deaths each year and PM2.5-related illness and death costs the nation $537–$678 billion annually. In 2019, water pollution caused 1.4 million premature deaths.  Contamination of drinking water by untreated sewage in developing countries is an issue, for example, over 732 million Indians (56% of the population) and over 92 million Ethiopians (92.9% of the population) do not have access to basic sanitation.  In 2013 over 10 million people in India fell ill with waterborne illnesses in 2013, and 1,535 people died, most of them children.  As of 2007, nearly 500 million Chinese lack access to safe drinking water. Acute exposure to certain pollutants can have short and long term effects. Oil spills can cause skin irritations and rashes. Noise pollution induces hearing loss, high blood pressure, stress, and sleep disturbance. Mercury has been linked to developmental deficits in children and neurologic symptoms. Older people are significantly exposed to diseases induced by air pollution. Those with heart or lung disorders are at additional risk. Children and infants are also at serious risk. Lead and other heavy metals have been shown to cause neurological problems, intellectual disabilities and behavioural problems.  Chemical and radioactive substances can cause cancer and birth defects. The health impacts of pollution have both direct and lasting social consequences. A 2021 study found that exposure to pollution causes an increase in violent crime.  A 2019 paper linked pollution to adverse school outcomes for children. A number of studies show that pollution has an adverse effect on the productivity of both indoor and outdoor workers."|2023-08-27-09-39-07
Pollution|Environment| Pollution has been found to be present widely in the environment.A 2022 study published in Environmental Science & Technology found that levels of anthropogenic chemical pollution have exceeded planetary boundaries and now threaten entire ecosystems around the world. There are a number of effects of this:|2023-08-27-09-39-07
Pollution|Regulation and monitoring| To protect the environment from the adverse effects of pollution, many nations worldwide have enacted legislation to regulate various types of pollution as well as to mitigate the adverse effects of pollution. At the local level, regulation usually is supervised by environmental agencies or the broader public health system. Different jurisdictions often have different levels regulation and policy choices about pollution. Historically, polluters will lobby governments in less economically developed areas or countries to maintain lax regulation in order to protect industrialisation at the cost of human and environmental health.[citation needed] The modern environmental regulatory environment has its origins in the United States with the beginning of industrial regulations around Air and Water pollution connected to industry and mining during the 1960s and 1970s.|2023-08-27-09-39-07
Pollution|Control|" Pollution control is a term used in environmental management. It means the control of emissions and effluents into air, water or soil. Without pollution control, the waste products from overconsumption, heating, agriculture, mining, manufacturing, transportation and other human activities, whether they accumulate or disperse, will degrade the environment. In the hierarchy of controls, pollution prevention and waste minimization are more desirable than pollution control. In the field of land development, low impact development is a similar technique for the prevention of urban runoff. Policy, law and monitoring/transparency/life-cycle assessment-attached economics could be developed and enforced to control pollution.  A review concluded that there is a lack of attention and action such as work on a globally supported ""formal science–policy interface"", e.g. to ""inform intervention, influence research, and guide funding""."|2023-08-27-09-39-07
Pollution|Cost|" Pollution has a cost.    Manufacturing activities that cause air pollution impose health and clean-up costs on the whole of society. A manufacturing activity that causes air pollution is an example of a negative externality in production. A negative externality in production occurs ""when a firm's production reduces the well-being of others who are not compensated by the firm.""  For example, if a laundry firm exists near a polluting steel manufacturing firm, there will be increased costs for the laundry firm because of the dirt and smoke produced by the steel manufacturing firm.  If external costs exist, such as those created by pollution, the manufacturer will choose to produce more of the product than would be produced if the manufacturer were required to pay all associated environmental costs. Because responsibility or consequence for self-directed action lies partly outside the self, an element of externalization is involved. If there are external benefits, such as in public safety, less of the good may be produced than would be the case if the producer were to receive payment for the external benefits to others. Goods and services that involve negative externalities in production, such as those that produce pollution, tend to be overproduced and underpriced since the externality is not being priced into the market. Pollution can also create costs for the firms producing the pollution. Sometimes firms choose, or are forced by regulation, to reduce the amount of pollution that they are producing. The associated costs of doing this are called abatement costs, or marginal abatement costs if measured by each additional unit.  In 2005 pollution abatement capital expenditures and operating costs in the US amounted to nearly $27 billion."|2023-08-27-09-39-07
Pollution|Dirtiest industries| The Pure Earth, an international non-for-profit organization dedicated to eliminating life-threatening pollution in the developing world, issues an annual list of some of the world's most polluting industries. Below is the list for 2016: A 2018 report by the Institute for Agriculture and Trade Policy and GRAIN says that the meat and dairy industries are poised to surpass the oil industry as the world's worst polluters.|2023-08-27-09-39-07
Pollution|Textile industry|" The textile industry is one of the largest polluters in the globalized world of mostly free market dominated socioeconomic systems.[citation needed] Chemically polluted textile wastewater degrades the quality of the soil and water.  The pollution comes from the type of conduct of chemical treatments used e.g., in pretreatment, dyeing, printing, and finishing operations  that many or most market-driven companies use despite ""eco-friendly alternatives"". Textile industry wastewater is considered to be one the largest polluters of water and soil ecosystems, causing ""carcinogenic, mutagenic, genotoxic, cytotoxic and allergenic threats to living organisms"".   The textile industry uses over 8000 chemicals in its supply chain,  also polluting the environment with large amounts of microplastics  and has been identified in one review as the industry sector producing the largest amount of pollution. A campaign of big clothing brands like Nike, Adidas and Puma to voluntarily reform their manufacturing supply chains to commit to achieving zero discharges of hazardous chemicals by 2020 (global goal)   appears to have failed."|2023-08-27-09-39-07
Pollution|Fossil fuel related industries| Outdoor air pollution attributable to fossil fuel use alone causes ~3.61 million deaths annually, making it one of the top contributors to human death, beyond being a major driver of climate change whereby greenhouse gases are considered per se as a form of pollution (see above).|2023-08-27-09-39-07
Pollution|Socially optimal level|" Society derives some indirect utility from pollution; otherwise, there would be no incentive to pollute. This utility may come from the consumption of goods and services that inherently create pollution (albeit the level can vary) or lower prices or lower required efforts (or inconvenience) to abandon or substitute these goods and services. Therefore, it is important that policymakers attempt to balance these indirect benefits with the costs of pollution in order to achieve an efficient outcome. [additional citation(s) needed] It is possible to use environmental economics to determine which level of pollution is deemed the social optimum. For economists, pollution is an ""external cost and occurs only when one or more individuals suffer a loss of welfare"". There is a socially optimal level of pollution at which welfare is maximized.  This is because consumers derive utility from the good or service manufactured, which will outweigh the social cost of pollution until a certain point. At this point the damage of one extra unit of pollution to society, the marginal cost of pollution, is exactly equal to the marginal benefit of consuming one more unit of the good or service. Moreover, the feasibility of pollution reduction rates could also be a factor of calculating optimal levels. While a study puts the global mean loss of life expectancy (LLE; similar to YPLL) from air pollution in 2015 at 2.9 years (substantially more than, for example, 0.3 years from all forms of direct violence), it also indicated that a significant fraction of the LLE is unavoidable in terms of current economical-technological feasibility such as aeolian dust and wildfire emission control. In markets with pollution, or other negative externalities in production, the free market equilibrium will not account for the costs of pollution on society. If the social costs of pollution are higher than the private costs incurred by the firm, then the true supply curve will be higher. The point at which the social marginal cost and market demand intersect gives the socially optimal level of pollution. At this point, the quantity will be lower and the price will be higher in comparison to the free market equilibrium.  Therefore, the free market outcome could be considered a market failure because it ""does not maximize efficiency"". This model can be used as a basis to evaluate different methods of internalizing the externality. Some examples include tariffs, a carbon tax and cap and trade systems."|2023-08-27-09-39-07
Pollution|Prior to 19th century|" Air pollution has always accompanied civilizations. Pollution started from prehistoric times, when man created the first fires. According to a 1983 article in the journal Science, ""soot"" found on ceilings of prehistoric caves provides ample evidence of the high levels of pollution that was associated with inadequate ventilation of open fires."" Metal forging appears to be a key turning point in the creation of significant air pollution levels outside the home. Core samples of glaciers in Greenland indicate increases in pollution associated with Greek, Roman, and Chinese metal production. The burning of coal and wood, and the presence of many horses in concentrated areas made the cities the primary sources of pollution. King Edward I of England banned the burning of sea-coal by proclamation in London in 1272, after its smoke became a problem;   the fuel was so common in England that this earliest of names for it was acquired because it could be carted away from some shores by the wheelbarrow."|2023-08-27-09-39-07
Pollution|19th century| It was the Industrial Revolution that gave birth to environmental pollution as we know it today. London also recorded one of the earlier extreme cases of water quality problems with the Great Stink on the Thames of 1858, which led to construction of the London sewerage system soon afterward. Pollution issues escalated as population growth far exceeded viability of neighborhoods to handle their waste problem. Reformers began to demand sewer systems and clean water. In 1870, the sanitary conditions in Berlin were among the worst in Europe. August Bebel recalled conditions before a modern sewer system was built in the late 1870s: Waste-water from the houses collected in the gutters running alongside the curbs and emitted a truly fearsome smell. There were no public toilets in the streets or squares. Visitors, especially women, often became desperate when nature called. In the public buildings the sanitary facilities were unbelievably primitive....As a metropolis, Berlin did not emerge from a state of barbarism into civilization until after 1870.|2023-08-27-09-39-07
Pollution|20th and 21st century|" The primitive conditions were intolerable for a world national capital, and the Imperial German government brought in its scientists, engineers, and urban planners to not only solve the deficiencies, but to forge Berlin as the world's model city. A British expert in 1906 concluded that Berlin represented ""the most complete application of science, order and method of public life,"" adding ""it is a marvel of civic administration, the most modern and most perfectly organized city that there is."" The emergence of great factories and consumption of immense quantities of coal gave rise to unprecedented air pollution and the large volume of industrial chemical discharges added to the growing load of untreated human waste. Chicago and Cincinnati were the first two American cities to enact laws ensuring cleaner air in 1881. Pollution became a major issue in the United States in the early twentieth century, as progressive reformers took issue with air pollution caused by coal burning, water pollution caused by bad sanitation, and street pollution caused by the three million horses who worked in American cities in 1900, generating large quantities of urine and manure. As historian Martin Melosi notes, the generation that first saw automobiles replacing the horses saw cars as ""miracles of cleanliness"".  By the 1940s, automobile-caused smog was a major issue in Los Angeles. Other cities followed around the country until early in the 20th century, when the short lived Office of Air Pollution was created under the Department of the Interior. Extreme smog events were experienced by the cities of Los Angeles and Donora, Pennsylvania, in the late 1940s, serving as another public reminder. Air pollution would continue to be a problem in England, especially later during the industrial revolution, and extending into the recent past with the Great Smog of 1952. Awareness of atmospheric  pollution spread widely after World War II, with fears triggered by reports of radioactive fallout from atomic warfare and testing.  Then a non-nuclear event – the Great Smog of 1952 in London – killed at least 4000 people.  This prompted some of the first major modern environmental legislation: the Clean Air Act of 1956. Pollution began to draw major public attention in the United States between the mid-1950s and early 1970s, when Congress passed the Noise Control Act, the Clean Air Act, the Clean Water Act, and the National Environmental Policy Act. Severe incidents of pollution helped increase consciousness. PCB dumping in the Hudson River resulted in a ban by the EPA on consumption of its fish in 1974. National news stories in the late 1970s – especially the long-term dioxin contamination at Love Canal starting in 1947 and uncontrolled dumping in Valley of the Drums – led to the Superfund legislation of 1980.  The pollution of industrial land gave rise to the name brownfield, a term now common in city planning. The development of nuclear science introduced radioactive contamination, which can remain lethally radioactive for hundreds of thousands of years. Lake Karachay – named by the Worldwatch Institute as the ""most polluted spot"" on earth – served as a disposal site for the Soviet Union throughout the 1950s and 1960s. Chelyabinsk, Russia, is considered the ""Most polluted place on the planet"". Nuclear weapons continued to be tested in the Cold War, especially in the earlier stages of their development. The toll on the worst-affected populations and the growth since then in understanding about the critical threat to human health posed by radioactivity has also been a prohibitive complication associated with nuclear power. Though extreme care is practiced in that industry, the potential for disaster suggested by incidents such as those at Three Mile Island, Chernobyl, and Fukushima pose a lingering specter of public mistrust. Worldwide publicity has been intense on those disasters.  Widespread support for test ban treaties has ended almost all nuclear testing in the atmosphere. International catastrophes such as the wreck of the Amoco Cadiz oil tanker off the coast of Brittany in 1978 and the Bhopal disaster in 1984 have demonstrated the universality of such events and the scale on which efforts to address them needed to engage. The borderless nature of atmosphere and oceans inevitably resulted in the implication of pollution on a planetary level with the issue of global warming. Most recently the term persistent organic pollutant (POP) has come to describe a group of chemicals such as PBDEs and PFCs among others. Though their effects remain somewhat less well understood owing to a lack of experimental data, they have been detected in various ecological habitats far removed from industrial activity such as the Arctic, demonstrating diffusion and bioaccumulation after only a relatively brief period of widespread use. A much more recently discovered problem is the Great Pacific Garbage Patch, a huge concentration of plastics, chemical sludge and other debris which has been collected into a large area of the Pacific Ocean by the North Pacific Gyre. This is a less well known pollution problem than the others described above, but nonetheless has multiple and serious consequences such as increasing wildlife mortality, the spread of invasive species and human ingestion of toxic chemicals. Organizations such as 5 Gyres have researched the pollution and, along with artists like Marina DeBris, are working toward publicizing the issue. Pollution introduced by light at night is becoming a global problem, more severe in urban centres, but nonetheless contaminating also large territories, far away from towns. Growing evidence of local and global pollution and an increasingly informed public over time have given rise to environmentalism and the environmental movement, which generally seek to limit human impact on the environment."|2023-08-27-09-39-07
Pollution|See also| Air pollution Soil contamination Water pollution Other|2023-08-27-09-39-07
Acid rain|General| Lists Categories Acid rain is rain or any other form of precipitation that is unusually acidic, meaning that it has elevated levels of hydrogen ions (low pH). Most water, including drinking water, has a neutral pH that exists between 6.5 and 8.5, but acid rain has a pH level lower than this and ranges from 4–5 on average.   The more acidic the acid rain is, the lower its pH is.  Acid rain can have harmful effects on plants, aquatic animals, and infrastructure. Acid rain is caused by emissions of sulfur dioxide and nitrogen oxide, which react with the water molecules in the atmosphere to produce acids. Acid rain has been shown to have adverse impacts on forests, freshwaters, soils, microbes, insects and aquatic life-forms.  In ecosystems, persistent acid rain reduces tree bark durability, leaving flora more susceptible to environmental stressors such as drought, heat/cold and pest infestation. Acid rain is also capable of detrimenting soil composition by stripping it of nutrients such as calcium and magnesium which play a role in plant growth and maintaining healthy soil. In terms of human infrastructure, acid rain also causes paint to peel, corrosion of steel structures such as bridges, and weathering of stone buildings and statues as well as having impacts on human health. Some governments, including those in Europe and North America, have made efforts since the 1970s to reduce the release of sulfur dioxide and nitrogen oxide into the atmosphere through air pollution regulations. These efforts have had positive results due to the widespread research on acid rain starting in the 1960s and the publicized information on its harmful effects.   The main source of sulfur and nitrogen compounds that result in acid rain are anthropogenic, but nitrogen oxides can also be produced naturally by lightning strikes and sulfur dioxide is produced by volcanic eruptions.|2023-09-22-15-49-28
Acid rain|Definition|" ""Acid rain"" is a popular term referring to the deposition of a mixture from wet (rain, snow, sleet, fog,  cloudwater, and dew) and dry (acidifying particles and gases) acidic components. Distilled water, once carbon dioxide is removed, has a neutral pH of 7.  Liquids with a pH less than 7 are acidic, and those with a pH greater than 7 are alkaline. ""Clean"" or unpolluted rain has an acidic pH, but usually no lower than 5.7, because carbon dioxide and water in the air react together to form carbonic acid, a weak acid according to the following reaction: Carbonic acid then can ionize in water forming low concentrations of carbonate and hydronium ions: Unpolluted rain can also contain other chemicals which affect its pH (acidity level). A common example is nitric acid produced by electric discharge in the atmosphere such as lightning.  Acid deposition as an environmental issue (discussed later in the article) would include additional acids other than H2CO3. Occasional pH readings in rain and fog water of well below 2.4 have been reported in industrialized areas. The main sources of the SO2 and NOx pollution that causes acid rain are burning fossil fuels to generate electricity and power internal combustion vehicles, to refine oil, and in industrial manufacturing and other processes."|2023-09-22-15-49-28
Acid rain|History| Acid rain was first systematically studied in Europe, in the 1960s, and in the United States and Canada, the following decade.|2023-09-22-15-49-28
Acid rain|In Europe|" The corrosive effect of polluted, acidic city air on limestone and marble was noted in the 17th century by John Evelyn, who remarked upon the poor condition of the Arundel marbles. 
Since the Industrial Revolution, emissions of sulfur dioxide and nitrogen oxides into the atmosphere have increased.   In 1852, Robert Angus Smith was the first to show the relationship between acid rain and atmospheric pollution in Manchester, England.  Smith coined the term ""acid rain"" in 1872. In the late 1960s, scientists began widely observing and studying the phenomenon.  At first, the main focus in this research lay on local effects of acid rain. Waldemar Christofer Brøgger was the first to acknowledge long-distance transportation of pollutants crossing borders from the United Kingdom to Norway – a problem systematically studied by Brynjulf Ottar in the 1970s.  Ottar's work was strongly influenced  by Swedish soil scientist Svante Odén, who had drawn widespread attention to Europe's acid rain problem in popular newspapers and wrote a landmark paper on the subject in 1968."|2023-09-22-15-49-28
Acid rain|In the United States|" The earliest report about acid rain in the United States came from chemical evidence gathered from Hubbard Brook Valley; public awareness of acid rain in the US increased in the 1970s after The New York Times reported on these findings. In 1972, a group of scientists including Gene Likens discovered the rain that was deposited at White Mountains of New Hampshire was acidic. The pH of the sample was measured to be 4.03 at Hubbard Brook.  The Hubbard Brook Ecosystem Study followed up with a series of research studies that analyzed the environmental effects of acid rain. Acid rain that mixed with stream water at Hubbard Brook was neutralized by the alumina from soils.  The result of this research indicated that the chemical reaction between acid rain and aluminium leads to an increasing rate of soil weathering. Experimental research was done to examine the effects of increased acidity in streams on ecological species. In 1980, a group of scientists modified the acidity of Norris Brook, New Hampshire, and observed the change in species' behaviors. There was a decrease in species diversity, an increase in community dominants, and a decrease in the food web complexity. In 1980, the US Congress passed an Acid Deposition Act.  This Act established an 18-year assessment and research program under the direction of the National Acidic Precipitation Assessment Program (NAPAP). NAPAP enlarged a network of monitoring sites to determine how acidic the precipitation actually was, seeking to determine long-term trends, and established a network for dry deposition. Using a statistically based sampling design, NAPAP quantified the effects of acid rain on a regional basis by targeting research and surveys to identify and quantify the effects of acid precipitation on freshwater and terrestrial ecosystems. NAPAP also assessed the effects of acid rain on historical buildings, monuments, and building materials. It also funded extensive studies on atmospheric processes and potential control programs. From the start, policy advocates from all sides attempted to influence NAPAP activities to support their particular policy advocacy efforts, or to disparage those of their opponents.   For the US Government's scientific enterprise, a significant impact of NAPAP were lessons learned in the assessment process and in environmental research management to a relatively large group of scientists, program managers, and the public. In 1981, the National Academy of Sciences was looking into research about the controversial issues regarding acid rain.  President Ronald Reagan dismissed the issues of acid rain  until his personal visit to Canada and confirmed that the Canadian border suffered from the drifting pollution from smokestacks originating in the US Midwest. Reagan honored the agreement to Canadian Prime Minister Pierre Trudeau's enforcement of anti-pollution regulation.  In 1982, Reagan commissioned William Nierenberg to serve on the National Science Board.  Nierenberg selected scientists including Gene Likens to serve on a panel to draft a report on acid rain. In 1983, the panel of scientists came up with a draft report, which concluded that acid rain is a real problem and solutions should be sought.  White House Office of Science and Technology Policy reviewed the draft report and sent Fred Singer's suggestions of the report, which cast doubt on the cause of acid rain.  The panelists revealed rejections against Singer's positions and submitted the report to Nierenberg in April. In May 1983, the House of Representatives voted against legislation that aimed to control sulfur emissions. There was a debate about whether Nierenberg delayed to release the report. Nierenberg himself denied the saying about his suppression of the report and stated that the report was withheld after the House's vote because it was not ready to be published. In 1991, the US National Acid Precipitation Assessment Program (NAPAP) provided its first assessment of acid rain in the United States.  It reported that 5% of New England Lakes were acidic, with sulfates being the most common problem. They noted that 2% of the lakes could no longer support Brook Trout, and 6% of the lakes were unsuitable for the survival of many species of minnow. Subsequent Reports to Congress have documented chemical changes in soil and freshwater ecosystems, nitrogen saturation, decreases in amounts of nutrients in soil, episodic acidification, regional haze, and damage to historical monuments. Meanwhile, in 1990, the US Congress passed a series of amendments to the Clean Air Act.  Title IV of these amendments established a cap and trade system designed to control emissions of sulfur dioxide and nitrogen oxides.  Title IV called for a total reduction of about 10 million tons of SO2 emissions from power plants, close to a 50% reduction.  It was implemented in two phases. Phase I began in 1995, and limited sulfur dioxide emissions from 110 of the largest power plants to a combined total of 8.7 million tons of sulfur dioxide. One power plant in New England (Merrimack) was in Phase I. Four other plants (Newington, Mount Tom, Brayton Point, and Salem Harbor) were added under other provisions of the program. Phase II began in 2000, and affects most of the power plants in the country. During the 1990s, research continued. On March 10, 2005, the EPA issued the Clean Air Interstate Rule (CAIR). This rule provides states with a solution to the problem of power plant pollution that drifts from one state to another. CAIR will permanently cap emissions of SO2 and NOx in the eastern United States. When fully implemented[when?], CAIR will reduce SO2 emissions in 28 eastern states and the District of Columbia by over 70% and NOx emissions by over 60% from 2003 levels. Overall, the program's cap and trade program has been successful in achieving its goals. Since the 1990s, SO2 emissions have dropped 40%, and according to the Pacific Research Institute, acid rain levels have dropped 65% since 1976.  Conventional regulation was used in the European Union, which saw a decrease of over 70% in SO2 emissions during the same time period. In 2007, total SO2 emissions were 8.9 million tons, achieving the program's long-term goal ahead of the 2010 statutory deadline. In 2007 the EPA estimated that by 2010, the overall costs of complying with the program for businesses and consumers would be $1 billion to $2 billion a year, only one fourth of what was originally predicted.  Forbes says: ""In 2010, by which time the cap and trade system had been augmented by the George W. Bush administration's Clean Air Interstate Rule, SO2 emissions had fallen to 5.1 million tons."" The term citizen science can be traced back as far as January 1989 to a campaign by the Audubon Society to measure acid rain. Scientist Muki Haklay cites in a policy report for the Wilson Center entitled 'Citizen Science and Policy: A European Perspective' a first use of the term 'citizen science' by R. Kerson in the magazine MIT Technology Review from January 1989.   Quoting from the Wilson Center report: ""The new form of engagement in science received the name ""citizen science"". The first recorded example of the use of the term is from 1989, describing how 225 volunteers across the US collected rain samples to assist the Audubon Society in an acid-rain awareness raising campaign. The volunteers collected samples, checked for acidity, and reported back to the organization. The information was then used to demonstrate the full extent of the phenomenon."""|2023-09-22-15-49-28
Acid rain|In Canada|" Canadian Harold Harvey was among the first to research a ""dead"" lake. In 1971, he and R. J. Beamish published a report, ""Acidification of the La Cloche Mountain Lakes"", documenting the gradual deterioration of fish stocks in 60 lakes in Killarney Park in Ontario, which they had been studying systematically since 1966. In the 1970s and 80s, acid rain was a major topic of research at the Experimental Lakes Area (ELA) in Northwestern Ontario, Canada.  Researchers added sulfuric acid to whole lakes in controlled ecosystem experiments to simulate the effects of acid rain. Because its remote conditions allowed for whole-ecosystem experiments, research at the ELA showed that the effect of acid rain on fish populations started at concentrations much lower than those observed in laboratory experiments.  In the context of a food web, fish populations crashed earlier than when acid rain had direct toxic effects to the fish because the acidity led to crashes in prey populations (e.g. mysids).  As experimental acid inputs were reduced, fish populations and lake ecosystems recovered at least partially, although invertebrate populations have still not completely returned to the baseline conditions.  This research showed both that acidification was linked to declining fish populations and that the effects could be reversed if sulfuric acid emissions decreased, and influenced policy in Canada and the United States. In 1985, seven Canadian provinces (all except British Columbia, Alberta, and Saskatchewan) and the federal government signed the Eastern Canada Acid Rain Program.  The provinces agreed to limit their combined sulfur dioxide emissions to 2.3 million tonnes by 1994. The Canada-US Air Quality Agreement was signed in 1991.  In 1998, all federal, provincial, and territorial Ministers of Energy and Environment signed The Canada-Wide Acid Rain Strategy for Post-2000, which was designed to protect lakes that are more sensitive than those protected by earlier policies."|2023-09-22-15-49-28
Acid rain|Emissions of chemicals leading to acidification| The most important gas which leads to acidification is sulfur dioxide. Emissions of nitrogen oxides which are oxidized to form nitric acid are of increasing importance due to stricter controls on emissions of sulfur compounds. 70 Tg(S) per year in the form of SO2 comes from fossil fuel combustion and industry, 2.8 Tg(S) from wildfires, and 7–8 Tg(S) per year from volcanoes.|2023-09-22-15-49-28
Acid rain|Natural phenomena| The principal natural phenomena that contribute acid-producing gases to the atmosphere are emissions from volcanoes.  Thus, for example, fumaroles from the Laguna Caliente crater of Poás Volcano create extremely high amounts of acid rain and fog, with acidity as high as a pH of 2, clearing an area of any vegetation and frequently causing irritation to the eyes and lungs of inhabitants in nearby settlements. Acid-producing gasses are also created by biological processes that occur on the land, in wetlands, and in the oceans. The major biological source of sulfur compounds is dimethyl sulfide. Nitric acid in rainwater is an important source of fixed nitrogen for plant life, and is also produced by electrical activity in the atmosphere such as lightning. Acidic deposits have been detected in glacial ice thousands of years old in remote parts of the globe.|2023-09-22-15-49-28
Acid rain|Human activity| The principal cause of acid rain is sulfur and nitrogen compounds from human sources, such as electricity generation, animal agriculture, factories, and motor vehicles. Industrial acid rain is a substantial problem in China and Russia   and areas downwind from them. These areas all burn sulfur-containing coal to generate heat and electricity. The problem of acid rain has not only increased with population and industrial growth, but has become more widespread. The use of tall smokestacks to reduce local pollution has contributed to the spread of acid rain by releasing gases into regional atmospheric circulation; dispersal from these taller stacks causes pollutants to be carried farther, causing widespread ecological damage.   Often deposition occurs a considerable distance downwind of the emissions, with mountainous regions tending to receive the greatest deposition (because of their higher rainfall). An example of this effect is the low pH of rain which falls in Scandinavia.|2023-09-22-15-49-28
Acid rain|Chemical processes| Combustion of fuels produces sulfur dioxide and nitric oxides. They are converted into sulfuric acid and nitric acid.|2023-09-22-15-49-28
Acid rain|Gas phase chemistry| In the gas phase sulfur dioxide is oxidized by reaction with the hydroxyl radical via an intermolecular reaction: which is followed by: In the presence of water, sulfur trioxide (SO3) is converted rapidly to sulfuric acid: Nitrogen dioxide reacts with OH to form nitric acid:|2023-09-22-15-49-28
Acid rain|Chemistry in cloud droplets| When clouds are present, the loss rate of SO2 is faster than can be explained by gas phase chemistry alone. This is due to reactions in the liquid water droplets. Sulfur dioxide dissolves in water and then, like carbon dioxide, hydrolyses in a series of equilibrium reactions: There are a large number of aqueous reactions that oxidize sulfur from S(IV) to S(VI), leading to the formation of sulfuric acid. The most important oxidation reactions are with ozone, hydrogen peroxide and oxygen (reactions with oxygen are catalyzed by iron and manganese in the cloud droplets).|2023-09-22-15-49-28
Acid rain|Wet deposition| Wet deposition of acids occurs when any form of precipitation (rain, snow, and so on) removes acids from the atmosphere and delivers it to the Earth's surface. This can result from the deposition of acids produced in the raindrops (see aqueous phase chemistry above) or by the precipitation removing the acids either in clouds or below clouds. Wet removal of both gases and aerosols are both of importance for wet deposition.|2023-09-22-15-49-28
Acid rain|Dry deposition| Acid deposition also occurs via dry deposition in the absence of precipitation. This can be responsible for as much as 20 to 60% of total acid deposition.  This occurs when particles and gases stick to the ground, plants or other surfaces.|2023-09-22-15-49-28
Acid rain|Adverse effects| Acid rain has been shown to have adverse impacts on forests, freshwaters and soils, killing insect and aquatic life-forms as well as causing damage to buildings and having impacts on human health.|2023-09-22-15-49-28
Acid rain|Surface waters and aquatic animals|" Both the lower pH and higher aluminium concentrations in surface water that occur as a result of acid rain can cause damage to fish and other aquatic animals. At pH lower than 5 most fish eggs will not hatch and lower pH can kill adult fish. As lakes and rivers become more acidic biodiversity is reduced. Acid rain has eliminated insect life and some fish species, including the brook trout in some lakes, streams, and creeks in geographically sensitive areas, such as the Adirondack Mountains of the United States.  However, the extent to which acid rain contributes directly or indirectly via runoff from the catchment to lake and river acidity (i.e., depending on characteristics of the surrounding watershed) is variable. The United States Environmental Protection Agency's (EPA) website states: ""Of the lakes and streams surveyed, acid rain caused acidity in 75% of the acidic lakes and about 50% of the acidic streams"".  Lakes hosted by silicate basement rocks are more acidic than lakes within limestone or other basement rocks with a carbonate composition (i.e. marble) due to buffering effects by carbonate minerals, even with the same amount of acid rain. [citation needed]"|2023-09-22-15-49-28
Acid rain|Soils| Soil biology and chemistry can be seriously damaged by acid rain. Some microbes are unable to tolerate changes to low pH and are killed.  The enzymes of these microbes are denatured (changed in shape so they no longer function) by the acid. The hydronium ions of acid rain also mobilize toxins, such as aluminium, and leach away essential nutrients and minerals such as magnesium. Soil chemistry can be dramatically changed when base cations, such as calcium and magnesium, are leached by acid rain, thereby affecting sensitive species, such as sugar maple (Acer saccharum). Soil acidification Impacts of acidic water and soil acidification on plants could be minor or in most cases major. Most minor cases which do not result in fatality of plant life can be attributed to the plants being less susceptible to acidic conditions and/or the acid rain being less potent. However, even in minor cases, the plant will eventually die due to the acidic water lowering the plant's natural pH.  Acidic water enters the plant and causes important plant minerals to dissolve and get carried away; which ultimately causes the plant to die of lack of minerals for nutrition. In major cases, which are more extreme, the same process of damage occurs as in minor cases, which is removal of essential minerals, but at a much quicker rate.  Likewise, acid rain that falls on soil and on plant leaves causes drying of the waxy leaf cuticle, which ultimately causes rapid water loss from the plant to the outside atmosphere and eventually results in death of the plant.  To see if a plant is being affected by soil acidification, one can closely observe the plant leaves. If the leaves are green and look healthy, the soil pH is normal and acceptable for plant life. But if the plant leaves have yellowing between the veins on their leaves, that means the plant is suffering from acidification and is unhealthy.  Moreover, a plant suffering from soil acidification cannot photosynthesize; the acid-water-induced process of drying out of the plant can destroy chloroplast organelles.  Without being able to photosynthesize, a plant cannot create nutrients for its own survival or oxygen for the survival of aerobic organisms, which affects most species on Earth and ultimately ends the purpose of the plant's existence.|2023-09-22-15-49-28
Acid rain|Forests and other vegetation| Adverse effects may be indirectly related to acid rain, like the acid's effects on soil (see above) or high concentration of gaseous precursors to acid rain. High altitude forests are especially vulnerable as they are often surrounded by clouds and fog which are more acidic than rain. Other plants can also be damaged by acid rain, but the effect on food crops is minimized by the application of lime and fertilizers to replace lost nutrients. In cultivated areas, limestone may also be added to increase the ability of the soil to keep the pH stable, but this tactic is largely unusable in the case of wilderness lands. When calcium is leached from the needles of red spruce, these trees become less cold tolerant and exhibit winter injury and even death.|2023-09-22-15-49-28
Acid rain|Ocean acidification|" Acid rain has a much less harmful effect on oceans on a global scale, but it creates an amplified impact in the shallower waters of coastal waters.  Acid rain can cause the ocean's pH to fall, known as ocean acidification, making it more difficult for different coastal species to create their exoskeletons that they need to survive. These coastal species link together as part of the ocean's food chain, and without them being a source for other marine life to feed off of, more marine life will die.  Coral's limestone skeleton is particularly sensitive to pH decreases, because the calcium carbonate, a core component of the limestone skeleton, dissolves in acidic (low pH) solutions. In addition to acidification, excess nitrogen inputs from the atmosphere promote increased growth of phytoplankton and other marine plants, which, in turn, may cause more frequent harmful algal blooms and eutrophication (the creation of oxygen-depleted ""dead zones"") in some parts of the ocean."|2023-09-22-15-49-28
Acid rain|Human health effects| Acid rain does not directly affect human health. The acid in the rainwater is too dilute to have direct adverse effects. The particulates responsible for acid rain (sulfur dioxide and nitrogen oxides) do have an adverse effect. Increased amounts of fine particulate matter in the air contribute to heart and lung problems, including asthma and bronchitis.|2023-09-22-15-49-28
Acid rain|Other adverse effects| Acid rain can damage buildings, historic monuments, and statues, especially those made of rocks, such as limestone and marble, that contain large amounts of calcium carbonate. Acids in the rain react with the calcium compounds in the stones to create gypsum, which then flakes off. The effects of this are commonly seen on old gravestones, where acid rain can cause the inscriptions to become completely illegible. Acid rain also increases the corrosion rate of metals, in particular iron, steel, copper and bronze.|2023-09-22-15-49-28
Acid rain|Affected areas| Places significantly impacted by acid rain around the globe include most of eastern Europe from Poland northward into Scandinavia,  the eastern third of the United States,  and southeastern Canada. Other affected areas include the southeastern coast of China and Taiwan.|2023-09-22-15-49-28
Acid rain|Technical solutions| Many coal-firing power stations use flue-gas desulfurization (FGD) to remove sulfur-containing gases from their stack gases. For a typical coal-fired power station, FGD will remove 95% or more of the SO2 in the flue gases. An example of FGD is the wet scrubber which is commonly used. A wet scrubber is basically a reaction tower equipped with a fan that extracts hot smoke stack gases from a power plant into the tower. Lime or limestone in slurry form is also injected into the tower to mix with the stack gases and combine with the sulfur dioxide present. The calcium carbonate of the limestone produces pH-neutral calcium sulfate that is physically removed from the scrubber. That is, the scrubber turns sulfur pollution into industrial sulfates. In some areas the sulfates are sold to chemical companies as gypsum when the purity of calcium sulfate is high. In others, they are placed in landfill. The effects of acid rain can last for generations, as the effects of pH level change can stimulate the continued leaching of undesirable chemicals into otherwise pristine water sources, killing off vulnerable insect and fish species and blocking efforts to restore native life. Fluidized bed combustion also reduces the amount of sulfur emitted by power production. Vehicle emissions control reduces emissions of nitrogen oxides from motor vehicles.|2023-09-22-15-49-28
Acid rain|International treaties| International treaties on the long-range transport of atmospheric pollutants have been agreed upon by western countries for some time now. Beginning in 1979, European countries convened in order to ratify general principles discussed during the UNECE Convention. The purpose was to combat Long-Range Transboundary Air Pollution.  The 1985 Helsinki Protocol on the Reduction of Sulfur Emissions under the Convention on Long-Range Transboundary Air Pollution furthered the results of the convention. Results of the treaty have already come to fruition, as evidenced by an approximate 40 percent drop in particulate matter in North America.  The effectiveness of the Convention in combatting acid rain has inspired further acts of international commitment to prevent the proliferation of particulate matter. Canada and the US signed the Air Quality Agreement in 1991. Most European countries and Canada signed the treaties. Activity of the Long-Range Transboundary Air Pollution Convention remained dormant after 1999, when 27 countries convened to further reduce the effects of acid rain.  In 2000, foreign cooperation to prevent acid rain was sparked in Asia for the first time. Ten diplomats from countries ranging throughout the continent convened to discuss ways to prevent acid rain.  Following these discussions, the Acid Deposition Monitoring Network in East Asia (EANET) was established in 2001 as an intergovernmental initiative to provide science-based inputs for decision makers and promote international cooperation on acid deposition in East Asia.  In 2023, the EANET member countries include Cambodia, China, Indonesia, Japan, Lao PDR, Malaysia, Mongolia, Myanmar, the Philippines, Republic of Korea, Russia, Thailand and Vietnam.|2023-09-22-15-49-28
Acid rain|Emissions trading| In this regulatory scheme, every current polluting facility is given or may purchase on an open market an emissions allowance for each unit of a designated pollutant it emits. Operators can then install pollution control equipment, and sell portions of their emissions allowances they no longer need for their own operations, thereby recovering some of the capital cost of their investment in such equipment. The intention is to give operators economic incentives to install pollution controls. The first emissions trading market was established in the United States by enactment of the Clean Air Act Amendments of 1990.  The overall goal of the Acid Rain Program established by the Act  is to achieve significant environmental and public health benefits through reductions in emissions of sulfur dioxide (SO2) and nitrogen oxides (NOx), the primary causes of acid rain. To achieve this goal at the lowest cost to society, the program employs both regulatory and market based approaches for controlling air pollution.|2023-09-22-15-49-28
Aerosol|General| An aerosol is a suspension of fine solid particles or liquid droplets in air or another gas.  Aerosols can be natural or anthropogenic. The term aerosol commonly refers to the particulate/air mixture, as opposed to the particulate matter alone.  Examples of natural aerosols are fog or mist, dust, forest exudates, and geyser steam. Examples of anthropogenic aerosols include particulate air pollutants, mist from the discharge at hydroelectric dams, irrigation mist, perfume from atomizers, smoke, dust, steam from a kettle, sprayed pesticides, and medical treatments for respiratory illnesses.  When a person inhales the contents of a vape pen or e-cigarette, they are inhaling an anthropogenic aerosol. The liquid or solid particles in an aerosol have diameters typically less than 1 μm (larger particles with a significant settling speed make the mixture a suspension, but the distinction is not clear-cut). In general conversation, aerosol often refers to a dispensing system that delivers a consumer product from a can. Diseases can spread by means of small droplets in the breath,  sometimes called bioaerosols.|2023-09-14-09-49-53
Aerosol|Definitions| Aerosol is defined as a suspension system of solid or liquid particles in a gas. An aerosol includes both the particles and the suspending gas, which is usually air.  Meteorologists usually refer them as particle matter - PM2.5 or PM10, depending on their size.  Frederick G. Donnan presumably first used the term aerosol during World War I to describe an aero-solution, clouds of microscopic particles in air. This term developed analogously to the term hydrosol, a colloid system with water as the dispersed medium.  Primary aerosols contain particles introduced directly into the gas; secondary aerosols form through gas-to-particle conversion. Key aerosol groups include sulfates, organic carbon, black carbon, nitrates, mineral dust, and sea salt, they usually clump together to form a complex mixture.  Various types of aerosol, classified according to physical form and how they were generated, include dust, fume, mist, smoke and fog. There are several measures of aerosol concentration. Environmental science and environmental health often use the mass concentration (M), defined as the mass of particulate matter per unit volume, in units such as μg/m3. Also commonly used is the number concentration (N), the number of particles per unit volume, in units such as number per m3 or number per cm3. Particle size has a major influence on particle properties, and the aerosol particle radius or diameter (dp) is a key property used to characterise aerosols. Aerosols vary in their dispersity. A monodisperse aerosol, producible in the laboratory, contains particles of uniform size. Most aerosols, however, as polydisperse colloidal systems, exhibit a range of particle sizes.  Liquid droplets are almost always nearly spherical, but scientists use an equivalent diameter to characterize the properties of various shapes of solid particles, some very irregular. The equivalent diameter is the diameter of a spherical particle with the same value of some physical property as the irregular particle.  The equivalent volume diameter (de) is defined as the diameter of a sphere of the same volume as that of the irregular particle.  Also commonly used is the aerodynamic diameter,  da.|2023-09-14-09-49-53
Aerosol|Generation and applications| People generate aerosols for various purposes, including: Some devices for generating aerosols are:|2023-09-14-09-49-53
Aerosol|In the atmosphere| Several types of atmospheric aerosol have a significant effect on Earth's climate: volcanic, desert dust, sea-salt, that originating from biogenic sources and human-made. Volcanic aerosol forms in the stratosphere after an eruption as droplets of sulfuric acid that can prevail for up to two years, and reflect sunlight, lowering temperature. Desert dust, mineral particles blown to high altitudes, absorb heat and may be responsible for inhibiting storm cloud formation. Human-made sulfate aerosols, primarily from burning oil and coal, affect the behavior of clouds. Although all hydrometeors, solid and liquid, can be described as aerosols, a distinction is commonly made between such dispersions (i.e. clouds) containing activated drops and crystals, and aerosol particles. The atmosphere of Earth contains aerosols of various types and concentrations, including quantities of: Aerosols can be found in urban ecosystems in various forms, for example: The presence of aerosols in the earth's atmosphere can influence its climate, as well as human health.|2023-09-14-09-49-53
Aerosol|Size distribution| For a monodisperse aerosol, a single number—the particle diameter—suffices to describe the size of the particles. However, more complicated particle-size distributions describe the sizes of the particles in a polydisperse aerosol. This distribution defines the relative amounts of particles, sorted according to size.  One approach to defining the particle size distribution uses a list of the sizes of every particle in a sample. However, this approach proves tedious to ascertain in aerosols with millions of particles and awkward to use. Another approach splits the size range into intervals and finds the number (or proportion) of particles in each interval. These data can be presented in a histogram with the area of each bar representing the proportion of particles in that size bin, usually normalised by dividing the number of particles in a bin by the width of the interval so that the area of each bar is proportionate to the number of particles in the size range that it represents.  If the width of the bins tends to zero, the frequency function is: where Therefore, the area under the frequency curve between two sizes a and b represents the total fraction of the particles in that size range: It can also be formulated in terms of the total number density N: Assuming spherical aerosol particles, the aerosol surface area per unit volume (S) is given by the second moment: And the third moment gives the total volume concentration (V) of the particles: The particle size distribution can be approximated. The normal distribution usually does not suitably describe particle size distributions in aerosols because of the skewness associated with a long tail of larger particles. Also for a quantity that varies over a large range, as many aerosol sizes do, the width of the distribution implies negative particles sizes, which is not physically realistic. However, the normal distribution can be suitable for some aerosols, such as test aerosols, certain pollen grains and spores. A more widely chosen log-normal distribution gives the number frequency as: where: The log-normal distribution has no negative values, can cover a wide range of values, and fits many observed size distributions reasonably well. Other distributions sometimes used to characterise particle size include: the Rosin-Rammler distribution, applied to coarsely dispersed dusts and sprays; the Nukiyama–Tanasawa distribution, for sprays of extremely broad size ranges; the power function distribution, occasionally applied to atmospheric aerosols; the exponential distribution, applied to powdered materials; and for cloud droplets, the Khrgian–Mazin distribution.|2023-09-14-09-49-53
Aerosol|Terminal velocity of a particle in a fluid| For low values of the Reynolds number (<1), true for most aerosol motion, Stokes' law describes the force of resistance on a solid spherical particle in a fluid. However, Stokes' law is only valid when the velocity of the gas at the surface of the particle is zero. For small particles (< 1 μm) that characterize aerosols, however, this assumption fails. To account for this failure, one can introduce the Cunningham correction factor, always greater than 1. Including this factor, one finds the relation between the resisting force on a particle and its velocity: where This allows us to calculate the terminal velocity of a particle undergoing gravitational settling in still air. Neglecting buoyancy effects, we find: where The terminal velocity can also be derived for other kinds of forces. If Stokes' law holds, then the resistance to motion is directly proportional to speed. The constant of proportionality is the mechanical mobility (B) of a particle: A particle traveling at any reasonable initial velocity approaches its terminal velocity exponentially with an e-folding time equal to the relaxation time: where: To account for the effect of the shape of non-spherical particles, a correction factor known as the dynamic shape factor is applied to Stokes' law. It is defined as the ratio of the resistive force of the irregular particle to that of a spherical particle with the same volume and velocity: where:|2023-09-14-09-49-53
Aerosol|Aerodynamic diameter| The aerodynamic diameter of an irregular particle is defined as the diameter of the spherical particle with a density of 1000 kg/m3 and the same settling velocity as the irregular particle. Neglecting the slip correction, the particle settles at the terminal velocity proportional to the square of the aerodynamic diameter, da: where This equation gives the aerodynamic diameter: One can apply the aerodynamic diameter to particulate pollutants or to inhaled drugs to predict where in the respiratory tract such particles deposit. Pharmaceutical companies typically use aerodynamic diameter, not geometric diameter, to characterize particles in inhalable drugs.[citation needed]|2023-09-14-09-49-53
Aerosol|Dynamics|" The previous discussion focused on single aerosol particles. In contrast, aerosol dynamics explains the evolution of complete aerosol populations. The concentrations of particles will change over time as a result of many processes. External processes that move particles outside a volume of gas under study include diffusion, gravitational settling, and electric charges and other external forces that cause particle migration. A second set of processes internal to a given volume of gas include particle formation (nucleation), evaporation, chemical reaction, and coagulation. A differential equation called the Aerosol General Dynamic Equation (GDE) characterizes the evolution of the number density of particles in an aerosol due to these processes. Change in time = Convective transport + brownian diffusion + gas-particle interactions + coagulation + migration by external forces Where: As particles and droplets in an aerosol collide with one another, they may undergo coalescence or aggregation. This process leads to a change in the aerosol particle-size distribution, with the mode increasing in diameter as total number of particles decreases.  On occasion, particles may shatter apart into numerous smaller particles; however, this process usually occurs primarily in particles too large for consideration as aerosols. The Knudsen number of the particle define three different dynamical regimes that govern the behaviour of an aerosol: where  is the mean free path of the suspending gas and  is the diameter of the particle.  For particles in the free molecular regime, Kn >> 1; particles small compared to the mean free path of the suspending gas.  In this regime, particles interact with the suspending gas through a series of ""ballistic"" collisions with gas molecules. As such, they behave similarly to gas molecules, tending to follow streamlines and diffusing rapidly through Brownian motion. The mass flux equation in the free molecular regime is: where a is the particle radius, P∞ and PA are the pressures far from the droplet and at the surface of the droplet respectively, kb is the Boltzmann constant, T is the temperature, CA is mean thermal velocity and α is mass accommodation coefficient.[citation needed] The derivation of this equation assumes constant pressure and constant diffusion coefficient. Particles are in the continuum regime when Kn << 1.  In this regime, the particles are big compared to the mean free path of the suspending gas, meaning that the suspending gas acts as a continuous fluid flowing round the particle.  The molecular flux in this regime is: where a is the radius of the particle A, MA is the molecular mass of the particle A, DAB is the diffusion coefficient between particles A and B, R is the ideal gas constant, T is the temperature (in absolute units like kelvin), and PA∞ and PAS are the pressures at infinite and at the surface respectively.[citation needed] The transition regime contains all the particles in between the free molecular and continuum regimes or Kn ≈ 1. The forces experienced by a particle are a complex combination of interactions with individual gas molecules and macroscopic interactions. The semi-empirical equation describing mass flux is: where Icont is the mass flux in the continuum regime.[citation needed] This formula is called the Fuchs-Sutugin interpolation formula. These equations do not take into account the heat release effect. Aerosol partitioning theory governs condensation on and evaporation from an aerosol surface, respectively. Condensation of mass causes the mode of the particle-size distributions of the aerosol to increase; conversely, evaporation causes the mode to decrease. Nucleation is the process of forming aerosol mass from the condensation of a gaseous precursor, specifically a vapor. Net condensation of the vapor requires supersaturation, a partial pressure greater than its vapor pressure. This can happen for three reasons:[citation needed] There are two types of nucleation processes. Gases preferentially condense onto surfaces of pre-existing aerosol particles, known as heterogeneous nucleation. This process causes the diameter at the mode of particle-size distribution to increase with constant number concentration.  With sufficiently high supersaturation and no suitable surfaces, particles may condense in the absence of a pre-existing surface, known as homogeneous nucleation. This results in the addition of very small, rapidly growing particles to the particle-size distribution. Water coats particles in aerosols, making them activated, usually in the context of forming a cloud droplet (such as natural cloud seeding by aerosols from trees in a forest).  Following the Kelvin equation (based on the curvature of liquid droplets), smaller particles need a higher ambient relative humidity to maintain equilibrium than larger particles do. The following formula gives relative humidity at equilibrium: where  is the saturation vapor pressure above a particle at equilibrium (around a curved liquid droplet), p0 is the saturation vapor pressure (flat surface of the same liquid) and S is the saturation ratio. Kelvin equation for saturation vapor pressure above a curved surface is: where rp droplet radius, σ surface tension of droplet, ρ density of liquid, M molar mass, T temperature, and R molar gas constant. There are no general solutions to the general dynamic equation (GDE);  common methods used to solve the general dynamic equation include:"|2023-09-14-09-49-53
Aerosol|Detection| Aerosol can either be measured in-situ or with remote sensing techniques.|2023-09-14-09-49-53
Aerosol|In situ observations| Some available in situ measurement techniques include:|2023-09-14-09-49-53
Aerosol|Remote sensing approach| Remote sensing approaches include:|2023-09-14-09-49-53
Aerosol|Size selective sampling|" Particles can deposit in the nose, mouth, pharynx and larynx (the head airways region), deeper within the respiratory tract (from the trachea to the terminal bronchioles), or in the alveolar region.  The location of deposition of aerosol particles within the respiratory system strongly determines the health effects of exposure to such aerosols.  This phenomenon led people to invent aerosol samplers that select a subset of the aerosol particles that reach certain parts of the respiratory system. Examples of these subsets of the particle-size distribution of an aerosol, important in occupational health, include the inhalable, thoracic, and respirable fractions. The fraction that can enter each part of the respiratory system depends on the deposition of particles in the upper parts of the airway.  The inhalable fraction of particles, defined as the proportion of particles originally in the air that can enter the nose or mouth, depends on external wind speed and direction and on the particle-size distribution by aerodynamic diameter.  The thoracic fraction is the proportion of the particles in ambient aerosol that can reach the thorax or chest region.  The respirable fraction is the proportion of particles in the air that can reach the alveolar region.  To measure the respirable fraction of particles in air, a pre-collector is used with a sampling filter. The pre-collector excludes particles as the airways remove particles from inhaled air. The sampling filter collects the particles for measurement. It is common to use cyclonic separation for the pre-collector, but other techniques include impactors, horizontal elutriators, and large pore membrane filters. Two alternative size-selective criteria, often used in atmospheric monitoring, are PM10 and PM2.5. PM10 is defined by ISO as particles which pass through a size-selective inlet with a 50% efficiency cut-off at 10 μm aerodynamic diameter and PM2.5 as particles which pass through a size-selective inlet with a 50% efficiency cut-off at 2.5 μm aerodynamic diameter. PM10 corresponds to the ""thoracic convention"" as defined in ISO 7708:1995, Clause 6; PM2.5 corresponds to the ""high-risk respirable convention"" as defined in ISO 7708:1995, 7.1.  The United States Environmental Protection Agency replaced the older standards for particulate matter based on Total Suspended Particulate with another standard based on PM10 in 1987  and then introduced standards for PM2.5 (also known as fine particulate matter) in 1997."|2023-09-14-09-49-53
Air pollution|General| Lists Categories Air pollution is the contamination of air due to the presence of substances in the atmosphere that are harmful to the health of humans and other living beings, or cause damage to the climate or to materials.   It is also the contamination of indoor or outdoor surrounding either by chemical activities, physical or biological agents that alters the natural features of the atmosphere.  There are many different types of air pollutants, such as gases (including ammonia, carbon monoxide, sulfur dioxide, nitrous oxides, methane and chlorofluorocarbons), particulates (both organic and inorganic), and biological molecules. Air pollution can cause diseases, allergies, and even death to humans; it can also cause harm to other living organisms such as animals and crops, and may damage the natural environment (for example, climate change, ozone depletion or habitat degradation) or built environment (for example, acid rain).  Air pollution can be caused by both human activities  and natural phenomena. Air quality is closely related to the earth's climate and ecosystems globally. Many of the contributors of air pollution are also sources of greenhouse emission i.e., burning of fossil fuel. Air pollution is a significant risk factor for a number of pollution-related diseases, including respiratory infections, heart disease, chronic obstructive pulmonary disease (COPD), stroke, and lung cancer.  [Growing evidence suggests that air pollution exposure may be associated with reduced IQ scores, impaired cognition,  increased risk for psychiatric disorders such as depression  and detrimental perinatal health.  The human health effects of poor air quality are far reaching, but principally affect the body's respiratory system and the cardiovascular system. Individual reactions to air pollutants depend on the type of pollutant a person is exposed to,   the degree of exposure, and the individual's health status and genetics. Outdoor air pollution attributable to fossil fuel use alone causes ~3.61 million deaths annually, making it one of the top contributors to human death,   with anthropogenic ozone and PM2.5 causing ~2.1 million.   Overall, air pollution causes the deaths of around 7 million people worldwide each year, or a global mean loss of life expectancy (LLE) of 2.9 years,  and is the world's largest single environmental health risk, which has not shown significant progress since at least 2015.     Indoor air pollution and poor urban air quality are listed as two of the world's worst toxic pollution problems in the 2008 Blacksmith Institute World's Worst Polluted Places report.  The scope of the air pollution crisis is large: In 2018, WHO estimated that “9 out of 10 people breathe air containing high levels of pollutants.”  Although the health consequences are extensive, the way the problem is handled is considered largely haphazard    or neglected. Productivity losses and degraded quality of life caused by air pollution are estimated to cost the world economy $5 trillion per year    but, along with health and mortality impacts, are an externality to the contemporary economic system and most human activity, albeit sometimes being moderately regulated and monitored.   Various pollution control technologies and strategies are available to reduce air pollution.   Several international and national legislation and regulation have been developed to limit the negative effects of air pollution.  Local rules, when properly executed, have resulted in significant advances in public health.  Some of these efforts have been successful at the international level, such as the Montreal Protocol,  which reduced the release of harmful ozone depleting chemicals, and the 1985 Helsinki Protocol,  which reduced sulfur emissions,  while others, such as international action on climate change,    have been less successful.|2023-09-15-14-45-55
Air pollution|Anthropogenic (human-made) sources| There are also sources from processes other than combustion:|2023-09-15-14-45-55
Air pollution|Emission factors| Air pollutant emission factors are reported representative values that aim to link the quantity of a pollutant released into the ambient air to an activity connected with that pollutant's release.     The weight of the pollutant divided by a unit weight, volume, distance, or time of the activity generating the pollutant is how these factors are commonly stated (e.g., kilograms of particulate emitted per tonne of coal burned). These criteria make estimating emissions from diverse sources of pollution easier. Most of the time, these components are just averages of all available data of acceptable quality, and they are thought to be typical of long-term averages. There are 12 compounds in the list of persistent organic pollutants. Dioxins and furans are two of them and intentionally created by combustion of organics, like open burning of plastics. These compounds are also endocrine disruptors and can mutate the human genes. The United States Environmental Protection Agency has published a compilation of air pollutant emission factors for a wide range of industrial sources.  The United Kingdom, Australia, Canada, and many other countries have published similar compilations, as well as the European Environment Agency.|2023-09-15-14-45-55
Air pollution|Pollutants| An air pollutant is a material in the air that can have adverse effects on humans and the ecosystem.  The substance can be solid particles, liquid droplets, or gases, and often takes the form of an aerosol (solid particles or liquid droplets dispersed and carried by a gas).  A pollutant can be of natural origin or man-made. Pollutants are classified as primary or secondary. Primary pollutants are usually produced by processes such as ash from a volcanic eruption. Other examples include carbon monoxide gas from motor vehicle exhausts or sulfur dioxide released from factories. Secondary pollutants are not emitted directly. Rather, they form in the air when primary pollutants react or interact. Ground level ozone is a prominent example of a secondary pollutant. Some pollutants may be both primary and secondary: they are both emitted directly and formed from other primary pollutants.|2023-09-15-14-45-55
Air pollution|Primary pollutants| Pollutants emitted into the atmosphere by human activity include:|2023-09-15-14-45-55
Air pollution|Secondary pollutants| Secondary pollutants include:|2023-09-15-14-45-55
Air pollution|Other pollutants|" There are many other chemicals classed as hazardous air pollutants. Some of these are regulated in the USA under the Clean Air Act and in Europe under numerous directives (including the Air ""Framework"" Directive, 96/62/EC, on ambient air quality assessment and management, Directive 98/24/EC, on risks related to chemical agents at work, and Directive 2004/107/EC covering heavy metals and polycyclic aromatic hydrocarbons in ambient air). Before flue-gas desulfurization was installed, the emissions from this power plant in New Mexico contained excessive amounts of sulfur dioxide. Thermal oxidisers are air pollution abatement options for hazardous air pollutants (HAPs), volatile organic compounds (VOCs), and odorous emissions. This video provides an overview of a NASA study on the human fingerprint on global air quality."|2023-09-15-14-45-55
Air pollution|Exposure| The risk of air pollution is determined by the pollutant's hazard and the amount of exposure to that pollutant. Air pollution exposure can be measured for a person, a group, such as a neighborhood or a country's children, or an entire population. For example, one would want to determine a geographic area's exposure to a dangerous air pollution, taking into account the various microenvironments and age groups. This can be calculated  as an inhalation exposure. This would account for daily exposure in various settings, e.g. different indoor micro-environments and outdoor locations. The exposure needs to include different ages and other demographic groups, especially infants, children, pregnant women, and other sensitive subpopulations. For each specific time that the subgroup is in the setting and engaged in particular activities, the exposure to an air pollutant must integrate the concentrations of the air pollutant with regard to the time spent in each setting and the respective inhalation rates for each subgroup, playing, cooking, reading, working, spending time in traffic, etc. A little child's inhaling rate, for example, will be lower than that of an adult. A young person engaging in strenuous exercise will have a faster rate of breathing than a child engaged in sedentary activity. The daily exposure must therefore include the amount of time spent in each micro-environmental setting as well as the kind of activities performed there. The air pollutant concentration in each microactivity/microenvironmental setting is summed to indicate the exposure. For some pollutants such as black carbon, traffic related exposures may dominate total exposure despite short exposure times since high concentrations coincide with proximity to major roads or participation in (motorized) traffic.  A large portion of total daily exposure occurs as short peaks of high concentrations, but it remains unclear how to define peaks and determine their frequency and health impact. In 2021, the WHO halved its recommended guideline limit for tiny particles from burning fossil fuels. The new limit for nitrogen dioxide (NO2) is 75% lower.  Growing evidence that air pollution—even when experienced at very low levels—hurts human health, led the WHO to revise its guideline (from 10 µg/m³ to 5 µg/m³) for what it considers a safe level of exposure of particulate pollution, bringing most of the world—97.3 percent of the global population—into the unsafe zone.|2023-09-15-14-45-55
Air pollution|Indoor air quality| A lack of ventilation indoors concentrates air pollution where people often spend the majority of their time. Radon (Rn) gas, a carcinogen, is exuded from the Earth in certain locations and trapped inside houses. Building materials including carpeting and plywood emit formaldehyde (H-CHO) gas. Paint and solvents give off volatile organic compounds (VOCs) as they dry. Lead paint can degenerate into dust and be inhaled. Intentional air pollution is introduced with the use of air fresheners, incense, and other scented items. Controlled wood fires in cook stoves and fireplaces can add significant amounts of harmful smoke particulates into the air, inside and out.   Indoor pollution fatalities may be caused by using pesticides and other chemical sprays indoors without proper ventilation. Also the kitchen in a modern produce harmful particles and gases, with equipment like toasters being one of the worst sources. Carbon monoxide poisoning and fatalities are often caused by faulty vents and chimneys, or by the burning of charcoal indoors or in a confined space, such as a tent.  Chronic carbon monoxide poisoning can result even from poorly-adjusted pilot lights. Traps are built into all domestic plumbing to keep sewer gas and hydrogen sulfide, out of interiors. Clothing emits tetrachloroethylene, or other dry cleaning fluids, for days after dry cleaning. Though its use has now been banned in many countries, the extensive use of asbestos in industrial and domestic environments in the past has left a potentially very dangerous material in many localities. Asbestosis is a chronic inflammatory medical condition affecting the tissue of the lungs. It occurs after long-term, heavy exposure to asbestos from asbestos-containing materials in structures. Those with asbestosis have severe dyspnea (shortness of breath) and are at an increased risk regarding several different types of lung cancer. As clear explanations are not always stressed in non-technical literature, care should be taken to distinguish between several forms of relevant diseases. According to the World Health Organization,  these may be defined as asbestosis, lung cancer, and peritoneal mesothelioma (generally a very rare form of cancer, when more widespread it is almost always associated with prolonged exposure to asbestos). Biological sources of air pollution are also found indoors, as gases and airborne particulates. Pets produce dander, people produce dust from minute skin flakes and decomposed hair, dust mites in bedding, carpeting and furniture produce enzymes and micrometre-sized fecal droppings, inhabitants emit methane, mold forms on walls and generates mycotoxins and spores, air conditioning systems can incubate Legionnaires' disease and mold, and houseplants, soil and surrounding gardens can produce pollen, dust, and mold. Indoors, the lack of air circulation allows these airborne pollutants to accumulate more than they would otherwise occur in nature.|2023-09-15-14-45-55
Air pollution|Health effects| Even at levels lower than those considered safe by United States regulators, exposure to three components of air pollution, fine particulate matter, nitrogen dioxide and ozone, correlates with cardiac and respiratory illness.  In 2020, pollution (including air pollution) was a contributing factor to one in eight deaths in Europe, and was a significant risk factor for pollution-related diseases including heart disease, stroke and lung cancer.  The health effects caused by air pollution may include difficulty in breathing, wheezing, coughing, asthma  and worsening of existing respiratory and cardiac conditions. These effects can result in increased medication use, increased doctor or emergency department visits, more hospital admissions and premature death. The human health effects of poor air quality are far reaching, but principally affect the body's respiratory system and the cardiovascular system. Individual reactions to air pollutants depend on the type of pollutant a person is exposed to, the degree of exposure, and the individual's health status and genetics.  The most common sources of air pollution include particulates, ozone, nitrogen dioxide, and sulfur dioxide. Children aged less than five years who live in developing countries are the most vulnerable population in terms of total deaths attributable to indoor and outdoor air pollution.  Under the Clean Air Act, U.S. EPA sets limits on certain air pollutants, including setting limits on how much can be in the air anywhere in the United States.  New research demonstrates that the biological and health outcomes of mixed exposures (Example PM + Ozone) could be significantly greater than individual exposures. Air pollution has both acute and chronic effects on human health, affecting a number of different systems and organs. It ranges from minor upper respiratory irritation to chronic respiratory and heart disease, lung cancer, acute respiratory infections in children and chronic bronchitis in adults, aggravating pre-existing heart and lung disease, or asthmatic attacks. Short and long term exposures have been linked with premature mortality and reduced life expectancy.  Diseases that develop from persistent exposure to air pollution are environmental health diseases, which develop when a health environment is not maintained.|2023-09-15-14-45-55
Air pollution|Mortality|" The World Health Organization estimated in 2014 that every year air pollution causes the premature death of some 7 million people worldwide.  Studies published in March 2019 indicated that the number may be around 8.8 million.  A 2022 review concluded that air pollution was responsible for 6.67 (5.90–7.49) million premature deaths in 2019. It concluded that since 2015 little real progress against (superordinate) pollution, which remained at ~9 million earlier deaths, can be identified.  
Causes of deaths include strokes, heart disease, COPD, lung cancer, and lung infections. Urban outdoor air pollution is estimated to cause 1.3 million deaths worldwide per year.  Children are particularly at risk due to the immaturity of their respiratory organ systems.  In 2015, outdoor air pollution, mostly by PM2.5, was estimated to lead to 3.3 (95% CI 1.61–4.81) million premature deaths per year worldwide, predominantly in Asia.  In 2021, the WHO reported that outdoor air pollution was estimated to cause 4.2 million premature deaths worldwide in 2016. A 2020 study indicates that the global mean loss of life expectancy (LLE; similar to YPLL) from air pollution in 2015 was 2.9 years, substantially more than, for example, 0.3 years from all forms of direct violence, albeit a significant fraction of the LLE is unavoidable.  Communities with the most exceptional aging have low ambient air pollution, suggesting a link between air pollution levels and longevity. A study published in 2022 in GeoHealth concluded that eliminating energy-related fossil fuel emissions in the United States would prevent 46,900–59,400 premature deaths each year and provide $537–$678 billion in benefits from avoided PM2.5-related illness and death. India and China have the highest death rate due to air pollution.   India also has more deaths from asthma than any other nation according to the World Health Organization. In 2019, 1.6 million deaths in India were caused by air pollution.  In December 2013, air pollution was estimated to kill 500,000 people in China each year.  There is a positive correlation between pneumonia-related deaths and air pollution from motor vehicle emissions. Annual premature European deaths caused by air pollution are estimated at 430,000  to 800,000.  An important cause of these deaths is nitrogen dioxide and other nitrogen oxides (NOx) emitted by road vehicles.  In a 2015 consultation document the UK government disclosed that nitrogen dioxide is responsible for 23,500 premature UK deaths per annum.  Across the European Union, air pollution is estimated to reduce life expectancy by almost nine months. The US EPA has estimated that limiting ground-level ozone concentration to 65 parts per billion (ppb), would avert 1,700 to 5,100 premature deaths nationwide in 2020 compared with the 75 ppb standard. The agency projected the more protective standard would also prevent an additional 26,000 cases of aggravated asthma, and more than a million cases of missed work or school.   Following this assessment, the EPA acted to protect public health by lowering the National Ambient Air Quality Standards (NAAQS) for ground-level ozone to 70 ppb. A 2008 economic study of the health impacts and associated costs of air pollution in the Los Angeles Basin and San Joaquin Valley of Southern California shows that more than 3,800 people die prematurely (approximately 14 years earlier than normal) each year because air pollution levels violate federal standards. The number of annual premature deaths is considerably higher than the fatalities related to auto collisions in the same area, which average fewer than 2,000 per year.    A 2021 study found that outdoor air pollution is associated with substantially increased mortality ""even at low pollution levels below the current European and North American standards and WHO guideline values"" shortly before the WHO adjusted its guidelines. The largest cause is air pollution generated by fossil fuel combustion  – mostly the production and use of cars, electricity production, and heating.  A study by Greenpeace estimates there are 4.5 million annual premature deaths worldwide because of pollutants released by high-emission power stations and vehicle exhausts. Diesel exhaust (DE) is a major contributor to combustion-derived particulate matter air pollution. In several human experimental studies, using a well-validated exposure chamber setup, DE has been linked to acute vascular dysfunction and increased thrombus formation. A study concluded that PM2.5 air pollution induced by the contemporary free trade and consumption by the 19 G20 nations causes two million premature deaths annually, suggesting that the average lifetime consumption of about ~28 people in these countries causes at least one premature death (average age ~67) while developing countries ""cannot be expected"" to implement or be able to implement countermeasures without external support or internationally coordinated efforts. The WHO estimates that in 2016, ~58% of outdoor air pollution-related premature deaths were due to ischaemic heart disease and stroke.  The mechanisms linking air pollution to increased cardiovascular mortality are uncertain, but probably include pulmonary and systemic inflammation. A 2021 study by scientists of U.K. and U.S. universities that uses a high spatial resolution model and an updated concentration-response function concluded that 10.4 million global excess deaths in 2012 and 8.7 million in 2018 – or a fifth[dubious  – discuss] – were due to air pollution generated by fossil fuel combustion, significantly higher than earlier estimates and with spatially subdivided mortality impacts. According to the WHO, air pollution accounts for 1 in 8 deaths worldwide."|2023-09-15-14-45-55
Air pollution|Cardiovascular disease| A 2007 review of evidence found that for the general population, ambient air pollution exposure is a risk factor correlating with increased total mortality from cardiovascular events (range: 12% to 14% per 10 µg/m3 increase). [clarification needed] Air pollution is emerging as a risk factor for stroke, particularly in developing countries where pollutant levels are highest.  A 2007 study found that in women, air pollution is not associated with hemorrhagic but with ischemic stroke.  Air pollution was found to be associated with increased incidence and mortality from coronary stroke in a cohort study in 2011.  Associations are believed to be causal and effects may be mediated by vasoconstriction, low-grade inflammation and atherosclerosis.  Other mechanisms such as autonomic nervous system imbalance have also been suggested.|2023-09-15-14-45-55
Air pollution|Lung disease|" Research has demonstrated increased risk of developing asthma  and chronic obstructive pulmonary disease (COPD)  from increased exposure to traffic-related air pollution. Air pollution has been associated with increased hospitalization and mortality from asthma and COPD.   COPD includes diseases such as chronic bronchitis and emphysema.  The risk of lung disease from air pollution is greatest for the following groups of people: infants and young children, whose normal breathing is faster than that of older children and adults; the elderly; those who work outside or spend a lot of time outside; and those who have heart or lung disease. A study conducted in 1960–1961 in the wake of the Great Smog of 1952 compared 293 London residents with 477 residents of Gloucester, Peterborough, and Norwich, three towns with low reported death rates from chronic bronchitis. All subjects were male postal truck drivers aged 40 to 59. Compared to the subjects from the outlying towns, the London subjects exhibited more severe respiratory symptoms (including cough, phlegm, and dyspnea), reduced lung function (FEV1 and peak flow rate), and increased sputum production and purulence. The differences were more pronounced for subjects aged 50 to 59. The study controlled for age and smoking habits, so concluded that air pollution was the most likely cause of the observed differences. 
More studies have shown that air pollution exposure from traffic reduces lung function development in children  and lung function may be compromised by air pollution even at low concentrations. It is believed that much like cystic fibrosis, by living in a more urban environment serious health hazards become more apparent. Studies have shown that in urban areas people experience mucus hypersecretion, lower levels of lung function, and more self-diagnosis of chronic bronchitis and emphysema."|2023-09-15-14-45-55
Air pollution|Cancer (lung cancer)| Around 300,000 lung cancer deaths were attributed globally in 2019 to exposure to fine particulate matter, PM2.5, contained in air pollution. A review of evidence regarding whether ambient air pollution exposure is a risk factor for cancer in 2007 found solid data to conclude that long-term exposure to PM2.5 (fine particulates) increases the overall risk of non-accidental mortality by 6% per a 10 μg/m3 increase. Exposure to PM2.5 was also associated with an increased risk of mortality from lung cancer (range: 15–21% per 10 μg/m3 increase) and total cardiovascular mortality (range: 12–14% per a 10 μg/m3 increase). The review further noted that living close to busy traffic appears to be associated with elevated risks of these three outcomes – increase in lung cancer deaths, cardiovascular deaths, and overall non-accidental deaths. The reviewers also found suggestive evidence that exposure to PM2.5 is positively associated with mortality from coronary heart diseases and exposure to SO2 increases mortality from lung cancer, but the data was insufficient to provide solid conclusions.  Another investigation showed that higher activity level increases deposition fraction of aerosol particles in human lung and recommended avoiding heavy activities like running in outdoor space at polluted areas. In 2011, a large Danish epidemiological study found an increased risk of lung cancer for people who lived in areas with high nitrogen oxide concentrations. In this study, the association was higher for non-smokers than smokers.  An additional Danish study, also in 2011, likewise noted evidence of possible associations between air pollution and other forms of cancer, including cervical cancer and brain cancer. A study presented in 2022 outlined the biological basis for how air pollution causes cancer.|2023-09-15-14-45-55
Air pollution|Kidney disease| In 2021, a study of 163,197 Taiwanese residents over the period of 2001–2016 estimated that every 5 μg/m3 decrease in the ambient concentration of PM2.5 was associated with a 25% reduced risk of chronic kidney disease development.  According to a chord study involving 10,997 atherosclerosis patients, higher PM 2.5 exposure is associate with increased albuminuria.|2023-09-15-14-45-55
Air pollution|Fertility| In women undergoing IVF treatment, increases in NO2 both at the patient's address and by the IVF lab were significantly associated with a lower live birth rate. In the general population, there is a significant increase in miscarriage rate in women exposed to NO2 compared to the non-exposed group. CO exposure is significantly associated with stillbirth in the second and third trimester. Polycyclic aromatic hydrocarbons (PAHs) have been associated with reduced fertility. Benzo(a)pyrene (BaP) is a well-known PAH and carcinogen which is often found in exhaust fumes and cigarette smoke.  PAHs have been reported to administer their toxic effects through oxidative stress by increasing the production of Reactive Oxygen Species (ROS) which can result in inflammation and cell death. More long-term exposure to PAHs can result in DNA damage and reduced repair. Exposure to BaP has been reported to reduce sperm motility and increasing the exposure worsens this effect. Research has demonstrated that more BaPs were found in men with reported fertility issues compared to men without. Studies have shown that BaPs can affect folliculogenesis and ovarian development by reducing the number of ovarian germ cells via triggering cell death pathways and inducing inflammation which can lead to ovarian damage. Particulate matter (PM) refers to the collection of solids and liquids suspended in the air. These can be harmful to humans when exposed to in day-to-day life, and more research has shown that these effects may be more extensive than first thought; particularly on male fertility. Within the spectrum of PM there are different weights, such as PM2.5 which are tiny particles of 2.5 microns in width or smaller, compared with PM10 which are classified as 10 microns in diameter or less. In a study based in California it was found that as exposure to PM2.5 increased sperm motility decreased and morphology became more abnormal. Similarly, in Poland exposure to PM2.5 and PM10 lead to an increase in the percentage of cells with immature chromatin (DNA that has not fully developed or has developed abnormally). In Turkey, a study looked at the fertility of men who work as toll collectors and are therefore exposed to high levels of traffic pollutants daily. Traffic pollution often has high levels of PM10 alongside carbon monoxide and nitrous oxides.  In this study group there were significant differences in sperm count and motility when compared to a control group with limited air pollution exposure. In women, whilst overall effects on fertility did not seem significant there was an association was found between increased exposure to PM10 and early miscarriage. Exposure to smaller particulate matter, PM2.5, was seen to have an effect on conception rates in women undergoing IVF but not with live birth rates. Ground-level ozone (O3), when in high concentrations, is regarded as an air pollutant and is often found in smog in industrial areas. O3 is largely produced by chemical reactions involving NOx gases (nitrous oxides, especially from combustion) and volatile organic compounds in the presence of sunlight. There is limited research about the effect that ozone pollution has on fertility.  At present, there is no evidence to suggest that ozone exposure poses a deleterious effect on spontaneous fertility in either females or males. However, there have been studies which suggest that high levels of ozone pollution, often a problem in the summer months, exert an effect on in vitro fertilisation (IVF) outcomes. Within an IVF population, NOx and ozone pollutants were linked with reduced rates of live birth. While most research on this topic is focused on the direct human exposure of air pollution, other studies have analysed the impact of air pollution on gametes and embryos within IVF laboratories. Multiple studies have reported a marked improvement in embryo quality, implantation and pregnancy rates after IVF laboratories have implemented air filters in a concerted effort to reduce levels of air pollution.  Therefore, ozone pollution is considered to have a negative impact on the success of assisted reproductive technologies (ART) when occurring at high levels. Ozone is thought to act in a biphasic manner where a positive effect on live birth is observed when ozone exposure is limited to before IVF embryo implantation.  Conversely, a negative effect is demonstrated upon exposure to ozone after embryo implantation. Retrospective and prospective studies evaluating the effect of several traffic pollutants (of which ground-level ozone is one) highlighted a significant decrease in live birth rates and miscarriages. In terms of male fertility, ozone is reported to cause a significant decrease in sperm concentration measured in semen after exposure.  Similarly, sperm vitality, the proportion of alive spermatozoa in a sample, was demonstrated to be diminished in a handful of studies.  This demonstrates that ozone air pollution exhibits a significantly negative effect of air pollution on this parameter. However, findings on the effect of ozone exposure on male fertility are somewhat discordant, highlighting the need for further research.|2023-09-15-14-45-55
Air pollution|Children|" In the United States, despite the passage of the Clean Air Act in 1970, in 2002 at least 146 million Americans were living in non-attainment areas – regions in which the concentration of certain air pollutants exceeded federal standards.  These dangerous pollutants are known as the criteria pollutants, and include ozone, particulate matter, sulfur dioxide, nitrogen dioxide, carbon monoxide, and lead. Protective measures to ensure children's health are being taken in cities such as New Delhi, India, where buses now use compressed natural gas to help eliminate the ""pea-soup"" smog.  A 2015 study in Europe has found that exposure to ultrafine particles can increase blood pressure in children.  In a 2018 WHO report, polluted air leads to the poisoning of millions of children under the age of 15, resulting in the death of some six hundred thousand children annually. Prenatal exposure to polluted air has been linked to a variety of neurodevelopmental disorders in children. For example, exposure to polycyclic aromatic hydrocarbons (PAH) was associated with reduced IQ scores and symptoms of anxiety and depression.  They can also lead to detrimental perinatal health outcomes that are often fatal in developing countries.  A 2014 study found that PAHs might play a role in the development of childhood attention deficit hyperactivity disorder (ADHD). Researchers have begun to find evidence for air pollution as a risk factor for autism spectrum disorder (ASD). In Los Angeles, children who were living in areas with high levels of traffic-related air pollution were more likely to be diagnosed with autism between 3–5 years of age.  The connection between air pollution and neurodevelopmental disorders in children is thought to be related to epigenetic dysregulation of the primordial germ cells, embryo, and fetus during a critical period. Some PAHs are considered endocrine disruptors and are lipid soluble. When they build up in adipose tissue, they can be transferred across the placenta.  Air pollution has been associated with the prevalence of preterm births. Ambient levels of air pollution have been associated with preterm birth and low birth weight. A 2014 WHO worldwide survey on maternal and perinatal health found a statistically significant association between low birth weights (LBW) and increased levels of exposure to PM2.5. Women in regions with greater than average PM2.5 levels had statistically significant higher odds of pregnancy resulting in a low-birth weight infant even when adjusted for country-related variables.  The effect is thought to be from stimulating inflammation and increasing oxidative stress. A study by the University of York found that in 2010 exposure to PM2.5 was strongly associated with 18% of preterm births globally, which was approximately 2.7 million premature births. The countries with the highest air pollution associated preterm births were in South and East Asia, the Middle East, North Africa, and West sub-Saharan Africa.  In 2019, ambient particulate matter pollution in Africa resulted in at least 383,000 early deaths, according to new estimates of the cost of air pollution in the continent. This increased from 3.6% in 1990 to around 7.4% of all premature deaths in the area. The source of PM2.5 differs greatly by region. In South and East Asia, pregnant women are frequently exposed to indoor air pollution because of wood and other biomass fuels being used for cooking, which are responsible for more than 80% of regional pollution. In the Middle East, North Africa and West sub-Saharan Africa, fine PM comes from natural sources, such as dust storms.  The United States had an estimated 50,000 preterm births associated with exposure to PM2.5 in 2010. A study between 1988 and 1991 found a correlation between sulfur dioxide (SO2) and total suspended particulates (TSP) and preterm births and low birth weights in Beijing. A group of 74,671 pregnant women, in four separate regions of Beijing, were monitored from early pregnancy to delivery along with daily air pollution levels of sulfur Dioxide and TSP (along with other particulates). The estimated reduction in birth weight was 7.3 g for every 100 µg/m3 increase in SO2 and 6.9 g for each 100 µg/m3 increase in TSP. These associations were statistically significant in both summer and winter, although, summer was greater. The proportion of low birth weight attributable to air pollution, was 13%. This is the largest attributable risk ever reported for the known risk factors of low birth weight.  Coal stoves, which are in 97% of homes, are a major source of air pollution in this area. Brauer et al. studied the relationship between air pollution and proximity to a highway with pregnancy outcomes in a Vancouver cohort of pregnant woman using addresses to estimate exposure during pregnancy. Exposure to NO, NO2, CO, PM10 and PM2.5 were associated with infants born small for gestational age (SGA). Women living less than 50 meters away from an expressway or highway were 26% more likely to give birth to a SGA infant."|2023-09-15-14-45-55
Air pollution|"""Clean"" areas"| Even in areas with relatively low levels of air pollution, public health effects can be significant and costly, since a large number of people breathe in such pollutants. A study published in 2017 found that even in areas of the U.S. where ozone and PM2.5 meet federal standards, Medicare recipients who are exposed to more air pollution have higher mortality rates. A 2005 scientific study for the British Columbia Lung Association showed that a small improvement in air quality (1% reduction of ambient PM2.5 and ozone concentrations) would produce $29 million in annual savings in the Metro Vancouver region in 2010.  This finding is based on health valuation of lethal (death) and sub-lethal (illness) affects. A study published in 2022 found that rural population in India, like those in urban areas, are also exposed to high levels of air pollution. In 2020, scientists found that the boundary layer air over the Southern Ocean around Antarctica is 'unpolluted' by humans.|2023-09-15-14-45-55
Air pollution|Central nervous system|" Data is accumulating that air pollution exposure also affects the central nervous system. Air pollution increases the risk of dementia in people over 50 years old.  Childhood indoor air pollution may negatively affect cognitive function and neurodevelopment.   Prenatal exposure may also affect neurodevelopment.   Studies show that air pollution is associated with a variety of developmental disabilities, oxidative stress, and neuro-inflammation and that it may contribute to Alzheimer's disease and Parkinson's disease. Researchers at the University of Rochester Medical Center found that early exposure to air pollution causes the same changes in the brain as autism and schizophrenia. This study was published in the journal Environmental Health Perspectives, in June 2014. It also showed that air pollution also affected short-term memory, learning ability, and impulsivity. Lead researcher Deborah Cory-Slechta said that: When we looked closely at the ventricles, we could see that the white matter that normally surrounds them hadn't fully developed. It appears that inflammation had damaged those brain cells and prevented that region of the brain from developing, and the ventricles simply expanded to fill the space. Our findings add to the growing body of evidence that air pollution may play a role in autism, as well as in other neurodevelopmental disorders. Exposure to fine particulate matter can increase levels of cytokines - neurotransmitters produced in response to infection and inflammation that are also associated with depression and suicide. Pollution has been associated with inflammation of the brain, which may disrupt mood regulation. According to a study of Washington DC's American University, heightened PM2.5 levels are linked to more self-reported depressive symptoms, and increases in daily suicide rates. In a study of mice, air pollution also has a larger negative impact on males than on females. In 2015, experimental studies reported the detection of significant episodic (situational) cognitive impairment from impurities in indoor air breathed by test subjects who were not informed about changes in the air quality. Researchers at the Harvard University and SUNY Upstate Medical University and Syracuse University measured the cognitive performance of 24 participants in three different controlled laboratory atmospheres that simulated those found in ""conventional"" and ""green"" buildings, as well as green buildings with enhanced ventilation. Performance was evaluated objectively using the widely used Strategic Management Simulation software simulation tool, which is a well-validated assessment test for executive decision-making in an unconstrained situation allowing initiative and improvisation. Significant deficits were observed in the performance scores achieved in increasing concentrations of either volatile organic compounds (VOCs) or carbon dioxide, while keeping other factors constant. The highest impurity levels reached are not uncommon in some classroom or office environments.   Higher PM2.5 and CO2 concentrations were shown to be associated with slower response times and reduced accuracy in tests."|2023-09-15-14-45-55
Air pollution|Agricultural effects| Various studies have estimated the impacts of air pollution on agriculture, especially ozone. A 2020  study showed that ozone pollution in California may reduce yields of certain perennial crops such as table grapes by as much as 22% per year, translating into economic damages of more than $1 billion per year.  After air pollutants enter the agricultural environment, they not only directly affect agricultural production and quality, but also enter agricultural waters and soil.  The COVID-19 induced lockdown served as a natural experiment to expose the close links between air quality and surface greenness. In India, the lockdown induced improvement in air quality, enhanced surface greenness and photosynthetic activity, with the positive response of vegetation to reduce air pollution was dominant in croplands.  On the other hand, agriculture in its traditional form is one of the primary contributors to the emission of trace gases like atmospheric ammonia.|2023-09-15-14-45-55
Air pollution|Economic effects|" Air pollution costs the world economy $5 trillion per year as a result of productivity losses and degraded quality of life, in a 2016 joint study by the World Bank and the Institute for Health Metrics and Evaluation (IHME) at the University of Washington.    These productivity losses are caused by deaths due to diseases caused by air pollution. One out of ten deaths in 2013 was caused by diseases associated with air pollution and the problem is getting worse. The problem is even more acute in the developing world. ""Children under age 5 in lower-income countries are more than 60 times as likely to die from exposure to air pollution as children in high-income countries.""   The report states that additional economic losses caused by air pollution, including health costs  and the adverse effect on agricultural and other productivity were not calculated in the report, and thus the actual costs to the world economy are far higher than $5 trillion. A study published in 2022 found “a strong and significant connection between air pollution and construction site accidents” and that “a 10-ppb increase in NO₂ levels increases the likelihood of an accident by as much as 25 percent”."|2023-09-15-14-45-55
Air pollution|Other effects| Artificial air pollution may be detectable on Earth from distant vantage points such as other planetary systems via atmospheric SETI – including NO2 pollution levels and with telescopic technology close to today. It may also be possible to detect extraterrestrial civilizations this way.|2023-09-15-14-45-55
Air pollution|Historical disasters| The world's worst short-term civilian pollution crisis was the 1984 Bhopal Disaster in India.  Leaked industrial vapours from the Union Carbide factory, belonging to Union Carbide, Inc., U.S.A. (later bought by Dow Chemical Company), killed at least 3787 people and injured from 150,000 to 600,000. The United Kingdom suffered its worst air pollution event when the 4 December Great Smog of 1952 formed over London. In six days more than 4,000 died and more recent estimates put the figure at nearer 12,000. An accidental leak of anthrax spores from a biological warfare laboratory in the former USSR in 1979 near Yekaterinburg (formerly Sverdlovsk) is believed to have caused at least 64 deaths.  The worst single incident of air pollution to occur in the US occurred in Donora, Pennsylvania, in late October 1948, when 20 people died and over 7,000 were injured.|2023-09-15-14-45-55
Air pollution|Reduction and regulation| Global depletion of the surrounding air pollution will require valiant leadership, a surplus of combined resources from the international community, and extensive societal changes.  Pollution prevention seeks to prevent pollution such as air pollution and could include adjustments to industrial and business activities such as designing sustainable manufacturing processes (and the products' designs)  and related legal regulations as well as efforts towards renewable energy transitions. Efforts to reduce particulate matter in the air may result in better health. The 9-Euro-Ticket scheme in Germany which allowed people to buy a monthly pass allowing use on all local and regional transport (trains, trams and busses) for 9 euro (€) for one month of unlimited travel saved 1.8 million tons of CO2 emissions during its three-month implementation from June to August 2022.|2023-09-15-14-45-55
Air pollution|Pollution control| Various pollution control technologies and strategies are available to reduce air pollution.   At its most basic level, land-use planning is likely to involve zoning and transport infrastructure planning. In most developed countries, land-use planning is an important part of social policy, ensuring that land is used efficiently for the benefit of the wider economy and population, as well as to protect the environment.  Stringent environmental regulations, effective control technologies and shift towards the renewable source of energy also helping countries like China and India to reduce their sulfur dioxide pollution. Titanium dioxide has been researched for its ability to reduce air pollution. Ultraviolet light will release free electrons from material, thereby creating free radicals, which break up VOCs and NOx gases. One form is superhydrophilic. Pollution-eating nanoparticles placed near a busy road were shown to absorb toxic emission from around 20 cars each day.|2023-09-15-14-45-55
Air pollution|Energy transition| Since a large share of air pollution is caused by combustion of fossil fuels such as coal and oil, the reduction of these fuels can reduce air pollution drastically. Most effective is the switch to clean power sources such as wind power, solar power, hydro power which do not cause air pollution.  Efforts to reduce pollution from mobile sources includes expanding regulation to new sources (such as cruise and transport ships, farm equipment, and small gas-powered equipment such as string trimmers, chainsaws, and snowmobiles), increased fuel efficiency (such as through the use of hybrid vehicles), conversion to cleaner fuels, and conversion to electric vehicles. A very effective means to reduce air pollution is the transition to renewable energy. According to a study published in Energy and Environmental Science in 2015 the switch to 100% renewable energy in the United States would eliminate about 62,000 premature mortalities per year and about 42,000 in 2050, if no biomass were used. This would save about $600 billion in health costs a year due to reduced air pollution in 2050, or about 3.6% of the 2014 U.S. gross domestic product.  Air quality improvement is a near-term benefit among the many societal benefits from climate change mitigation.|2023-09-15-14-45-55
Air pollution|Alternatives to pollution| There are now practical alternatives to the principal causes of air pollution:|2023-09-15-14-45-55
Air pollution|Control devices| The following items are commonly used as pollution control devices in industry and transportation. They can either destroy contaminants or remove them from an exhaust stream before it is emitted into the atmosphere.|2023-09-15-14-45-55
Air pollution|Monitoring| Spatiotemporal monitoring of air quality may be necessary for improving air quality, and thereby the health and safety of the public, and assessing impacts of interventions.  Such monitoring is done to different extents with different regulatory requirements with discrepant regional coverage by a variety of organizations and governance entities such as using a variety of technologies for use of the data and sensing such mobile IoT sensors,  satellites,    and monitoring stations.   Some websites attempt to map air pollution levels using available data.|2023-09-15-14-45-55
Air pollution|Air quality modeling| Numerical models either on a global scale using tools such as GCMs (general circulation models coupled with a pollution module) or CTMs (Chemical transport model) can be used to simulate the levels of different pollutants in the atmosphere. These tools can have several types (Atmospheric model) and different uses. These models can be used in forecast mode which can help policy makers to decide on appropriate actions when an air pollution episode is detected. They can also be used for climate modeling including evolution of air quality in the future, for example the IPCC (Intergovernmental Panel on Climate Change) provides climate simulations including air quality assessments in their reports (latest report accessible through their site).|2023-09-15-14-45-55
Air pollution|Regulations| In general, there are two types of air quality standards. The first class of standards (such as the U.S. National Ambient Air Quality Standards and E.U. Air Quality Directive ) set maximum atmospheric concentrations for specific pollutants. Environmental agencies enact regulations which are intended to result in attainment of these target levels. The second class (such as the North American air quality index) take the form of a scale with various thresholds, which is used to communicate to the public the relative risk of outdoor activity. The scale may or may not distinguish between different pollutants. In Canada, air pollution and associated health risks are measured with the Air Quality Health Index (AQHI).  It is a health protection tool used to make decisions to reduce short-term exposure to air pollution by adjusting activity levels during increased levels of air pollution. The AQHI is a federal program jointly coordinated by Health Canada and Environment Canada. However, the AQHI program would not be possible without the commitment and support of the provinces, municipalities and NGOs. From air quality monitoring to health risk communication and community engagement, local partners are responsible for the vast majority of work related to AQHI implementation. The AQHI provides a number from 1 to 10+ to indicate the level of health risk associated with local air quality. Occasionally, when the amount of air pollution is abnormally high, the number may exceed 10. The AQHI provides a local air quality current value as well as a local air quality maximums forecast for today, tonight and tomorrow and provides associated health advice. As it is now known that even low levels of air pollution can trigger discomfort for the sensitive population, the index has been developed as a continuum: The higher the number, the greater the health risk and need to take precautions. The index describes the level of health risk associated with this number as 'low', 'moderate', 'high' or 'very high', and suggests steps that can be taken to reduce exposure. The measurement is based on the observed relationship of nitrogen dioxide (NO2), ground-level ozone (O3) and particulates (PM2.5) with mortality, from an analysis of several Canadian cities. Significantly, all three of these pollutants can pose health risks, even at low levels of exposure, especially among those with pre-existing health problems. When developing the AQHI, Health Canada's original analysis of health effects included five major air pollutants: particulates, ozone, and nitrogen dioxide (NO2), as well as sulfur dioxide (SO2), and carbon monoxide (CO). The latter two pollutants provided little information in predicting health effects and were removed from the AQHI formulation. The AQHI does not measure the effects of odour, pollen, dust, heat or humidity. TA Luft is the German air quality regulation.|2023-09-15-14-45-55
Air pollution|Governing urban air pollution|" In Europe, Council Directive 96/62/EC on ambient air quality assessment and management provides a common strategy against which member states can ""set objectives for ambient air quality in order to avoid, prevent or reduce harmful effects on human health and the environment ... and improve air quality where it is unsatisfactory"". In July 2008, in the case Dieter Janecek v. Freistaat Bayern, the European Court of Justice ruled that under this directive  citizens have the right to require national authorities to implement a short term action plan that aims to maintain or achieve compliance to air quality limit values. This important case law appears to confirm the role of the EC as centralised regulator to European nation-states as regards air pollution control. It places a supranational legal obligation on the UK to protect its citizens from dangerous levels of air pollution, furthermore superseding national interests with those of the citizen. In 2010, the European Commission (EC) threatened the UK with legal action against the successive breaching of PM10 limit values.  The UK government has identified that if fines are imposed, they could cost the nation upwards of £300 million per year. In March 2011, the Greater London Built-up Area remained the only UK region in breach of the EC's limit values, and was given three months to implement an emergency action plan aimed at meeting the EU Air Quality Directive.  The City of London has dangerous levels of PM10 concentrations, estimated to cause 3000 deaths per year within the city.  As well as the threat of EU fines, in 2010 it was threatened with legal action for scrapping the western congestion charge zone, which is claimed to have led to an increase in air pollution levels. In response to these charges, mayor of London Boris Johnson has criticised the current need for European cities to communicate with Europe through their nation state's central government, arguing that in future ""A great city like London"" should be permitted to bypass its government and deal directly with the European Commission regarding its air quality action plan. This can be interpreted as recognition that cities can transcend the traditional national government organisational hierarchy and develop solutions to air pollution using global governance networks, for example through transnational relations. Transnational relations include but are not exclusive to national governments and intergovernmental organisations,  allowing sub-national actors including cities and regions to partake in air pollution control as independent actors. Global city partnerships can be built into networks, for example the C40 Cities Climate Leadership Group, of which London is a member. The C40 is a public 'non-state' network of the world's leading cities that aims to curb their greenhouse emissions.  The C40 has been identified as 'governance from the middle' and is an alternative to intergovernmental policy.  It has the potential to improve urban air quality as participating cities ""exchange information, learn from best practices and consequently mitigate carbon dioxide emissions independently from national government decisions"".  A criticism of the C40 network is that its exclusive nature limits influence to participating cities and risks drawing resources away from less powerful city and regional actors."|2023-09-15-14-45-55
Air pollution|Hotspots| Air pollution hotspots are areas where air pollution emissions expose individuals to increased negative health effects.  They are particularly common in highly populated, urban areas, where there may be a combination of stationary sources (e.g. industrial facilities) and mobile sources (e.g. cars and trucks) of pollution. Emissions from these sources can cause respiratory disease, childhood asthma,  cancer, and other health problems. Fine particulate matter such as diesel soot, which contributes to more than 3.2 million premature deaths around the world each year, is a significant problem. It is very small and can lodge itself within the lungs and enter the bloodstream. Diesel soot is concentrated in densely populated areas, and one in six people in the U.S. live near a diesel pollution hot spot. While air pollution hotspots affect a variety of populations, some groups are more likely to be located in hotspots. Previous studies have shown disparities in exposure to pollution by race and/or income. Hazardous land uses (toxic storage and disposal facilities, manufacturing facilities, major roadways) tend to be located where property values and income levels are low. Low socioeconomic status can be a proxy for other kinds of social vulnerability, including race, a lack of ability to influence regulation and a lack of ability to move to neighborhoods with less environmental pollution. These communities bear a disproportionate burden of environmental pollution and are more likely to face health risks such as cancer or asthma. Studies show that patterns in race and income disparities not only indicate a higher exposure to pollution but also higher risk of adverse health outcomes.  Communities characterized by low socioeconomic status and racial minorities can be more vulnerable to cumulative adverse health impacts resulting from elevated exposure to pollutants than more privileged communities.  Blacks and Latinos generally face more pollution than Whites and Asians, and low-income communities bear a higher burden of risk than affluent ones.  Racial discrepancies are particularly distinct in suburban areas of the Southern United States and metropolitan areas of the Midwestern and Western United States.  Residents in public housing, who are generally low-income and cannot move to healthier neighborhoods, are highly affected by nearby refineries and chemical plants.|2023-09-15-14-45-55
Air pollution|Cities| Air pollution is usually concentrated in densely populated metropolitan areas, especially in developing countries where cities are experiencing rapid growth and environmental regulations are relatively lax or nonexistent. Urbanization leads to a rapid rise in premature mortality due to anthropogenic air pollution in fast-growing tropical cities.  However, even populated areas in developed countries attain unhealthy levels of pollution, with Los Angeles and Rome being two examples.  Between 2002 and 2011 the incidence of lung cancer in Beijing near doubled. While smoking remains the leading cause of lung cancer in China, the number of smokers is falling while lung cancer rates are rising . Tehran was declared the most polluted city in the world on May 24, 2022.|2023-09-15-14-45-55
Air pollution|Projections| In a 2019 projection, by 2030 half of the world's pollution emissions could be generated by Africa.  Potential contributors to such an outcome include increased burning activities (such as the burning of open waste), traffic, agri-food and chemical industries, sand dust from the Sahara, and overall population growth. In a 2012 study, by 2050 outdoor air pollution (particulate matter and ground-level ozone) is projected to become the top cause of environmentally related deaths worldwide.|2023-09-15-14-45-55
Ammonia pollution|General| Ammonia pollution is pollution by the chemical ammonia (NH3) – a compound of nitrogen and hydrogen which is a byproduct of agriculture and industry.  Common forms include air pollution by the ammonia gas emitted by rotting agricultural slurry and fertilizer factories while natural sources include the burning coal mines of Jharia, the caustic Lake Natron and the guano of seabird colonies. Gaseous ammonia reacts with other pollutants in the air to form fine particles of ammonium salts, which affect human breathing. Ammonia gas can also affect the chemistry of the soil on which it settles and will, for example, degrade the conditions required by the sphagnum moss and heathers of peatland. Ammonia detection is facilitated through the use of filter packs and fabric denuders (a gas separator). Techniques such as satellite imaging and rainwater analysis are also used.  Much is still unknown about the impact of ammonia pollution, but rising emission rates concern scientists. The level of ammonia in the atmosphere was more than twice as large in 2010 as it was in 1940.  Ammonia is now recognized by many countries as a major pollutant and some have begun taking steps to limit their emissions.|2023-09-22-06-53-56
Ammonia pollution|Sources| The table below lists sources of ammonia pollution and their percent contribution to global ammonia emissions. The sources are also classified as either anthropogenic (resulting from humans) or natural. About half of the NH4+ is converted to ammonia gas, which then enters the atmosphere or dissolves in runoff. Volatilization is increased in moist, warm, and acidic environments. 2. Artificial fertilizer usage such as in slurry is manufactured to have high nutrient contents. This includes nitrogen based compounds like ammonium (NH4+) which, similar to manure, is released as ammonia through volatilization (into the atmosphere or in runoff). (⅔ from livestock) (⅓  from fertilizer)|2023-09-22-06-53-56
Ammonia pollution|Effects| Ammonia decreases the biodiversity of terrestrial and aquatic ecosystems and also forms aerosols in the atmosphere which can cause human health complications if inhaled.|2023-09-22-06-53-56
Ammonia pollution|Biodiversity| Gaseous ammonia emissions enter Earth’s soil and water through both wet and dry deposition. Aqueous ammonia, another form of the compound, may seep directly into the ground or flow into aquatic ecosystems. Both terrestrial and aquatic ammonia pollution decrease biodiversity mainly through the process of nitrification. In terrestrial settings, ammonia increases soil acidity (decreased pH) and causes eutrophication (an overabundance of nutrients). Both of these occur as a direct result of nitrification. In this process, ammonia is converted into nitrate by bacteria  (usually of genera Nitrosomonas and Nitrobacter) performing the following two step reaction: Step 1: Ammonia (NH3 ) is oxidized into nitrite (NO2−) by: Step 2: Nitrite (NO2−) is oxidized into nitrate(NO3−) The products of this reaction include hydrogen (H+) ions which lower the soil pH and lead to acidification. Increased soil acidity in the ecosystem leads to decreased protection against cold temperatures, drought, disease, and invasive species. The other product, nitrate (NO3−), is a key nutrient for plant growth. This excess nitrate from ammonia nitrification favors nitrophilous plants (those that prefer high nitrate concentrations) and disadvantages others. For example, an increase in nitrophilous plant populations shade other plants from necessary sunlight. Sensitive plant groups such as lichen and moss are particularly susceptible to ammonia pollution and habitats such as bogs, peatlands, grasslands, heathlands, and forests are mainly affected. In aquatic settings, ammonia causes nitrogenous oxygen demand, eutrophication, and changes in fish health. Nitrogenous biological oxygen demand (NBOD) occurs as a direct result of nitrification (see terrestrial effects). Dissolved oxygen (O2) is used in nitrification to react with NH3. This results in less O2 available to organisms that depend on it. Nitrification also releases nitrate which leads to eutrophication as in terrestrial settings. Nitrophilous algae and macrophytes create large blooms in standing water. This puts stress on resources and also can indirectly poison organisms through toxic algae formation. In contrast, ammonia can also directly harm organisms with permeable skin if they absorb it. Fish kills and changes in fish growth, gill condition, organ weights, and hematocrit (red blood cell) levels are linked to ammonia exposure.|2023-09-22-06-53-56
Ammonia pollution|Human health| Gaseous ammonia that is not deposited forms aerosols by combining with other emissions such as sulfur dioxide (SO2) and nitrogen oxides (NOX). Atmospheric reactions among sulfur dioxide, nitrogen oxides, intermediate products, and other gases eventually result in formation of ammonium nitrate (NH4NO3) and ammonium bisulfate (NH4HSO4) by the following: These resulting ammonium (NH4) aerosols are classified as fine particulate matter (PM2.5 or particulate matter less than 2.5 microns in size). The small size of PM2.5 particles allows them to enter the lungs and bloodstream through inhalation. Ammonium particles can then cause complications including asthma, lung cancer, cardiovascular issues, birth defects, and premature death in humans. The smaller ammonium PM2.5 can also travel further distances (100–1000 km) when compared to unreacted ammonia (less than 10–100 km) in the atmosphere.  Some countries like China have focused on reducing SO2 and NOX emissions, however increased NH3 pollution still results in PM2.5 formation and reduces air quality.|2023-09-22-06-53-56
Ammonia pollution|Monitoring techniques| Ammonia pollution is most commonly measured by its presence in the atmosphere. It has no automatic relay system as with other pollutant measurements such as carbon dioxide; therefore, ammonia samples must be collected through other methods including filter packs, fabric denuders, satellite imaging, and rainwater analysis.|2023-09-22-06-53-56
Ammonia pollution|Filter packs|" Filter packs consist of an air pump fitted with a Teflon and glass fiber filter. The pump sucks in air and the filters remove ammonia particles. The Teflon and glass fiber filter are coated in citric acid which reacts with the slightly basic ammonia particles. This reaction essentially ""glues"" the ammonia in place. Later, the filter is tested with Nessler’s reagent (an ammonia indicator) and a spectrophotometer reads the amount of ammonia present."|2023-09-22-06-53-56
Ammonia pollution|Fabric denuders| Fabric denuders function through passive sampling (no pump is used and collection depends only on airflow). A pipe fitted with cloth filters on either side serves as a tunnel for air to diffuse through. The cloth is coated in phosphoric acid which attracts ammonia gas (a base). Air flows through the tube and ammonia sticks to the filters which can then be tested for NH3 concentrations using Nessler's reagent and a spectrophotometer.|2023-09-22-06-53-56
Ammonia pollution|Satellite imaging| Systems of satellites measure gas signatures in the atmosphere over time. Ammonia’s signature is charted giving an estimate of its prevalence in the air and where it is most concentrated. NASA has been using satellite imaging to monitor ammonia emissions since 2008.|2023-09-22-06-53-56
Ammonia pollution|Rainwater analysis| Buckets of rain are collected and then tested for ammonia using techniques described above. This provides the concentration of ammonia gas trapped in atmospheric water vapor.|2023-09-22-06-53-56
Ammonia pollution|Regulations| Although ammonia is now recognized as a potentially hazardous air pollutant, only some countries have taken further action to reduce their emission. Reduction strategies predominantly focus on controlling agricultural practices.|2023-09-22-06-53-56
Ammonia pollution|Policy| The European Union has had two policies in place since 1999 to prevent ammonia pollution. These include the Gothenburg Protocol (1999) and the Directive on Integrated Pollution Protection and Control (1999). The National Emission Ceilings Directive was also put into effect in 2001 by the EU to further reduce NH3 emissions. The Gothenburg Protocol was revised in 2012 to set new, stricter, ceiling limits on ammonia until 2020 and to include all EU-27 countries. The United Kingdom in particular has announced that they plan to cut emissions by 16% by 2030, however no new policies have been enacted. Other countries like China and the United States acknowledge ammonia as a pollutant, but have no policies in place to regulate it. Ammonia pollution regulations mainly focus on mitigation through better farming practices. One suggested change is keeping manure and fertilizer in large storage tanks to prevent runoff and volatilization into the air. Another strategy involves feeding livestock diets less dense in protein. This would result in less nitrogen proteins (including ammonia) ending up in manure. A final idea is using less urea and ammonium based fertilizers which are prone to volatilization into ammonia.|2023-09-22-06-53-56
Area source pollution|General| Area sources are sources of pollution which emit a substance or radiation from a specified area.|2023-07-29-16-49-56
Area source pollution|Air pollution| For example, area sources of air pollution are air pollutant emission sources which operate within a certain locale. The U.S. Environmental Protection Agency has categorized 70 different categories of air pollution area source.  Locomotives operating on certain linear tracks are examples of a line source, whereas locomotives operating within a railyard are an example of an area source of pollution. Other area sources of air pollution are:|2023-07-29-16-49-56
Area source pollution|Water pollution| Water pollution manifestations of an area source—often called nonpoint source pollution—include: In the 1950s or earlier hydrology transport models appeared to calculate surface runoff, primarily for flood forecasting. Beginning in the early 1970s computer models were developed to analyze the transport of runoff carrying water pollutants, which considered dissolution rates of various chemicals, infiltration into soils and ultimate pollutant load delivered to receiving waters. One of the earliest models addressing chemical dissolution in runoff and resulting transport was developed in the early 1970s by the United States Environmental Protection Agency. [full citation needed] This computer model formed the basis of much of the regulatory framework that led to strategies for water pollution control via land use and chemical handling techniques. People produce so much trash that half of it goes in water sources.[citation needed]|2023-07-29-16-49-56
Asbestosis|General| Asbestosis is long-term inflammation and scarring of the lungs due to asbestos fibers.  Symptoms may include shortness of breath, cough, wheezing, and chest tightness.  Complications may include lung cancer, mesothelioma, and pulmonary heart disease. Asbestosis is caused by breathing in asbestos fibers. It requires a relatively large exposure over a long period of time, which typically only occur in those who directly work with asbestos.   All types of asbestos fibers are associated with an increased risk.  It is generally recommended that currently existing and undamaged asbestos be left undisturbed.  Diagnosis is based upon a history of exposure together with medical imaging.  Asbestosis is a type of interstitial pulmonary fibrosis. There is no specific treatment.  Recommendations may include influenza vaccination, pneumococcal vaccination, oxygen therapy, and stopping smoking.  Asbestosis affected about 157,000 people and resulted in 3,600 deaths in 2015.   Asbestos use has been banned in a number of countries in an effort to prevent disease. Statistics from the UK's Health and Safety Executive showed that in 2019, there were 490 asbestosis deaths.|2023-09-09-08-14-16
Asbestosis|Signs and symptoms|" The signs and symptoms of asbestosis typically manifest after a significant amount of time has passed following asbestos exposure, often several decades under current conditions in the US.  The primary symptom of asbestosis is generally the slow onset of shortness of breath, especially with physical activity.  Clinically advanced cases of asbestosis may lead to respiratory failure. When a stethoscope is used to listen to the lungs of a person with asbestosis, they may hear inspiratory ""crackles"". The characteristic pulmonary function finding in asbestosis is a restrictive ventilatory defect.  This manifests as a reduction in lung volumes, particularly the vital capacity (VC) and total lung capacity (TLC). The TLC may be reduced through alveolar wall thickening; however, this is not always the case.  Large airway function, as reflected by FEV1/FVC, is generally well preserved.  In severe cases, the drastic reduction in lung function due to the stiffening of the lungs and reduced TLC may induce right-sided heart failure (cor pulmonale).   In addition to a restrictive defect, asbestosis may produce reduction in diffusion capacity and a low amount of oxygen in the blood of the arteries."|2023-09-09-08-14-16
Asbestosis|Cause| The cause of asbestosis is the inhalation of microscopic asbestos mineral fibers suspended in the air.  In the 1930s, E. R. A. Merewether found that greater exposure resulted in greater risk.|2023-09-09-08-14-16
Asbestosis|Risk factors|" Those who worked in the production, milling, manufacturing, installation, or removal of asbestos products before the late 1970s are at an increased risk of exposure to asbestos. This includes people who worked in these jobs in the United States and Canada. For example: Construction workers who inhale asbestos from contaminated building materials such paint, spackling, roof shingles, masonry compounds, and drywall may get asbestosis.
The amount and length of an individual's exposure to asbestos are the primary factors that determine the level of risk. The longer one is exposed to the substance, the higher their risk of developing lung damage. Families of exposed workers can be affected because asbestos fibers from clothing and hair can end up in the home. People who live near mines can also be exposed to airborne asbestos fibers"|2023-09-09-08-14-16
Asbestosis|Pathogenesis| Asbestosis is the scarring of lung tissue (beginning around terminal bronchioles and alveolar ducts and extending into the alveolar walls) resulting from the inhalation of asbestos fibers. There are two types of fibers: amphibole (thin and straight) and serpentine (curly). All forms of asbestos fibers are responsible for human disease as they are able to penetrate deeply into the lungs.  When such fibers reach the alveoli (air sacs) in the lung, where oxygen is transferred into the blood, the foreign bodies (asbestos fibers) cause the activation of the lungs' local immune system and provoke an inflammatory reaction dominated by lung macrophages that respond to chemotactic factors activated by the fibers.  This inflammatory reaction can be described as chronic rather than acute, with a slow ongoing progression of the immune system attempting to eliminate the foreign fibers. Macrophages phagocytose (ingest) the fibers and stimulate fibroblasts to deposit connective tissue. Due to the asbestos fibers' natural resistance to digestion, some macrophages are killed and others release inflammatory chemical signals, attracting further lung macrophages and fibrolastic cells that synthesize fibrous scar tissue, which eventually becomes diffuse and can progress in heavily exposed individuals. This tissue can be seen microscopically soon after exposure in animal models.   Some asbestos fibers become layered by an iron-containing proteinaceous material (ferruginous body) in cases of heavy exposure where about 10% of the fibers become coated.  Most inhaled asbestos fibers remain uncoated.  About 20% of the inhaled fibers are transported by cytoskeletal components of the alveolar epithelium to the interstitial compartment of the lung where they interact with macrophages and mesenchymal cells. The cytokines, transforming growth factor beta and tumor necrosis factor alpha, appear to play major roles in the development of  scarring inasmuch as the process can be blocked in animal models by preventing the expression of the growth factors.   The result is fibrosis in the interstitial space, thus asbestosis. This fibrotic scarring causes alveolar walls to thicken, which reduces elasticity and gas diffusion, reducing oxygen transfer to the blood as well as the removal of carbon dioxide.  This can result in shortness of breath, a common symptom exhibited by individuals with asbestosis.  Those with asbestosis may be more vulnerable to tumor growth (mesothelioma), because asbestos decreases the cytotoxicity of natural killer cells and impairs the functioning of T helper cells, which detect abnormal cell growth.|2023-09-09-08-14-16
Asbestosis|Diagnosis|" According to the American Thoracic Society (ATS), the general diagnostic criteria for asbestosis are: The abnormal chest x-ray and its interpretation remain the most important factors in establishing the presence of pulmonary fibrosis.  The findings usually appear as small, irregular parenchymal opacities, primarily in the lung bases. Using the ILO Classification system, ""s"", ""t"", and/or ""u"" opacities predominate. CT or high-resolution CT (HRCT) are more sensitive than plain radiography at detecting pulmonary fibrosis (as well as any underlying pleural changes). More than 50% of people affected with asbestosis develop plaques in the parietal pleura, the space between the chest wall and lungs. Once apparent, the radiographic findings in asbestosis may slowly progress or remain static, even in the absence of further asbestos exposure.  Rapid progression suggests an alternative diagnosis. Asbestosis resembles many other diffuse interstitial lung diseases, including other pneumoconiosis. The differential diagnosis includes idiopathic pulmonary fibrosis (IPF), hypersensitivity pneumonitis, sarcoidosis, and others. The presence of pleural plaques may provide supportive evidence of causation by asbestos. Although lung biopsy is usually not necessary, the presence of asbestos bodies in association with pulmonary fibrosis establishes the diagnosis.  Conversely, interstitial pulmonary fibrosis in the absence of asbestos bodies is most likely not asbestosis.  Asbestos bodies in the absence of fibrosis indicate exposure, not disease. Figure A shows the location of the lungs, airways, pleura, and diaphragm in the body. Figure B shows lungs with asbestos-related diseases, including pleural plaque, lung cancer, asbestosis, plaque on the diaphragm, and mesothelioma. Extensive fibrosis of pleura and lung parenchyma. The arrow points to an uncoated segment of asbestos fiber in this ferruginous body. Severe pleural fibrosis with focal calcification. The black arrows point to ferrugionous bodies that are located at the periphery of a focus of non-small cell lung carcinoma, NOS. 61 yr old working industrially with asbestos for decades."|2023-09-09-08-14-16
Asbestosis|Treatment| There is no cure available for asbestosis.  Oxygen therapy at home is often necessary to relieve the shortness of breath and correct underlying low blood oxygen levels. Supportive treatment of symptoms includes respiratory physiotherapy to remove secretions from the lungs by postural drainage, chest percussion, and vibration. Nebulized medications may be prescribed in order to loosen secretions or treat underlying chronic obstructive pulmonary disease. Immunization against pneumococcal pneumonia and annual influenza vaccination is administered due to increased sensitivity to the diseases. Those with asbestosis are at increased risk for certain cancers. If the person smokes, quitting the habit reduces further damage. Periodic pulmonary function tests, chest x-rays, and clinical evaluations, including cancer screening/evaluations, are given to detect additional hazards.|2023-09-09-08-14-16
Asbestosis|Legal issues| On 21 December 1906 H. Montague Murray, M.D., F.R.C.P.,  testified before a British committee concerning a patient who died in April 1900. Murray indicated that fibrosis of the lungs caused by asbestos dust was a plausible cause of the patient's death. The death of English textile worker Nellie Kershaw in 1924 from pulmonary asbestosis was the first case to be described in medical literature, and the first published account of disease definitely attributed to occupational asbestos exposure. However, her former employers (Turner Brothers Asbestos) denied that asbestosis even existed because the medical condition was not officially recognised at the time. As a result, they accepted no liability for her injuries and paid no compensation, either to Kershaw during her final illness or to her family after her death. Even so, the findings of the inquest into her death were highly influential insofar as they led to a parliamentary enquiry by the British Parliament. The enquiry formally acknowledged the existence of asbestosis, recognised that it was hazardous to health and concluded that it was irrefutably linked to the prolonged inhalation of asbestos dust. Having established the existence of asbestosis on a medical and judicial basis, the report resulted in the first Asbestos Industry Regulations being published in 1931, which came into effect on 1 March 1932. The first lawsuits against asbestos manufacturers occurred in 1929.  Since then, many lawsuits have been filed against asbestos manufacturers and employers, for neglecting to implement safety measures after the link between asbestos, asbestosis and mesothelioma became known (some reports seem to place this as early as 1898 in modern times).  The liability resulting from the sheer number of lawsuits and people affected has reached billions of U.S. dollars.  The amounts and method of allocating compensation have been the source of many court cases, and government attempts at resolution of existing and future cases. To date, about 100 companies have declared bankruptcy at least partially due to asbestos-related liability. In accordance with Chapter 11 and § 524(g) of the U.S. federal bankruptcy code, a company may transfer its liabilities and certain assets to an asbestos personal injury trust, which is then responsible for compensating present and future claimants. Since 1988, 60 trusts have been established to pay claims with about $37 billion in total assets. From 1988 through 2010, analysis from the United States Government Accountability Office indicates that trusts have paid about 3.3 million claims valued at about $17.5 billion.|2023-09-09-08-14-16
Asbestosis|Notable people| This is a partial list of notable people who have died from lung fibrosis associated with asbestos:|2023-09-09-08-14-16
Atmospheric dispersion modeling|General| Lists Categories Atmospheric dispersion modeling is the mathematical simulation of how air pollutants disperse in the ambient atmosphere. It is performed with computer programs that include algorithms to solve the mathematical equations that govern the pollutant dispersion. The dispersion models are used to estimate the downwind ambient concentration of air pollutants or toxins emitted from sources such as industrial plants, vehicular traffic or accidental chemical releases. They can also be used to predict future concentrations under specific scenarios (i.e. changes in emission sources). Therefore, they are the dominant type of model used in air quality policy making. They are most useful for pollutants that are dispersed over large distances and that may react in the atmosphere. For pollutants that have a very high spatio-temporal variability (i.e. have very steep distance to source decay such as black carbon) and for epidemiological studies statistical land-use regression models are also used. Dispersion models are important to governmental agencies tasked with protecting and managing the ambient air quality. The models are typically employed to determine whether existing or proposed new industrial facilities are or will be in compliance with the National Ambient Air Quality Standards (NAAQS) in the United States and other nations. The models also serve to assist in the design of effective control strategies to reduce emissions of harmful air pollutants. During the late 1960s, the Air Pollution Control Office of the U.S. EPA initiated research projects that would lead to the development of models for the use by urban and transportation planners.  A major and significant application of a roadway dispersion model that resulted from such research was applied to the Spadina Expressway of Canada in 1971. Air dispersion models are also used by public safety responders and emergency management personnel for emergency planning of accidental chemical releases. Models are used to determine the consequences of accidental releases of hazardous or toxic materials, Accidental releases may result in fires, spills or explosions that involve hazardous materials, such as chemicals or radionuclides. The results of dispersion modeling, using worst case accidental release source terms and meteorological conditions, can provide an estimate of location impacted areas, ambient concentrations, and be used to determine protective actions appropriate in the event a release occurs. Appropriate protective actions may include evacuation or shelter in place for persons in the downwind direction. At industrial facilities, this type of consequence assessment or emergency planning is required under the U.S. Clean Air Act (CAA) codified in Part 68 of Title 40 of the Code of Federal Regulations. The dispersion models vary depending on the mathematics used to develop the model, but all require the input of data that may include: Many of the modern, advanced dispersion modeling programs include a pre-processor module for the input of meteorological and other data, and many also include a post-processor module for graphing the output data and/or plotting the area impacted by the air pollutants on maps. The plots of areas impacted may also include isopleths showing areas of minimal to high concentrations that define areas of the highest health risk. The isopleths plots are useful in determining protective actions for the public and responders. The atmospheric dispersion models are also known as atmospheric diffusion models, air dispersion models, air quality models, and air pollution dispersion models.|2023-01-13-20-57-33
Atmospheric dispersion modeling|Atmospheric layers| Discussion of the layers in the Earth's atmosphere is needed to understand where airborne pollutants disperse in the atmosphere. The layer closest to the Earth's surface is known as the troposphere. It extends from sea-level to a height of about 18 km (11 mi) and contains about 80 percent of the mass of the overall atmosphere. The stratosphere is the next layer and extends from 18 km (11 mi) to about 50 km (31 mi). The third layer is the mesosphere which extends from 50 km (31 mi) to about 80 km (50 mi). There are other layers above 80 km, but they are insignificant with respect to atmospheric dispersion modeling. The lowest part of the troposphere is called the atmospheric boundary layer (ABL) or the planetary boundary layer (PBL) . The air temperature of the atmosphere decreases with increasing altitude until it reaches what is called an inversion layer (where the temperature increases with increasing altitude) that caps the Convective Boundary Layer, typically to about 1.5 to 2 km (0.93 to 1.24 mi) in height. The upper part of the troposphere (i.e., above the inversion layer) is called the free troposphere and it extends up to the tropopause (the boundary in the Earth's atmosphere between the troposphere and the stratosphere). In tropical and mid-latitudes during daytime, the Free convective layer can comprise the entire troposphere, which is up to 10 to 18 km (6.2 to 11.2 mi) in the Intertropical convergence zone. The ABL is of the most important with respect to the emission, transport and dispersion of airborne pollutants. The part of the ABL between the Earth's surface and the bottom of the inversion layer is known as the mixing layer. Almost all of the airborne pollutants emitted into the ambient atmosphere are transported and dispersed within the mixing layer. Some of the emissions penetrate the inversion layer and enter the free troposphere above the ABL. In summary, the layers of the Earth's atmosphere from the surface of the ground upwards are: the ABL made up of the mixing layer capped by the inversion layer; the free troposphere; the stratosphere; the mesosphere and others. Many atmospheric dispersion models are referred to as boundary layer models because they mainly model air pollutant dispersion within the ABL. To avoid confusion, models referred to as mesoscale models have dispersion modeling capabilities that extend horizontally up to a few hundred kilometres. It does not mean that they model dispersion in the mesosphere.|2023-01-13-20-57-33
Atmospheric dispersion modeling|Gaussian air pollutant dispersion equation|" The technical literature on air pollution dispersion is quite extensive and dates back to the 1930s and earlier. One of the early air pollutant plume dispersion equations was derived by Bosanquet and Pearson.  Their equation did not assume Gaussian distribution nor did it include the effect of ground reflection of the pollutant plume. Sir Graham Sutton derived an air pollutant plume dispersion equation in 1947  which did include the assumption of Gaussian distribution for the vertical and crosswind dispersion of the plume and also included the effect of ground reflection of the plume. Under the stimulus provided by the advent of stringent environmental control regulations, there was an immense growth in the use of air pollutant plume dispersion calculations between the late 1960s and today. A great many computer programs for calculating the dispersion of air pollutant emissions were developed during that period of time and they were called ""air dispersion models"". The basis for most of those models was the Complete Equation For Gaussian Dispersion Modeling Of Continuous, Buoyant Air Pollution Plumes shown below: The above equation not only includes upward reflection from the ground, it also includes downward reflection from the bottom of any inversion lid present in the atmosphere. The sum of the four exponential terms in  converges to a final value quite rapidly. For most cases, the summation of the series with m = 1, m = 2 and m = 3 will provide an adequate solution. and  are functions of the atmospheric stability class (i.e., a measure of the turbulence in the ambient atmosphere) and of the downwind distance to the receptor. The two most important variables affecting the degree of pollutant emission dispersion obtained are the height of the emission source point and the degree of atmospheric turbulence. The more turbulence, the better the degree of dispersion. Equations   for  and  are: (x) = exp(Iy + Jyln(x) + Ky[ln(x)]2) (x) = exp(Iz + Jzln(x) + Kz[ln(x)]2) (units of , and , and x are in meters) The classification of stability class is proposed by F. Pasquill.  The six stability classes are referred to:
A-extremely unstable
B-moderately unstable
C-slightly unstable
D-neutral
E-slightly stable
F-moderately stable The resulting calculations for air pollutant concentrations are often expressed as an air pollutant concentration contour map in order to show the spatial variation in contaminant levels over a wide area under study.  In this way the contour lines can overlay sensitive receptor locations and reveal the spatial relationship of air pollutants to areas of interest. Whereas older models rely on stability classes (see air pollution dispersion terminology) for the determination of  and , more recent models increasingly rely on the Monin-Obukhov similarity theory to derive these parameters."|2023-01-13-20-57-33
Atmospheric dispersion modeling|Briggs plume rise equations|" The Gaussian air pollutant dispersion equation (discussed above) requires the input of H which is the pollutant plume's centerline height above ground level—and H is the sum of Hs (the actual physical height of the pollutant plume's emission source point) plus ΔH (the plume rise due to the plume's buoyancy). To determine ΔH, many if not most of the air dispersion models developed between the late 1960s and the early 2000s used what are known as the Briggs equations. G.A. Briggs first published his plume rise observations and comparisons in 1965.  In 1968, at a symposium sponsored by CONCAWE (a Dutch organization), he compared many of the plume rise models then available in the literature.  In that same year, Briggs also wrote the section of the publication edited by Slade  dealing with the comparative analyses of plume rise models.  That was followed in 1969 by his classical critical review of the entire plume rise literature,  in which he proposed a set of plume rise equations which have become widely known as ""the Briggs equations"".  Subsequently, Briggs modified his 1969 plume rise equations in 1971 and in 1972. Briggs divided air pollution plumes into these four general categories: Briggs considered the trajectory of cold jet plumes to be dominated by their initial velocity momentum, and the trajectory of hot, buoyant plumes to be dominated by their buoyant momentum to the extent that their initial velocity momentum was relatively unimportant.  Although Briggs proposed plume rise equations for each of the above plume categories, it is important to emphasize that ""the Briggs equations"" which become widely used are those that he proposed for bent-over, hot buoyant plumes. In general, Briggs's equations for bent-over, hot buoyant plumes are based on observations and data involving plumes from typical combustion sources such as the flue gas stacks from steam-generating boilers burning fossil fuels in large power plants.  Therefore, the stack exit velocities were probably in the range of 20 to 100 ft/s (6 to 30 m/s) with exit temperatures ranging from 250 to 500 °F (120 to 260 °C). A logic diagram for using the Briggs equations  to obtain the plume rise trajectory of bent-over buoyant plumes is presented below: The above parameters used in the Briggs' equations are discussed in Beychok's book."|2023-01-13-20-57-33
Atmospheric dispersion modeling|Atmospheric dispersion models| List of atmospheric dispersion models provides a more comprehensive list of models than listed below. It includes a very brief description of each model.|2023-01-13-20-57-33
Basel Convention|General|" Lists Categories The Basel Convention on the Control of Transboundary Movements of Hazardous Wastes and Their Disposal, usually known as the Basel Convention, is an international treaty that was designed to reduce the movements of hazardous waste between nations, and specifically to prevent transfer of hazardous waste from developed to less developed countries. It does not, however, address the movement of radioactive waste. The convention is also intended to minimize the rate and toxicity of wastes generated, to ensure their environmentally sound management as closely as possible to the source of generation, and to assist developing countries in environmentally sound management of the hazardous and other wastes they generate. The convention was opened for signature on 21 March 1989, and entered into force on 5 May 1992. As of June 2023, there are 191 parties to the convention. In addition, Haiti and the United States have signed the convention but not ratified it. Following a petition urging action on the issue signed by more than a million people around the world, most of the world's countries, but not the United States, agreed in May 2019 to an amendment of the Basel Convention to include plastic waste as regulated material.   Although the United States is not a party to the treaty, export shipments of plastic waste from the United States are now ""criminal traffic as soon as the ships get on the high seas,"" according to the Basel Action Network (BAN), and carriers of such shipments may face liability, because the transportation of plastic waste is prohibited in just about every other country."|2023-06-03-13-36-07
Basel Convention|History|" With the tightening of environmental laws (for example, RCRA) in developed nations in the 1970s, disposal costs for hazardous waste rose dramatically. At the same time, the globalization of shipping made cross-border movement of waste easier, and many less developed countries were desperate for foreign currency. Consequently, the trade in hazardous waste, particularly to poorer countries, grew rapidly. In 1990, OECD countries exported around 1.8 million tons of hazardous waste. Although most of this waste was shipped other developed countries, a number of high-profile incidents of hazardous waste-dumping led to calls for regulation. One of the incidents which led to the creation of the Basel Convention was the Khian Sea waste disposal incident, in which a ship carrying incinerator ash from the city of Philadelphia in the United States dumped half of its load on a beach in Haiti before being forced away. It sailed for many months, changing its name several times. Unable to unload the cargo in any port, the crew was believed to have dumped much of it at sea. Another incident was a 1988 case in which five ships transported 8,000 barrels of hazardous waste from Italy to the small Nigerian town of Koko in exchange for $100 monthly rent which was paid to a Nigerian for the use of his farmland. At its meeting that took place from 27 November to 1 December 2006, the parties of the Basel Agreement focused on issues of electronic waste and the dismantling of ships. Increased trade in recyclable materials has led to an increase in a market for used products such as computers. This market is valued in billions of dollars. At issue is the distinction when used computers stop being a ""commodity"" and become a ""waste"". As of June 2023, there are 191 parties to the treaty, which includes 188 UN member states, the Cook Islands, the European Union, and the State of Palestine. The five UN member states that are not party to the treaty are East Timor, Fiji, Haiti, South Sudan, and United States."|2023-06-03-13-36-07
Basel Convention|Definition of hazardous waste|" Waste falls under the scope of the convention if it is within the category of wastes listed in Annex I of the convention and it exhibits one of the hazardous characteristics contained in Annex III. 
In other words, it must both be listed and possess a characteristic such as being explosive, flammable, toxic, or corrosive. The other way that a waste may fall under the scope of the convention is if it is defined as or considered to be a hazardous waste under the laws of either the exporting country, the importing country, or any of the countries of transit. The definition of the term disposal is made in Article 2 al 4 and just refers to annex IV, which gives a list of operations which are understood as disposal or recovery. Examples of disposal are broad, including recovery and recycling. Alternatively, to fall under the scope of the convention, it is sufficient for waste to be included in Annex II, which lists other wastes, such as household wastes and residue that comes from incinerating household waste. Radioactive waste that is covered under other international control systems and wastes from the normal operation of ships are not covered. Annex IX attempts to define wastes which are not considered hazardous wastes and which would be excluded from the scope of the Basel Convention. If these wastes however are contaminated with hazardous materials to an extent causing them to exhibit an Annex III characteristic, they are not excluded."|2023-06-03-13-36-07
Basel Convention|Obligations|" In addition to conditions on the import and export of the above wastes, there are stringent requirements for notice, consent and tracking for movement of wastes across national boundaries. It is of note that the convention places a general prohibition on the exportation or importation of wastes between parties and non-parties. The exception to this rule is where the waste is subject to another treaty that does not take away from the Basel Convention. The United States is a notable non-party to the convention and has a number of such agreements for allowing the shipping of hazardous wastes to Basel Party countries. The OECD Council also has its own control system that governs the transboundary movement of hazardous materials between OECD member countries. This allows, among other things, the OECD countries to continue trading in wastes with countries like the United States that have not ratified the Basel Convention. Parties to the convention must honor import bans of other parties. Article 4 of the Basel Convention calls for an overall reduction of waste generation. By encouraging countries to keep wastes within their boundaries and as close as possible to its source of generation, the internal pressures should provide incentives for waste reduction and pollution prevention. Parties are generally prohibited from exporting covered wastes to, or importing covered waste from, non-parties to the convention. The convention states that illegal hazardous waste traffic is criminal but contains no enforcement provisions. According to Article 12, parties are directed to adopt a protocol that establishes liability rules and procedures that are appropriate for damage that comes from the movement of hazardous waste across borders. The current consensus is that as space is not classed as a ""country"" under the specific definition, export of e-waste to non-terrestrial locations would not be covered."|2023-06-03-13-36-07
Basel Convention|Basel Ban Amendment|" After the initial adoption of the convention, some least developed countries and environmental organizations argued that it did not go far enough. Many nations and NGOs argued for a total ban on shipment of all hazardous waste to developing countries. In particular, the original convention did not prohibit waste exports to any location except Antarctica but merely required a notification and consent system known as ""prior informed consent"" or PIC. Further, many waste traders sought to exploit the good name of recycling and begin to justify all exports as moving to recycling destinations. Many believed a full ban was needed including exports for recycling. These concerns led to several regional waste trade bans, including the Bamako Convention. Lobbying at 1995 Basel conference by developing countries, Greenpeace and several European countries such as Denmark, led to the adoption of an amendment to the convention in 1995 termed the Basel Ban Amendment to the Basel Convention. The amendment has been accepted by 86 countries  and the European Union, but has not entered into force (as that requires ratification by three-fourths of the member states to the convention). On 6 September 2019, Croatia became the 97th country to ratify the amendment which will enter into force after 90 days on 5 December 2019. The amendment prohibits the export of hazardous waste from a list of developed (mostly OECD) countries to developing countries. The Basel Ban applies to export for any reason, including recycling. An area of special concern for advocates of the amendment was the sale of ships for salvage, shipbreaking. The Ban Amendment was strenuously opposed by a number of industry groups as well as nations including Australia and Canada. The number of ratification for the entry-into force of the Ban Amendment is under debate: Amendments to the convention enter into force after ratification of ""three-fourths of the Parties who accepted them"" [Art. 17.5]; so far, the parties of the Basel Convention could not yet agree whether this would be three-fourths of the parties that were party to the Basel Convention when the ban was adopted, or three-fourths of the current parties of the convention [see Report of COP 9 of the Basel Convention]. The status of the amendment ratifications can be found on the Basel Secretariat's web page.  The European Union fully implemented the Basel Ban in its Waste Shipment Regulation (EWSR), making it legally binding in all EU member states. Norway and Switzerland have similarly fully implemented the Basel Ban in their legislation. In the light of the blockage concerning the entry into force of the Ban Amendment, Switzerland and Indonesia have launched a ""Country-led Initiative"" (CLI) to discuss in an informal manner a way forward to ensure that the trans boundary movements of hazardous wastes, especially to developing countries and countries with economies in the transition, do not lead to an unsound management of hazardous wastes. This discussion aims at identifying and finding solutions to the reasons why hazardous wastes are still brought to countries that are not able to treat them in a safe manner. It is hoped that the CLI will contribute to the realization of the objectives of the Ban Amendment. The Basel Convention's website informs about the progress of this initiative."|2023-06-03-13-36-07
Basel Convention|Regulation of plastic waste|" In the wake of popular outcry, in May 2019 most of the world's countries, but not the United States, agreed to amend the Basel Convention to include plastic waste as a regulated material.  The world's oceans are estimated to contain 100 million metric tons of plastic, with up to 90% of this quantity originating in land-based sources. The United States, which produces an annual 42 million metric tons of plastic waste, more than any other country in the world,  opposed the amendment, but since it is not a party to the treaty it did not have an opportunity to vote on it to try to block it. Information about, and visual images of, wildlife, such as seabirds, ingesting plastic, and scientific findings that nanoparticles do penetrate through the blood–brain barrier were reported to have fueled public sentiment for coordinated international legally binding action. Over a million people worldwide signed a petition demanding official action.    Although the United States is not a party to the treaty, export shipments of plastic waste from the United States are now ""criminal traffic as soon as the ships get on the high seas,"" according to the Basel Action Network (BAN), and carriers of such shipments may face liability, because the Basel Convention as amended in May 2019 prohibits the transportation of plastic waste to just about every other country. The Basel Convention contains three main entries on plastic wastes in Annex II, VIII and IX of the Convention. The Plastic Waste Amendments of the convention are now binding on 186 States. In addition to ensuring the trade in plastic waste is more transparent and better regulated, under the Basel Convention governments must take steps not only to ensure the environmentally sound management of plastic waste, but also to tackle plastic waste at its source."|2023-06-03-13-36-07
Basel Convention|Basel watchdog| The Basel Action Network (BAN) is a charitable civil society non-governmental organization that works as a consumer watchdog for implementation of the Basel Convention. BAN's principal aims is fighting exportation of toxic waste, including plastic waste, from industrialized societies to developing countries. BAN is based in Seattle, Washington, United States, with a partner office in the Philippines. BAN works to curb trans-border trade in hazardous electronic waste, land dumping, incineration, and the use of prison labor.|2023-06-03-13-36-07
Basel Convention|References| This article incorporates text from a free content work.  Licensed under Cc BY-SA 3.0 IGO (license statement/permission). Text taken from Drowning in Plastics – Marine Litter and Plastic Waste Vital Graphics​,   United Nations Environment Programme.|2023-06-03-13-36-07
Bioaccumulation|General| Bioaccumulation is the gradual accumulation of substances, such as pesticides or other chemicals, in an organism.   Bioaccumulation occurs when an organism absorbs a substance faster than it can be lost or eliminated by catabolism and excretion. Thus, the longer the biological half-life of a toxic substance, the greater the risk of chronic poisoning, even if environmental levels of the toxin are not very high.  Bioaccumulation, for example in fish, can be predicted by models.   Hypothesis for molecular size cutoff criteria for use as bioaccumulation potential indicators are not supported by data.  Biotransformation can strongly modify bioaccumulation of chemicals in an organism. Toxicity induced by metals is associated with bioaccumulation and biomagnification.  Storage or uptake of a metal faster than it is metabolized and excreted leads to the accumulation of that metal.  The presence of various chemicals and harmful substances in the environment can be analyzed and assessed with a proper knowledge on bioaccumulation helping with chemical control and usage. An organism can take up chemicals by breathing, absorbing through skin or swallowing.  When the concentration of a chemical is higher within the organism compared to its surroundings (air or water), it is referred to as bioconcentration.  Biomagnification is another process related to bioaccumulation as the concentration of the chemical or metal increases as it moves up from one trophic level to another.  Naturally, the process of bioaccumulation is necessary for an organism to grow and develop; however, accumulation of harmful substances can also occur.|2023-08-24-06-28-13
Bioaccumulation|Terrestrial examples|" An example of poisoning in the workplace can be seen from the phrase ""mad as a hatter"" (18th and 19th century England). Mercury was used in stiffening the felt used to make hats involved.  This forms organic species such as methylmercury, which is lipid-soluble (fat-soluble), and tends to accumulate in the brain, resulting in mercury poisoning. Other lipid-soluble poisons include tetraethyllead compounds (the lead in leaded petrol), and DDT. These compounds are stored in the body fat, and when the fatty tissues are used for energy, the compounds are released and cause acute poisoning. Strontium-90, part of the fallout from atomic bombs, is chemically similar enough to calcium that it is taken up in forming bones, where its radiation can cause damage for a long time. Some animal species use bioaccumulation as a mode of defense: by consuming toxic plants or animal prey, an animal may accumulate the toxin, which then presents a deterrent to a potential predator. One example is the tobacco hornworm, which concentrates nicotine to a toxic level in its body as it consumes tobacco plants. Poisoning of small consumers can be passed along the food chain to affect the consumers later in the chain. Other compounds that are not normally considered toxic can be accumulated to toxic levels in organisms. The classic example is vitamin A, which becomes concentrated in livers of carnivores, e.g. polar bears: as a pure carnivore that feeds on other carnivores (seals), they accumulate extremely large amounts of vitamin A in their livers. It was known by the native peoples of the Arctic that the livers of carnivores should not be eaten, but Arctic explorers have suffered hypervitaminosis A from eating the livers of bears; and there has been at least one example of similar poisoning of Antarctic explorers eating husky dog livers.  One notable example of this is the expedition of Sir Douglas Mawson, whose exploration companion died from eating the liver of one of their dogs."|2023-08-24-06-28-13
Bioaccumulation|Aquatic examples|" Coastal fish (such as the smooth toadfish) and seabirds (such as the Atlantic puffin) are often monitored for heavy metal bioaccumulation. Methylmercury gets into freshwater systems through industrial emissions and rain. As its concentration increases up the food web, it can reach dangerous levels for both fish and the humans who rely on fish as a food source. Fish are typically assessed for bioaccumulation when they have been exposed to chemicals that are in their aqueous phases.  Commonly tested fish species include the common carp, rainbow trout, and bluegill sunfish.  Generally, fish are exposed to bioconcentration and bioaccumulation of organic chemicals in the environment through lipid layer uptake of water-borne chemicals.  In other cases, the fish are exposed through ingestion/digestion of substances or organisms in the aquatic environment which contain the harmful chemicals. Naturally produced toxins can also bioaccumulate. The marine algal blooms known as ""red tides"" can result in local filter-feeding organisms such as mussels and oysters becoming toxic; coral reef fish can be responsible for the poisoning known as ciguatera when they accumulate a toxin called ciguatoxin from reef algae. In some eutrophic aquatic systems, biodilution can occur. This is a decrease in a contaminant with an increase in trophic level, due to higher concentrations of algae and bacteria to dilute the concentration of the pollutant. Wetland acidification can raise the chemical or metal concentrations, which leads to an increased bioavailability in marine plants and freshwater biota.  Plants situated there which includes both rooted and submerged plants can be influenced by the bioavailability of metals."|2023-08-24-06-28-13
Bioaccumulation|Studies of turtles as model species| Bioaccumulation in turtles occurs when synthetic organic contaminants (i.e., PFAS), heavy metals, or high levels of trace elements enter a singular organism, potentially affecting their health. Although there are ongoing studies of bioaccumulation in turtles, factors like pollution, climate change, and shifting landscape can affect the amounts of these toxins in the ecosystem. The most common elements studied in turtles are mercury, cadmium, argon[dubious  – discuss], and selenium. Heavy metals are released into rivers, streams, lakes, oceans, and other aquatic environments, and the plants that live in these environments will absorb the metals. Since the levels of trace elements are high in aquatic ecosystems, turtles will naturally consume various trace elements throughout various aquatic environments by eating plants and sediments.  Once these substances enter the bloodstream and muscle tissue, they will increase in concentration and will become toxic to the turtles, perhaps causing metabolic, endocrine system, and reproductive failure. Some marine turtles are used as experimental subjects to analyze bioaccumulation because of their shoreline habitats, which facilitate the collection of blood samples and other data.  The turtle species are very diverse and contribute greatly to biodiversity, so many researchers find it valuable to collect data from various species. Freshwater turtles are another model species for investigating bioaccumulation.  Due to their relatively limited home-range freshwater turtles can be associated with a particular catchment and its chemical contaminant profile.|2023-08-24-06-28-13
Bioaccumulation|Developmental effects of turtles| Toxic concentrations in turtle eggs may damage the developmental process of the turtle. For example, in the Australian freshwater short-neck turtle (Emydura macquarii macquarii), environmental PFAS concentrations were bioaccumulated by the mother and then offloaded into their eggs that impacted developmental metabolic processes and fat stores.  Furthermore, there is evidence PFAS impacted the gut microbiome in exposed turtles. In terms of toxic levels of heavy metals, it was observed to decrease egg-hatching rates in the Amazon River turtle, Podocnemis expansa.   In this particular turtle egg, the heavy metals reduce the fat in the eggs and change how water is filtered throughout the embryo; this can affect the survival rate of the turtle egg.|2023-08-24-06-28-13
Biofouling|General| Biofouling or biological fouling is the accumulation of microorganisms, plants, algae, or small animals where it is not wanted on surfaces such as ship and submarine hulls, devices such as water inlets, pipework, grates, ponds, and rivers that cause degradation to the primary purpose of that item. Such accumulation is referred to as epibiosis when the host surface is another organism and the relationship is not parasitic. Since biofouling can occur almost anywhere water is present, biofouling poses risks to a wide variety of objects such as boat hulls and equipment, medical devices and membranes, as well as to entire industries, such as paper manufacturing, food processing, underwater construction, and desalination plants. Anti-fouling is the ability of specifically designed materials (such as toxic biocide paints, or non-toxic paints)  to remove or prevent biofouling. The buildup of biofouling on marine vessels poses a significant problem. In some instances, the hull structure and propulsion systems can be damaged.  The accumulation of biofoulers on hulls can increase both the hydrodynamic volume of a vessel and the hydrodynamic friction, leading to increased drag of up to 60%.  The drag increase has been seen to decrease speeds by up to 10%, which can require up to a 40% increase in fuel to compensate.  With fuel typically comprising up to half of marine transport costs, antifouling methods save the shipping industry a considerable amount of money. Further, increased fuel use due to biofouling contributes to adverse environmental effects and is predicted to increase emissions of carbon dioxide and sulfur dioxide between 38% and 72% by 2020, respectively.|2023-06-21-10-00-34
Biofouling|Biology|" Biofouling organisms are highly diverse, and extend far beyond the attachment of barnacles and seaweeds. According to some estimates, over 1,700 species comprising over 4,000 organisms are responsible for biofouling.  Biofouling is divided into microfouling—biofilm formation and bacterial adhesion—and macrofouling—attachment of larger organisms. Due to the distinct chemistry and biology that determine what prevents them from settling, organisms are also classified as hard- or soft-fouling types. Calcareous (hard) fouling organisms include barnacles, encrusting bryozoans, mollusks, polychaete and other tube worms, and zebra mussels. Examples of non-calcareous (soft) fouling organisms are seaweed, hydroids, algae and biofilm ""slime"".  Together, these organisms form a fouling community."|2023-06-21-10-00-34
Biofouling|Ecosystem formation| Marine fouling is typically described as following four stages of ecosystem development. Within the first minute the van der Waals interaction causes the submerged surface to be covered with a conditioning film of organic polymers. In the next 24 hours, this layer allows the process of bacterial adhesion to occur, with both diatoms and bacteria (e.g. Vibrio alginolyticus, Pseudomonas putrefaciens) attaching, initiating the formation of a biofilm. By the end of the first week, the rich nutrients and ease of attachment into the biofilm allow secondary colonizers of spores of macroalgae (e.g. Enteromorpha intestinalis, Ulothrix) and protozoans (e.g. Vorticella, Zoothamnium sp.) to attach themselves. Within two to three weeks, the tertiary colonizers—the macrofoulers—have attached. These include tunicates, mollusks and sessile cnidarians.|2023-06-21-10-00-34
Biofouling|Impact|" Governments and industry spend more than US$5.7 billion annually to prevent and control marine biofouling. 
Biofouling occurs everywhere but is most significant economically to the shipping industries, since fouling on a ship's hull significantly increases drag, reducing the overall hydrodynamic performance of the vessel, and increases the fuel consumption. Biofouling is also found in almost all circumstances where water-based liquids are in contact with other materials. Industrially important impacts are on the maintenance of mariculture, membrane systems (e.g., membrane bioreactors and reverse osmosis spiral wound membranes) and cooling water cycles of large industrial equipment and power stations. Biofouling can occur in oil pipelines carrying oils with entrained water, especially those carrying used oils, cutting oils, oils rendered water-soluble through emulsification, and hydraulic oils.[citation needed] Other mechanisms impacted by biofouling include microelectrochemical drug delivery devices, papermaking and pulp industry machines, underwater instruments, fire protection system piping, and sprinkler system nozzles.   In groundwater wells, biofouling buildup can limit recovery flow rates, as is the case in the exterior and interior of ocean-laying pipes where fouling is often removed with a tube cleaning process. Besides interfering with mechanisms, biofouling also occurs on the surfaces of living marine organisms, when it is known as epibiosis. [citation needed] Medical devices often include fan-cooled heat sinks, to cool their electronic components. While these systems sometimes include HEPA filters to collect microbes, some pathogens do pass through these filters, collect inside the device and are eventually blown out and infect other patients. Devices used in operating rooms rarely include fans, so as to minimize the chance of transmission. Also, medical equipment, HVAC units, high-end computers, swimming pools, drinking-water systems and other products that utilize liquid lines run the risk of biofouling as biological growth occurs inside them. Historically, the focus of attention has been the severe impact due to biofouling on the speed of marine vessels. In some instances the hull structure and propulsion systems can become damaged.  Over time, the accumulation of biofoulers on hulls increases both the hydrodynamic volume of a vessel and the frictional effects leading to increased drag of up to 60%  The additional drag can decrease speeds up to 10%, which can require up to a 40% increase in fuel to compensate.  With fuel typically comprising up to half of marine transport costs, biofouling is estimated to cost the US Navy alone around $1 billion per year in increased fuel usage, maintenance and biofouling control measures.  Increased fuel use due to biofouling contributes to adverse environmental effects and is predicted to increase emissions of carbon dioxide and sulfur dioxide between 38 and 72 percent by 2020. Biofouling also impacts aquaculture, increasing production and management costs, while decreasing product value.  Fouling communities may compete with shellfish directly for food resources,  impede the procurement of food and oxygen by reducing water flow around shellfish, or interfere with the operational opening of their valves.  Consequently, stock affected by biofouling can experience reduced growth, condition and survival, with subsequent negative impacts on farm productivity.  Although many methods of removal exist, they often impact the cultured species, sometimes more so than the fouling organisms themselves."|2023-06-21-10-00-34
Biofouling|Detection| Shipping companies have historically relied on scheduled biofouler removal to keep such accretions to a manageable level. However, the rate of accretion can vary widely between vessels and operating conditions, so predicting acceptable intervals between cleanings is difficult. LED manufacturers have developed a range of UVC (250–280 nm) equipment that can detect biofouling buildup, and can even prevent it. Fouling detection relies on the biomass' property of fluorescence. All microorganisms contain natural intracellular fluorophores, which radiate in the UV range when excited. At UV-range wavelengths, such fluorescence arises from three aromatic amino acids—tyrosine, phenylalanine, and tryptophan. The easiest to detect is tryptophan, which radiates at 350 nm when irradiated at 280 nm.|2023-06-21-10-00-34
Biofouling|Antifouling| Antifouling is the process of preventing accumulations from forming. In industrial processes, biodispersants can be used to control biofouling. In less controlled environments, organisms are killed or repelled with coatings using biocides, thermal treatments, or pulses of energy. Nontoxic mechanical strategies that prevent organisms from attaching include choosing a material or coating with a slippery surface, creating an ultra-low fouling surface with the use of zwitterions, or creating nanoscale surface topologies similar to the skin of sharks and dolphins, which only offer poor anchor points. Non-toxic anti-sticking coatings prevent attachment of microorganisms thus negating the use of biocides. These coatings are usually based on organic polymers. There are two classes of non-toxic anti-fouling coatings. The most common class relies on low friction and low surface energies. Low surface energies result in hydrophobic surfaces. These coatings create a smooth surface, which can prevent attachment of larger microorganisms. For example, fluoropolymers and silicone coatings are commonly used.  These coatings are ecologically inert but have problems with mechanical strength and long-term stability. Specifically, after days biofilms (slime) can coat the surfaces, which buries the chemical activity and allows microorganisms to attach.  The current standard for these coatings is polydimethylsiloxane, or PDMS, which consists of a non-polar backbone made of repeating units of silicon and oxygen atoms.  The non-polarity of PDMS allows for biomolecules to readily adsorb to its surface in order to lower interfacial energy. However, PDMS also has a low modulus of elasticity that allows for the release of fouling organisms at speeds of greater than 20 knots. The dependence of effectiveness on vessel speed prevents use of PDMS on slow-moving ships or those that spend significant amounts of time in port. The second class of non-toxic antifouling coatings are hydrophilic coatings. They rely on high amounts of hydration in order to increase the energetic penalty of removing water for proteins and microorganisms to attach. The most common examples of these coatings are based on highly hydrated zwitterions, such as glycine betaine and sulfobetaine. These coatings are also low-friction, but are considered by some to be superior to hydrophobic surfaces because they prevent bacteria attachment, preventing biofilm formation.  These coatings are not yet commercially available and are being designed as part of a larger effort by the Office of Naval Research to develop environmentally safe biomimetic ship coatings. Biocides are chemical substances that kill or deter microorganisms responsible for biofouling. The biocide is typically applied as a paint, i.e. through physical adsorption. The biocides prevent the formation of biofilms.  Other biocides are toxic to larger organisms in biofouling, such as algae. Formerly, the so-called tributyltin (TBT) compounds were used as biocides (and thus anti-fouling agents).  TBTs are toxic to both microorganisms and larger aquatic organisms.  The international maritime community has phased out the use of organotin-based coatings.  Replacing organotin compounds is dichlorooctylisothiazolinone. This compound, however, also suffers from broad toxicity to marine organisms. Ultrasonic transducers may be mounted in or around the hull of small to medium-sized boats. Research has shown these systems can help reduce fouling, by initiating bursts of ultrasonic waves through the hull medium to the surrounding water, killing or denaturing the algae and other microorganisms that form the beginning of the fouling sequence. The systems cannot work on wooden-hulled boats, or boats with a soft-cored composite material, such as wood or foam. The systems have been loosely based on technology proven to control algae blooms. Pulsed laser irradiation is commonly used against diatoms. Plasma pulse technology is effective against zebra mussels and works by stunning or killing the organisms with microsecond-duration energizing of the water with high-voltage electricity. Similarly, another method shown to be effective against algae buildups bounces brief high-energy acoustic pulses down pipes. Regimens to periodically use heat to treat exchanger equipment and pipes have been successfully used to remove mussels from power plant cooling systems using water at 105 °F (40 °C) for 30 minutes. The medical industry utilizes a variety of energy methods to address bioburden issues associated with biofouling. Autoclaving typically involves heating a medical device to 121 °C (249 °F) for 15–20 minutes. Ultrasonic cleaning, UV light, and chemical wipe-down or immersion can also be used for different types of devices. Medical devices used in operating rooms, ICUs, isolation rooms, biological analysis labs, and other high-contamination-risk areas have negative pressure (constant exhaust) in the rooms, maintain strict cleaning protocols, require equipment with no fans, and often drape equipment in protective plastic. UVC irradiation is a noncontact, nonchemical solution that can be used across a range of instruments. Radiation in the UVC range prevents biofilm formation by deactivating the DNA in bacteria, viruses, and other microbes. Preventing biofilm formation prevents larger organisms from attaching themselves to the instrument and eventually rendering it inoperable.|2023-06-21-10-00-34
Biofouling|History|" Biofouling, especially of ships, has been a problem for as long as humans have been sailing the oceans. The earliest attestations of attempts to counter fouling, and thus also the earliest attestation of knowledge if it, is the use of pitch and copper plating as anti-fouling solutions that were attributed to ancient seafaring nations, such as the Phoenicians and Carthaginians (1500- 300BC). Wax, tar and asphaltum have been used since early times.  An Aramaic record dating from 412 B.C. tells of a ship's bottom being coated with a mixture of arsenic, oil and sulphur.  In Deipnosophistae, Athenaeus described the anti-fouling efforts taken in the construction of the great ship of Hieron of Syracuse (died 467 BC). A recorded explanation by Plutarch of the impact fouling had on ship speed goes as follows: ""when weeds, ooze, and filth stick upon its sides, the stroke of the ship is more obtuse and weak; and the water, coming upon this clammy matter, doth not so easily part from it; and this is the reason why they usually calk their ships."" Before the 18th century, various anti-fouling techniques were used, with three main substances employed: ""White stuff"", a mixture of train oil (Whale oil), rosin and sulfur; ""Black stuff"", a mixture of tar and pitch; and ""Brown stuff"", which was simply sulfur added to Black stuff.  In many of these cases, the purpose of these treatments is ambiguous. There is dispute whether many of these treatments were actual anti-fouling techniques, or whether, when they were used in conjunction with lead and wood sheathing, they were simply intended to combat wood-boring shipworms. In 1708, Charles Perry suggested copper sheathing explicitly as an anti-fouling device but the first experiments were not made until 1761 with the sheathing of HMS Alarm, after which the bottoms and sides of several ships' keels and false keels were sheathed with copper plates. The copper performed well in protecting the hull from invasion by worm, and in preventing the growth of weed, for when in contact with water, the copper produced a poisonous film, composed mainly of oxychloride, that deterred these marine creatures. Furthermore, as this film was slightly soluble, it gradually washed away, leaving no way for marine life to attach itself to the ship.[citation needed]
From about 1770, the Royal Navy set about coppering the bottoms of the entire fleet and continued to the end of the use of wooden ships. The process was so successful that the term copper-bottomed came to mean something that was highly dependable or risk free. With the rise of iron hulls in the 19th century, copper sheathing could no longer be used due to its galvanic corrosive interaction with iron. Anti-fouling paints were tried, and in 1860, the first practical paint to gain widespread use was introduced in Liverpool and was referred to as ""McIness"" hot plastic paint.  These treatments had a short service life, were expensive, and relatively ineffective by modern standards. By the mid-twentieth century, copper oxide-based paints could keep a ship out of drydock for as much as 18 months, or as little as 12 in tropical waters.  The shorter service life was due to rapid leaching of the toxicant, and chemical conversion into less toxic salts, which accumulated as a crust that would inhibit further leaching of active cuprous oxide from the layer under the crust. The 1960s brought a breakthrough, with self-polishing paints that slowly hydrolyze, slowly releasing toxins. These paints employed organotin chemistry (""tin-based"") biotoxins such as tributyltin oxide (TBT) and were effective for up to four years. These biotoxins were subsequently banned by the International Maritime Organization when they were found to be very toxic to diverse organisms.   TBT in particular has been described as the most toxic pollutant ever deliberately released in the ocean. As an alternative to organotin toxins, there has been renewed interest in copper as the active agent in ablative or self polishing paints, with reported service lives up to 5 years; yet also other methods that do not involve coatings. Modern adhesives permit application of copper alloys to steel hulls without creating galvanic corrosion. However, copper alone is not impervious to diatom and algae fouling. Some studies indicate that copper may also present an unacceptable environmental impact. Study of biofouling began in the early 19th century with Davy's experiments linking the effectiveness of copper to its solute rate.  In the 1930s microbiologist Claude ZoBell showed that the attachment of organisms is preceded by the adsorption of organic compounds now referred to as extracellular polymeric substances. One trend of research is the study of the relationship between wettability and anti-fouling effectiveness. Another trend is the study of living organisms as the inspiration for new functional materials. For example, the mechanisms used by marine animals to inhibit biofouling on their skin. Materials research into superior antifouling surfaces for fluidized bed reactors suggest that low wettability plastics such as polyvinyl chloride (PVC), high-density polyethylene and polymethylmethacrylate (""plexiglas"") demonstrate a high correlation between their resistance to bacterial adhesion and their hydrophobicity. A study of the biotoxins used by organisms has revealed several effective compounds, some of which are more powerful than synthetic compounds. Bufalin, a bufotoxin, was found to be over 100 times as potent as TBT, and over 6,000 times more effective in anti-settlement activity against barnacles. One approach to antifouling entails coating surfaces with polyethylene glycol (PEG).  Growing chains of PEG on surfaces is challenging. The resolution to this problem may come from understanding the mechanisms by which mussels adhere to solid surfaces in marine environments. Mussels utilize adhesive proteins, or MAPs.  The service life of PEG coatings is also doubtful."|2023-06-21-10-00-34
Biological oxidizer|General| A biological oxidizer is a device that uses micro-organisms to treat wastewater and the volatile organic compounds produced by commercial and industrial operations. Biological oxidation devices convert biodegradable organic compounds into carbon dioxide and water. This is a natural occurring process which differs from traditional chemical and thermal oxidizing agents and methods. Some of the more commonly used micro-organisms are heterotrophic bacteria, which play an important role in biological degradation processes. Generally, these micro-organisms are rod shaped and facultative. Biological oxidizers provide a stable environment which allows bacteria to naturally oxidize and stabilize a large number of organics in a more efficient manner. Some of the emissions that may be treated biologically include: The prompt removal of a wide range of wastes and pollutants from the environment is the foremost requisite leading to minimal negative environmental impact and sustainability. Microorganisms offer excellent anabolic and catabolic adaptability to degrade and produce stabilized organic matters from contaminants. Microbiology is providing significant views of regulatory metabolic pathways as well as effectiveness to adaption and biological degradation in our changing environment.|2023-03-05-04-16-08
Biological oxidizer|Biological destruction mechanism of hazardous air pollutants| Micro-organisms are utilized in biological remediation to control industrial and commercial vapor effluents. When utilizing biological oxidation systems for the remediation emissions, the off gases or vapors are passed through a packed bed having a thin biological film at the surface. The micro-organisms are immobilized into the thin biological film, as the vapor passes over the film they become attached and are oxidized or stabilized. The biological film accomplishes the degradation process, as the biological sump water is reprocessed over the biomedia it creates additional biological growth and as the film increases so does the biological oxidizers efficiency. Large surface area and footprint were once required to treat waste water vapor and industrial plant emissions, with the advent of advanced biological oxidation equipment a smaller footprint is required. The footprint will typically occupy the same space as conventional thermal oxidizers.|2023-03-05-04-16-08
Biological oxidizer|Biological controls| Excessive formation of the biological film may lead to certain problems such as sloughing, it is an important factor to maintain optimum biological film. Maintaining the biological film is accomplished by proper moisture content. For this purpose the humidity of the air is adjusted within the reaction chamber before the vapor flows over the packing media. The biological packing media may be natural or made of synthetic plastic. Recirculation of the water is always completed in the biological oxidation system to make the system more cost-effective. Biochemical oxygen demand (BOD) indirectly measures the amount of easily biodegradable organic matters thus very low values indicate direct waste water disposal. The prompt removal of a wide range of wastes and pollutants from the waste gas flow is the foremost requirement of biological oxidizers to meet regulatory permitting requirements. Micro-organisms differ in their ability to rapidly metabolize different pollutants, so the selection of the proper mix of organisms is critical. Research is underway to genetically modify various organisms to improve their performance in biological oxidation.|2023-03-05-04-16-08
Biological oxidizer|Benefits of biological oxididation| Biological oxidation of organic matters has led to the innovation of a low cost secondary treatment of the waste water emissions and industrial air emissions. The process of biodegradation offers a very fast method which typically offers 4,000 catalytic cycles per minute. Destruction rate efficiency is generally greater than 99% on most biodegradable organics emissions. The biological oxidation technology is free from secondary emissions (NOx) with limited CO2 production. While other oxidation technologies such as thermal oxidation produces CO, NO2 and CO2.|2023-03-05-04-16-08
Biological oxidizer|List of manufacturers| The following manufacturers have been involved in the development, design and planning of waste gas purification systems for a wide range of industries: Global manufacture of turnkey systems.|2023-03-05-04-16-08
Biomagnification|General| Biomagnification, also known as bioamplification or biological magnification, is the increase in concentration of a substance, e.g a pesticide, in the tissues of organisms at successively higher levels in a food chain.  This increase can occur as a result of: Biological magnification often refers to the process whereby  substances such as pesticides or heavy metals work their way into lakes, rivers and the ocean, and then move up the food chain in progressively greater concentrations as they are incorporated into the diet of aquatic organisms such as zooplankton, which in turn are eaten perhaps by fish, which then may be eaten by bigger fish, large birds, animals, or humans. The substances become increasingly concentrated in tissues or internal organs as they move up the chain. Bioaccumulants are substances that increase in concentration in living organisms as they take in contaminated air, water, or food because the substances are very slowly metabolized or excreted.|2023-01-30-09-17-56
Biomagnification|Processes|" Although sometimes used interchangeably with ""bioaccumulation"", an important distinction is drawn between the two, and with bioconcentration. Thus, bioconcentration and bioaccumulation occur within an organism, and biomagnification occurs across trophic (food chain) levels. Biodilution is also a process that occurs to all trophic levels in an aquatic environment; it is the opposite of biomagnification, thus when a pollutant gets smaller in concentration as it progresses up a food web. Many chemicals that bioaccumulate are highly soluble in fats (lipophilic) and insoluble in water (hydrophobic).  Lipophilic substances cannot be diluted, broken down, or excreted in urine, a water-based medium, and so accumulate in fatty tissues of an organism, if the organism lacks enzymes to degrade them. When eaten by another organism, fats are absorbed in the gut, carrying the substance, which then accumulates in the fats of the predator. Since at each level of the food chain there is a lot of energy loss, a predator must consume many prey, including all of their lipophilic substances. For example, though mercury is only present in small amounts in seawater, it is absorbed by algae (generally as methylmercury). Methylmercury is one of the most harmful mercury molecules. It is efficiently absorbed, but only very slowly excreted by organisms.  Bioaccumulation and bioconcentration result in buildup in the adipose tissue of successive trophic levels: zooplankton, small nekton, larger fish, etc. Anything which eats these fish also consumes the higher level of mercury the fish have accumulated. This process explains why predatory fish such as swordfish and sharks or birds like osprey and eagles have higher concentrations of mercury in their tissue than could be accounted for by direct exposure alone. For example, herring contains mercury at approximately 0.01 parts per million (ppm) and shark contains mercury at greater than 1 ppm. DDT is a pesticide known to biomagnify, which is one of the most significant reasons it was deemed harmful to the environment by the EPA and other organizations. DDT is one of the least soluble chemicals known and accumulates progressively in adipose tissue, and as the fat is consumed by predators, the amounts of DDT biomagnify. A well known example of the harmful effects of DDT biomagnification is the significant decline in North American populations of predatory birds such as bald eagles and peregrine falcons due to DDT caused eggshell thinning in the 1950s.   DDT is now a banned substance in many parts of the world."|2023-01-30-09-17-56
Biomagnification|Current status| In a review, a large number of studies, Suedel et al.  concluded that although biomagnification is probably more limited in occurrence than previously thought, there is good evidence that DDT, DDE, PCBs, toxaphene, and the organic forms of mercury and arsenic do biomagnify in nature. For other contaminants, bioconcentration and bioaccumulation account for their high concentrations in organism tissues. More recently, Gray  reached a similar substances remaining in the organisms and not being diluted to non-threatening concentrations. The success of top predatory-bird recovery (bald eagles, peregrine falcons) in North America following the ban on DDT use in agriculture is testament to the importance of recognizing and responding to biomagnification.|2023-01-30-09-17-56
Biomagnification|Substances that biomagnify|" Two common groups that are known to biomagnify are chlorinated hydrocarbons, also known as organochlorines, and inorganic compounds like methylmercury or heavy metals.  Both are lipophilic and not easily degraded. Novel organic substances like organochlorines are not easily degraded because organisms lack previous exposure and have thus not evolved specific detoxification and excretion mechanisms, as there has been no selection pressure from them. These substances are consequently known as ""persistent organic pollutants"" or POPs. Metals are not degradable because they are chemical elements. Organisms, particularly those subject to naturally high levels of exposure to metals, have mechanisms to sequester and excrete metals. Problems arise when organisms are exposed to higher concentrations than usual, which they cannot excrete rapidly enough to prevent damage. Persistent heavy metals, such as lead, cadmium, mercury, and arsenic, can have a wide variety of adverse health effects across species."|2023-01-30-09-17-56
Black sugar|General|" Black sugar is a form of crystalline precipitate or a particulate pollution by-product unique from creosote caused by the burning of low grade coal in the 1940–1960s. The outgas from burning coal would be a by-product (burning coal was used in residential forced hot air furnaces) for the heating of family housing. Black sugar was often found on porches and the cold ductwork and caused the high carbon, soot-loaded gas to precipitate  on the floors and walls and crystallize. Outward appearance was that of very fine, shiny coal, but would not combust when exposed to flames. After a time, the crystalline material would become powdered, to the consistency of white cane sugar, but retain a fine, shiny, black texture, thus the name ""black sugar"".  The material would often be cleaned from duct work by chimney sweeps or cleaning staff on a regular basis in the Beaver-Ambridge-Monaca area of Pennsylvania along the Ohio River near Pittsburgh."|2021-11-22-16-25-16
Blackwater (coal)|General| Blackwater is a form of pollution produced in coal preparation.    In its purification, coal is crushed in a coal preparation plant and then separated and transported as a coal slurry,  From the slurry, incombustible materials are removed and the coal can be sized.  After the recovery of the coal particles from this slurry, the remaining water is black, contains very fine particles of coal.  This blackwater cannot be processed in a water treatment plant. [why?]|2023-07-15-04-11-22
Blackwater (coal)|Disasters|" Impoundments for storage of blackwater and other coal-related wastes have a troubled history with often severe environmental consequences. In February 1972, three dams holding a mixture of coal slurry in Logan County, West Virginia, failed in succession: 130,000,000 US gallons (490,000 m3) of toxic water were released in the Buffalo Creek Flood.  As discussed in the book The Buffalo Creek Flood: An Act of Man, out of a population of 5,000 people, 125 people were killed, 1,121 were injured, and over 4,000 were left homeless. The flood caused 50 million dollars in damages. Despite evidence of negligence, the Pittston Coal Company, which owned the compromised dam, called the event an ""Act of God"". In 2002, a 900-foot (270 m) high, 2,000-foot (610 m) long valley fill in Lyburn, West Virginia, failed and slid into a sediment pond at the toe of the fill, generating a large wave of water and sediment that destroyed several cars and houses."|2023-07-15-04-11-22
Blackwater (coal)|Future technologies| The ultimate solution to the blackwater problem is to process coal without the use of water.[according to whom?]  Such dry-separation technologies are under development.[when?][by whom?]|2023-07-15-04-11-22
Blue ice (aviation)|General| In aviation, blue ice is frozen sewage material that has leaked mid-flight from commercial aircraft lavatory waste systems. It is a mixture of human biowaste and liquid disinfectant that freezes at high altitude. The name comes from the blue color of the disinfectant. Airlines are not allowed to dump their waste tanks mid-flight, and pilots have no mechanism by which to do so;  however, leaks sometimes do occur from a plane's septic tank.|2023-08-01-13-24-47
Blue ice (aviation)|Danger of ground impact|" There were at least 27 documented incidents of blue ice impacts in the United States between 1979 and 2003.  These incidents typically happen under airport landing paths as the mass warms sufficiently to detach from the plane during its descent. A rare incident of falling blue ice causing damage to the roof of a home was reported on October 20, 2006 in Chino, California.  A similar incident was reported in Leicester, UK, in 2007. In 1971, a chunk of ice from an aircraft tore a large hole in the roof of the Essex Street Chapel in Kensington, London, and was one trigger for the demolition of the building. In November 2011, a chunk of ice, the size of an orange, broke through the roof of a private house in Ratingen-Hösel, Germany.[citation needed] In February 2013, a ""football sized"" ball of blue ice smashed through a conservatory roof in Clanfield, Hampshire, causing around £10,000 worth of damage. In October 2016, a chunk of ice tore a hole in a private house in Amstelveen, The Netherlands. In two incidents in May 2018, chunks of blue ice fell onto residents in Kelowna, British Columbia. In November 2018, a chunk of ice fell from the sky and crashed through the roof of a home in Bristol, England."|2023-08-01-13-24-47
Blue ice (aviation)|Danger to aircraft|" Blue ice can also be dangerous to the aircraft itself — the National Transportation Safety Board has recorded three very similar incidents where waste from lavatories caused damage to the leaking aircraft,    all involving Boeing 727s. In all three cases, waste from a leaking lavatory hit one (or the other) of the three engines the 727 has mounted in the rear, causing a power loss.     The flights made safe emergency landings with the two remaining engines; nobody was injured.
Only one report specifically mentions ice,  while another mentions ""soft body FOD"" (foreign object damage),  indicating that the damage was caused by a relatively soft object like a bird, or even ice, as opposed to (e.g.) a stone or an object made of metal."|2023-08-01-13-24-47
Blue ice (aviation)|In popular culture|" Blue ice became known to many people from the 2003 Season 3 finale of the HBO television series Six Feet Under, in which a foot-sized chunk drops on a woman, killing her. A similar incident occurs in the 1996 television series Early Edition episode “Frostbite”, when the main character saves a man from being crushed by a chunk of blue ice. It was also mentioned in The Big Bang Theory. This also happened in an episode of CSI: NY. The title of the 1992 film Blue Ice is a reference to the phenomenon.  The 2001 film Joe Dirt finds the title character (played by David Spade) proudly displaying a large chunk of ""blue ice"" which he has mistaken for a meteorite, and the topic has also been covered on the TV show MANswers. Blue ice was also featured in  an episode of the television series MythBusters. Blue ice is a cause of death in season 4 of 1000 Ways to Die. Blue ice also features in Series 6 Episode 1 of the BBC Series The Brittas Empire, in which a block of blue ice falls on the Whitbury Newtown Leisure Centre."|2023-08-01-13-24-47
Brother Nut|General|" Brother Nut is an internationally-known performance artist based in Beijing, China.  He is known only by his pseudonym, 坚果兄弟, which is also sometimes translated in English as ""Nut Brother."" He was born in Shenzhen, Guangdong in 1981."|2022-11-22-03-52-12
Brother Nut|Career|" His best known work is 2015's ""Project Dust,"" which consisted of the creation of a brick made entirely from particulate matter vacuumed out of heavily polluted Beijing city air. This project spotlighted Beijing's ongoing air pollution problems at a time when China sought to recast itself as an environmentally-aware nation. Project Dust received press coverage in the United States   and Europe,  as well as in Asian press.  An online photo gallery is also maintained to summarize the activity of the project. In 2016 Nut Brother was a speaker at the Enter the Anthropocene Creative Time Summit in Washington DC. In 2018, he made headlines again with his project, “Nongfu Spring Market” in which he exhibited 9000 bottles of polluted water from the village of Xiaohaotu, in Shaanxi. The bottled water was then exhibited in the form of a street market set up in the 798 Art District in Beijing. While the art show was shut down by authorities, the government also began investigations into the town's water pollution problems. In 2020, he was profiled on the Al Jazeera documentary 101 East: China's Activist Artist,      which showed him collecting toys from children in Baishizhou which would be used for an art work involving a giant game of grab (as in an arcade claw machine), which would highlight the eviction of families in the area."|2022-11-22-03-52-12
Certificate of Financial Responsibility|General| The Certificate of Financial Responsibility (COFR) program was created to ensure that tankers, barges, and other vessels used to transport oil and chemical-based products on U.S. should bear any ensuing cleanup costs from spills or leaks.  This is based on the Oil Pollution Act of 1990 and other environmental statutes.|2021-02-02-19-18-22
Certificate of Financial Responsibility|Administration| The COFR program is administered by The U.S. Coast Guard’s National Pollution Funds Center (NPFC). The Vessel Certification Division of the NPFC ensures that responsible parties are identified and held responsible for the expenses incurred during a water pollution incident. A COFR is issued to vessel operators once they have shown the can pay cleanup and damage costs up to the liability limits required by the Oil Pollution Act.|2021-02-02-19-18-22
Certificate of Financial Responsibility|Conditions of Compliance| With a few limited exceptions, vessels greater than 300 gross tons and vessels of any size that are transferring oil between vessels or shipping oil in the Exclusive Economic Zone (EEZ) are required to comply with the COFR regulations in order to operate in U.S. waters.|2021-02-02-19-18-22
Certificate of Financial Responsibility|Penalties| Operators who do not comply with the COFR requirements are subject to:|2021-02-02-19-18-22
Chakr Innovation|General| Chakr Innovation is a cleantech startup based in India specializing in material science technology. The company was founded by graduates from IIT Delhi and works in the field of air and environmental protection.  Chakr Innovation is the first company in India to receive type approval certification for their retrofit emission control device (RECD) from labs approved by Central Pollution Control Board (CPCB).  They have over 15 patents filed and their work has been recognized across the globe by reputed organizations like United Nations, WWF, Forbes, and the likes.|2023-07-22-08-47-58
Chakr Innovation|History| Chakr Innovation was founded by graduates froFtimesm IIT Delhi in 2016 to reduce pollution with the help of innovation and technology.  The idea began with a group of friends having sugarcane juice at a shop with a wall turned black because of soot particles coming out from the diesel generator exhaust used for crushing sugarcane. Chakr Innovation launched Chakr Shield in 2017, one year after its incorporation. The device could reduce the Particulate Matter 2.5 (PM 2.5) emission from a diesel generator by up to 90%. In 2022 the company introduced a Dual Fuel Kit that would allow a diesel generator to run on fossil fuel and Natural gas simultaneously in a 30 to 70 ratio.|2023-07-22-08-47-58
Chakr Innovation|Products| Chakr Shield is a patented retrofit emission control device (RECD) by Chakr Innovation, it was also the first in India to get a Type Approval certification by CPCB certified labs like ICAT and ARAI for its capability to reduce the pollution by diesel generators by up to 90%. The Chakr Dual Fuel Kit uses technology to allow a diesel generator set to operate on a mixture of gas and diesel as a fuel, with 70% natural gas and 30% fossil fuel.  This can be a perfect conversion kit for industries with access to gas pipeline networks. With the launch of this product, Chakr Innovation reportedly became the only turnkey solution provider in India to control the emissions from diesel generators. In 2020, Chakr Innovation launched a decontamination cabinet for N95 masks with the help of ozone gas. Ozone is a strong oxidizing agent that destroyed viruses and bacteria by diffusing through their protein coats. Chakr DeCoV reportedly inactivated SARS-CoV-2 and reduced bacterial load by 99.9999%, allowing N95 masks to be reused up to 10 times.|2023-07-22-08-47-58
Chlorofluorocarbon|General| Lists Categories Chlorofluorocarbons (CFCs) and hydrochlorofluorocarbons (HCFCs) are fully or partly halogenated hydrocarbons that contain carbon (C), hydrogen (H), chlorine (Cl), and fluorine (F), produced as volatile derivatives of methane, ethane, and propane. The most common example is dichlorodifluoromethane (R-12). R-12 is also commonly called Freon and is used as a refrigerant. Many CFCs have been widely used as refrigerants, propellants (in aerosol applications), gaseous fire suppression systems, and solvents. As a result of CFCs contributing to ozone depletion in the upper atmosphere, the manufacture of such compounds has been phased out under the Montreal Protocol, and they are being replaced with other products such as hydrofluorocarbons (HFCs) and Hydrofluoroolefins (HFOs)  including R-410A,R-134a and R-1234yf.|2023-09-27-12-53-46
Chlorofluorocarbon|Structure, properties and production|" As in simpler alkanes, carbon in the CFCs bond with tetrahedral symmetry. Because the fluorine and chlorine atoms differ greatly in size and effective charge from hydrogen and from each other, the methane-derived CFCs deviate from perfect tetrahedral symmetry. The physical properties of CFCs and HCFCs are tunable by changes in the number and identity of the halogen atoms. In general, they are volatile but less so than their parent alkanes. The decreased volatility is attributed to the molecular polarity induced by the halides, which induces intermolecular interactions. Thus, methane boils at −161 °C whereas the fluoromethanes boil between −51.7 (CF2H2) and −128 °C (CF4). The CFCs have still higher boiling points because the chloride is even more polarizable than fluoride. Because of their polarity, the CFCs are useful solvents, and their boiling points make them suitable as refrigerants. The CFCs are far less flammable than methane, in part because they contain fewer C-H bonds and in part because, in the case of the chlorides and bromides, the released halides quench the free radicals that sustain flames. The densities of CFCs are higher than their corresponding alkanes. In general, the density of these compounds correlates with the number of chlorides. CFCs and HCFCs are usually produced by halogen exchange starting from chlorinated methanes and ethanes. Illustrative is the synthesis of chlorodifluoromethane from chloroform: Brominated derivatives are generated by free-radical reactions of hydrochlorofluorocarbons, replacing C-H bonds with C-Br bonds. The production of the anesthetic 2-bromo-2-chloro-1,1,1-trifluoroethane (""halothane"") is illustrative:"|2023-09-27-12-53-46
Chlorofluorocarbon|Applications| CFCs and HCFCs are used in various applications because of their low toxicity, reactivity and flammability.  Every permutation of fluorine, chlorine and hydrogen based on methane and ethane has been examined and most have been commercialized. Furthermore, many examples are known for higher numbers of carbon as well as related compounds containing bromine. Uses include refrigerants, blowing agents, aerosol propellants in medicinal applications, and degreasing solvents. Billions of kilograms of chlorodifluoromethane are produced annually as a precursor to tetrafluoroethylene, the monomer that is converted into Teflon.|2023-09-27-12-53-46
Chlorofluorocarbon|Numbering system| A special numbering system is to be used for fluorinated alkanes, prefixed with Freon-, R-, CFC- and HCFC-, where the rightmost value indicates the number of fluorine atoms, the next value to the left is the number of hydrogen atoms plus 1, and the next value to the left is the number of carbon atoms less one (zeroes are not stated), and the remaining atoms are chlorine. Freon-12, for example, indicates a methane derivative (only two numbers) containing two fluorine atoms (the second 2) and no hydrogen (1-1=0). It is therefore CCl2F2. Another equation that can be applied to get the correct molecular formula of the CFC/R/Freon class compounds is this to take the numbering and add 90 to it. The resulting value will give the number of carbons as the first numeral, the second numeral gives the number of hydrogen atoms, and the third numeral gives the number of fluorine atoms. The rest of the unaccounted carbon bonds are occupied by chlorine atoms. The value of this equation is always a three figure number. An easy example is that of CFC-12, which gives: 90+12=102 -> 1 carbon, 0 hydrogens, 2 fluorine atoms, and hence 2 chlorine atoms resulting in CCl2F2. The main advantage of this method of deducing the molecular composition in comparison with the method described in the paragraph above is that it gives the number of carbon atoms of the molecule. Freons containing bromine are signified by four numbers. Isomers, which are common for ethane and propane derivatives, are indicated by letters following the numbers:|2023-09-27-12-53-46
Chlorofluorocarbon|Reactions| The most important reaction[citation needed] of the CFCs is the photo-induced scission of a C-Cl bond: The chlorine atom, written often as Cl., behaves very differently from the chlorine molecule (Cl2). The radical Cl. is long-lived in the upper atmosphere, where it catalyzes the conversion of ozone into O2. Ozone absorbs UV-B radiation, so its depletion allows more of this high energy radiation to reach the Earth's surface. Bromine atoms are even more efficient catalysts; hence brominated CFCs are also regulated.|2023-09-27-12-53-46
Chlorofluorocarbon|Impact as greenhouse gases| CFCs were phased out via the Montreal Protocol due to their part in ozone depletion. The atmospheric impacts of CFCs are not limited to their role as ozone-depleting chemicals. Infrared absorption bands prevent heat at that wavelength from escaping earth's atmosphere. CFCs have their strongest absorption bands from C-F and C-Cl bonds in the spectral region of 7.8–15.3 µm —referred to as the “atmospheric window” due to the relative transparency of the atmosphere within this region. The strength of CFC absorption bands and the unique susceptibility of the atmosphere at wavelengths where CFCs (indeed all covalent fluorine compounds) absorb radiation  creates a “super” greenhouse effect from CFCs and other unreactive fluorine-containing gases such as perfluorocarbons, HFCs, HCFCs, bromofluorocarbons, SF6, and NF3.  This “atmospheric window” absorption is intensified by the low concentration of each individual CFC. Because CO2 is close to saturation with high concentrations and few infrared absorption bands, the radiation budget and hence the greenhouse effect has low sensitivity to changes in CO2 concentration;  the increase in temperature is roughly logarithmic.  Conversely, the low concentration of CFCs allow their effects to increase linearly with mass,  so that chlorofluorocarbons are greenhouse gases with a much higher potential to enhance the greenhouse effect than CO2. Groups are actively disposing of legacy CFCs to reduce their impact on the atmosphere. According to NASA in 2018, the hole in the ozone layer has begun to recover as a result of CFC bans.  However, new research released in 2023 reports an alarming increase in CFCs, pointing to unregulated use in China.|2023-09-27-12-53-46
Chlorofluorocarbon|History|" Carbon tetrachloride (CCl4) was used in fire extinguishers and glass ""anti-fire grenades"" from the late nineteenth century until around the end of World War II. Experimentation with chloroalkanes for fire suppression on military aircraft began at least as early as the 1920s. Freon is a trade name for a group of CFCs which are used primarily as refrigerants, but also have uses in fire-fighting and as propellants in aerosol cans. Bromomethane is widely used as a fumigant. Dichloromethane is a versatile industrial solvent. The Belgian scientist Frédéric Swarts pioneered the synthesis of CFCs in the 1890s. He developed an effective exchange agent to replace chloride in carbon tetrachloride with fluoride to synthesize CFC-11 (CCl3F) and CFC-12 (CCl2F2). In the late 1920s, Thomas Midgley Jr. improved the process of synthesis and led the effort to use CFC as a refrigerant to replace ammonia (NH3), chloromethane (CH3Cl), and sulfur dioxide (SO2), which are toxic but were in common use. In searching for a new refrigerant, requirements for the compound were: low boiling point, low toxicity, and to be generally non-reactive. In a demonstration for the American Chemical Society, Midgley flamboyantly demonstrated all these properties by inhaling a breath of the gas and using it to blow out a candle  in 1930."|2023-09-27-12-53-46
Chlorofluorocarbon|Commercial development and use| During World War II, various chloroalkanes were in standard use in military aircraft, although these early halons suffered from excessive toxicity. Nevertheless, after the war they slowly became more common in civil aviation as well. In the 1960s, fluoroalkanes and bromofluoroalkanes became available and were quickly recognized as being highly effective fire-fighting materials. Much early research with Halon 1301 was conducted under the auspices of the US Armed Forces, while Halon 1211 was, initially, mainly developed in the UK. By the late 1960s they were standard in many applications where water and dry-powder extinguishers posed a threat of damage to the protected property, including computer rooms, telecommunications switches, laboratories, museums and art collections. Beginning with warships, in the 1970s, bromofluoroalkanes also progressively came to be associated with rapid knockdown of severe fires in confined spaces with minimal risk to personnel. By the early 1980s, bromofluoroalkanes were in common use on aircraft, ships, and large vehicles as well as in computer facilities and galleries. However, concern was beginning to be expressed about the impact of chloroalkanes and bromoalkanes on the ozone layer. The Vienna Convention for the Protection of the Ozone Layer did not cover bromofluoroalkanes as it was thought, at the time, that emergency discharge of extinguishing systems was too small in volume to produce a significant impact, and too important to human safety for restriction.|2023-09-27-12-53-46
Chlorofluorocarbon|Regulation|" Since the late 1970s, the use of CFCs has been heavily regulated because of their destructive effects on the ozone layer. After the development of his electron capture detector, James Lovelock was the first to detect the widespread presence of CFCs in the air, finding a mole fraction of 60 ppt of CFC-11 over Ireland. In a self-funded research expedition ending in 1973, Lovelock went on to measure CFC-11 in both the Arctic and Antarctic, finding the presence of the gas in each of 50 air samples collected, and concluding that CFCs are not hazardous to the environment. The experiment did however provide the first useful data on the presence of CFCs in the atmosphere. The damage caused by CFCs was discovered by Sherry Rowland and Mario Molina who, after hearing a lecture on the subject of Lovelock's work, embarked on research resulting in the first publication suggesting the connection in 1974. It turns out that one of CFCs' most attractive features—their low reactivity—is key to their most destructive effects. CFCs' lack of reactivity gives them a lifespan that can exceed 100 years, giving them time to diffuse into the upper stratosphere.   Once in the stratosphere, the sun's ultraviolet radiation is strong enough to cause the homolytic cleavage of the C-Cl bond. In 1976, under the Toxic Substances Control Act, the EPA banned commercial manufacturing and use of CFCs and aerosol propellants. This was later superseded in the 1990 amendments to the Clean Air Act to address stratospheric ozone depletion. By 1987, in response to a dramatic seasonal depletion of the ozone layer over Antarctica, diplomats in Montreal forged a treaty, the Montreal Protocol, which called for drastic reductions in the production of CFCs. On 2 March 1989, 12 European Community nations agreed to ban the production of all CFCs by the end of the century. In 1990, diplomats met in London and voted to significantly strengthen the Montreal Protocol by calling for a complete elimination of CFCs by 2000. By 2010, CFCs should have been completely eliminated from developing countries as well. Because the only CFCs available to countries adhering to the treaty is from recycling, their prices have increased considerably. A worldwide end to production should also terminate the smuggling of this material. However, there are current CFC smuggling issues, as recognized by the United Nations Environmental Programme (UNEP) in a 2006 report titled ""Illegal Trade in Ozone Depleting Substances"". UNEP estimates that between 16,000–38,000 tonnes of CFCs passed through the black market in the mid-1990s. The report estimated between 7,000 and 14,000 tonnes of CFCs are smuggled annually into developing countries. Asian countries are those with the most smuggling; as of 2007, China, India and South Korea were found to account for around 70% of global CFC production,  South Korea later to ban CFC production in 2010.  Possible reasons for continued CFC smuggling were also examined: the report noted that many banned CFC producing products have long lifespans and continue to operate. The cost of replacing the equipment of these items is sometimes cheaper than outfitting them with a more ozone-friendly appliance. Additionally, CFC smuggling is not considered a significant issue, so the perceived penalties for smuggling are low. In 2018 public attention was drawn to the issue, that at an unknown place in east Asia an estimated amount of 13,000 metric tons annually of CFCs have been produced since about 2012 in violation of the protocol.   While the eventual phaseout of CFCs is likely, efforts are being taken to stem these current non-compliance problems. By the time of the Montreal Protocol, it was realised that deliberate and accidental discharges during system tests and maintenance accounted for substantially larger volumes than emergency discharges, and consequently halons were brought into the treaty, albeit with many exceptions. While the production and consumption of CFCs are regulated under the Montreal Protocol, emissions from existing banks of CFCs are not regulated under the agreement. In 2002, there were an estimated 5,791 kilotons of CFCs in existing products such as refrigerators, air conditioners, aerosol cans and others.  Approximately one-third of these CFCs are projected to be emitted over the next decade if action is not taken, posing a threat to both the ozone layer and the climate.  A proportion of these CFCs can be safely captured and destroyed. In 1978 the United States banned the use of CFCs such as Freon in aerosol cans, the beginning of a long series of regulatory actions against their use. The critical DuPont manufacturing patent for Freon (""Process for Fluorinating Halohydrocarbons"", U.S. Patent #3258500) was set to expire in 1979. In conjunction with other industrial peers DuPont formed a lobbying group, the ""Alliance for Responsible CFC Policy,"" to combat regulations of ozone-depleting compounds.  In 1986 DuPont, with new patents in hand, reversed its previous stance and publicly condemned CFCs.  DuPont representatives appeared before the Montreal Protocol urging that CFCs be banned worldwide and stated that their new HCFCs would meet the worldwide demand for refrigerants."|2023-09-27-12-53-46
Chlorofluorocarbon|Phasing-out of CFCs|" Use of certain chloroalkanes as solvents for large scale application, such as dry cleaning, have been phased out, for example, by the IPPC directive on greenhouse gases in 1994 and by the volatile organic compounds (VOC) directive of the EU in 1997. Permitted chlorofluoroalkane uses are medicinal only. Bromofluoroalkanes have been largely phased out and the possession of equipment for their use is prohibited in some countries like the Netherlands and Belgium, from 1 January 2004, based on the Montreal Protocol and guidelines of the European Union. Production of new stocks ceased in most (probably all) countries in 1994.[citation needed] However many countries still require aircraft to be fitted with halon fire suppression systems because no safe and completely satisfactory alternative has been discovered for this application. There are also a few other, highly specialized uses. These programs recycle halon through ""halon banks"" coordinated by the Halon Recycling Corporation  to ensure that discharge to the atmosphere occurs only in a genuine emergency and to conserve remaining stocks. The interim replacements for CFCs are hydrochlorofluorocarbons (HCFCs), which deplete stratospheric ozone, but to a much lesser extent than CFCs.  Ultimately, hydrofluorocarbons (HFCs) will replace HCFCs. Unlike CFCs and HCFCs, HFCs have an ozone depletion potential (ODP) of 0.   DuPont began producing hydrofluorocarbons as alternatives to Freon in the 1980s. These included Suva refrigerants and Dymel propellants.  Natural refrigerants are climate friendly solutions that are enjoying increasing support from large companies and governments interested in reducing global warming emissions from refrigeration and air conditioning."|2023-09-27-12-53-46
Chlorofluorocarbon|Phasing-out of HFCs and HCFCs| Hydrofluorocarbons are included in the Kyoto Protocol and are regulated under the Kigali Amendment to the Montreal Protocol  due to their very high Global Warming Potential and the recognition of halocarbon contributions to climate change. On September 21, 2007, approximately 200 countries agreed to accelerate the elimination of hydrochlorofluorocarbons entirely by 2020 in a United Nations-sponsored Montreal summit. Developing nations were given until 2030. Many nations, such as the United States and China, who had previously resisted such efforts, agreed with the accelerated phase out schedule.  India successfully phased out HCFCs by 2020.|2023-09-27-12-53-46
Chlorofluorocarbon|Properly collecting, controlling, and destroying CFCs and HCFCs| While new production of these refrigerants has been banned, large volumes still exist in older systems and pose an immediate threat to our environment.  Preventing the release of these harmful refrigerants has been ranked as one of the single most effective actions we can take to mitigate catastrophic climate change.|2023-09-27-12-53-46
Chlorofluorocarbon|Development of alternatives for CFCs|" Work on alternatives for chlorofluorocarbons in refrigerants began in the late 1970s after the first warnings of damage to stratospheric ozone were published. The hydrochlorofluorocarbons (HCFCs) are less stable in the lower atmosphere, enabling them to break down before reaching the ozone layer. Nevertheless, a significant fraction of the HCFCs do break down in the stratosphere and they have contributed to more chlorine buildup there than originally predicted. Later alternatives lacking the chlorine, the hydrofluorocarbons (HFCs) have an even shorter lifetimes in the lower atmosphere.  One of these compounds, HFC-134a, were used in place of CFC-12 in automobile air conditioners. Hydrocarbon refrigerants (a propane/isobutane blend) were also used extensively in mobile air conditioning systems in Australia, the US and many other countries, as they had excellent thermodynamic properties and performed particularly well in high ambient temperatures. 1,1-Dichloro-1-fluoroethane (HCFC-141b) has replaced HFC-134a, due to its low ODP and GWP values. And according to the Montreal Protocol, HCFC-141b is supposed to be phased out completely and replaced with zero ODP substances such as cyclopentane, HFOs, and HFC-345a before January 2020.[citation needed] Among the natural refrigerants (along with ammonia and carbon dioxide), hydrocarbons have negligible environmental impacts and are also used worldwide in domestic and commercial refrigeration applications, and are becoming available in new split system air conditioners. 
Various other solvents and methods have replaced the use of CFCs in laboratory analytics. In Metered-dose inhalers (MDI), a non-ozone effecting substitute was developed as a propellant, known as ""hydrofluoroalkane."""|2023-09-27-12-53-46
Chlorofluorocarbon|Tracer of ocean circulation| Because the time history of CFC concentrations in the atmosphere is relatively well known, they have provided an important constraint on ocean circulation. CFCs dissolve in seawater at the ocean surface and are subsequently transported into the ocean interior. Because CFCs are inert, their concentration in the ocean interior reflects simply the convolution of their atmospheric time evolution and ocean circulation and mixing.|2023-09-27-12-53-46
Chlorofluorocarbon|CFC and SF6 tracer-derived age of ocean water|" Chlorofluorocarbons (CFCs) are anthropogenic compounds that have been released into the atmosphere since the 1930s in various applications such as in air-conditioning, refrigeration, blowing agents in foams, insulations and packing materials, propellants in aerosol cans, and as solvents.   The entry of CFCs into the ocean makes them extremely useful as transient tracers to estimate rates and pathways of ocean circulation and mixing processes.  However, due to production restrictions of CFCs in the 1980s, atmospheric concentrations of CFC-11 and CFC-12 has stopped increasing, and the CFC-11 to CFC-12 ratio in the atmosphere have been steadily decreasing, making water dating of water masses more problematic.  Incidentally, production and release of sulfur hexafluoride (SF6) have rapidly increased in the atmosphere since the 1970s.  Similar to CFCs, SF6 is also an inert gas and is not affected by oceanic chemical or biological activities.  Thus, using CFCs in concert with SF6 as a tracer resolves the water dating issues due to decreased CFC concentrations. Using CFCs or SF6 as a tracer of ocean circulation allows for the derivation of rates for ocean processes due to the time-dependent source function. The elapsed time since a subsurface water mass was last in contact with the atmosphere is the tracer-derived age.  Estimates of age can be derived based on the partial pressure of an individual compound and the ratio of the partial pressure of CFCs to each other (or SF6). The age of a water parcel can be estimated by the CFC partial pressure (pCFC) age or SF6 partial pressure (pSF6) age. The pCFC age of a water sample is defined as: where [CFC] is the measured CFC concentration (pmol kg−1) and F is the solubility of CFC gas in seawater as a function of temperature and salinity.  The CFC partial pressure is expressed in units of 10–12 atmospheres or parts-per-trillion (ppt).  The solubility measurements of CFC-11 and CFC-12 have been previously measured by Warner and Weiss  Additionally, the solubility measurement of CFC-113 was measured by Bu and Warner  and SF6 by Wanninkhof et al.  and Bullister et al.   Theses authors mentioned above have expressed the solubility (F) at a total pressure of 1 atm as: where F = solubility expressed in either mol l−1 or mol kg−1 atm−1,
T = absolute temperature,
S = salinity in parts per thousand (ppt),
a1, a2, a3, b1, b2, and b3 are constants to be determined from the least squares fit to the solubility measurements.  This equation is derived from the integrated Van 't Hoff equation and the logarithmic Setchenow salinity dependence. It can be noted that the solubility of CFCs increase with decreasing temperature at approximately 1% per degree Celsius. Once the partial pressure of the CFC (or SF6) is derived, it is then compared to the atmospheric time histories for CFC-11, CFC-12, or SF6 in which the pCFC directly corresponds to the year with the same.  The difference between the corresponding date and the collection date of the seawater sample is the average age for the water parcel.  The age of a parcel of water can also be calculated using the ratio of two CFC partial pressures or the ratio of the SF6 partial pressure to a CFC partial pressure."|2023-09-27-12-53-46
Chlorofluorocarbon|Safety|" According to their material safety data sheets, CFCs and HCFCs are colorless, volatile, non-toxic liquids and gases with a faintly sweet ethereal odor. Overexposure at concentrations of 11% or more may cause dizziness, loss of concentration, central nervous system depression or cardiac arrhythmia. Vapors displace air and can cause asphyxiation in confined spaces. Although non-flammable, their combustion products include hydrofluoric acid and related species. 
Normal occupational exposure is rated at 0.07% and does not pose any serious health risks."|2023-09-27-12-53-46
Contaminated land|General| Contaminated land contains substances in or under the land that are definitively or potentially hazardous to health or the environment. These areas often have a long history of industrial production and industrial farming. Many sites may be affected by their former uses such as mining, industry, chemical and oil spills and waste disposal. Areas that were previously industrial areas, called brownfield sites, are higher risk areas. Contamination can also occur naturally as a result of the geology of the area, or through agricultural use.|2023-09-05-08-20-12
Contaminated land|Overview| Land can be contaminated by the following:|2023-09-05-08-20-12
Contaminated land|United Kingdom| A requirement was placed on all local authorities in England, Wales and Scotland to investigate potentially contaminated sites and, where necessary, ensure they are remediated by Part IIA of the Environmental Protection Act 1990,  which was inserted by the Environment Act 1995 The regime in Part IIA did not apply to radioactive contamination, but section 78YC permitted Ministers to make regulations to apply Part IIA to such contamination. Such Regulations have been made. The Waste and Contaminated Land (Northern Ireland) Order 2007 made similar provision for Northern Ireland|2023-09-05-08-20-12
Contaminated land|Legal definition|" Section 78A(2) of the Environmental Protection Act 1990  defines ""Contaminated Land"" as: The Contaminated Land Report (CLR) series of documents have been produced by the Department for Environment, Food and Rural Affairs (DEFRA) and the Environment Agency, to provide regulators with ""relevant, appropriate, authoritative and scientifically based information and advice on the assessment of risk from contamination in soils"". The Environment Agency has issued a number of Soil Guideline Values (SGVs) which, whilst non-binding, may be used as guidance in the environmental risk assessment  of land and in setting remediation targets.  They should only be applied to human health assessments."|2023-09-05-08-20-12
Contaminated land|The Process| Assessment of contaminated land in the UK is predominantly undertaken under the planning system.  The National Planning Policy Framework  (NPPF) sets out that, following development, a site should not be capable of being determined as ‘contaminated land’ under Part IIA of the Environmental Protection Act.   In addition, the risks from contamination should be assessed within the context of a site's end-use and upon completion, the site should be ‘suitable’ for its new use. A technical framework for identifying and dealing with land affected by contamination is detailed within DEFRA and Environment Agency guidance entitled Model Procedures for the Management of Land Contamination  (CLR11). The process can broadly be divided into three stages: risk assessment, remedial options appraisal, and implementation of remediation. A 'phased approach' to risk assessment is encouraged within CLR11 and should typically include the following: Should the risk assessment demonstrate that unacceptable risks to human health or the surrounding environment are likely to exist, then some remedial work will be necessary.  This process involves three key stages: Once the remedial strategy has been approved by relevant regulatory authorities then it should be implemented.  A verification report should be produced upon completion of the work to demonstrate that remedial targets have been achieved.  This work may include testing of remedial excavations, results of post-remedial monitoring, certification for imported material or membrane integrity testing, amongst other things.  Details of ongoing/long-term monitoring may also need to be agreed at this stage, possibly under a Section 106 Agreement. Upon completion of this process, the site should not pose a significant risk to future users or the surrounding environment and should be suitable for its end use.  Once this process of site assessment has been completed successfully then any associated planning conditions can be discharged.|2023-09-05-08-20-12
Disposable product|General|" A disposable (also called disposable product) is a product designed for a single use after which it is recycled or is disposed as solid waste. The term is also sometimes used for products that may last several months (e.g. disposable air filters) to distinguish from similar products that last indefinitely (e.g. washable air filters). The word ""disposables"" is not to be confused with the word ""consumables"", which is widely used in the mechanical world. For example, welders consider welding rods, tips, nozzles, gas, etc. to be ""consumables"", as they last only a certain amount of time before needing to be replaced. Consumables are needed for a process to take place, such as inks for printing and welding rods for welding, while disposable products are products that can be thrown away after it becomes damaged or otherwise unuseful."|2023-09-22-00-47-58
Disposable product|Etymology|" ""Disposable"" is an adjective meaning something which is not reusable but is disposed of after use. Many people now use the term as a noun or substantive, i.e. ""a disposable"" but in reality this is still an adjective as the noun (product, nappy, etc.) is implied. Disposable income is the amount of money left over from one's salary or pay for spending, saving or whatever, after all living costs have been taken out; this term uses the word ""disposable"" in a different sense, as the money is available to be ""disposed"" (i.e. allocated or committed) freely according to one's discretion."|2023-09-22-00-47-58
Disposable product|Materials|" Disposable products are most often made from paper, plastic, cotton, or polystyrene foam.  Products made from composite materials such as laminations are difficult to recycle and are more likely to be disposed of at the end of their use.They are typically disposed of using landfills because it is a cheap option. But in 2004 the European Union passed a law where they stopped allowing disposals in landfills. In 2021, Australia's Minderoo Foundation produced a report called the ""Plastic Waste Makers Index"" concluding that half of the world's single-use plastic waste is produced by just 20 companies.   China is the biggest consumer of single-use plastics."|2023-09-22-00-47-58
Disposable product|Packaging| Packages are usually intended for a single use. The waste hierarchy calls for minimization of materials.  Many packages and materials are suited to recycling, although the actual recycling percentages are relatively low in many regions. For example, in Chile, only 1% of plastic is recycled.  Reuse and repurposing of packaging is increasing, but eventually containers will be recycled, composted, incinerated, or landfilled. There are many container forms such as boxes, bottles, jars, bags, etc.  Materials include paper, plastics, metals, fabrics, composites, etc.|2023-09-22-00-47-58
Disposable product|Food service industry disposables|" In 2002, Taiwan began taking action to reduce the use of disposable tableware at institutions and businesses, and to reduce the use of plastic bags. Yearly, the nation of 17.7 million people was producing 59,000 tons of disposable tableware waste and 105,000 tons of waste plastic bags, and increasing measures have been taken in the years since then to reduce the amount of waste.  In 2013 Taiwan's Environmental Protection Administration (EPA) banned outright the use of disposable tableware in the nation's 968 schools, government agencies and hospitals.  The ban is expected to eliminate 2,600 metric tons of waste yearly. In Germany, Austria, and Switzerland, laws banning use of disposable food and drink containers at large-scale events have been enacted. Such a ban has been in place in Munich, Germany, since 1991, applying to all city facilities and events. This includes events of all sizes, including very large ones (Christmas market, Auer-Dult Faire, Oktoberfest and Munich City Marathon). For small events of a few hundred people, the city has arranged for a corporation offer rental of crockery and dishwasher equipment. In part through this regulation, Munich reduced the waste generated by Oktoberfest, which attracts tens of thousands of people, from 11,000 metric tons in 1990 to 550 tons in 1999. China produces about 57 billion pairs of single-use chopsticks yearly, of which half are exported. About 45 percent are made from trees – about 3.8 million of them – mainly cotton wood, birch, and spruce, the remainder being made from bamboo. Japan uses about 24 billion pairs of these disposables per year, and globally the use is about 80 billion pairs are thrown away by about 1.4 million people. Reusable chopsticks in restaurants have a lifespan of 130 meals. In Japan, with disposable ones costing about 2 cents and reusable ones costing typically $1.17, the reusables better at the $2.60 breakeven cost. Campaigns in several countries to reduce this waste are beginning to have some effect. Israel is considered the world's largest user of disposables food containers and dinnerware. Each month, 250 million plastic cups and more than 12 million paper cups are used, manufactured and disposed.  In Israel there are no laws about manufacturing or importing of food disposable containers. A kulhar  is a traditional handle-less clay cup from South Asia that is typically unpainted and unglazed, and meant to be disposable.  Since kulhars are made by firing in a kiln and are almost never reused, they are inherently sterile and hygienic.  Bazaars and food stalls in the Indian subcontinent traditionally served hot beverages, such as tea, in kuhlars, which suffused the beverage with an ""earthy aroma"" that was often considered appealing.  Yoghurt, hot milk with sugar as well as some regional desserts, such as kulfi (traditional ice-cream), are also served in kulhars.  Kulhars have gradually given way to polystyrene and coated paper cups, because the latter are lighter to carry in bulk and cheaper.⁠ ⁠"|2023-09-22-00-47-58
Disposable product|Medical and hygiene products|" Medical and surgical device manufacturers worldwide produce a multitude of items that are intended for one use only.  The primary reason is infection control; when an item is used only once it cannot transmit infectious agents to subsequent patients.  Manufacturers of any type of medical device are obliged to abide by numerous standards and regulations. ISO 15223: Medical Devices and EN 980 cite that single use instruments or devices be labelled as such on their packaging with a universally recognized symbol to denote ""do not re-use"", ""single use"", or ""use only once"". This symbol is the numeral 2, within a circle with a 45° line through it. Examples of single use items include:"|2023-09-22-00-47-58
Disposable product|Laws and policies| Many governments[which?] are scaling up their efforts to phase out single-use plastic packaging and to manage plastic packaging waste in an environmentally sound manner. A number of countries have legislation to ensure that plastic packaging waste collected from households is sorted, reprocessed, compounded, and reused or recycled. There are also bans on single-use plastic food packaging in many countries.|2023-09-22-00-47-58
Disposable product|Sources| This article incorporates text from a free content work.  Licensed under Cc BY-SA 3.0 IGO (license statement/permission). Text taken from Drowning in Plastics – Marine Litter and Plastic Waste Vital Graphics​,   United Nations Environment Programme.|2023-09-22-00-47-58
Drug pollution|General|" Drug pollution or pharmaceutical pollution is pollution of the environment with pharmaceutical drugs and their metabolites, which reach the aquatic environment (groundwater, rivers, lakes, and oceans) through wastewater. Drug pollution is therefore mainly a form of water pollution. ""Pharmaceutical pollution is now detected in waters throughout the world,"" said a scientist at the Cary Institute of Ecosystem Studies in Millbrook, New York.  ""Causes include aging infrastructure, sewage overflows and agricultural runoff. Even when wastewater makes it to sewage treatment facilities, they aren't equipped to remove pharmaceuticals."""|2023-09-21-17-49-57
Drug pollution|Sources and effects| Most simply from the drugs having been cleared and excreted in the urine. The portion that comes from expired or unneeded drugs that are flushed unused down the toilet is smaller, but it is also important, especially in hospitals (where its magnitude is greater than in residential contexts). This includes drug molecules that are too small to be filtered out by existing water treatment plants. The process of upgrading existing plants to use advanced oxidation processes that are able to remove these molecules can be expensive. Drugs such as antidepressants have been found in the United States Great Lakes. Researchers from the University of Buffalo have found high traces of antidepressants in the brains of fish. Fish behavior on antidepressants have been noted to have similar impacts and reducing risk-averse behavior, and thereby reducing survival through predation. Other sources include agricultural runoff (because of antibiotic use in livestock) and pharmaceutical manufacturing. Drug pollution is implicated in the sex effects of water pollution. It is a suspected a contributor (besides industrial pollution) in fish kills, amphibian dieoffs, and amphibian pathomorphology.|2023-09-21-17-49-57
Drug pollution|Pollution of water systems| In the early 1990s, pharmaceuticals were found to be present in the environment, which resulted in massive scientific research, new regulations, and public attention.  Also during the 1990s, it was discovered that for the synthesis of one kilogram of an active pharmaceutical compound the amount of waste produced was fifty to hundred times that one kilogram,  which was ending up in the environment. During the late 1990s, estrogens were discovered in wastewater. It was concluded that this was the cause of feminization of fish. This was another factor that caused greater attention to pharmaceuticals in the environment.  Reviews and information on pharmaceuticals present in the environment date back to at least the 1980s.  The majority of pharmaceuticals are intended to cause slight adverse effects for the target population.  Low concentrations of pharmaceuticals can have negative effects on the freshwater ecosystems.|2023-09-21-17-49-57
Drug pollution|Pharmaceuticals in the environment|" In the United States, Spain, Germany and the United Kingdom over 101 different pharmaceuticals were present in ground water, surface water, drinking water or tap water. Between 30 and 100 different pharmaceuticals were found present in the aforementioned waters in Thailand, Canada, Australia, India, China, South Korea, Japan, Sweden, Poland, Italy, the Netherlands, France and Brazil. In 2022, the most comprehensive study of pharmaceutical pollution of the world's rivers finds that it threatens ""environmental and/or human health in more than a quarter of the studied locations"". It investigated 1,052 sampling sites along 258 rivers in 104 countries, representing the river pollution of 470 million people. It found that ""the most contaminated sites were in low- to middle-income countries and were associated with areas with poor wastewater and waste management infrastructure and pharmaceutical manufacturing"" and lists the most frequently detected and concentrated pharmaceuticals. Groundwater contamination by pharmaceuticals, which belong to the category of contaminants of emerging concern (CEC) or emerging organic pollutants (EOP), has been receiving increasing attention in the fields of environmental engineering, hydrology and hydrogeochemistry since the last decades of the twentieth century. Pharmaceuticals  are suspected to provoke long-term effects in aquatic ecosystems even at low concentration ranges (trace concentrations) because of their bioactive and chemically stable nature, which leads to recalcitrant behaviours in the aqueous compartments, a feature that is typically associated with the difficulty in degrading these compounds to innocuous molecules, similarly with the behaviour exhibited by persistent organic pollutants.   Furthermore, continuous release of medical products in the water cycle poses concerns about bioaccumulation and biomagnification phenomena.  As the vulnerability of groundwater systems is increasingly recognized even from the regulating authority (the European Medicines Agency, EMA), environmental risk assessment (ERA) procedures, which is required for pharmaceuticals appliance for marketing authorization and preventive actions urged to preserve these environments. In the last decades of the twentieth century, scientific research efforts have been fostered towards deeper understanding of the interactions of groundwater transport and attenuation mechanisms with the chemical nature of polluting agents.  Amongst the multiple mechanisms governing solutes mobility in groundwater, biotransformation and biodegradation play a crucial role in determining the evolution of the system (as identified by developing concentration fields) in the presence of organic compounds, such as pharmaceuticals.  Other processes that might impact on pharmaceuticals fate in groundwater include classical advective-dispersive mass transfer, as well as geochemical reactions, such as adsorption onto soils and dissolution / precipitation. One major goal in the field of environmental protection and risk mitigation is the development of mathematical formulations yielding reliable predictions of the fate of pharmaceuticals in aquifer systems, eventually followed by an appropriate quantification of predictive uncertainty and estimation of the risks associated with this kind of contamination."|2023-09-21-17-49-57
Drug pollution|Prevention| Drug pollution still remains to be a global problem, since current policy techniques are not adequate enough. Most policy approaches remain to be individualized, expensive, and reactive.  Biomarkers could be extremely helpful in the risk assessment of pharmaceuticals for decision making in regulations. Biomarkers could help explain if a non-target organism was exposed to a pharmaceutical and the toxicity levels of the pharmaceutical in the organism if it is present. The main action for preventing drug pollution is to incinerate unwanted pharmaceutical drugs. Burning them chemically degrades their active molecules, with few exceptions. The resulting ash can be further processed before landfilling, such as to remove and recycle any heavy metals that may be present.[citation needed] There are now programs in many cities that provide collection points at places including drug stores, grocery stores, and police stations. People can bring their unwanted pharmaceuticals there for safe disposal, instead of flushing them (externalizing them to the waterways) or throwing them in the trash (externalizing them to a landfill, where they can become leachate). Another aspect of drug pollution prevention is environmental law and regulation, although this faces the problems of enforcement costs, enforcement corruption and negligence (see below), and, where enforcement succeeds, increased costs of doing business. The lobbying of pros and cons is ongoing.|2023-09-21-17-49-57
Drug pollution|Manufacturing|" One extreme example of drug pollution was found in India in 2009 in an area where pharmaceutical manufacturing activity is concentrated.  Not all pharmaceutical manufacturing contributes to the problem. In places where environmental law and regulation are adequately enforced, the wastewater from the factories is cleaned to a safe level.  But to the extent that the market rewards ""looking the other way"" in developing nations, whether through local corruption (bribed inspectors or regulators) or plausible deniability, such protections are circumvented. This problem belongs to everyone, because consumers in well-regulated places constitute the biggest customers of the factories that operate in the inadequately regulated or inspected places, meaning that externality is involved."|2023-09-21-17-49-57
Duct (industrial exhaust)|General|" Industrial exhaust ducts are pipe systems that connect hoods to industrial chimneys through other components of exhaust systems like fans, collectors, etc. Ducts are low-pressure pneumatic conveyors to convey dust, particles, shavings, fumes, or chemical hazardous components from air in the vicinity to a shop floor or any other specific locations like tanks, sanding machines, or laboratory hoods. Ducts can be fabricated from a variety of materials including carbon steel, stainless steel, PVC, and fiberglass.   They can be fabricated through rolling (preferable for ducts of 12"" or more in diameter) or extruded (for ducts up to 18""). HVAC systems do not include this category of industrial application, namely exhaust systems.  A distinction from HVAC system ducts is that the fluid (air) conveyed through the duct system may not be homogeneous. An industrial exhaust duct system is primarily a pneumatic conveying system and is basically governed by laws of flow of fluids."|2021-12-14-01-44-56
Duct (industrial exhaust)|Fluid flow| The conveying fluid that flows through the duct system is air. Air transports materials from the hood to a destination. It is also instrumental in capturing the material into the flow system. Air is a compressible fluid, but for engineering calculations, air is considered as incompressible as a simplification, without any significant errors.|2021-12-14-01-44-56
Duct (industrial exhaust)|Design| Process design of exhaust system will include The goal is to keep contaminants out using minimum airflow. It is estimated that increase in an inch wg[clarification needed] of static pressure can add a few thousands of dollars to the operation cost per annum.|2021-12-14-01-44-56
Dustiness|General| Dustiness is the tendency of particles to become airborne in response to a mechanical or aerodynamic stimulus.  Dustiness is affected by the particle shape, size, and inherent electrostatic forces.  Dustiness increases the risk of inhalation exposure. Dusty materials tend to generate aerosols with high particle concentrations measured in number or in mass. The tendency of powdered materials to release airborne particles under external energies indicates their dustiness level. The dusty level of powders directly affects worker exposure scenarios and associated health risks in occupational settings. Powder-based aerosol particles can pose adverse effects when deposited in human respiratory systems via inhalation.|2023-09-05-13-02-42
Dustiness|Motivation| A significant motivation for quantifying and measuring the dustiness of materials comes from the area of workplace safety. The potential health impacts of suspended particles, particularly by inhalation, can be significant.|2023-09-05-13-02-42
Dustiness|Dustiness testing| The amount of dust produced during handling or processing of a powder can be affected by the nature of the handling process, the ambient humidity, the particle size and water content of the powder, and other factors. To measure dustiness of a particular powder in a replicable way, standardized testing procedures have been created and published.|2023-09-05-13-02-42
Dustiness|European Committee for Standardization - Continuous Drop and Rotating Drum| Various laboratory systems have been developed to test dustiness of fine powders. A European standard on dustiness testing has been established by the European Committee for Standardization (CEN) since April 2006.  This standard is especially related to human exposure in workplace (EN 15051). It describes two methods: the rotating drum system and continuous drop system, both of which use gravity to stimulate the material and generate aerosols.   The rotating drum method involves placing the powder in a cylinder containing baffles, while the continuous drop system involves allowing a stream of powder to fall onto a surface. While the drum approach has been successfully scaled down by some researchers, published standards call for tens or hundreds of grams of material, a stipulation that can prove problematic for nanomaterials, pharmaceuticals and other expensive powders.|2023-09-05-13-02-42
Dustiness|Aerosol generation system| Recently, an aerosol generation system based on laboratory funnel (resembling a fluidized bed) has been developed, which has the potential to become an alternative or supplementary method to the existing systems in dustiness testing.   Its performance was compared to other three aerosolization systems using the same test materials.|2023-09-05-13-02-42
Dustiness|Nanomaterials dustiness| The dustiness of the nanomaterials can influence potential exposures and the selection of the appropriate engineering control during the manufacturing production.   Electrostatic forces influence the stability of particle dispersion in air and effect the dustiness.   Nanomaterials in dry powder form tend to pose the greatest risk for inhalation exposure, while nanomaterials suspended in a liquid typically present less risk via inhalation.|2023-09-05-13-02-42
Dustiness|Safety measures| The full life cycle of a nanomaterial should be considered when planning to control for dust exposure. Nanomaterial synthesis reactors, nanoparticle collection and handling, product fabrication with nanomaterials, product use, and product disposal are potential sources of dust exposure. National Institute for Occupational Safety and Health recommends the use of high-efficiency particulate air (HEPA) filters on local exhaust ventilation, laboratory chemical hoods, lowflow enclosures, and any other containment enclosures as a best practice during the handling of engineered nanomaterials.|2023-09-05-13-02-42
Dutch pollutant standards|General|" Dutch Standards are environmental pollutant reference values (i.e., concentrations in environmental medium) used in environmental remediation, investigation and cleanup. Barring a few exceptions, the target values are underpinned by an environmental risk analysis wherever possible and apply to individual substances. In most cases, target values for the various substances are related to a national background concentration that was determined for the Netherlands. Groundwater target values provide an indication of the benchmark for environmental quality in the long term, assuming that there are negligible risks for the ecosystem. For metals a distinction is made between deep and shallow groundwater. This is because deep and shallow groundwater contain different background concentrations. An arbitrary limit of 10 metres has been adopted. The target values shown below are for 'shallow' groundwater, 0 – 10 m depth. The soil remediation intervention values indicate when the functional properties of the soil for humans, plants and animals is seriously impaired or threatened. They are representative of the
level of contamination above which a serious case of soil contamination is deemed to exist. The target values for soil are adjusted for the organic matter (humus) content and soil fraction <0.2 µm (Lutum - Latin, meaning ""mud"" or ""clay""). The values below are calculated for a 'Standard Soil' with 10% organic matter and 25% lutum. A case of environmental contamination is defined as 'serious' if >25 m³ soil or >100 m³ groundwater is contaminated above the intervention value. The values presented below are from Annex 1, Table 1, ""Groundwater target values and soil and groundwater intervention values"". In previous versions of the Dutch Standards, target values for soil were also present. However, in the 2009 version, target values for soils have been deleted for all compounds except metals."|2023-07-08-13-52-37
Ecological light pollution|General| Lists Categories Ecological light pollution  is the effect of artificial light on individual organisms and on the structure of ecosystems as a whole. The effect that artificial light has upon organisms is highly variable,  and ranges from beneficial (e.g. increased ability for predator species to observe prey) to immediately fatal (e.g. moths that are attracted to incandescent lanterns and are killed by the heat).  It is also possible for light at night to be both beneficial and damaging for a species.  As an example, humans benefit from using indoor artificial light to extend the time available for work and play, but the light disrupts the human circadian rhythm, and the resulting stress is damaging to health. Through the various effects that light pollution has on individual species, the ecology of regions is affected.  In the case where two species occupy an identical niche, the population frequency of each species may be changed by the introduction of artificial light if they are not equally affected by light at night.  For example, some species of spiders avoid lit areas, while other species willingly build webs directly on lamp posts. Since lamp posts attract many flying insects,  the spiders that tolerate light gain an advantage over the spiders that avoid it, and may become more dominant in the environment as a result.   Changes in these species frequencies can then have knock-on effects, as the interactions between these species and others in the ecosystem are affected and food webs are altered. These ripple effects can eventually affect diurnal plants and animals. As an example, changes in the activity of night active insects can change the survival rates of night blooming plants,  which may provide food or shelter for diurnal animals. The introduction of artificial light at night is one of the most drastic anthropogenic changes to the Earth, comparable to toxic pollution, land use change, and climate change due to increases in the concentration of green house gases.|2023-09-20-09-07-01
Ecological light pollution|Natural light cycles| The introduction of artificial light disrupts several natural light cycles that arise from the movements of the Earth, Moon, and Sun, as well as from meteorological factors.|2023-09-20-09-07-01
Ecological light pollution|Diurnal (solar) cycle| The most obvious change in introducing light at night is the end of darkness in general. The day/night cycle is probably the most powerful environmental behavioral signal, as almost all animals can be categorized as nocturnal or diurnal. If a nocturnal animal is only active in extreme dark, it will be unable to survive in lit areas.  The most acute affects are directly next to streetlights and lit buildings, but the diffuse light of skyglow can extend out to hundreds of kilometers away from city centres.|2023-09-20-09-07-01
Ecological light pollution|Seasonal (solar) cycles|" The axial tilt of the Earth results in seasons outside of the tropics. The change in the length of the day, or photoperiod, is the key signal for seasonal behavior (e.g. mating season) in non-tropical animals and plants.  The presence of light at night can result in ""seasons out of time"",   changing the behavior, thermoregulation, and hormonal functioning of affected organisms. This may result in a disconnect between body functioning and seasonality, causing disruptions to reproduction, dormancy, and migration."|2023-09-20-09-07-01
Ecological light pollution|Lunar cycles| The behavior of some animals (e.g. coyotes,  bats,  toads,  insects) is keyed to the lunar cycle.  Near city centers the level of skyglow often exceeds that of the full moon,  so the presence of light at night can alter these behaviors, potentially reducing fitness.|2023-09-20-09-07-01
Ecological light pollution|Cloud coverage| In pristine areas, clouds blot out the stars and darken the night sky, resulting in the darkest possible nights. In urban and suburban areas, in contrast, clouds enhance the effect of skyglow,  particularly for longer wavelengths.  This means that the typical level of light is much higher near cities, but it also means that truly dark nights never occur in these areas.|2023-09-20-09-07-01
Ecological light pollution|Terrestrial Environment| The attraction of insects to artificial light is one of the most well known examples of the effect of light at night on organisms. When insects are attracted to lamps they can be killed by exhaustion or contact with the lamp itself, and they are also vulnerable to predators like bats. Insects are affected differently by the varying wavelengths of light, and many species can see ultraviolet and infrared light that is invisible to humans. Because of variances in perception, moths are more attracted to broad spectrum white and bluish light sources than they are to the yellow light emitted by low pressure sodium-vapor lamps. The compound eye of moths results in fatal attraction to light. Dragonflies perceive horizontally polarized light as a sign of water. For this reason, sources of water are indistinguishable from asphalt roads with polarized light pollution to them. Dragonflies searching for water either to drink or in which to lay eggs often land on roads or other dark flat reflective surfaces such as cars and remain there until they die of dehydration and hyperthermia. Light pollution may hamper the mating rituals of fireflies, once they depend on their own light for courtship, resulting in decreased populations. Fireflies are charismatic (which is a rare quality amongst insects) and are easily spotted by nonexperts, providing thus good flagship species to attract public attention; good investigation models for the effects of light on nocturnal wildlife; and finally, due to their sensibility and rapid response to environmental changes, good bioindicators for artificial night lighting. Lights on tall structures can disorient migrating birds leading to fatalities. An estimated 365-988 million fatal bird collisions with buildings occur annually in North America, making human-made structures a large contributor to the decline in bird species.  The surface area of glass emitting artificial light at night is a major factor for fatal bird collisions with buildings, and turning off lights at night can minimize these fatalities.  The Fatal Light Awareness Program (FLAP) works with building owners in Toronto, Canada and other cities to reduce mortality of birds by turning out lights during migration periods. Similar disorientation has also been noted for bird species migrating close to offshore production and drilling facilities. Studies carried out by Nederlandse Aardolie Maatschappij b.v. (NAM) and Shell have led to development and trial of new lighting technologies in the North Sea. In early 2007, the lights were installed on the Shell production platform L15. The experiment proved a great success since the number of birds circling the platform declined by 50–90%.[56] Juvenile seabirds may also be disoriented by lights as they leave their nests and fly out to sea causing events of high mortality.  To minimise mortality rescue programs are conducted on many islands giving a second chance to thousands of seabird fledglings. Birds migrate at night for several reasons. Save water from dehydration in hot day flying and part of the bird's navigation system works with stars in some way. With city light outshining the night sky, birds (and also about mammals) no longer navigate by stars. Ceilometers (searchlights) can be particularly deadly traps for birds,   as they become caught in the beam and risk exhaustion and collisions with other birds. In the worst recorded ceilometer kill-off, on October 7–8, 1954, 50,000 birds from 53 different species were killed at Warner Robins Air Force Base. Lights from seashore developments repel nesting Sea turtle mothers, and their hatchlings are fatally attracted to street and hotel lights rather than to the ocean. Artificial lighting has many negative impacts on trees and plants, particularly in fall and autumn phenology. Trees and herbaceous plants rely on the photoperiod, or the amount of time in a day where sunlight is available for photosynthesis, to help determine the changing seasons. When the hours of sunlight decrease, plants can recognize that autumn is underway and begin to make preparations for winter dormancy. For example, deciduous trees  shift the colour of their leaves to maximize different wavelengths of light that are more prevalent in the fall before eventually dropping them as light becomes too scarce for photosynthesis to be worthwhile. When deciduous trees are exposed to light pollution, they mistake the artificial light for sunlight and retain their green leaves later into the autumn season. This can be dangerous for the tree, as it wastes energy trying to photosynthesize that should be preserved for winter survival. Light pollution can also cause leaf stoma to remain open into the night, which leaves the tree vulnerable to infection and disease. Similarly, light pollution in the spring can also be dangerous for trees and herbaceous plants. Artificial light causes plants to think that spring has arrived and it is time to begin producing leaves for photosynthesizing again. However, temperatures may not yet be warm enough to support the new leaf buds, and they are susceptible to frost, which can impair future leaf production. Small herbaceous plants that are exposed to artificial lighting potentially face a greater risk, as more of their body is illuminated. Therefore, only the root system is protected, and could potentially not be enough to sustain the whole plant as it tries to remain green through the fall and winter.|2023-09-20-09-07-01
Ecological light pollution|Aquatic Environment| Ecological light pollution has also critical effects on marine ecosystems. Zooplankton (e.g. Daphnia) exhibit diel vertical migration. That is, they actively change their vertical position inside of lakes throughout the day. In lakes with fish, the primary driver for their migration is light level, because small fish visually prey on them.  The introduction of light through skyglow reduces the height to which they can ascend during the night.   Because zooplankton feed on the phytoplankton that form algae, the decrease in their predation upon phytoplankton may increase the chance of algal blooms, which can kill off the lakes' plants and lower water quality. Light pollution impacts migration in some species of fish. For example, juvenile chinook salmon are attracted to and slowed down by artificial light. It is possible that artificial light draws them closer to the shoreline, where they face a greater risk of predation from birds and mammals. Artificial lighting also attracts a greater density of piscivorous fish, which have an advantage due to the slower movement of the juvenile fish.  Light pollution also has impacts on the hormonal functioning of some fish; European perch and roach both experience reductions in the production of reproductive hormones when exposed to artificial lighting in a rural environment.  Artificial light has also been shown to cause disruptions to fish (and zooplankton) in the high Arctic, where fishing boats with lights resulted in a lack of fish up to 200 metres below the water's surface.|2023-09-20-09-07-01
Ecological light pollution|Humans| At the turn of the century it was discovered that human eyes contain a non-imaging photosensor that is the primary regulator of the human circadian rhythm.   This photosensor is particularly affected by blue light, and when it observes light the pineal gland stops the secretion of melatonin.  The presence of light at night in human dwellings (or for shift workers) makes going to sleep more difficult and reduces the overall level of melatonin in the bloodstream, and exposure to a low-level incandescent bulb for 39 minutes is sufficient to suppress melatonin levels to 50%.   Because melatonin is a powerful anti-oxidant, it is hypothesized that this reduction can result in an increased risk of breast and prostate cancer. Other human health effects may include increased headache incidence, worker fatigue,  medically defined stress, decrease in sexual function and increase in anxiety.     Likewise, animal models have been studied demonstrating unavoidable light to produce adverse effect on mood and anxiety. == Effects of different wavelengths == The effect that artificial light has upon organisms is wavelength dependent.  While human beings cannot see ultraviolet light, it is often used by entomologists to attract insects. Generally speaking, blue light is more likely to be damaging to mammals because the non-imaging photoreceptors in mammalian eyes are most sensitive in the blue region.   This means that if traditional vapor discharge streetlamps are replaced by white LEDs (which generally emit more of their radiation in the blue part of the spectrum), the ecological impact could be greater even if the total amount of radiated light is decreased.|2023-09-20-09-07-01
Ecological light pollution|Polarized light pollution|" Artificial planar surfaces, such as glass windows or asphalt reflect highly polarized light. Many insects are attracted to polarized surfaces, because polarization is usually an indicator for water. This effect is called polarized light pollution,  and although it is certainly a form of ecological photopollution, ""ecological light pollution"" usually refers to the impact of artificial light on organisms. In the night, the polarization of the moonlit sky is very strongly reduced in the presence of urban light pollution, because scattered urban light is not strongly polarized.  Since polarized moonlight is believed to be used by many animals for navigation, this screening is another negative effect of light pollution on ecology."|2023-09-20-09-07-01
Ecological light pollution|Prevents and Controls| To regulate and manage the problem of light pollution, it needs to establish a mature management system. Based on Zhou's studies, posing regulations such as green lighting, strengthening the propaganda and education by governors could help stop or reduce the adverse impacts of light pollution.|2023-09-20-09-07-01
Environmental effects of illicit drug production|General| The environmental impacts caused by the production of illicit drugs is an often neglected topic when analysing the effects of such substances. However, due to the clandestine nature of illicit drug production, its effects can be highly destructive yet difficult to detect and measure. The consequences differ depending upon the drug being produced but can be largely categorised into impacts caused by natural drugs or caused by synthetic/semi-synthetic drugs. Natural drugs refer to drugs which are primarily extracted from a natural source such as cocaine or cannabis.  Synthetic drugs are produced from material that can't be found in nature and semi-synthetic drugs are made from both natural and synthetic materials such as methamphetamine and MDMA.  Drug policy is a large determinant on how organisations produce drugs and thereby, how their processes affect the environment, thus prompting Government bodies to analyse the current drug policy.  It is inevitable that solutions to such environmental impacts are synonymous with solutions to overall illicit drug production, however many have noted the reactionary measures undertaken by government bodies and elevate the need of preventative measures instead.|2022-11-17-03-09-39
Environmental effects of illicit drug production|Environmental impacts of natural drugs| Natural drugs are those whose constituents are primarily extracted from natural sources such as cocaine or marijuana. The environmental impacts associated with such drugs include deforestation, watershed depletion and greenhouse gas emissions.|2022-11-17-03-09-39
Environmental effects of illicit drug production|Marijuana|" With the ease of access to marijuana increasing due to legalisation in parts of North America and Canada  many have noted the increasing importance of measuring its possible environmental ramifications. As marijuana has been previously illegal in these areas there is now an opportunity to measure these outcomes. However, there have already been a variety of known consequences caused by the production of marijuana. Watershed depletion is a serious issue that can be caused by marijuana production. Marijuana cultivation requires large amounts of water, where a single plant can require 8-10 gallons of water per day.  This sparks concern, especially in areas susceptible to water shortages such as California.  California is the largest producer of marijuana in the U.S  yet has had issues surrounding water supply and sanitation   for a number of years. In 2012, it was estimated that at least 3,177,241,050 gallons of water were used in the production of marijuana in California.  Thus, marijuana production can have severe implications on watershed levels with a number of organisations calling for stricter regulations as marijuana becomes more widespread. The production of marijuana also requires large amounts of energy due to the controlling of environmental conditions.  This further causes high levels of greenhouse gas emissions and energy consumption.  ""In 2015, the average electricity consumption of a 5,000-square-foot indoor facility in Boulder County was 41,808 kilowatt-hours per month, while an average household in the county used about 630 kilowatt-hours"".  Such high levels of energy consumption in turn, result in high greenhouse gas emissions. In 2016, it was estimated that on average the production of one kilogram of marijuana produced 4,600 kilograms of carbon dioxide. Thus, marijuana cultivation produces 15 million metric tons of carbon dioxide in the United States in a single year."|2022-11-17-03-09-39
Environmental effects of illicit drug production|Cocaine| Most of the world's cocaine is produced in South America, particularly in the Andean region.  The environmental destruction caused by the production of cocaine has been well documented, with reports made the UN and other government bodies.  Due to the illegal nature of coca production, farmers make little effort in soil conservation and sustainability practices as seen in the high mobility and short life of coca plots in Colombia. One of the major implications of cocaine production is deforestation as large areas of forest are cleared for coca cultivation. The UNODC approximated that 97,622 hectares of primary forest were cleared for coca cultivation during 2001-2004 in the Andean region.  This further causes habitat destruction, especially in biodiversity hotspots, areas rich in a variety of species. Such areas are chosen for coca cultivation due to their remote locations, minimising chances of detection.  Deforestation has further impacts of soil erosion which further inhibits the survival of native species. The use of pesticides can also cause severely affect the environment. Farmers are able to sue un-regulated and highly toxic pesticides due to the clandestine nature of drug production.  The use of such pesticides can have both direct and indirect effects on the ecosystem. Where lethal levels of exposure directly cause the death of fauna, which is further carried up the food chain where secondary feeders who consume the poisoned animals are also impacted. Furthermore, non-lethal levels of exposure can also cause weaker immune system development and neurological issues, further increasing mortality rates.|2022-11-17-03-09-39
Environmental effects of illicit drug production|Environmental impacts of synthetic/semi-synthetic drugs| Synthetic drugs are those which are primarily derived from inorganic substances.  Semi-synthetic are a hybrid of both synthetic and natural drugs, however as both synthetic and semi-synthetic drugs undergo an array of chemical processes during production, their environmental impact are quite similar.|2022-11-17-03-09-39
Environmental effects of illicit drug production|Methamphetamine| Methamphetamine or meth is a synthetic drug which can be produced on a domestic scale. The dumping of toxic waste is a major issue associated with the production of meth.  It has been approximated that for each pound of meth produced, five pounds of toxic waste are also generated. The methods of disposal of these substances can be extremely damaging to the environment as producers may simply pour them down the sink or toilet. However, such methods allow producers to be more easily detected thus, producers sometimes adopt more environmentally destructive methods such as leaving waste in remote locations such as forests or buried underground where the waste can harm flora and fauna.  Producers have also used specialised trucks or vans, equipped with pumps and hose to drain waste onto the road as the vehicle moves. This decreases their chance of detection yet spreads the damage caused by the toxic waste. The production of meth also produces a number of toxic gases that can harm the human respiratory system and devastate the environment.  High levels of phosphine gas can be produced during meth production which can further cause headaches, convulsions and death.  The production of meth further produces hydrogen chloride gas, which when released into the environment cause damage metal structures and buildings. Hydrogen chloride is also highly soluble and readily dissolves into water bodies where it can harm the aquatic life.  This high solubility also causes it to be quickly washed out by rain in the atmosphere, further causing acid rain where high levels of such rain can have drastic impacts on the environment.|2022-11-17-03-09-39
Environmental effects of illicit drug production|Environmental impacts of drug policy| Drug policy is a determining factor on drug production as it partially dictates the methods through which illicit drugs are produced and transported. Thus, when determining such policies the environmental consequences are sometimes overlooked, resulting in effects which magnify the damage done unto the environment.  This is apparent in coca cultivation in the Andean Region, where drug policy has forced producers into more remote locations to avoid detection. In such ungoverned areas, producers maximise their damage through deforestation and toxic pesticide use, destroying these resource rich areas. These effects of drug policy have been noted by a number of government bodies including the UNDP who stated that some eradication campaigns “have not eradicated illicit production but rather displaced it to new areas of greater environmental significance.” Policies involving drug trafficking have also had adverse effects on the environment.  One key aspect of drug trafficking is the need to establish landing areas, usually by clearing land and deforestation. Once established, such areas further accelerate other illegal trafficking activities such as wildlife, marine and timber trafficking as drug traffickers may diversify their operations to expand their networks. Furthermore, as governments policies restrict the movement of traffickers, they must find alternate and more remote routes to transport their materials. These alternate routes typically require further land clearing and habitat destruction, thus further harming the environment. Drug policy can further inhibit biodiversity conservation. As drug policy can displace the actions of traffickers and producers into more biodiverse locations, their impact on global biodiversity is magnified.  As producers relocate into more remote locations, their actions of deforestation and dumping of toxic materials such as kerosene and hydrochloric acid can greatly damage biodiversity.  Furthermore, anti-drug initiatives and policies can further drain funding and diminish resources available for environmental protection initiatives.  Areas known for illicit-drug production can further discourage tourism, conservation activists and local law enforcement. This allows drug producers to conduct themselves with more freedom and thereby increase their damage. Furthermore, the lack of tourism in such areas limits the revenue of local conservation efforts and the transparency of theses issues.|2022-11-17-03-09-39
Environmental effects of illicit drug production|Possible solutions| Due to the nature of illicit drug production, it is inevitable that solutions to these environmental issues are synonymous with overall drug production prevention. However,  by taking environmental impacts into account when formulating drug policies it is possible to better mitigate this damage.  Changes in approach have been highlighted as a key method to help target these environmental concerns. This involves analysing and these environmental impacts when assessing the effects of illicit drugs and informing the illicit drug consumer base and law-makers of these impacts. Improved cooperation between international, national and regional-level organisations allows for a more-informed and sustainable solution to drug production. Previous collaborative efforts have involved more reactory responses which moreso displaced drug operations rather than prevented. A more integrated response between different organisations allows for more preventative measures to be implemented.   Furthermore, as much of the environmental impacts occur in transit countries, not just countries of origin, greater integration between different organisations could allow for preventative policies in transit countries to be established.  An example of this improved cooperation can be seen in Plan Colombia, which saw the collaboration between the U.S and the Colombian Government to combat drug production.  The project saw a decrease in coca cultivation in Colombia from 160,000 hectares to 48,000 hectares and a decrease in the drug-related economy from US$7.5 billion to US$4.5 billion from 2008-2013. Quelling the demand for illicit drugs has also been considered as a solution to the environmental impacts involved with drug production.  That is, by reassessing current anti-drug propaganda and intertwining drug-related health issues with the environmental impacts of illicit drug production a decrease in demand may be achieved.  Shifting the approach of current advertisements to focus on such issues may better inform the public and consumers of illicit drugs of these environmental problems. This notion can be further carried into children's drug education, where placing greater emphasis on the environmental effects alongside the traditional and well known health effects may incite a greater reaction.  It has also been suggested that besides just revealing these issues it is important for advertising bodies to communicate the contribution individuals make by consuming illicit drugs, thereby increasing their sense of self-value and lessening their dependence on illicit drugs.   Enlightening more consumers of such problems may also accrue a larger audience and support for anti-drug solutions.  However, even if such a response fails to stem demand, shedding light on these issues may foster voter concerns who still may appeal to legislators.|2022-11-17-03-09-39
Environmental effects of paint|General| The environmental effects of paint can vary depending on the type of paint used and mitigation measures. Traditional painting materials and processes can have harmful effects on the environment, including those from the use of lead and other additives.  Measures can be taken to reduce its environmental effects, including accurately estimating paint quantities so waste is minimized, and use of environmentally preferred paints, coating, painting accessories, and techniques. The United States Environmental Protection Agency guidelines and Green Star  standards  can be applied.[not verified in body]|2023-09-26-22-13-48
Environmental effects of paint|Low-VOC and other environmentally preferred paints| Volatile organic compounds (VOCs) are gases emitted by various solids or liquids, many of which have short- and long-term adverse health effects. Solvents in traditional paints often contain high quantities of VOCs. Low VOC paints improve indoor air quality and reduce urban smog.[citation needed]  The beneficial characteristics of such paints include low odor, clean air, and safer technology, as well as excellent durability and a washable finish. Low-VOC paint types include latex (water-based), recycled latex (water-based), acrylic, and milk paint. The labels of paint cans can be checked for the following information: In the US, items containing toxic ingredients have registration numbers with either the:|2023-09-26-22-13-48
Environmental effects of paint|Antifouling paint| Antifouling paint (or bottom paint) is used to protect the hulls of boats from fouling by marine organisms. Antifouling paint protects the surface from corrosion and prevents drag on the ship from any build-up of marine organisms. These paints have contained organotin compounds such as tributyltin, which are considered to be toxic chemicals with negative effects on humans and the environment.  Tributyltin compounds are moderately to highly persistent organic pollutants that bioconcentrate up the marine predators' food chain.  One common example is it leaching from marine paints into the aquatic environment, causing irreversible damage to the aquatic life. Tributyltin has also been linked to obesity in humans, as it triggers genes that cause the growth of fat cells. [medical citation needed] Tributyltin is harmful to some marine organisms, including the dog whelk, it causes dog whelks to suffer from imposex; females develop male sexual characteristics such as a penis.  This causes them to become infertile or even die. In severe cases, males can develop egg sacs. Alternatives include biomimetic antifouling coatings.|2023-09-26-22-13-48
Environmental effects of paint|Heavy metals| Heavy metals are used in paints and have raised concerns due to their toxicity at high levels of exposure and since they build up in the food chain. Lead paint contains lead as pigment. Lead is also added to paint to speed drying, increase durability, retain a fresh appearance, and resist moisture that causes corrosion. Although banned in many countries, paint with significant lead content is still used in areas such as Eastern Europe and Asia, most commonly for industry purposes like anticorrosive paint.   For example, leaded paint is sometimes used to paint roadways and parking lot lines. Lead, a poisonous metal, can damage nerve connections (especially in young children) and cause blood and brain disorders. Because of lead's low reactivity and solubility, lead poisoning usually only occurs in cases when it is dispersed, such as when sanding lead-based paint prior to repainting. Primer paint containing hexavalent chromium is still widely used for aerospace and automobile refinishing applications.[citation needed] Zinc chromate has been used as a pigment for artists' paint, known as zinc yellow or yellow 36. It is highly toxic and now rarely used.[citation needed]|2023-09-26-22-13-48
Environmental effects of paint|Types of Pollution| Production of gas leads to three main forms of pollution and waste: solid, liquid and gas.|2023-09-26-22-13-48
Environmental effects of paint|Liquid| The process of creating paint consumes a large amount of water and chemicals which leads to the production of large amounts of wastewater.   Roughly 70% of the wastewater produced by the paint manufacturing industry is released into natural bodies of water which causes the destination to be polluted.   The industry generates between 75 and 85 million gallons of wastewater per day.   The wastewater generated during production has high levels of chemical oxygen demand (COD) because of all the substances used in the creation process.|2023-09-26-22-13-48
Environmental effects of paint|Solid| The paint manufacturing process produces solid waste.   Most of the solid waste generated is formed in the dispersion, filing, and fluid transport stages of paint production.   Examples of solid waste that is formed during production include adhesives, plastic, and resins.|2023-09-26-22-13-48
Environmental effects of paint|Gas| Hazardous gases are released during the creation and use of paint and can be harmful to people. Total suspended particulate matter (TSPM or TSP) is one of these pollutants.   The World Health Organization has determined that exposure to total suspended particulate matter can lead to acute respiratory infection, asthma, emphysema, lung cancer, cardiovascular disease, and chronic obstructive lung disease to people working with it. The volatile organic compounds emitted can potentially turn into hazardous air pollutants.   VOCs can cause people to experience eye irritation, breathing difficulties, kidney damage, and cancer.   VOCs can negatively affect the environment by polluting groundwater and drinking water.|2023-09-26-22-13-48
Environmental effects of paint|Mitigation| As a response to the environmental and health concerns, some paint manufacturers now offer environmentally friendly alternatives. Also, in some countries, paint recycling is carried out on surplus paints and resold. Paint and coating manufacturers can modify their operations to reduce and mitigate air pollution.  The first step manufacturers can take to reduce air pollution is to eliminate the use of heavy metals in coating mixtures and reformulate coatings to be non-hazardous.   In place of heavy metals, non-hazardous biocides can be used to kill bacteria.   Manufacturers can also reduce emissions during the creation process by covering materials and tanks to reduce spills and leakage during the blending, mixing, and packaging of the paint.  Paint manufacturers can also start recycling paint; recycling paint allows new paint to be created with less emission.|2023-09-26-22-13-48
Environmental impact of fashion|General| The fashion industry, particularly manufacture and use of apparel and footwear, is a significant driver of greenhouse gas emissions and plastic pollution.  The rapid growth of fast fashion has led to around 80 billion items of clothing being consumed annually, with a large number going to landfill. Less than one percent of clothing is recycled to make new clothes.  The industry produces an estimated 10% of all greenhouse gas emissions.  The production and distribution of the crops, fibers, and garments used in fashion all contribute to differing forms of environmental pollution, including water, air, and soil degradation.[citation needed] The textile industry is the second greatest polluter of local freshwater in the world,  and is culpable for roughly one-fifth of all industrial water pollution.  Some of the main factors that contribute to this industrial caused pollution are the vast overproduction of fashion items,  the use of synthetic fibers, the agriculture pollution of fashion crops,  and the proliferation of microfibers across global water sources. Efforts have been made by some retailers and consumers to promote sustainable fashion practices, such as reducing waste, and improving energy and water efficiency.|2023-09-27-05-50-35
Environmental impact of fashion|Fast fashion|" Fast fashion is defined as ""an approach to the design, creation, and marketing of clothing fashions that emphasizes making fashion trends quickly and cheaply available to consumers.""  The amount of new garments bought by Americans has tripled since the 1960s. Globalization has encouraged the rapid growth of the fast fashion industry. Global retail sales of apparel in 2019 reached 1.9 trillion U.S dollars, a new high – this number is expected to double to three trillion U.S. dollars by the year 2030. The world consumes more than 80 billion items of clothing annually."|2023-09-27-05-50-35
Environmental impact of fashion|Production and disposal of waste|" One concern with fast fashion is the clothes waste it produces. According to the Environmental Protection Agency,  15.1 million tons of textile clothing waste was produced in 2013 alone.  In the United States, 64.5% of textile waste is discarded in landfills, 19.3% is incinerated with energy recovery, only 16.2% is recycled.  When textile clothing ends up in landfills, chemicals on the clothes such as the dye can leech into the ground and cause environmental damage. When unsold clothing is burned,  it releases CO2  into the atmosphere. According to a report from the World Bank Group, the fashion industry is responsible for 10% of yearly global carbon emissions. 
In 2019, France announced that it was making an effort to prevent companies from this practice of burning unsold fashion items.   Fashion is produced at such high and fast rates, that more than 40% of fashion goods are sold at a markdown."|2023-09-27-05-50-35
Environmental impact of fashion|Polyester| Polyester was one of the most popular fibers used in fashion in 2017, found in about 60% of garments in retail stores and equalling about 21.3 million tons of polyester fiber.  There was a 157% increase of polyester clothing consumption from 2000 to 2015.  Synthetic polyester is made from a chemical reaction of coal, petroleum, air, and water,  two of which are fossil fuels. When coal is burned it creates heavy amounts of air pollution containing carbon dioxide.[clarification needed] When petroleum is used[clarification needed]it creates several air pollutants such as particulate matter, nitrogen oxides, carbon monoxide, hydrogen sulfide, and sulfur dioxide.  The creation of polyester creates pollution,[citation needed] in addition to waste from the finished product at the end of its life cycle. Polyester is non-biodegradable  meaning it can never be converted by bacteria to a state that will not damage the environment. Washing polyester clothing leads to shedding of microfiber plastics which enter water systems, including oceans.|2023-09-27-05-50-35
Environmental impact of fashion|Cotton| Cotton is the most common crop in the world aside from food.  Half of all textiles produced are made of the fiber.  Cotton is a water-intensive crop, requiring 3644 cubic meters of water to grow one ton of fiber, or 347 gallons per pound.  Growing cotton requires 25% of insecticides and 10-16% of pesticides of what is used globally every year.   Half of the top pesticides used in growing cotton in the US are deemed likely to be carcinogenic by the United States Environmental Protection Agency.  Cotton production degrades the quality of the soil, leading to exhausted fields and expansion into new areas.  Expansion into new areas leads to the destruction of local habitats and the associated pollution affects biodiversity.|2023-09-27-05-50-35
Environmental impact of fashion|Animal fibers and textiles| Animal-based fibers such as wool and leather were responsible for 14.5% of global greenhouse gas emissions in 2005.  Cattle have digestive systems that use a process known as foregut fermentation, which creates the greenhouse gas methane as a byproduct. In addition to the CH4 released from the ruminants, CO2 and N2O are released into the atmosphere as byproducts of raising the animals. In total, 44% of emissions caused by livestock are from enteric fermentation, 41% comes from the feed needed to raise the livestock, 10% comes from manure, and 5% comes from energy consumption. Energy use here is measured in megajoules needed to produce one kilogram of the given textile. Water use here is measured in liters of water needed to produce one kilogram of the given textile.|2023-09-27-05-50-35
Environmental impact of fashion|Marine impact| Improperly disposing of clothing can harm the environment, especially through wastewater. Chemicals from decomposing clothing can leach into the air and into the ground, affecting both groundwater and surface water. Aside from plastic pollution, textiles also contributes significantly to marine pollution. Unlike plastic, textile pollution's impact on marine life occurs in its various supply chain processes.  Pollutants like pesticides and clothing manufacturing chemicals cling to particles that accumulate in the waters ecosystem and consequently enter into human food chains.|2023-09-27-05-50-35
Environmental impact of fashion|Microfiber pollution| Plastic and synthetic textile are both created from a chemical structure called polymer. The Merriam-Webster dictionary defines polymer as “a chemical compound or mixture of compounds formed by polymerization and consisting essentially of repeating structural units.” For plastic, the common polymer found is PET, polyethylene (PE), or polypropylene (PP), whereas for textile, the polymer found the most abundant in the collection of waste is polyester and nylon textiles. Textiles shed microfibers at every stage of their life cycle, from production, to use, to end of life disposal.  These fibers end up in the soil, air, lakes, and oceans.  Microfiber pollution has existed as long as the textile industry has, but only recently has it come under public scrutiny.  The Ocean Wise Conservation Association produced a study discussing the textile waste. For polyester, it stated that on average, humans shed around 20 to 800 mg micro polyester waste for every kg textile washed. A smaller amount for nylon is found; for every kg of fabrics washed, we shed around 11 to 63 mg nylon microfiber waste to the waters.  Washing synthetic textiles releases microplastics and microfibers into the oceans.  This type of waste is most commonly found from washing machine cycles, where fibers of clothes fall loose during the tumbling process.  An individual domestic load of laundry can shed up to 700,000 microfibers. The Association also released a study stating that on average, households in the United States and Canada produce around 135 grams of microfibers, which is equivalent to 22 kilotons of microfibers released to the wastewater annually. These wastewater will go through various waste water treatment plants, however, around 878 tons of those 22 kilotons were left untreated and hence, thrown into the ocean. For comparison, 878 tons of waste is equivalent to around 9 - 10 blue whales in the ocean. This is how much we pollute just from textile. Textiles are the main source of microfibers in the environment.  Thirty five percent of the microplastics that are found in marine ecosystems, such as shorelines, are from synthetic microfibers and nanofibers.  Such microfibers affect marine life in that fish or other species in the marine ecosystems consume them, which end up in the intestine and harm the animals.  Microfibers have been found in the digestive tracts of widely consumed fish and shellfish.  These fish are then consumed by humans, which leads to the absorption of micro pollutants in the fish in a process called biomagnification.  Predators of the affected marine individuals are also harmed, as they consume their prey who now contain the microfibers.  The yearly shellfish consumption of microplastics was found to be 11,000 pieces, and microfibers were found in eighty three percent of fish caught in one lake in Brazil.  Further, about two thirds of synthetic fibers from clothing production will be found in the ocean from 2015 to 2050.  In one study, the food consumption rates decreased in crabs who were eating food with plastic microfibers, which further lead to the available energy for growth to also decrease. Techniques to address the environmental impacts of the fashion industry include a marine algal bioabsorbent, which could be used for dye removal through rich algal surface chemistry through heteroatom containing functional groups.  Many techniques or potential solutions are difficult in their implementation, for instance the accuracy of marine sediment techniques to detect microplastics is not sufficiently tested among different soil samples or sources.|2023-09-27-05-50-35
Environmental impact of fashion|Eutrophication| Clothing often contains non-organic, excessively farmed cotton which is grown with chemicals that are known to cause eutrophication. Eutrophication is a process in which fresh water sources such as lakes and rivers become overly enriched with nutrients. This causes a dense growth of plant life that is harmful to the ecosystem, which can eventually kill all living things in the local ecosystem.  Two of the main ingredients in pesticides are nitrates and phosphates. When the pesticides leak into stream systems surrounding the cropland, the nitrates and phosphates contribute to water eutrophication.|2023-09-27-05-50-35
Environmental impact of fashion|Water use| The fashion industry consumes a large amount of water to produce fabrics and manufacture garments every year. The global fashion industry uses 93 billion cubic meters of water per year, or 20 trillion gallons.   This is four percent of all freshwater withdrawal globally.  This amount is set to double by 2030 if it follows the current trend.  According to the United Nations Environment Programme, the fashion industry is responsible for 20 percent of global wastewater.  Manufacturing a single pair of Levi jeans, will on average, consume about 3,781 liters of water to make.  On average, to produce one kilogram of textiles will require 200 liters of water.|2023-09-27-05-50-35
Environmental impact of fashion|Sustainability efforts| The consumer use phase in the life cycle of clothing and other textiles is a significant area of impact yet it is often overlooked.  While there is minimal research into energy efficient washers and dryers as a method of reducing impact on the consumer side,  wearing garments for 9 months longer could cut overall waste by 22% and water use by 33%.  On the producer side, choosing to make garments in popular colors and designs that consumers are more likely to buy is both a financially and environmentally responsible choice.  Designing clothing that is more likely to be purchased can reduce waste on the production side. In 2018 the fashion retailer H&M ended up with $4.3 billion of unsold merchandise.|2023-09-27-05-50-35
Environmental impact of pharmaceuticals and personal care products|General|" Lists Categories The environmental effect of pharmaceuticals and personal care products (PPCPs) is being investigated since at least the 1990s. PPCPs include substances used by individuals for personal health or cosmetic reasons and the products used by agribusiness to boost growth or health of livestock. More than twenty million tons of PPCPs are produced every year.  The European Union has declared pharmaceutical residues with the potential of contamination of water and soil to be ""priority substances"".[3] PPCPs have been detected in water bodies throughout the world. More research is needed to evaluate the risks of toxicity, persistence, and bioaccumulation, but the current state of research shows that personal care products impact the environment and other species, such as coral reefs    and fish.   PPCPs encompass environmental persistent pharmaceutical pollutants (EPPPs) and are one type of persistent organic pollutants. They are not removed in conventional sewage treatment plants but require a fourth treatment stage which not many plants have. In 2022, the most comprehensive study of pharmaceutical pollution of the world's rivers found that it threatens ""environmental and/or human health in more than a quarter of the studied locations"". It investigated 1,052 sampling sites along 258 rivers in 104 countries, representing the river pollution of 470 million people. It found that ""the most contaminated sites were in low- to middle-income countries and were associated with areas with poor wastewater and waste management infrastructure and pharmaceutical manufacturing"" and lists the most frequently detected and concentrated pharmaceuticals."|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Overview|" Since the 1990s, water pollution by pharmaceuticals has been an environmental issue of concern.  Many public health professionals in the United States began writing reports of pharmaceutical contamination in waterways in the 1970s.""  Most pharmaceuticals are deposited in the environment through human consumption and excretion, and are often filtered ineffectively by municipal sewage treatment plants which are not designed to manage them. Once in the water, they can have diverse, subtle effects on organisms, although research is still limited. Pharmaceuticals may also be deposited in the environment through improper disposal, runoff from sludge fertilizer and reclaimed wastewater irrigation, and leaky sewer pipes.  In 2009, an investigative report by Associated Press concluded that U.S. manufacturers had legally released 271 million pounds of compounds used as drugs into the environment, 92% of which was the industrial chemicals phenol and hydrogen peroxide, which are also used as antiseptics. It could not distinguish between drugs released by manufacturers as opposed to the pharmaceutical industry. It also found that an estimated 250 million pounds of pharmaceuticals and contaminated packaging were discarded by hospitals and long-term care facilities.  The series of articles led to a hearing[when?] conducted by the U.S. Senate Subcommittee on Transportation Safety, Infrastructure Security, and Water Quality. This hearing was designed to address the levels of pharmaceutical contaminants in U.S. drinking water. This was the first time that pharmaceutical companies were questioned about their waste disposal methods. ""No federal regulations or laws were created as a result of the hearing.""[citation needed] ""Between the years of 1970-2018 more than 3000 pharmaceutical chemicals were manufactured, but only 17 are screened or tested for in waterways.""[citation needed] Alternately, ""There are no studies designed to examine the effects of pharmaceutical contaminated drinking water on human health.""  In parallel, the European Union is the second biggest consumer in the world (24% of the world total) after the USA and in the majority of EU Member States, around 50% of unused human medicinal products is not collected to be disposed of properly. In the EU, between 30 and 90% of the orally administered doses are estimated to be excreted as the active substances in the urine. The term environmental persistent pharmaceutical pollutants (EPPP) was suggested in the 2010 nomination of pharmaceuticals and environment as an emerging issue to Strategic Approach to International Chemicals Management (SAICM) by the International Society of Doctors for the Environment (ISDE).[citation needed]"|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Safe disposal|" Depending on the sources and ingredients, there are various ways in which the public can dispose of pharmaceutical and personal care products in acceptable ways. The most environmentally safe disposal method is to take advantage of community drug take-back programs that collect drugs at a central location for proper disposal.  Several local public health departments in the United States have initiated these programs.[examples  needed] In addition, the United States Drug Enforcement Administration (DEA) periodically promotes local take-back programs, as well as the National Take Back Initiative. Take-back programs in the US are funded by state or local health departments or are volunteer programs through pharmacies or health care providers. In recent years, the proposition that pharmaceutical manufacturers should be responsible for their products ""from the cradle to the grave"" has been gaining attention.  Where there is no local take-back program, the U.S. Environmental Protection Agency (EPA) and the Office of National Drug Control Policy suggested in a 2009 guidance that consumers do the following: The intent of the recommended practices is that the chemicals would be separated from the open environment, especially water bodies, long enough for them to naturally break down. When these substances find their way into water, it is much more difficult to deal with them. Water treatment facilities use different processes in order to minimize or fully eliminate these pollutants. This is done by using sorption where suspended solids are removed by sedimentation.   Another method used is biodegradation, and through this method microorganisms, such as bacteria and fungi, feed on or break down these pollutants thus eliminating them from the contaminated media."|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Types| Pharmaceuticals, or prescription and over-the-counter medications made for human use or veterinary or agribusiness purposes, are common PPCPs found in the environment.  There are nine classes of pharmaceuticals included in PPCPs: hormones, antibiotics, lipid regulators, nonsteroidal anti-inflammatory drugs, beta-blockers, antidepressants, anticonvulsants, antineoplastics, and diagnostic contrast media.[2] Personal care products have four classes: fragrances, preservatives, disinfectants, and sunscreen agents.  These products may be found in cosmetics, perfumes, menstrual care products, lotions, shampoos, soaps, toothpastes, and sunscreen. These products typically enter the environment when passed through or washed off the body and into the ground or sewer lines, or when disposed of in the trash, septic tank, or sewage system.[3] Traces of illicit drugs can be found in waterways and may even be carried by money.[4]|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Routes into the environment|" More attention has been devoted since 2016 to PPCPs in the environment. Two causes may contribute to this: PPCPs are actually increasing in the environment due to widespread use and/or analytical technology is better able to detect PPCPs in the environment.  These substances enter the environment directly or indirectly. Direct methods include contamination of surface water by hospitals, households, industries, or sewage treatment plants. Direct contamination can also affect the sediment and soil. It is generally assumed (albeit hardly verified) that the production of pharmaceuticals in industrialised countries is well controlled and unharmful to the environment, due to the local legal restrictions usually required to permit production. However, a substantial fraction of the global production of pharmaceuticals takes place in low-cost production countries like India and China. Recent reports from India demonstrate that such production sites may emit very large quantities of e.g. antibiotics, yielding levels of the drugs in local surface waters higher than those found in the blood of patients under treatment. The major route for pharmaceutical residues to reach the aquatic environment is most probably by excretion from patients undergoing pharma treatment. Since many pharmaceutical substances are not metabolized in the body they may be excreted in biologically active form, usually via the urine. Furthermore, many pharmaceutical substances are not fully taken up from the intestine (following oral administration in patients) into their blood stream. The fraction not taken up into the blood stream will remain in the gut and eventually be excreted via the feces. Hence, both urine and feces from treated patients contain pharmaceutical residues. Between 30 and 90% of the orally administered dose is generally excreted as active substance in the urine. An additional source to environmental pollution with pharmaceuticals is improper disposal of unused or expired drug residues. In European countries take-back systems for such residues are usually in place (although not always utilized to full extent), while in the US only voluntary initiatives on a local basis exist. Though most of the waste goes to incineration and people are asked to throw unused or expired pharmaceuticals into their household waste, investigations in Germany showed that up to 24% of liquid pharmaceuticals and 7% of tablets or ointments are disposed always or at least ""rarely"" via the toilet or sink. Proper destruction of pharma residues should yield rest products without any pharmaceutical or ecotoxic activity. Furthermore, the residues should not act as components in the environmental formation of new such products. Incineration at a high temperature (>1000 degrees Celsius) is considered to fulfill the requirements, but even following such incineration residual ashes from the incineration should be properly taken care of. Pharmaceuticals used in veterinary medicine, or as additives to animal food, pose a different problem, since they are excreted into soil or possibly open surface waters. It is well known that such excretions may affect terrestrial organisms directly, leading to extinction of exposed species (e.g. dung-beetles). Lipid-soluble pharma residues from veterinary use may bind strongly to soil particles, with little tendency to leak out to ground water or to local surface waters. More water-soluble residues may be washed out with rain or melting snow and reach both ground water and surface water streams."|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Presence in the environment| The use of pharmaceuticals and personal care products (PPCPs) is on the rise with an estimated increase from 2 billion to 3.9 billion annual prescriptions between 1999 and 2009 in the United States alone.  PPCPs enter into the environment through individual human activity and as residues from manufacturing, agribusiness, veterinary use, and hospital and community use. In Europe, the input of pharmaceutical residues via domestic waste water is estimated to be around 80% whereas 20% is coming from hospitals.  Individuals may add PPCPs to the environment through waste excretion and bathing as well as by directly disposing of unused medications to septic tanks, sewers, or trash.  Because PPCPs tend to dissolve relatively easily and do not evaporate at normal temperatures, they often end up in soil and water bodies. Some PPCPs are broken down or processed easily by a human or animal body and/or degrade quickly in the environment. However, others do not break down or degrade easily. The likelihood or ease with which an individual substance will break down depends on its chemical makeup and the metabolic pathway of the compound.|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|In rivers|" In 2022, the most comprehensive study of pharmaceutical pollution of the world's rivers finds that it threatens ""environmental and/or human health in more than a quarter of the studied locations"". It investigated 1,052 sampling sites along 258 rivers in 104 countries, representing the river pollution of 470 million people. It found that ""the most contaminated sites were in low- to middle-income countries and were associated with areas with poor wastewater and waste management infrastructure and pharmaceutical manufacturing"" and lists the most frequently detected and concentrated pharmaceuticals. Locations of studied rivers/catchments (n = 137).Points = groups of sampling sites across respective river catchments;Shades of countries = total number of sampling sites. Concentrations Detection frequencies and number of active pharmaceutical ingredients detected Cumulative concentrations of active pharmaceutical ingredients Sites exceeding ""safe"" limits"|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|In groundwater|" Trace amounts of pharmaceuticals from treated wastewater infiltrating into the aquifer are among emerging ground-water contaminants being studied throughout the United States.  Popular pharmaceuticals such as antibiotics, anti-inflammatories, antidepressants, decongestants,
tranquilizers, etc. are normally found in treated wastewater.  This wastewater is discharged from the treatment facility, and often makes its way into the aquifer or source of surface water used for drinking water."|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Recreational drugs| A study published in 2014 reported a spike in the levels of ecstasy, ketamine, caffeine and acetaminophen in nearby rivers coinciding with a Taiwanese youth event attended by around 600,000 people.  In 2018, shellfish in Puget Sound, waters that receive treated sewage from the Seattle area, tested positive for oxycodone.  The occurrence of pharmaceuticals and personal care products in wastewater is frequent and ubiquitous enough that PPCPs in wastewater can be measured to estimate their use in a community.|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Studies prior to 2006| A 2002 study by the U.S. Geological Survey found detectable quantities of one or more chemicals in 80 percent of a sampling of 139 susceptible streams in 30 states.  The most common pharmaceuticals detected were  nonprescription drugs; detergents, fire retardants, pesticides, natural and synthetic hormones, and an assortment of antibiotics and prescription medications were also found. A 2006 study found detectable concentrations of 28 pharmaceutical compounds in sewage treatment plant effluents, surface water, and sediment. The therapeutic classes included antibiotics, analgesics and anti-inflammatories, lipid regulators, beta-blockers, anti-convulsant, and steroid hormones. Although most chemical concentrations were detected at low levels (nano-grams/Liter (ng/L)), there are uncertainties that remain regarding the levels at which toxicity occurs and the risks of bioaccumulation of these pharmaceutical compounds.|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Other|" Besides the identified input from human medicine, there appears to be diffuse pollution from pharmaceuticals used in other areas such as agriculture. Investigations in Germany, France and Scotland showed traces of PPCPs upstream of waste water treatment plant effluents to rivers. The noPILLS report found that ""the
whole medicinal product chain needs to be considered for multi-point, targeted intervention""."|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Human|" The scope of human exposure to pharmaceuticals and personal care products from the environment is a complex function of many factors. These factors include the concentrations, types, and distribution of pharmaceuticals in the environment; the pharmacokinetics of each drug; the structural transformation of the chemical compounds either through metabolism or natural degradation processes; and the potential bioaccumulation of the drugs.  More research is needed to determine the effects on humans of long-term exposure to low levels of PPCPs.  The full effects of mixtures of low concentrations of different PPCPs is also unknown. ""The U.S. EPA risk assessment states that the acceptable daily intake (ADI) of pharmaceuticals is around 0.0027 mg/kg‐day.""[citation needed] Due to the lack of research of toxicity guidelines and their effects on human health it is difficult to determine a healthy dosage for water contaminated by pharmaceuticals. ""The pharmaceutical sample size tested does not give a full representation of human exposure. Only 17 out of 3000 prescriptions are screened for in drinking water.""[citation needed] In addition, ""The EPA and FDA regulations state that a drug or chemical is not considered harmful until clear evidence shows that a substance causes harm"".  This means that the U.S. is not testing or screening for thousands of potential contaminants in drinking water. Health risk assessments have not been conducted to provide concrete evidence to link pharmaceutical contamination and adverse human health effects. ""However, adverse health outcomes are displayed in aquatic organisms. Fish living near water treatment plants have been reported to be feminized.""  ""Some male fish started to develop ovaries and other feminized characteristic due to pharmaceutical pollution, and some species have decreased in population due to exposure of EE2 and other hormonal ECD substances.""[citation needed] Although research has shown that PPCPs are present in water bodies throughout the world, no studies have shown a direct effect on human health. However, the absence of empirical data cannot rule out the possibility of adverse outcomes due to interactions or long-term exposures to these substances. Because the amounts of these chemicals in the water supply may be in the parts per trillion or parts per billion, it is difficult to chemically determine the exact amounts present. Many studies  have therefore been focused on determining if the concentrations of these pharmaceuticals exist at or above the accepted daily intake (ADI) at which the designed biological outcomes can occur. In addition to the growing concerns about human health risks from pharmaceutical drugs via environmental exposures, many researchers have speculated about the potential for inducing antibiotic resistance. One study found 10 different antibiotics in sewage treatment effluents, surface water, and sediments.  Some microbiologists believe that if antibiotic concentrations are higher than the minimum inhibitory concentrations (MICs) of a species of pathogenic bacteria, a selective pressure would be exerted and, as a result, antibiotic resistance would be selectively promoted. It has also been demonstrated that at even sub-inhibitory concentrations (e.g., one-fourth of the MIC), several antibiotics are able to have an effect on gene expression (e.g., as shown for the modulation of expression of toxin-encoding genes in Staphylococcus aureus). For reference the MIC of erythromycin that is effective against 90 percent of lab grown Campylobacter bacteria, the most common food-borne pathogen in the United States, is 60 ng/mL.  One study found that the average concentration of erythromycin, a commonly prescribed antibiotic, was 0.09 ng/mL in water treatment plant effluents.  Additionally, transfer of genetic elements among bacteria has been observed under natural conditions in wastewater treatment plants, and selection of resistant bacteria has been documented in sewers receiving wastewaters from pharmaceutical plants.  Moreover, antibiotic resistant bacteria may also remain in sewage sludge and enter the food chain if the sludge is not incinerated but used as fertilizer on agricultural land. The relationship between risk perception and behavior is multifaceted. Risk management is most effective once the motivation behind the behavior of disposing unused pharmaceuticals is understood. There was little correlation found between the perception of risk and knowledge regarding pharmaceutical waste according to a study conducted by Cook and Bellis in 2001.   This study cautioned against the effectiveness of attempting to change the public's behavior on these health issues by warning them of the risks associated with their actions. It is advised to take careful measures to inform the public in a way that does not impart guilt but rather public awareness. For example, a study carried out by Norlund and Garvill in Sweden (2003)   that found that some people may make a personal sacrifice in terms of comfort because they feel that it would be helpful to reduce further environmental damage caused by the use of cars. Awareness of air pollution problems was a factor in their decision to take action on a more environmentally favorable choice of transportation.  Thus, the goal of Bound's project encapsulates whether the perception of risk associated with pharmaceuticals has an effect on the way in which medication is commonly disposed. In order to conduct this study, the pharmaceuticals were grouped by their therapeutic action in order to help participants identify them. The eight therapeutic groups are listed below: antibacterials, antidepressants, antihistamines, antiepileptics, hormone treatments, and lipid regulators. Next, a survey was created to examine the disposal patterns of the participants and their perception of the existing risk or threat against the environment. Respondents were asked the following questions in part one of the survey: 1. When and how they disposed of pharmaceuticals. 2. How they perceive the risk to the environment posed by pharmaceuticals. 3. To differentiate between the risks associated with different classed of pharmaceuticals.  Part two of the survey involved each of the eight pharmaceutical groups described above individually. Finally, the third part asked information about the age, sex, profession, postcode, and education of participants.  The sample size of participants was precise in comparison to the actual distribution of males and females in the UK: Sample- 54.8 percent were female and 45.2 percent male vs. Actual- the UK of 51.3 percent female to 48.7 percent male. Results showed that when a medication must be discarded, 63.2 percent of participants throw them in a bin, 21.8 percent return them to a pharmacist, and 11.5 percent dispose of them via the toilet/sink, while the remaining 3.5 percent keep them. Only half of the respondents felt like pharmaceuticals could potentially be harmful to the environment. Upon examination of factors relevant to risk perception, there was no definite link found between perception and education or income. Dr. Bound noted that participation in altruistic activities such as Environmental Conservation groups may provide members with the ability to better grasp the effects of their actions in the environment. In regards to the aquatic environment, it is hard for one to perceive the favorable effects of properly disposing medication. There also exists the plausibility that a person's behavior will only be affected if there is a severe risk to themselves or humans as opposed to an environmental threat. Even though there are serious threats of pharmaceutical pollution resulting in the feminization of certain fish, they have a lower priority because they are not easily understood or experienced by the general public. In Jonathan P. Bound's opinion, the provision of information about exactly how to go about disposing unused medication properly in conjunction with risk education may have a more positive and forceful effect."|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Recommendations|" Several recommendations and initiatives have been made to prevent pharmaceutical pollution in the environment. Important practices include: First, it is imperative that patients become educated on pharmaceutical pollution and its hazardous effects on humans, animals, and the overall environment.  By educating patients on proper disposal of unused drugs, steps are being taken to further prevent pharmaceutical waste in the environment.  Consumers should take precautions before tossing out drugs in the trash or flushing them down the toilet.[citation needed]  Community take-back programs have been set up for consumers to bring back unused drugs for proper disposal.[citation needed]  Another initiative is for pharmacies to serve as a take-back site for proper drug disposal such as implementing recycling bins for customers to bring back unused or expired medicines while they're shopping.  In addition, medical foundations could receive these medicines to administer them to people who need them, while destroying those that are in excess or expired. Furthermore, educating physicians and patients on the importance of proper drug disposal and the environmental concern will help further reduce pharmaceutical waste. Also, implementing initiatives for hospitals to focus on better practices for hazardous waste disposal may prove to be beneficial.  The US EPA encourages hospitals to develop efficient pharmaceutical disposal practices by giving them grants.   This incentive may be very beneficial to other hospitals worldwide. Additionally, ""It is critical for us to develop an analytical method of identifying, testing, and regulating the amount of pharmaceuticals in the water systems"".  Data must be collected in order to accurately measure the prevalence of pharmaceuticals in drinking water. ""Multiple Health risk assessments should be conducted to understand the effects of prolonged exposure to pharmaceuticals in drinking water"". Community-based programs should be developed to monitor exposure and health outcomes.  We should encourage the pharmaceutical industry to develop technology that extracts pharmaceuticals from waterways. ""Extensive research must be conducted to determine the amount of pharmaceutical contamination in the environment and its effects on  animals and marine life"". Many pharmaceuticals pass through the human body unchanged, so there are advantages when human excreta does not go into waterways, even after conventional sewage treatment, which also does not remove most of these chemicals. It is therefore preferable for human feces and urine to go into fertile soil, where they will receive more effective treatment by numerous microbes found there, over longer amounts of time, and stay away from waterways.   This can be achieved via the use of urine-diverting dry toilets, composting toilets, and Arborloos."|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Environmental| While the full effects of most PPCPs on the environment are not understood, there is concern about the potential they have for harm because they may act unpredictably when mixed with other chemicals from the environment or concentrate in the food chain. Additionally, some PPCPs are active at very low concentrations, and are often released continuously in large or widespread quantities. Because of the high solubility of most PPCPs, aquatic organisms are especially vulnerable to their effects.  Researchers have found that a class of antidepressants may be found in frogs and can significantly slow their development.[medical citation needed]  The increased presence of estrogen and other synthetic hormones in waste water due to birth control and hormonal therapies has been linked to increased feminization of exposed fish and other aquatic organisms.  The chemicals within these PPCP products could either affect the feminization or masculinization of different fishes, therefore affecting their reproductive rates. In addition to being found only in waterways, the ingredients of some PPCPs can also be found in the soil. Since some of these substances take a long time or cannot be degraded biologically, they make their way up the food chain.[medical citation needed] Information pertaining to the transport and fate of these hormones and their metabolites in dairy waste disposal is still being investigated, yet research suggest that the land application of solid wastes is likely linked with more hormone contamination problems.  Not only does the pollution from PPCPs affect marine ecosystems, but also those habitats that depend on this polluted water. There are various concerns about the effects of pharmaceuticals found in surface waters and specifically the threats against rainbow trout exposed to treated sewage effluents.  Analysis of these pharmaceuticals in the blood plasma of fish compared to human therapeutic plasma levels have yielded vital information providing a means of assessing risk associated with medication waste in water. Rainbow trout were exposed to undiluted, treated sewage water at three different sites in Sweden.  They were exposed for a total of 14 days while 25 pharmaceuticals were measured in the blood plasma at different levels for analysis.  The progestin Levonorgestrel was detected in fish blood plasma at concentrations between 8.5 and 12 ng mL-1 which exceed the human therapeutic plasma level. The measured effluent level of Levonorgestrel in the three areas was shown to reduce the fertility of the rainbow trout.[non-primary source needed] The three sites chosen for field exposures were in located in Stockholm, Gothenburg, and Umeå.  They were chosen according to their varying degrees of treatment technologies, geographic locations, and size.  The effluent treatment includes active sludge treatment, nitrogen and phosphorus removal (except in Umeå), primary clarification, and secondary clarification.  Juvenile rainbow trout were procured from Antens fiskodling AB, Sweden and Umlax AB, Sweden.  The fish were exposed to aerated, undiluted, treated effluent.  Since all of the sites underwent sludge treatment, it can be inferred that they are not representative of the low end of treatment efficacy. Of the 21 pharmaceuticals that were detected in the water samples, 18 were identified in the effluent, 17 in the plasma portion, and 14 pharmaceuticals were found in both effluent and plasma.[non-primary source needed]|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Current research| Starting in the mid-1960s, ecologists and toxicologists began to express concern about the potential adverse effects of pharmaceuticals in the water supply, but it wasn't until a decade later that the presence of pharmaceuticals in water was well documented. Studies in 1975 and 1977 found clofibric acid and salicylic acids at trace concentrations in treated water.  Widespread concern about and research into the effect of PPCPs largely started in the early 1990s.  Until this time, PPCPs were largely ignored because of their relative solubility and containment in waterways compared to more familiar pollutants like agrochemicals, industrial chemicals, and industrial waste and byproducts. Since then, a great deal of attention has been directed to the ecological and physiological risk associated with pharmaceutical compounds and their metabolites in water and the environment. In the last decade, most research in this area has focused on steroid hormones and antibiotics. There is concern that steroid hormones may act as endocrine disruptors. Some research suggests that concentrations of ethinylestradiol, an estrogen used in oral contraceptive medications and one of the most commonly prescribed pharmaceuticals, can cause endocrine disruption in aquatic and amphibian wildlife in concentrations as low as 1 ng/L. Current research on PPCPs aims to answer these questions:|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Pharmacoenvironmentology| Pharmacoenvironmentology is an extension of pharmacovigilance as it deals specifically with the environmental and ecological effects of drugs given at therapeutic doses.  Pharmacologists with this particular expertise (known as a pharmacoenvironmentologist) become a necessary component of any team assessing different aspects of drug safety in the environment.  We must look at the effects of drugs not only in medical practice, but also at its environmental effects. Any good clinical trial should look at the impact of particular drugs on the environment. Things we need to address in pharmacoenvironmentology are drugs and their exact concentration in different parts of the environment. Pharmacoenvironmentology is a specific domain of pharmacology and not of environmental studies. This is because it deals with drugs entering through living organisms through elimination.|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Ecopharmacovigilance| Pharmacovigilance is a new branch of science, which was born in 1960 after the incidence of the thalidomide disaster. Thalidomide is a teratogen and caused horrific birth abnormalities. The thalidomide disaster lead to the present day approach to drug safety and adverse event reporting. According to the EPA, pharamacovigilance is science aiming to capture any adverse effects of pharmaceuticals in humans after use. However, ecopharmacovigilance is  the science, and activities concerning detection, assessment, understanding, and prevention of adverse effects of pharmaceuticals in the environment which affect humans and other animal species.[citation needed] There has been a growing focus among scientists about the impact of drugs on the environment.  In recent years, we have been able to see human pharmaceuticals that are being detected in the environment which most are typically found on surface water.[citation needed] The importance of ecopharmacovigilance is to monitor adverse effects of pharmaceuticals on humans through environmental exposure.[citation needed] Due to this relatively new field of science, researchers are continuously developing and understanding the impacts of pharmaceuticals in the environment and its risk on human and animal exposure. Environmental risk assessment is a regulatory requirement in the launch of any new drug.[citation needed]  This precaution has become a necessary step towards the understanding and prevention of adverse effects of pharmaceutical residue in the environment.  It is important to note that pharmaceuticals enter the environment from the excretion of drugs after human use, hospitals, and improper disposal of unused drugs from patients.[citation needed]|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Ecopharmacology|" Ecopharmacology concerns the entry of chemicals or drugs into the environment through any route and at any concentration disturbing the balance of ecology (ecosystem), as a consequence. Ecopharmacology is a broad term that includes studies of ""PPCPs"" irrespective of doses and route of entry into environment. The geology of a karst aquifer area assists with the movement of PPCPs from the surface to the ground water. Relatively soluble bedrock creates sinkholes, caves and sinking streams into which surface water easily flows, with minimal filtering.  Since 25% of the population get their drinking water from karst aquifers, this affects a large number of people.   A 2016 study of karst aquifers in southwest Illinois found that 89% of water samples had one or more PPCP measured. Triclocarban (an antimicrobial) was the most frequently detected PPCP, with gemfibrozil (a cardiovascular drug) the second most frequently detected.  Other PPCPs detected were trimethoprim, naproxen, carbamazepine, caffeine, sulfamethoxazole, and fluoxetine. The data suggests that septic tank effluent is a probable source of PPCPs."|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Fate of pharmaceuticals in sewage treatment plants|" Sewage treatment plants (STP) work with physical, chemical, and biological processes to remove nutrients and contaminants from waste water. Usually the STP is equipped with an initial mechanical separation of solid particles (cotton buds, cloth, hygiene articles etc.) appearing in the incoming water. Following this there may be filters separating finer particles either occurring in the incoming water or developing as a consequence of chemical treatment of the water with flocculating agents. Many STPs also include one or several steps of biological treatment. By stimulating the activity of various strains of microorganisms physically their activity may be promoted to degrade the organic content of the sewage by up to 90% or more. In certain cases more advanced techniques are used as well. The today most commonly used advanced treatment steps especially in terms of micropollutants are PPCPs are difficult to remove from wastewater with conventional methods. Some research shows the concentration of such substances is even higher in water leaving the plant than water entering the plant. Many factors including environmental pH, seasonal variation, and biological properties affect the ability of an STP to remove PPCPs. A 2013 study of a drinking water treatment plant found that of 30 PPCPs measured at both the source water and the drinking water locations, 76% of PPCPs were removed, on average, in the water treatment plant. Ozonation was found to be an efficient treatment process for the removal of many PPCPs.  However, there are some PPCPs that were not removed, such as DEET used as mosquito spray, nonylphenol which is a surfactant used in detergents, the antibiotic erythromycin, and the herbicide atrazine. Several research projects are running to optimize the use of advanced sewage treatment techniques under different conditions. The advanced techniques will increase the costs for the sewage treatment substantially.
In a European cooperation project between 2008 and 2012 in comparison four hospital waste water treatment facilities were developed in Switzerland, Germany, The Netherlands and Luxembourg to investigate the elimination rates of concentrated waste water with pharmaceutical ""cocktails"" by using different and combined advanced treatment technologies.  Especially the German STP at Marienhospital Gelsenkirchen showed the effects of a combination of membranes, ozone, powdered activated carbon and sand filtration.  But even a maximum of installed technologies could not eliminate 100% of all substances and especially radiocontrast agents are nearly impossible to eliminate. The investigations showed that depending on the installed technologies the treatment costs for such a hospital treatment facility may be up to 5.50 € per m3.  Other studies and comparisons expect the treatment costs to increase up to 10%, mainly due to energy demand.  It is therefore important to define best available technique before extensive infrastructure investments are introduced on a wide basis. The fate of incoming pharmaceutical residues in the STP is unpredictable. Some substances seem to be more or less completely eliminated, while others pass the different steps in the STP unaffected. There is no systematic knowledge at hand to predict how and why this happens. Pharmaceutical residues that have been conjugated (bound to a bile acid) before being excreted from the patients may undergo de-conjugation in the STP, yielding higher levels of free pharmaceutical substance in the outlet from the STP than in its incoming water. Some pharmaceuticals with large sales volumes have not been detected in the incoming water to the STP, indicating that complete metabolism and degradation must have occurred already in the patient or during the transport of sewage from the household to the STP."|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Regulation|" In the United States, EPA has published wastewater regulations for pharmaceutical manufacturing plants.  
EPA has also issued air pollution regulations for manufacturing facilities. EPA published regulations for hazardous waste disposal of pharmaceuticals by health care facilities in 2019.  The agency also studied disposal practices for health care facilities where unused pharmaceuticals might be flushed rather than placed in solid waste, but did not develop wastewater regulations. There are no national regulations covering disposal by consumers to sewage treatment plants (i.e., disposed down the drain). To address pharmaceuticals that may be present in drinking water, in 2009 EPA added three birth control substances and one antibiotic to its Contaminant Candidate List (CCL 3) for possible regulation under the Safe Drinking Water Act. In 2019, the United States Virgin Islands banned coral damaging sunscreens, in a growing trend to try to protect coral reefs."|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Blister packs|" 80% of pills in the world are packed with blister packaging, which is the most convenient type for several reasons.  Blister packs have two main components, the ""lid"" and the ""blister"" (cavity). Lid is mainly manufactured with aluminum (Al) and paper. The Cavity consists of polyvinyl chloride (PVC), polypropylene (PP), polyester (PET) or aluminum (Al).  If users employ proper disposal methods, all these materials can be recycled and the harmful effects to the environment can be minimized. However, a problem arises with the improper disposal either by burning or disposing as normal household waste. Burning of blister packs directly causes air pollution by the combustion products of polypropylene ([C3H6]n), polyester ([C10H8O4]n), and polyvinyl chloride ([CH2CHCl]n). The combustion reactions and products of these chemicals are mentioned below. [C3H6]n + 9n/2 O2 → 3n CO2 +3n H2O [C10H8O4]n + 10n O2 → 10n CO2 +4n H2O [CH2CHCl]n + 2n O2 → n CO2 + n H2O + n HCl + n CO Even though polypropylene and polyester is harmful to the environment, the most toxic effect is due to the combustion of polyvinyl chloride since it produces hydrochloric acid (HCl) which is an irritant in the lower and upper respiratory tract that can cause adverse effects to human beings. The disposal of blister packs as normal waste, will forbid recycling process and eventually accumulate in soil or water, which will result soil and water pollution since bio-degradation processes of compounds like PVC, PP and PET are very slow. As a result, ecologically damaging effects like disturbances of the habitats and movements can be seen. Ingestion by the animals, affect the secretion of gastric enzymes and steroid hormones  that can decrease the feeding stimuli and may also cause problems in reproduction.  At low pH, aluminum can increase its solubility according to the following equation. As a result, the negative effects of both aquatic and terrestrial ecosystems  can be generated. 2Al(s)+ 6H+ → 2Al3+ (aq) + 3H2 (g) By employing proper disposal methods, all manufacturing materials of blister packs like PP, PE, PVC and Al can be recycled and the adverse effects to the environment can be minimized.[citation needed] Even though, the synthesis of these polymers relatively simple, the recycling process can be very complex since the blister packs contain metals and polymers together. As the first step of recycling, separation of Al and Polymers using the hydrometallurgical method which uses hydrochloric acid (HCl)   can be incorporated. Then PVC can be recycled by using mechanical or chemical methods.  The most recent trend is to use biodegradable, eco-friendly ""bio plastics"" which are also called as biopolymers such as derivatives of starch, cellulose, protein, chitin and xylan for pharmaceutical packaging, to reduce the hostile effects to the environment.[citation needed]"|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Nail polish| In nail salons, employees can be exposed to dozens of chemicals found in nail polish and nail polish removers.    Nail polishes have many ingredients which are considered toxic, including solvents, resins, colorants and pigments,  among others.  [1]  In the early 2000's some of the toxic components found in nail polish (toluene, formaldehyde and dibutyl phthalate) started being replaced by other substances. One of the new components was triphenyl phosphate which is known as a endocrine-disrupting plasticizer.  Now many labels are available including not only 3-Free but higher, for example 5-Free or 12-Free.  There are few studies on the possible health outcomes of nail polish exposures; these include skin problems, respiratory disorders, neurologic disorders, and reproductive disorders.|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Nail polish remover| Nail polish remover has the ability to enter bodies of water and soil after entering landfills or by precipitation, such as rain or snow. However, due to acetone's high volatility, most of it that enters the bodies of water and soil will evaporate again and re-enter the atmosphere. Not all of the acetone molecules will evaporate again, and so, when acetone remains in the bodies of water or soil, a reaction will occur. Nail polish remover evaporates easily because acetone's intermolecular forces are weak. An acetone molecule can't attract other acetone molecules easily because its hydrogens are not slightly positive. The only force that holds acetone molecules together is its permanent dipoles which are weaker than hydrogen bonds. Since nail polish remover is a solvent, it will dissolve in water. When acetone dissolves in water, it hydrogen bonds with water. The more nail polish remover that enters the hydrosphere will increase the concentration of acetone and then increase the concentration of the solution created when acetone and water bonds. If enough nail polish remover is disposed, it can reach the lethal dose level for aquatic life. Nail polish remover can also enter the lithosphere by landfills and by precipitation. However, it will not bind to the soil. Microorganisms in the soil will decompose acetone.  The consequence of microorganisms decomposing acetone is the risk it has to cause oxygen depletion in bodies of water. The more acetone readily available for microorganism decomposition leads to more microorganisms reproduced and thus oxygen depletion because more microorganisms use up the available oxygen. When nail polish remover evaporates, acetone enters the atmosphere in the gaseous phase. In the gaseous phase, acetone can undergo photolysis and breakdown into carbon monoxide, methane, and ethane.  When temperatures are between 100 - 350 degrees Celsius, the following mechanism  occurs: (CH3)2CO + hv → CH3 + CH3CO CH3CO → CH3+ CO CH3+ (CH3)2CO → CH4 + CH2COCH3 2CH3 → C2H6 A second pathway that nail polish remover can enter in the atmosphere is reacting with hydroxyl radicals. When acetone reacts with hydroxyl radicals, its main product is methylglyoxal.  Methylglyoxal is an organic compound that is a by-product of many metabolic pathways. It is an intermediate precursor for many advanced glycation end-products, that are formed for diseases such as diabetes or neurodegenerative diseases. The following reaction occurs: (CH3)2CO + ·OH → CH3C(O)OH + ·CH3 CH3C(O)OH + ·CH3→  CH3C(O)COH + 3H+[clarification needed]|2023-09-21-17-54-56
Environmental impact of pharmaceuticals and personal care products|Sunscreens| Sunscreens use a variety of chemical compounds to prevent UV radiation, like benzophenone, octocrylene, octinoxate, among others. These chemical compounds affect the life of coral reefs in different stages of their life and contributes to coral bleaching.|2023-09-21-17-54-56
Environmental effects of mining|General| Lists Categories Environmental effects of mining can occur at local, regional, and global scales through direct and indirect mining practices. Mining can cause in erosion, sinkholes, loss of biodiversity, or the contamination of soil, groundwater, and surface water by chemicals emitted from mining processes. These processes also affect the atmosphere through carbon emissions which contributes to climate change.  Some mining methods (lithium mining, phosphate mining, coal mining, mountaintop removal mining, and sand mining) may have such significant environmental and public health effects that mining companies in some countries are required to follow strict environmental and rehabilitation codes to ensure that the mined area returns to its original state.|2023-09-22-20-01-00
Environmental effects of mining|Erosion| Erosion of exposed hillsides, mine dumps, tailings dams and resultant siltation of drainages, creeks and rivers can significantly affect the surrounding areas, a prime example being the giant Ok Tedi Mine in Papua New Guinea.  Soil erosion can decrease the water availability for plant growth, resulting in a population decline in the plant ecosystem.  Soil erosion is mainly caused by excessive rainfall, lack of soil management and chemical exposure from mining.  In wilderness areas, mining may cause destruction of ecosystems and habitats, and in areas of farming, it may disturb or destroy productive grazing and croplands.|2023-09-22-20-01-00
Environmental effects of mining|Sinkholes| A sinkhole at or near a mine site is typically caused from the failure of a mine roof from the extraction of resources, weak overburden or geological discontinuities.  The overburden at the mine site can develop cavities in the subsoil or rock, which can infill with sand and soil from the overlying strata. These cavities in the overburden have the potential to eventually cave in, forming a sinkhole at the surface. The sudden failure of earth creates a large depression at the surface without warning, this can be seriously hazardous to life and property.  Sinkholes at a mine site can be mitigated with the proper design of infrastructure such as mining supports and better construction of walls to create a barrier around an area prone to sinkholes. Back-filling and grouting can be done to stabilize abandoned underground workings.|2023-09-22-20-01-00
Environmental effects of mining|Water pollution| Mining can have harmful effects on surrounding surface and groundwater.  If proper precautions are not taken, unnaturally high concentrations of chemicals, such as arsenic, sulphuric acid, and mercury can spread over a significant area of surface or subsurface water.  Large amounts of water used for mine drainage, mine cooling,  aqueous extraction and other mining processes increases the potential for these chemicals to contaminate ground and surface water. As mining produces copious amounts of waste water, disposal methods are limited due to contaminates within the waste water. Runoff containing these chemicals can lead to the devastation of the surrounding vegetation. The dumping of the runoff in surface waters or in a lot of forests is the worst option. Therefore, submarine tailings disposal are regarded as a better option (if the waste is pumped to great depth).  Land storage and refilling of the mine after it has been depleted is even better, if no forests need to be cleared for the storage of debris. The contamination of watersheds resulting from the leakage of chemicals also has an effect on the health of the local population. In well-regulated mines, hydrologists  and geologists  take careful measurements of water to take precaution to exclude any type of water contamination that could be caused by the mine's operations. The minimization of environmental degradation is enforced in American mining practices by federal and state law, by restricting operators to meet standards for the protection of surface and groundwater from contamination.  This is best done through the use of non-toxic extraction processes as bioleaching.|2023-09-22-20-01-00
Environmental effects of mining|Air pollution| The mining industry contributes between 4 and 7% of global greenhouse gas emissions. Air pollutants have a negative impact on plant growth, primarily through interfering with resource accumulation. Once leaves are in close contact with the atmosphere, many air pollutants, such as O3 and NOx, affect the metabolic function of the leaves and interfere with net carbon fixation by the plant canopy. Air pollutants that are first deposited on the soil, such as heavy metals, first affect the functioning of roots and interfere with soil resource capture by the plant. These reductions in resource capture (production of carbohydrate through photosynthesis, mineral nutrient uptake and water uptake from the soil) will affect plant growth through changes in resource allocation to the various plant structures. When air pollution stress co-occurs with other stresses, e.g. water stress, the outcome on growth will depend on a complex interaction of processes within the plant. At the ecosystem level, air pollution can shift the competitive balance among the species present and may lead to changes in the composition of the plant community. In agroecosystems these changes may be manifest in reduced economic yield.|2023-09-22-20-01-00
Environmental effects of mining|Acid rock drainage|" Sub-surface mining often progresses below the water table, so water must be constantly pumped out of the mine in order to prevent flooding. When a mine is abandoned, the pumping ceases, and water floods the mine. This introduction of water is the initial step in most acid rock drainage situations. Acid rock drainage occurs naturally within some environments as part of the weathering process but is exacerbated by large-scale earth disturbances characteristic of mining and other large construction activities, usually within rocks containing an abundance of sulfide minerals. Areas where the earth has been disturbed (e.g. construction sites, subdivisions, and transportation corridors) may create acid rock drainage. In many localities, the liquid that drains from coal stocks, coal handling facilities, coal washeries, and coal waste tips can be highly acidic, and in such cases it is treated as acid mine drainage (AMD). The same type of chemical reactions and processes may occur through the disturbance of acid sulfate soils formed under coastal or estuarine conditions after the last major sea level rise, and constitutes a similar environmental hazard. The five principal technologies used to monitor and control water flow at mine sites are diversion systems, containment ponds, groundwater pumping systems, subsurface drainage systems, and subsurface barriers. In the case of AMD, contaminated water is generally pumped to a treatment facility that neutralizes the contaminants.  A 2006 review of environmental impact statements found that ""water quality predictions made after considering the effects of mitigation largely underestimated actual impacts to groundwater, seeps, and surface water""."|2023-09-22-20-01-00
Environmental effects of mining|Heavy metals| Heavy metals are naturally occurring elements that have a high atomic weight and a density at least 5 times greater than that of water. Their multiple industrial, domestic, agricultural, medical and technological applications have led to their wide distribution in the environment; raising concerns over their potential effects on human health and the environment. Naturally occurring heavy metals are displayed in shapes that are not promptly accessible for uptake by plants. They are ordinarily displayed in insoluble shapes, like in mineral structures, or in precipitated or complex shapes that are not promptly accessible for plant take-up. Normally happening heavy metals have an awesome adsorption capacity in soil and are hence not promptly accessible for living organisms. The holding vitality between normally happening heavy metals and soil is exceptionally high compared to that with anthropogenic sources. Dissolution and transport of metals and heavy metals by run-off and ground water is another example of environmental problems with mining, such as the Britannia Mine, a former copper mine near Vancouver, British Columbia. Tar Creek, an abandoned mining area in Picher, Oklahoma that is now an Environmental Protection Agency Superfund site, also suffers from heavy metal contamination. Water in the mine containing dissolved heavy metals such as lead and cadmium leaked into local groundwater, contaminating it.  Long-term storage of tailings and dust can lead to additional problems, as they can be easily blown off site by wind, as occurred at Skouriotissa, an abandoned copper mine in Cyprus. Environmental changes such as global warming and increased mining activity may increase the content of heavy metals in the stream sediments.|2023-09-22-20-01-00
Environmental effects of mining|Effect on biodiversity| The implantation of a mine is a major habitat modification, and smaller perturbations occur on a larger scale than exploitation site, mine-waste residuals contamination of the environment for example. Adverse effects can be observed long after the end of the mine activity.  Destruction or drastic modification of the original site and anthropogenic substances release can have major impact on biodiversity in the area.  Destruction of the habitat is the main component of biodiversity losses, but direct poisoning caused by mine-extracted material, and indirect poisoning through food and water, can also affect animals, vegetation and microorganisms. Habitat modification such as pH and temperature modification disturb communities in the surrounding area. Endemic species are especially sensitive, since they require very specific environmental conditions. Destruction or slight modification of their habitat put them at the risk of extinction. Habitats can be damaged when there is not enough terrestrial product as well as by non-chemical products, such as large rocks from the mines that are discarded in the surrounding landscape with no concern for impacts on natural habitat. Concentrations of heavy metals are known to decrease with distance from the mine,  and effects on biodiversity tend to follow the same pattern. Impacts can vary greatly depending on mobility and bioavailability of the contaminant: less-mobile molecules will stay inert in the environment while highly mobile molecules will easily move into another compartment or be taken up by organisms. For example, speciation of metals in sediments could modify their bioavailability, and thus their toxicity for aquatic organisms. Biomagnification plays an important role in polluted habitats: mining impacts on biodiversity, assuming that concentration levels are not high enough to directly kill exposed organisms, should be greater to the species on top of the food chain because of this phenomenon. Adverse mining effects on biodiversity depend a great extent on the nature of the contaminant, the level of concentration at which it can be found in the environment, and the nature of the ecosystem itself. Some species are quite resistant to anthropogenic disturbances, while some others will completely disappear from the contaminated zone. Time alone does not seem to allow the habitat to recover completely from the contamination.  Remediation practices take time,  and in most cases will not enable the recovery of the original diversity present before the mining activity took place.|2023-09-22-20-01-00
Environmental effects of mining|Aquatic organisms| The mining industry can impact aquatic biodiversity through different ways. One way can be direct poisoning;   a higher risk for this occurs when contaminants are mobile in the sediment  or bioavailable in the water. Mine drainage can modify water pH,  making it hard to differentiate direct impact on organisms from impacts caused by pH changes. Effects can nonetheless be observed and proven to be caused by pH modifications.  Contaminants can also affect aquatic organisms through physical effects:  streams with high concentrations of suspended sediment limit light, thus diminishing algae biomass.  Metal oxide deposition can limit biomass by coating algae or their substrate, thereby preventing colonization. Factors that impact communities in acid mine drainage sites vary temporarily and seasonally: temperature, rainfall, pH, salinisation and metal quantity all display variations on the long term, and can heavily affect communities. Changes in pH or temperature can affect metal solubility, and thereby the bioavailable quantity that directly impact organisms. Moreover, contamination persists over time: ninety years after a pyrite mine closure, water pH was still very low and microorganisms populations consisted mainly of acidophil bacteria. One big case study that was considered extremely toxic to aquatic organisms was the contamination that occurred in Minamata Bay.  Methylmercury was released into wastewater by industrial chemical company's and a disease called Minamata disease was discovered in Kumamoto, Japan.  This resulted in mercury poisoning in fishes and shellfishes and it was contaminating surrounding species and many died from it and it impacted anyone that ate the contaminated fishes.|2023-09-22-20-01-00
Environmental effects of mining|Microorganisms| Algae communities are less diverse in acidic water containing high zinc concentration,  and mine drainage stress decrease their primary production. Diatoms' community is greatly modified by any chemical change,  pH phytoplankton assemblage,  and high metal concentration diminishes the abundance of planktonic species.  Some diatom species may grow in high-metal-concentration sediments.  In sediments close to the surface, cysts suffer from corrosion and heavy coating.  In very polluted conditions, total algae biomass is quite low, and the planktonic diatom community is missing.  Similarly to phytoplankton, the zooplankton communities are heavily altered in cases where the mining impact is severe.  In case of functional complementary, however, it is possible that the phytoplankton and zooplankton mass remains stable.|2023-09-22-20-01-00
Environmental effects of mining|Macro-organisms| Water insect and crustacean communities are modified around a mine,  resulting in a low tropic completeness and their community being dominated by predators. However, biodiversity of macroinvertebrates can remain high, if sensitive species are replaced with tolerant ones.  When diversity within the area is reduced, there is sometimes no effect of stream contamination on abundance or biomass,  suggesting that tolerant species fulfilling the same function take the place of sensible species in polluted sites. pH diminution in addition to elevated metal concentration can also have adverse effects on macroinvertebrates' behaviour, showing that direct toxicity is not the only issue. Fish can also be affected by pH,  temperature variations, and chemical concentrations.|2023-09-22-20-01-00
Environmental effects of mining|Vegetation| Soil texture and water content can be greatly modified in disturbed sites,  leading to plants community changes in the area. Most of the plants have a low concentration tolerance for metals in the soil, but sensitivity differs among species. Grass diversity and total coverage is less affected by high contaminant concentration than forbs and shrubs.  Mine waste-materials rejects or traces due to mining activity can be found in the vicinity of the mine, sometimes far away from the source.  Established plants cannot move away from perturbations, and will eventually die if their habitat is contaminated by heavy metals or metalloids at a concentration that is too elevated for their physiology. Some species are more resistant and will survive these levels, and some non-native species that can tolerate these concentrations in the soil, will migrate in the surrounding lands of the mine to occupy the ecological niche. This can also leave the soil vulnerable to potential soil erosion, which would make it inhabitable for plants. Plants can be affected through direct poisoning, for example arsenic soil content reduces bryophyte diversity.  Vegetation can also be contaminated from other metals as well such as nickel and copper.  Soil acidification through pH diminution by chemical contamination can also lead to a diminished species number.  Contaminants can modify or disturb microorganisms, thus modifying nutrient availability, causing a loss of vegetation in the area.  Some tree roots divert away from deeper soil layers in order to avoid the contaminated zone, therefore lacking anchorage within the deep soil layers, resulting in the potential uprooting by the wind when their height and shoot weight increase.  In general, root exploration is reduced in contaminated areas compared to non-polluted ones.  Plant species diversity will remain lower in reclaimed habitats than in undisturbed areas.  Depending on what specific type of mining is done, all vegetation can be initially removed from the area before the actual mining is started. Cultivated crops might be a problem near mines. Most crops can grow on weakly contaminated sites, but yield is generally lower than it would have been in regular growing conditions. Plants also tend to accumulate heavy metals in their aerial organs, possibly leading to human intake through fruits and vegetables.  Regular consumption of contaminated crops might lead to health problems caused by long-term metal exposure.  Cigarettes made from tobacco growing on contaminated sites might also possibly have adverse effects on human population, as tobacco tends to accumulate cadmium and zinc in its leaves.|2023-09-22-20-01-00
Environmental effects of mining|Animals| Habitat destruction is one of the main issues of mining activity. Huge areas of natural habitat are destroyed during mine construction and exploitation, forcing animals to leave the site. Animals can be poisoned directly by mine products and residuals. Bioaccumulation in the plants or the smaller organisms they eat can also lead to poisoning: horses, goats and sheep are exposed in certain areas to potentially toxic concentration of copper and lead in grass.  There are fewer ant species in soil containing high copper levels, in the vicinity of a copper mine.  If fewer ants are found, chances are higher that other organisms living in the surrounding landscape are strongly affected by the high copper levels as well. Ants have good judgement whether an area is habitual as they live directly in the soil and are thus sensitive to environmental disruptions.|2023-09-22-20-01-00
Environmental effects of mining|Microorganisms| Microorganisms are extremely sensitive to environmental modification, such as modified pH,  temperature changes or chemical concentrations due to their size. For example, the presence of arsenic and antimony in soils have led to diminution in total soil bacteria.  Much like waters sensitivity, a small change in the soil pH can provoke the remobilization of contaminants,  in addition to the direct impact on pH-sensitive organisms. Microorganisms have a wide variety of genes among their total population, so there is a greater chance of survival of the species due to the resistance or tolerance genes in that some colonies possess,  as long as modifications are not too extreme. Nevertheless, survival in these conditions will imply a big loss of gene diversity, resulting in a reduced potential for adaptations to subsequent changes. Undeveloped soil in heavy metal contaminated areas could be a sign of reduced activity by soils microfauna and microflora, indicating a reduced number of individuals or diminished activity.  Twenty years after disturbance, even in rehabilitation area, microbial biomass is still greatly reduced compared to undisturbed habitat. Arbuscular mycorrhiza fungi are especially sensitive to the presence of chemicals, and the soil is sometimes so disturbed that they are no longer able to associate with root plants. However, some fungi possess contaminant accumulation capacity and soil cleaning ability by changing the biodisponibility of pollutants,  this can protect plants from potential damages that could be caused by chemicals.  Their presence in contaminated sites could prevent loss of biodiversity due to mine-waste contamination,  or allow for bioremediation, the removal of undesired chemicals from contaminated soils. On the contrary, some microbes can deteriorate the environment: which can lead to elevated SO4 in the water and can also increase microbial production of hydrogen sulfide, a toxin for many aquatic plants and organisms.|2023-09-22-20-01-00
Environmental effects of mining|Tailings| Mining processes produce an excess of waste materials known as tailings. The materials that are left over after are a result of separating the valuable fraction from the uneconomic fraction of ore. These large amounts of waste are a mixture of water, sand, clay, and residual bitumen. Tailings are commonly stored in tailings ponds made from naturally existing valleys or large engineered dams and dyke systems.  Tailings ponds can remain part of an active mine operation for 30–40 years. This allows for tailings deposits to settle, or for storage and water recycling. Tailings have great potential to damage the environment by releasing toxic metals by acid mine drainage or by damaging aquatic wildlife;  these both require constant monitoring and treatment of water passing through the dam. However, the greatest danger of tailings ponds is dam failure. Tailings ponds are typically formed by locally derived fills (soil, coarse waste, or overburden from mining operations and tailings) and the dam walls are often built up on to sustain greater amounts of tailings.  The lack of regulation for design criteria of the tailings ponds are what put the environment at risk for flooding from the tailings ponds.|2023-09-22-20-01-00
Environmental effects of mining|Spoil tip|" A spoil tip is a pile of accumulated overburden that was removed from a mine site during the extraction of coal or ore. These waste materials are composed of ordinary soil and rocks, with the potential to be contaminated with chemical waste
. Spoil is much different from tailings, as it is processed material that remains after the valuable components have been extracted from ore.  Spoil tip combustion can happen fairly commonly as, older spoil tips tend to be loose and tip over the edge of a pile. As spoil is mainly composed of carbonaceous material that is highly combustible, it can be accidentally ignited by the lighting fire or the tipping of hot ashes.  Spoil tips can often catch fire and be left burning underground or within the spoil piles for many years."|2023-09-22-20-01-00
Environmental effects of mining|Effects of mine pollution on humans| Humans are also affected by mining. There are many diseases that can come from the pollutants that are released into the air and water during the mining process. For example, during smelting operations large quantities of air pollutants, such as the suspended particulate matter, SOx, arsenic particles and cadmium, are emitted. Metals are usualldonoy emitted into the air as particulates as well. There are also many occupational health hazards that miners face. Most of miners suffer from various respiratory and skin diseases such as asbestosis, silicosis, or black lung disease. Furthermore, one of the biggest subset of mining that impacts humans is the pollutants that end up in the water, which results in poor water quality.  About 30% of the world has access to renewable freshwater which is used by industries that generate large amounts of waste containing chemicals in various concentrations that are deposited into the freshwater.  The concern of active chemicals in the water can pose a great risk to human health as it can accumulate within the water and fishes.  There was a study done on an abandon mine in China, Dabaoshan mine and this mine was not active to many years yet the impact of how metals can accumulate in water and soil was a major concern for neighboring villages.  Due to the lack of proper care of waste materials 56% of mortality rate is estimated within the regions around this mining sites, and many have been diagnosed with esophageal cancer and liver cancer.  It resulted that this mine till this day still has negative impacts on human health through crops and it is evident that there needs to be more cleaning up measures around surrounding areas. The long-term effects associated with air pollution are plenty including chronic asthma, pulmonary insufficiency, and cardiovascular mortality. According to a Swedish cohort study, diabetes seems to be induced after long-term air pollution exposure. Furthermore, air pollution seems to have various malign health effects in early human life, such as respiratory, cardiovascular, mental, and perinatal disorders, leading to infant mortality or chronic disease in adult age. Discuss contamination basically influences those living in huge urban zones, where street outflows contribute the foremost to the degradation of discuss quality. There's moreover a threat of mechanical mishaps, where the spread of a harmful haze can be lethal to the populaces of the encompassing regions. The scattering of poisons is decided by numerous parameters, most outstandingly barometrical soundness and wind.|2023-09-22-20-01-00
Environmental effects of mining|Deforestation|" With open cast mining the overburden, which may be covered in forest, must be removed before the mining can commence. Although the deforestation due to mining may be small compared to the total amount it may lead to species extinction if there is a high level of local endemism.
The lifecycle of mining coal is one of the filthiest cycles that causes deforestation due to the amount of toxins, and heavy metals that are released soil and water environment.  Although the effects of coal mining take a long time to impact the environment the burning of coals and fires which can burn up to decades can release flying ash and increase the greenhouse gasses. Specifically strip mining that can destroy landscapes, forests, and wildlife habitats that are near the sites.  Trees, plants and topsoil are cleared from the mining area and this can lead to destruction of agricultural land. Furthermore, when rainfall occurs the ashes and other materials are washed into streams that can hurt fish. These impacts can still occur after the mining site is completed which disturbs the presences of the land and restoration of the deforestation takes longer than usual because the quality of the land is degraded.  Legal mining, albeit more environmentally-controlled than illegal mining, contributes to some substantial percentage to the deforestation of tropical countries"|2023-09-22-20-01-00
Environmental effects of mining|Coal mining| The environmental factors of the coal industry are not only impacting air pollution, water management and land use but also is causing severe health effects by the burning of the coal. Air pollution is increasing in numbers of toxins such as mercury, lead, sulfur dioxide, nitrogen oxides and other heavy metals.  This is causing health issues involving breathing difficulties and is impacting the wildlife around the surrounding areas that needs clean air to survive.  The future of air pollution remains unclear as the Environmental Protection Agency have tried to prevent some emissions but don't have control measures in place for all plants producing mining of coal.  Water pollution is another factor that is being damaged throughout this process of mining coals, the ashes from coal is usually carried away in rainwater which streams into larger water sites. It can take up to 10 years to clean water sites that have coal waste and the potential of damaging clean water can only make the filtration much more difficult.|2023-09-22-20-01-00
Environmental effects of mining|Deep sea mining| Deep sea mining for manganese nodules and other resources have led to concerns from marine scientists and environmental groups over the impact on fragile deep sea ecosystems. Knowledge of potential impacts is limited due to limited research on deep sea life.|2023-09-22-20-01-00
Environmental effects of mining|Lithium mining| Lithium does not occur as the metal naturally since it is highly reactive, but is found combined in small amounts in rocks, soils, and bodies of water.  The extraction of lithium in rock form can be exposed to air, water, and soil.  Furthermore, batteries are globally demanded for containing lithium in regards to manufacturing, the toxic chemicals that lithium produce can negatively impact humans, soils, and marine species.  Lithium production increased by 25% between 2000 and 2007 for the use of batteries, and the major sources of lithium are found in brine lake deposits.  Lithium is discovered and extracted from 150 minerals, clays, numerous brines, and sea water, and although lithium extraction from rock-form is twice as expensive from that of lithium extracted from brines, the average brine deposit is greater than in comparison to an average lithium hard rock deposit.|2023-09-22-20-01-00
Environmental effects of mining|Phosphate mining| Phosphate-bearing rocks are mined to produce phosphorus, an essential element used in industry and agriculture.  The process of extraction includes removal of surface vegetation, thereby exposing phosphorus rocks to the terrestrial ecosystem, damaging the land area with exposed phosphorus, resulting in ground erosion.  The products released from phosphate ore mining are wastes, and tailings, resulting in human exposure to particulate matter from contaminated tailings via inhalation and the toxic elements that impact human health are (Cd, Cr, Zn, Cu and Pb).|2023-09-22-20-01-00
Environmental effects of mining|Oil shale mining| Oil shale is a sedimentary rock containing kerogen which hydrocarbons can be produced. Mining oil shale impacts the environment it can damage the biological land and ecosystems. The thermal heating and combustion generate a lot of material and waste that includes carbon dioxide and greenhouse gas. Many environmentalists are against the production and usage of oil shale because it creates large amounts of greenhouse gasses. Among air pollution, water contamination is a huge factor mainly because oil shales are dealing with oxygen and hydrocarbons.   There is changes in the landscape with mining sites due to oil shale mining and the production using chemical products.  The ground movements within the area of underground mining is a problem that is long-term because it causes non-stabilized areas. Underground mining causes a new formation that can be suitable for some plant growth, but rehabilitation could be required.|2023-09-22-20-01-00
Environmental effects of mining|Mountaintop removal mining| Mountaintop removal mining (MTR) occurs when trees are cut down, and coal seams are removed by machines and explosives.  As a result the landscape is more susceptible to flash flooding and causing potential pollution from the chemicals.  The critical zone disturbed by mountaintop removal causes degraded stream water quality towards the marine and terrestrial ecosystems and thus mountaintop removal mining affect hydrologic response and long-term watersheds.|2023-09-22-20-01-00
Environmental effects of mining|Sand mining| Sand mining and gravel mining creates large pits and fissures in the earth's surface. At times, mining can extend so deeply that it affects ground water, springs, underground wells, and the water table.  The major threats of sand mining activities include channel bed degradation, river formation and erosion.  Sand mining has resulted in an increase of water turbidity in the majority offshore of Lake Hongze, the fourth largest freshwater lake located in China.|2023-09-22-20-01-00
Environmental effects of mining|Mitigation| To ensure completion of reclamation, or restoring mine land for future use, many governments and regulatory authorities around the world require that mining companies post a bond to be held in escrow until productivity of reclaimed land has been convincingly demonstrated, although if cleanup procedures are more expensive than the size of the bond, the bond may simply be abandoned. Since 1978 the mining industry has reclaimed more than 2 million acres (8,000 km2) of land in the United States alone. This reclaimed land has renewed vegetation and wildlife in previous mining lands and can even be used for farming and ranching.|2023-09-22-20-01-00
Exhaust gas|General| Lists Categories Exhaust gas or flue gas is emitted as a result of the combustion of fuels such as natural gas, gasoline (petrol), diesel fuel, fuel oil, biodiesel blends,  or coal. According to the type of engine, it is discharged into the atmosphere through an exhaust pipe, flue gas stack, or propelling nozzle. It often disperses downwind in a pattern called an exhaust plume. It is a major component of motor vehicle emissions (and from stationary internal combustion engines), which can also include crankcase blow-by and evaporation of unused gasoline. Motor vehicle emissions are a common source of air pollution and are a major ingredient in the creation of smog in some large cities.  A 2013 study by the Massachusetts Institute of Technology (MIT) indicates that 53,000 early deaths occur per year in the United States alone because of vehicle emissions.  According to another study from the same university, traffic fumes alone cause the death of 5,000 people every year just in the United Kingdom.|2023-08-21-05-01-42
Exhaust gas|Composition|" The largest part of most combustion gas is nitrogen (N2), water vapor (H2O) (except with pure-carbon fuels), and carbon dioxide (CO2) (except for fuels without carbon); these are not toxic or noxious (although water vapor and carbon dioxide are greenhouse gases that contribute to climate change). A relatively small part of combustion gas is undesirable, noxious, or toxic substances, such as carbon monoxide (CO) from incomplete combustion, hydrocarbons (properly indicated as CxHy, but typically shown simply as ""HC"" on emissions-test slips) from unburnt fuel, nitrogen oxides (NOx) from excessive combustion temperatures, and particulate matter (mostly soot)."|2023-08-21-05-01-42
Exhaust gas|Exhaust gas temperature| Exhaust gas temperature (EGT) is important to the functioning of the catalytic converter of an internal combustion engine. It may be measured by an exhaust gas temperature gauge. EGT is also a measure of engine health in gas-turbine engines (see below).|2023-08-21-05-01-42
Exhaust gas|Cold engines| During the first two minutes after starting the engine of a car that has not been operated for several hours, the amount of emissions can be very high. This occurs for two main reasons:|2023-08-21-05-01-42
Exhaust gas|Passenger car emissions summary| Comparable with the European emission standards EURO III as it was applied on October 2000 In 2000, the United States Environmental Protection Agency began to implement more stringent emissions standards for light duty vehicles. The requirements were phased in beginning with 2004 vehicles and all new cars and light trucks were required to meet the updated standards by the end of 2007.|2023-08-21-05-01-42
Exhaust gas|Internal-combustion engines|" In spark-ignition engines the gases resulting from combustion of the fuel and air mix are called exhaust gases. The composition varies from petrol to diesel engines, but is around these levels: The 10% oxygen for ""diesel"" is likely if the engine was idling, e.g. in a test rig. It is much less if the engine is running under load, although diesel engines always operate with an excess of air over fuel.[citation needed]
The CO content for petrol engines varies from ~ 15 ppm for well tuned engine with fuel injection and a catalytic converter up to 100,000 ppm (10%) for a richly tuned carburetor engine, such as typically found on small generators and garden equipment. Exhaust gas from an internal combustion engine whose fuel includes nitromethane will contain nitric acid vapour, which is corrosive, and when inhaled causes a muscular reaction making it impossible to breathe. People who are likely to be exposed to it should wear a gas mask. In jet engines and rocket engines, exhaust from propelling nozzles which in some applications shows shock diamonds.[citation needed]"|2023-08-21-05-01-42
Exhaust gas|Other types| In steam engine terminology the exhaust is steam that is now so low in pressure that it can no longer do useful work.|2023-08-21-05-01-42
Exhaust gas|NOx| Mono-nitrogen oxides NO and NO2 (NOx)(whether produced this way or naturally by lightning) react with ammonia, moisture, and other compounds to form nitric acid vapor and related particles. Small particles can penetrate deeply into sensitive lung tissue and damage it, causing premature death in extreme cases. Inhalation of NO species increases the risk of lung cancer  and colorectal cancer.  and inhalation of such particles may cause or worsen respiratory diseases such as emphysema and bronchitis and heart disease. In a 2005 U.S. EPA study the largest emissions of NOx came from on road motor vehicles, with the second largest contributor being non-road equipment which is mostly gasoline and diesel stations. The resulting nitric acid may be washed into soil, where it becomes nitrate, which is useful to growing plants.|2023-08-21-05-01-42
Exhaust gas|Volatile organic compounds| When oxides of nitrogen (NOx) and volatile organic compounds (VOCs) react in the presence of sunlight, ground level ozone is formed, a primary ingredient in smog.  A 2005 U.S. EPA report gives road vehicles as the second largest source of VOCs in the U.S. at 26% and 19% are from non road equipment which is mostly gasoline and diesel stations.   27% of VOC emissions are from solvents which are used in the manufacturer of paints and paint thinners and other uses.|2023-08-21-05-01-42
Exhaust gas|Ozone| Ozone is beneficial in the upper atmosphere,  but at ground level ozone irritates the respiratory system, causing coughing, choking, and reduced lung capacity.  It also has many negative effects throughout the ecosystem.|2023-08-21-05-01-42
Exhaust gas|Carbon monoxide (CO)| Carbon monoxide poisoning is the most common type of fatal air poisoning in many countries.  Carbon monoxide is colorless, odorless and tasteless, but highly toxic. It combines with hemoglobin to produce carboxyhemoglobin, which blocks the transport of oxygen. At concentrations above 1000ppm it is considered immediately dangerous and is the most immediate health hazard from running engines in a poorly ventilated space. In 2011, 52% of carbon monoxide emissions were created by mobile vehicles in the U.S.|2023-08-21-05-01-42
Exhaust gas|Hazardous air pollutants (toxics)| Chronic (long-term) exposure to benzene (C6H6) damages bone marrow. It can also cause excessive bleeding and depress the immune system, increasing the chance of infection.  Benzene causes leukemia and is associated with other blood cancers and pre-cancers of the blood.|2023-08-21-05-01-42
Exhaust gas|Particulate matter (PM10 and PM2.5)| The health effects of inhaling airborne particulate matter have been widely studied in humans and animals and include asthma, lung cancer, cardiovascular issues, premature death.     Because of the size of the particles, they can penetrate the deepest part of the lungs.   A 2011 UK study estimates 90 deaths per year due to passenger vehicle PM.   In a 2006 publication, the U.S. Federal Highway Administration (FHWA) state that in 2002 about 1 per-cent of all PM10 and 2 per-cent of all PM2.5 emissions came from the exhaust of on-road motor vehicles (mostly from diesel engines).  In Chinese, European, and Indian markets, both diesel and gasoline vehicles are required to have a tailpipe filter installed, while the United States has mandated it for diesel only. In 2022, British testing specialist Emissions Analytics estimated that the 300 million or so gasoline vehicles in the US over the subsequent decade would emit around 1.6 septillion harmful particles.|2023-08-21-05-01-42
Exhaust gas|Carbon dioxide (CO2)| Carbon dioxide is a greenhouse gas.  Motor vehicle CO2 emissions are part of the anthropogenic contribution to the growth of CO2 concentrations in the atmosphere which according to the vast majority of the scientific community is causing climate change.   Motor vehicles are calculated to generate about 20% of the European Union's man-made CO2 emissions, with passenger cars contributing about 12%.  European emission standards limit the CO2 emissions of new passenger cars and light vehicles. The European Union average new car CO2 emissions figure dropped by 5.4% in the year to the first quarter of 2010, down to 145.6 g/km.|2023-08-21-05-01-42
Exhaust gas|Water vapour| Vehicle exhaust contains much water vapour. There has been research into ways that troops in deserts can recover drinkable water from their vehicles' exhaust gases.|2023-08-21-05-01-42
Exhaust gas|Pollution reduction| Emission standards focus on reducing pollutants contained in the exhaust gases from vehicles as well as from industrial flue gas stacks and other air pollution exhaust sources in various large-scale industrial facilities such as petroleum refineries, natural gas processing plants, petrochemical plants and chemical production plants.   However, these are often referred to as flue gases. Catalytic converters in cars intend to break down the pollution of exhaust gases using a catalyst. Scrubbers in ships intend to remove the sulfur dioxide (SO2) of marine exhaust gases. The regulations on marine sulfur dioxide emissions are tightening, however only a small number of special areas worldwide have been designated for low sulfur diesel fuel use only. One of the advantages claimed for advanced steam technology engines is that they produce smaller quantities of toxic pollutants (e.g. oxides of nitrogen) than petrol and diesel engines of the same power.[citation needed]  They produce larger quantities of carbon dioxide but less carbon monoxide due to more efficient combustion.|2023-08-21-05-01-42
Exhaust gas|Health studies| Researchers from the University of California, Los Angeles School of Public Health say preliminary results of their statistical study of children listed in the California Cancer Registry born between 1998 and 2007 found that traffic pollution may be associated with a 5% to 15% increase in the likelihood of some cancers.   A World Health Organization study found that diesel fumes cause an increase in lung cancer.|2023-08-21-05-01-42
Exhaust gas|Localised effects| The California Air Resources Board found in studies that 50% or more of the air pollution (smog) in Southern California is due to car emissions.[citation needed] Concentrations of pollutants emitted from combustion engines may be particularly high around signalized intersections because of idling and accelerations. Computer models often miss this kind of detail.|2023-08-21-05-01-42
Eyesore|General| An eyesore is something that is largely considered to look unpleasant or ugly. Its technical usage is as an alternative perspective to the notion of landmark. Common examples include dilapidated buildings, graffiti, litter, polluted areas, and excessive commercial signage such as billboards. Some eyesores may be a matter of opinion such as controversial modern architecture (see also spite house), transmission towers or wind turbines.    Natural eyesores include feces, mud and weeds.|2023-09-13-22-10-25
Eyesore|Effect on property values| In the US, the National Association of Realtors says an eyesore can shave about 10 percent off the value of a nearby listing.|2023-09-13-22-10-25
Eyesore|Remediation| Clean-up programmes to improve or remove eyesores are often started by local bodies or even national governments. These are frequently called Operation Eyesore.  High-profile international events such as the Olympic Games usually trigger such activity. Others contend that it is best to address these problems while they are small, since signs of neglect encourage anti-social behaviour such as vandalism and fly-tipping. This strategy is known as fixing broken windows.|2023-09-13-22-10-25
Eyesore|Controversy| Whether some constructions are eyesores is a matter of opinion which may change over time. Landmarks are often called eyesores.|2023-09-13-22-10-25
Firewater (fire fighting)|General| Lists Categories Firewater refers to water that has been used in firefighting and requires disposal. In many cases, it is a highly polluting material and requires special care in its disposal.|2023-03-04-10-33-41
Firewater (fire fighting)|Description| In many firefighting situations, large quantities of water remain after the fire has been extinguished. This firewater contains materials present in the building and also contains dissolved and particulate materials from combustion processes and materials generated through quenching. Firewater can be particularly polluting when the building or site being extinguished itself contains potentially polluting materials such as pesticides, organic and inorganic chemical reagents, fertilizers, etc. Certain premises, including farms and the chemical industry, pose special risks because of the types of materials present. Premises containing quantities of plastics can also cause severe problems because of the taste and odor imparted to the firewater. Releasing contaminated firewater into a river, lake or other body that supplies drinking water may render the untreated supply unsuitable for drinking or food preparation. Managing firewater frequently requires that the water be contained on the site until removed from a specialized treat statement. One of the recognized techniques is to contain the firewater in the drainage system using pneumatic bladders or lockable non-return valves, which can be activated automatically or manually.|2023-03-04-10-33-41
Firewater (fire fighting)|Containment| Firewater containment is the process of containing the run-off from fighting fires. Firewater contains many hazardous substances that result from combustion, which can turn safe materials into toxic, polluting and environmentally damaging substances. The preferred method of firewater containment is to use pneumatic bladders/drain stoppers that block the outflow from the drain or pneumatic non-return valves, both of which can convert the drains into containment vessels (called sumps) from which the firewater can be pumped away into tankers for safe disposal. Firewater containment is one of the many environmental factors considered alongside spill and pollution containment as an essential part of any company's environmental policy for ISO14001 accreditation. Firewater runoff often leaks into the surrounding environment through different routes such as rain, sprinkler systems (for example) or others.  Containment of firewater is an integral component of preventing contamination of drainage and sewage systems, rivers, streams, and more. Pollution caused by firewater can last for hundreds of years following the initial use, making cost-effective and practical innovations to the current firewater containment system necessary for both the environment and businesses. Many of the largest negative environmental impacts due to firefighting related activities occur because of firewater runoff, making its containment necessary. Firewater recycling is often considered a type of firewater containment and disposal to reduce water use and pollution, but the means to do so require further research.  Compact and mobile filtration units are proposed for this task to contribute to the spray and foaming of contaminated water for firefighters. Increased recycling of firewater has allowed a surplus of benefits that have not been fully researched. Although, recycling is highly recommended by several countries. The table below describes a corresponding overview of commercially available firewater in-drain spill and pollution containment system examples. Products such as Flapstopper and similar technology provide the latest efficient state-of-the-art technology. Isolation valves are often used to prevent firewater from escaping the site of a fire until it can properly be removed. CIRIA C736 Containment systems for the prevention of pollution, a central industry guidance document in the United Kingdom designed to assist owners/operators of facilities storing potentially hazardous substances, exists as a response to faulty containment aiming to aid commercial and industrial facilities in the containment of potential firewater use.|2023-03-04-10-33-41
Firewater (fire fighting)|Pollution and notable events| Firewater's main association with pollution is its ability to rapidly spread hazardous substances if not correctly contained following use for firefighting; firewater run-off is often the culprit in or a main contributor to many chemical spill pollution events (see Water pollution). The Sandoz chemical spill of 1986, for example, turned the Rhine river red with pollutants and affected much of the wildlife due to faulty containment of the firewater used in treating an agrochemical warehouse fire,  releasing 30 tonnes of toxic chemicals into the river. Firewater containment and retention is an important issue because they can prevent the carrying of contaminants far from their sources through to connected bodies of water and neighboring areas. Drinking water, fish stocks, and other water-related necessities are potentially polluted by firewater. The Sandoz fire affected bodies of water connected to the Rhine in Switzerland, France, and Germany, despite the fire occurring only in Switzerland. Often, damage to the environment following a fire at an industrial site occurs because of polluted firewater run-off. Water used in treating a fire may pick up contaminants from the burning object then leak into the surrounding environment when poorly contained.  Rain and other environmental factors can increase the firewater run-off spread of a containment area. The 2013 Smethwick fire involved the burning of 100,000 t of plastic recycling materials and required 14 dam3 of firewater used for treatment within the first 12 hours of the initial burning, all pumped from the Birmingham Canal with the potential to disrupt the natural state of the canal and aid in the carrying of contaminated materials from the fire. The UNECE Safety Guidelines and Good Practices for Fire-water Retention exist as a response to the Sandoz fire, outlining guidelines and proper practices for managing firewater and firewater retention.|2023-03-04-10-33-41
Firewater (fire fighting)|Fire prevention| Containment is the most commonly utilized methods of dealing with highly polluted fire-water, one other method would be the use of water distribution systems to give fire fighters an access to large quantities of water to combat large scale fires. This also gives firefighters access to high velocity water flow, which is known to have reduced toxicity and polluted levels. These, however, can still lead to polluted water. Even with high velocity water, it can still become polluted, even if the levels are indeed lower. In using water as a main source of fire fighting, it is clear that there will always be some level of toxicity in the water that is utilized in the process of stopping these fires. Ultimately, the best method of lessening fire-water is lessening fires. The most successful way of lessening toxicity of water after fire fighting, is giving proper education to the public on preventing fires, in domestic homes and outside.|2023-03-04-10-33-41
Global dimming|General|" Lists Categories The first systematic measurements of global direct irradiance at the Earth's surface began in the 1950s. A decline in irradiance was soon observed, and it was given the name of global dimming. It continued from 1950s until 1980s, with an observed reduction of 4–5% per decade,  even though solar activity did not vary more than the usual at the time.  Global dimming has instead been attributed to an increase in atmospheric particulate matter, predominantly sulfate aerosols, as the result of rapidly growing air pollution due to post-war industrialization. After 1980s, global dimming started to reverse, alongside reductions in particulate emissions, in what has been described as global brightening, although this reversal is only considered ""partial"" for now.  The reversal has also been globally uneven, as the dimming trend continued during the 1990s over some mostly developing countries like India, Zimbabwe, Chile and Venezuela.  Over China, the dimming trend continued at a slower rate after 1990,  and did not begin to reverse until around 2005. Global dimming has interfered with the hydrological cycle by lowering evaporation, which is likely to have reduced rainfall in certain areas,  and may have caused the observed southwards shift of the entire tropical rain belt between 1950 and 1985, with a limited recovery afterwards.  Since high evaporation at the tropics is needed to drive the wet season, cooling caused by particulate pollution appears to weaken Monsoon of South Asia, while reductions in pollution strengthen it.   Multiple studies have also connected record levels of particulate pollution in the Northern Hemisphere to the monsoon failure behind the 1984 Ethiopian famine,    although the full extent of anthropogenic vs. natural influences on that event is still disputed.   On the other hand, global dimming has also counteracted some of the greenhouse gas emissions, effectively ""masking"" the total extent of global warming experienced to date, with the most-polluted regions even experiencing cooling in the 1970s. Conversely, global brightening had contributed to the acceleration of global warming which began in the 1990s. In the near future, global brightening is expected to continue, as nations act to reduce the toll of air pollution on the health of their citizens. This also means that less of global warming would be masked in the future. Climate models are broadly capable of simulating the impact of aerosols like sulfates, and in the IPCC Sixth Assessment Report, they are believed to offset around 0.5 °C (0.90 °F) of warming. Likewise, climate change scenarios incorporate reductions in particulates and the cooling they offered into their projections, and this includes the scenarios for climate action required to meet 1.5 °C (2.7 °F) and 2 °C (3.6 °F) targets.  It is generally believed that the cooling provided by global dimming is similar to the warming derived from atmospheric methane, meaning that simultaneous reductions in both would effectively cancel each other out.  However, uncertainties remain about the models' representation of aerosol impacts on weather systems, especially over the regions with a poorer historical record of atmospheric observations. The processes behind global dimming are similar to those which drive reductions in direct sunlight after volcanic eruptions. In fact, the eruption of Mount Pinatubo in 1991 had temporarily reversed the brightening trend.  Both processes are considered an analogue for stratospheric aerosol injection, a solar geoengineering intervention which aims to counteract global warming through intentional releases of reflective aerosols, albeit at much higher altitudes, where lower quantities would be needed and the polluting effects would be minimized.  However, while that intervention may be very effective at stopping or reversing warming and its main consequences, it would also have substantial effects on the global hydrological cycle, as well as regional weather and ecosystems. Because its effects are only temporary, it would have to be maintained for centuries until the greenhouse gas concentrations are normalized to avoid a rapid and violent return of the warming, sometimes known as termination shock."|2023-09-27-06-13-59
Global dimming|History|" In the late 1960s, Mikhail Ivanovich Budyko worked with simple two-dimensional energy-balance climate models to investigate the reflectivity of ice.  He found that the ice–albedo feedback created a positive feedback loop in the Earth's climate system. The more snow and ice, the more solar radiation is reflected back into space and hence the colder Earth grows and the more it snows. Other studies suggested that sulfate pollution or a volcano eruption could provoke the onset of an ice age. In the 1980s, research in Israel and the Netherlands revealed an apparent reduction in the amount of sunlight,  and Atsumu Ohmura, a geography researcher at the Swiss Federal Institute of Technology, found that solar radiation striking the Earth's surface had declined by more than 10% over the three previous decades, even as the global temperature had been generally rising since the 1970s.  In the 1990s, this was followed by the papers describing multi-decade declines in Estonia,  Germany  and across the former Soviet Union,  which prompted the researcher Gerry Stanhill to coin the term ""global dimming"".  Subsequent research estimated an average reduction in sunlight striking the terrestrial surface of around 4–5% per decade over late 1950s–1980s, and 2–3% per decade when 1990s were included.     Notably, solar radiation at the top of the atmosphere did not vary by more than 0.1-0.3% in all that time, strongly suggesting that the reasons for the dimming were on Earth.  Additionally, only visible light and infrared radiation were dimmed, rather than the ultraviolet part of the spectrum."|2023-09-27-06-13-59
Global dimming|Reversal| Starting from 2005, scientific papers began to report that after 1990, the global dimming trend had clearly switched to global brightening.      This followed measures taken to combat air pollution by the developed nations, typically through flue-gas desulfurization installations at thermal power plants, such as wet scrubbers or fluidized bed combustion.   In the United States, sulfate aerosols have declined significantly since 1970 with the passage of the Clean Air Act, which was strengthened in 1977 and 1990. According to the EPA, from 1970 to 2005, total emissions of the six principal air pollutants, including sulfates, dropped by 53% in the US.  By 2010, this reduction in sulfate pollution led to estimated healthcare cost savings valued at $50 billion annually.  Similar measures were taken in Europe,  such as the 1985 Helsinki Protocol on the Reduction of Sulfur Emissions under the Convention on Long-Range Transboundary Air Pollution, and with similar improvements. On the other hand, a 2009 review found that dimming continued in China after stabilizing in the 1990s and intensified in India, consistent with their continued industrialization, while the US, Europe, and South Korea continued to brighten. Evidence from Zimbabwe, Chile and Venezuela also pointed to continued dimming during that period, albeit at a lower confidence level due to the lower number of observations.   Due to these contrasting trends, no statistically significant change had occurred on a global scale from 2001 to 2012.  Post-2010 observations indicate that the global decline in aerosol concentrations and global dimming continued, with pollution controls on the global shipping industry playing a substantial role in the recent years.  Since nearly 90% of the human population lives in the Northern Hemisphere, clouds there are far more affected by aerosols than in the Southern Hemisphere, but these differences have halved in the two decades since 2000, providing further evidence for the ongoing global brightening.|2023-09-27-06-13-59
Global dimming|Causes|" Global dimming had been widely attributed to the increased presence of aerosol particles in Earth's atmosphere, predominantly those of sulfates.  While natural dust is also an aerosol with some impacts on climate, and volcanic eruptions considerably increase sulfate concentrations in the short term, these effects have been dwarfed by increases in sulfate emissions since the start of the Industrial Revolution.  According to the IPCC First Assessment Report, the global human-caused emissions of sulfur into the atmosphere were less than 3 million tons per year in 1860, yet they increased to 15 million tons in 1900, 40 million tons in 1940 and about 80 millions in 1980. This meant that the human-caused emissions became ""at least as large"" as all natural emissions of sulfur-containing compounds: the largest natural source, emissions of dimethyl sulfide from the ocean, was estimated at 40 million tons per year, while volcano emissions were estimated at 10 million tons. Moreover, that was the average figure: according to the report, ""in the industrialized regions of Europe and North America, anthropogenic emissions dominate over natural emissions by about a factor of ten or even more"". Aerosols and other atmospheric particulates have direct and indirect effects on the amount of sunlight received at the surface. Directly, particles of sulfur dioxide reflect almost all sunlight, like tiny mirrors.  On the other hand, incomplete combustion of fossil fuels (such as diesel) and wood releases particles of black carbon (predominantly soot), which absorb solar energy and heat up, reducing the overall amount of sunlight received on the surface while also contributing to warming. Black carbon is an extremely small component of air pollution at land surface levels, yet it has a substantial heating effect on the atmosphere at altitudes above two kilometers (6,562 ft). Indirectly, the pollutants affect the climate by acting as nuclei, meaning that water droplets in clouds coalesce around the particles.  Increased pollution causes more particulates and thereby creates clouds consisting of a greater number of smaller droplets (that is, the same amount of water is spread over more droplets). The smaller droplets make clouds more reflective, so that more incoming sunlight is reflected back into space and less reaches the Earth's surface. This same effect also reflects radiation from below, trapping it in the lower atmosphere. In models, these smaller droplets also decrease rainfall.  In the 1990s, experiments comparing the atmosphere over the northern and southern islands of the Maldives, showed that the effect of macroscopic pollutants in the atmosphere at that time (blown south from India) caused about a 10% reduction in sunlight reaching the surface in the area under the Asian brown cloud – a much greater reduction than expected from the presence of the particles themselves.  Prior to the research being undertaken, predictions were of a 0.5–1% effect from particulate matter; the variation from prediction may be explained by cloud formation with the particles acting as the focus for droplet creation."|2023-09-27-06-13-59
Global dimming|Relationship to climate change|" It has been understood for a long time that any effect on solar irradiance from aerosols would necessarily impact Earth's radiation balance. Reductions in atmospheric temperatures have already been observed after large volcanic eruptions such as the 1963 eruption of Mount Agung in Bali, 1982 El Chichón eruption in Mexico, 1985 Nevado del Ruiz eruption in Colombia and 1991 eruption of Mount Pinatubo in the Philippines. However, even the major eruptions only result in temporary jumps of sulfur particles, unlike the more sustained increases caused by the anthropogenic pollution.  In 1990, the IPCC First Assessment Report acknowledged that ""Human-made aerosols, from sulphur emitted largely in fossil fuel combustion can modify clouds and this may act to lower temperatures"", while ""a decrease in emissions of sulphur might be expected to increase global temperatures"". However, lack of observational data and difficulties in calculating indirect effects on clouds left the report unable to estimate whether the total impact of all anthropogenic aerosols on the global temperature amounted to cooling or warming.  By 1995, the IPCC Second Assessment Report had confidently assessed the overall impact of aerosols as negative (cooling);  however, aerosols were recognized as the largest source of uncertainty in future projections in that report and the subsequent ones. At the peak of global dimming, it was able to counteract the warming trend completely, but by 1975, the continually increasing concentrations of greenhouse gases have overcome the masking effect and dominated ever since.  Even then, regions with high concentrations of sulfate aerosols due to air pollution had initially experienced cooling, in contradiction to the overall warming trend.  The eastern United States was a prominent example: the temperatures there declined by 0.7 °C (1.3 °F) between 1970 and 1980, and by up to 1 °C (1.8 °F) in the Arkansas and Missouri. As the sulfate pollution was reduced, the central and eastern United States had experienced warming of 0.3 °C (0.54 °F) between 1980 and 2010,  even as sulfate particles still accounted for around 25% of all particulates.  By 2021, the northeastern coast of the United States was instead one of the fastest-warming regions of North America, as the slowdown of the Atlantic Meridional Overturning Circulation increased temperatures in that part of the North Atlantic Ocean. Globally, the emergence of extreme heat beyond the preindustrial records was delayed by aerosol cooling, and hot extremes accelerated as global dimming abated: it has been estimated that since the mid-1990s, peak daily temperatures in northeast Asia and hottest days of the year in Western Europe would have been substantially less hot if aerosol concentrations had stayed the same as before.  In Europe, the declines in aerosol concentrations since the 1980s had also reduced the associated fog, mist and haze: altogether, it was responsible for about 10–20% of daytime warming across Europe, and about 50% of the warming over the more polluted Eastern Europe.  Because aerosol cooling depends on reflecting sunlight, air quality improvements had a negligible impact on wintertime temperatures,  but had increased temperatures from April to September by around 1 °C (1.8 °F) in Central and Eastern Europe.  Some of the acceleration of sea level rise, as well as Arctic amplification and the associated Arctic sea ice decline, was also attributed to the reduction in aerosol masking. Pollution from black carbon, mostly represented by soot, also contributes to global dimming. However, because it absorbs heat instead of reflecting it, it warms the planet instead of cooling it like sulfates. This warming is much weaker than that of greenhouse gases, but it can be regionally significant when black carbon is deposited over ice masses like mountain glaciers and the Greenland ice sheet, where it reduces their albedo and increases their absorption of solar radiation.  Even the indirect effect of soot particles acting as cloud nuclei is not strong enough to provide cooling: the ""brown clouds"" formed around soot particles were known to have a net warming effect since the 2000s.  Black carbon pollution is particularly strong over India, and as the result, it is considered to be one of the few regions where cleaning up air pollution would reduce, rather than increase, warming. Since changes in aerosol concentrations already have an impact on the global climate, they would necessarily influence future projections as well. In fact, it is impossible to fully estimate the warming impact of all greenhouse gases without accounting for the counteracting cooling from aerosols. Climate models started to account for the effects of sulfate aerosols around the IPCC Second Assessment Report; when the IPCC Fourth Assessment Report was published in 2007, every climate model had integrated sulfates, but only 5 were able to account for less impactful particulates like black carbon.  By 2021, CMIP6 models estimated total aerosol cooling in the range from 0.1 °C (0.18 °F) to 0.7 °C (1.3 °F);  The IPCC Sixth Assessment Report selected the best estimate of a 0.5 °C (0.90 °F) cooling provided by sulfate aerosols, while black carbon amounts to about 0.1 °C (0.18 °F) of warming.  While these values are based on combining model estimates with observational constraints, including those on ocean heat content,  the matter is not yet fully settled. The difference between model estimates mainly stems from disagreements over the indirect effects of aerosols on clouds.   While it is well known that aerosols increase the number of cloud droplets and this makes the clouds more reflective, calculating how liquid water path, an important cloud property, is affected by their presence is far more challenging, as it involves computationally heavy continuous calculations of evaporation and condensation within clouds. Climate models generally assume that aerosols increase liquid water path, which makes the clouds even more reflective. However, satellite observations taken in 2010s suggested that aerosols decreased liquid water path instead, and in 2018, this was reproduced in a model which integrated more complex cloud microphysics.  Yet, 2019 research found that earlier satellite observations were biased by failing to account for the thickest, most water-heavy clouds naturally raining more and shedding more particulates: very strong aerosol cooling was seen when comparing clouds of the same thickness.  Moreover, large-scale observations can be confounded by changes in other atmospheric factors, like humidity: i.e. it was found that while post-1980 improvements in air quality would have reduced the number of clouds over the East Coast of the United States by around 20%, this was offset by the increase in relative humidity caused by atmospheric response to AMOC slowdown.  Similarly, while the initial research looking at sulfates from the 2014–2015 eruption of Bárðarbunga found that they caused no change in liquid water path,  it was later suggested that this finding was confounded by counteracting changes in humidity.  To avoid confounders, many observations of aerosol effects focus on ship tracks, but post-2020 research found that visible ship tracks are a poor proxy for other clouds, and estimates derived from them overestimate aerosol cooling by as much as 200%.  At the same time, other research found that the majority of ship tracks are ""invisible"" to satellites, meaning that the earlier research had underestimated aerosol cooling by overlooking them.  Finally, 2023 research indicates that all climate models have underestimated sulfur emissions from volcanoes which occur in the background, outside of major eruptions, and so had consequently overestimated the cooling provided by anthropogenic aerosols, especially in the Arctic climate. Regardless of the current strength of aerosol cooling, all future climate change scenarios project decreases in particulates and this includes the scenarios where 1.5 °C (2.7 °F) and 2 °C (3.6 °F) targets are met: their specific emission reduction targets assume the need to make up for lower dimming.  Since models estimate that the cooling caused by sulfates is largely equivalent to the warming caused by atmospheric methane (and since methane is a relatively short-lived greenhouse gas), it is believed that simultaneous reductions in both would effectively cancel each other out.  Yet, in the recent years, methane concentrations had been increasing at rates exceeding their previous period of peak growth in the 1980s,   with wetland methane emissions driving much of the recent growth,   while air pollution is getting cleaned up aggressively.  These trends are some of the main reasons why 1.5 °C (2.7 °F) warming is now expected around 2030, as opposed to the mid-2010s estimates where it would not occur until 2040. It has also been suggested that aerosols are not given sufficient attention in regional risk assessments, in spite of being more influential on a regional scale than globally.  For instance, a climate change scenario with high greenhouse gas emissions but strong reductions in air pollution would see 0.2 °C (0.36 °F)  more global warming by 2050 than the same scenario with little improvement in air quality, but regionally, the difference would add 5 more tropical nights per year in northern China and substantially increase precipitation in northern China and northern India.  Likewise, a paper comparing current level of clean air policies with a hypothetical maximum technically feasible action under otherwise the same climate change scenario found that the latter would increase the risk of temperature extremes by 30–50% in China and in Europe.  Unfortunately, because historical records of aerosols are sparser in some regions than in others, accurate regional projections of aerosol impacts are difficult. Even the latest CMIP6 climate models can only accurately represent aerosol trends over Europe,  but struggle with representing North America and Asia, meaning that their near-future projections of regional impacts are likely to contain errors as well."|2023-09-27-06-13-59
Global dimming|Aircraft contrails and lockdowns|" In general, aircraft contrails (also called vapor trails) are believed to trap outgoing longwave radiation emitted by the Earth and atmosphere more than they reflect incoming solar radiation, resulting in a net increase in radiative forcing. In 1992, this warming effect was estimated between 3.5 mW/m2 and 17 mW/m2. 
Global radiative forcing impact of aircraft contrails has been calculated from the reanalysis data, climate models, and radiative transfer codes; estimated at 12 mW/m2 for 2005, with an uncertainty range of 5 to 26 mW/m2, and with a low level of scientific understanding.  Contrail cirrus may be air traffic's largest radiative forcing component, larger than all CO2 accumulated from aviation, and could triple from a 2006 baseline to 160–180 mW/m2 by 2050 without intervention.   For comparison, the total radiative forcing from human activities amounted to 2.72 W/m2 (with a range between 1.96 and 3.48W/m2) in 2019, and the increase from 2011 to 2019 alone amounted to 0.34W/m2. Contrail effects differ a lot depending on when they are formed, as they decrease the daytime temperature and increase the nighttime temperature, reducing their difference.  In 2006, it was estimated that night flights contribute 60 to 80% of contrail radiative forcing while accounting for 25% of daily air traffic, and winter flights contribute half of the annual mean radiative forcing while accounting for 22% of annual air traffic.  Starting from the 1990s, it was suggested that contrails during daytime have a strong cooling effect, and when combined with the warming from night-time flights, this would lead to a substantial diurnal temperature variation (the difference in the day's highs and lows at a fixed station).  When no commercial aircraft flew across the USA following the September 11 attacks, the diurnal temperature variation was widened by 1.1 °C (2.0 °F). 
Measured across 4,000 weather stations in the continental United States, this increase was the largest recorded in 30 years.  Without contrails, the local diurnal temperature range was 1 °C (1.8 °F) higher than immediately before.  In the southern US, the difference was diminished by about 3.3 °C (6 °F), and by 2.8 °C (5 °F) in the US midwest.   However, follow-up studies found that a natural change in cloud cover can more than explain these findings.  The authors of a 2008 study wrote, ""The variations in high cloud cover, including contrails and contrail-induced cirrus clouds, contribute weakly to the changes in the diurnal temperature range, which is governed primarily by lower altitude clouds, winds, and humidity."" A 2011 study of British meteorological records taken during World War II identified one event where the temperature was 0.8 °C (1.4 °F) higher than the day's average near airbases used by USAAF strategic bombers after they flew in a formation, although they cautioned it was a single event. The global response to the 2020 coronavirus pandemic led to a reduction in global air traffic of nearly 70% relative to 2019. Thus, it provided an extended opportunity to study the impact of contrails on regional and global temperature. Multiple studies found ""no significant response of diurnal surface air temperature range"" as the result of contrail changes, and either ""no net significant global ERF"" (effective radiative forcing) or a very small warming effect.    On the other hand, the decline in sulfate emissions caused by the curtailed road traffic and industrial output during the COVID-19 lockdowns did have a detectable warming impact: it was estimated to have increased global temperatures by 0.01–0.02 °C (0.018–0.036 °F) initially and up to 0.03 °C (0.054 °F) by 2023, before disappearing. Regionally, the lockdowns were estimated to increase temperatures by 0.05–0.15 °C (0.090–0.270 °F) in eastern China over January–March, and then by 0.04–0.07 °C (0.072–0.126 °F) over Europe, eastern United States, and South Asia in March–May, with the peak impact of 0.3 °C (0.54 °F) in some regions of the United States and Russia.   In the city of Wuhan, the urban heat island effect was found to have decreased by 0.24 °C (0.43 °F) at night and by 0.12 °C (0.22 °F) overall during the strictest lockdowns."|2023-09-27-06-13-59
Global dimming|Relationship to hydrological cycle|" On regional and global scale, air pollution can affect the water cycle, in a manner similar to some natural processes. One example is the impact of Sahara dust on hurricane formation: air laden with sand and mineral particles moves over the Atlantic Ocean, where they block some of the sunlight from reaching the water surface, slightly cooling it and dampening the development of hurricanes.  Likewise, it has been suggested since the early 2000s that since aerosols decrease solar radiation over the ocean and hence reduce evaporation from it, they would be ""spinning down the hydrological cycle of the planet.""   In 2011, it was found that anthropogenic aerosols had been the predominant factor behind 20th century changes in rainfall over the Atlantic Ocean sector,  when the entire tropical rain belt shifted southwards between 1950 and 1985, with a limited northwards shift afterwards.  Future reductions in aerosol emissions are expected to result in a more rapid northwards shift, with limited impact in the Atlantic but a substantially greater impact in the Pacific. Most notably, multiple studies connect aerosols from the Northern Hemisphere to the failed monsoon in sub-Saharan Africa during the 1970s and 1980s, which then led to the Sahel drought and the associated famine.    However, model simulations of Sahel climate are very inconsistent,  so it's difficult to prove that the drought would not have occurred without aerosol pollution, although it would have clearly been less severe.   Some research indicates that those models which demonstrate warming alone driving strong precipitation increases in the Sahel are the most accurate, making it more likely that sulfate pollution was to blame for overpowering this response and sending the region into drought. Another dramatic finding had connected the impact of aerosols with the weakening of the Monsoon of South Asia. It was first advanced in 2006,  yet it also remained difficult to prove.  In particular, some research suggested that warming itself increases the risk of monsoon failure, potentially pushing it past a tipping point.   By 2021, however, it was concluded that global warming consistently strengthened the monsoon,  and some strengthening was already observed in the aftermath of lockdown-caused aerosol reductions. In 2009, an analysis of 50 years of data found that light rains had decreased over eastern China, even though there was no significant change in the amount of water held by the atmosphere. This was attributed to aerosols reducing droplet size within clouds, which led to those clouds retaining water for a longer time without raining.  The phenomenon of aerosols suppressing rainfall through reducing cloud droplet size has been confirmed by subsequent studies.  Later research found that aerosol pollution over South and East Asia didn't just suppress rainfall there, but also resulted in more moisture transferred to Central Asia, where summer rainfall had increased as the result.  IPCC Sixth Assessment Report had also linked changes in aerosol concentrations to altered precipitation in the Mediterranean region."|2023-09-27-06-13-59
Global dimming|Solar geoengineering|" An increase in planetary albedo of 1% would eliminate most of radiative forcing from anthropogenic greenhouse gas emissions and thereby global warming, while a 2% albedo increase would negate the warming effect of doubling the atmospheric carbon dioxide concentration.  This is the theory behind solar geoengineering, and the high reflective potential of sulfate aerosols means that they were considered in this capacity for a long time. In 1974, Mikhail Budyko suggested that if global warming became a problem, the planet could be cooled by burning sulfur in the stratosphere, which would create a haze.  This approach would simply send the sulfates to the troposphere – the lowest part of the atmosphere. Using it today would be equivalent to more than reversing the decades of air quality improvements, and the world would face the same issues which prompted the introduction of those regulations in the first place, such as acid rain.  The suggestion of relying on tropospheric global dimming to curb warming has been described as a ""Faustian bargain"" and is not seriously considered by modern research. Instead, starting with the seminal 2006 paper by Paul Crutzen, the solution advocated is known as stratospheric aerosol injection, or SAI. It would transport sulfates into the next higher layer of the atmosphere – stratosphere, where they would last for years instead of weeks, so far less sulfur would have to be emitted.   It has been estimated that the amount of sulfur needed to offset a warming of around 4 °C (7.2 °F) relative to now (and 5 °C (9.0 °F) relative to the preindustrial), under the highest-emission scenario RCP 8.5 would be less than what is already emitted through air pollution today, and that reductions in sulfur pollution from future air quality improvements already expected under that scenario would offset the sulfur used for geoengineering.  The trade-off is increased cost. While there's a popular narrative that stratospheric aerosol injection can be carried out by individuals, small states, or other non-state rogue actors, scientific estimates suggest that cooling the atmosphere by 1 °C (1.8 °F) through stratospheric aerosol injection would cost at least $18 billion annually (at 2020 USD value), meaning that only the largest economies or economic blocs could afford this intervention.   Even so, these approaches would still be ""orders of magnitude"" cheaper than greenhouse gas mitigation,  let alone the costs of unmitigated effects of climate change. The main downside to SAI is that any such cooling would still cease 1–3 years after the last aerosol injection, while the warming from CO2 emissions lasts for hundreds to thousands of years unless they are reversed earlier. This means that neither stratospheric aerosol injection nor other forms of solar geoengineering can be used as a substitute for reducing greenhouse gas emissions, because if solar geoengineering were to cease while greenhouse gas levels remained high, it would lead to ""large and extremely rapid"" warming and similarly abrupt changes to the water cycle. Many thousands of species would likely go extinct as the result. Instead, any solar geoengineering would act as a temporary measure to limit warming while emissions of greenhouse gases are reduced and carbon dioxide is removed, which may well take hundreds of years. Other risks include limited knowledge about the regional impacts of solar geoengineering (beyond the certainty that even stopping or reversing the warming entirely would still result in significant changes in weather patterns in many areas) and, correspondingly, the impacts on ecosystems. It is generally believed that relative to now, crop yields and carbon sinks would be largely unaffected or may even increase slightly, because reduced photosynthesis due to lower sunlight would be offset by CO2 fertilization effect and the reduction in thermal stress, but there's less confidence about how specific ecosystems may be affected. Moreover, stratospheric aerosol injection is likely to somewhat increase mortality from skin cancer due to the weakened ozone layer, but it would also reduce mortality from ground-level ozone, with the net effect unclear. Changes in precipitation are also likely to shift the habitat of mosquitoes and thus substantially affect the distribution and spread of vector-borne diseases, with currently unclear consequences."|2023-09-27-06-13-59
Global distillation|General| Lists Categories Global distillation or the grasshopper effect is the geochemical process by which certain chemicals, most notably persistent organic pollutants (POPs), are transported from warmer to colder regions of the Earth, particularly the poles and mountain tops. Global distillation explains why relatively high concentrations of POPs have been found in the Arctic environment and in the bodies of animals and people who live there, even though most of the chemicals have not been used in the region in appreciable amounts.|2023-01-09-15-33-33
Global distillation|Mechanism| The global distillation process can be understood using the same principles that explain distillations used to make liquor or purify chemicals in a laboratory. In these processes, a substance is vapourised at a relatively high temperature, and then the vapour travels to an area of lower temperature where it condenses. A similar phenomenon occurs on a global scale for certain chemicals. When these chemicals are released into the environment, some evaporates when ambient temperatures are warm,  blows around on winds until temperatures are cooler, and then condensation occurs. Drops in temperature large enough to result in deposition can occur when chemicals are blown from warmer to cooler climates, or when seasons change.  The net effect is atmospheric transport from low to high latitude and altitude. Since global distillation is a relatively slow process that relies on successive evaporation/condensation cycles, it is only effective for semi-volatile chemicals that break down very slowly in the environment, like DDT, polychlorinated biphenyls, and lindane.|2023-01-09-15-33-33
Global distillation|Effect of global distillation| Several studies have measured the effect, usually by correlating the concentrations of a certain chemical in air, water, or biological specimens from various parts of the world with the latitude from which the samples were collected. For example, the levels of PCBs, hexachlorobenzene, and lindane measured in water, lichens, and tree bark have been shown to be greater in higher latitudes. The effect is also used to explain why certain pesticides are found in Arctic and high altitude samples even though there is no agricultural activity in these areas,  and why indigenous peoples of the Arctic have some of the highest body burdens of certain POPs ever measured. Recent studies conclude that for most pollutants slower degradation in colder temperatures is a more important factor in accounting for their accumulation in cold region than global distillation. Exceptions include highly volatile, persistent substances such as chlorofluorocarbons.|2023-01-09-15-33-33
Global plastic pollution treaty|General| UN Member States are currently negotiating a legally-binding, international agreement on plastics that will address the entire life cycle of plastics, from design to production and disposal. On March 2, 2022 UN Member States voted at the resumed fifth UN Environment Assembly (UNEA-5.2) to establish an Intergovernmental Negotiating Committee (INC) with the mandate of advancing a legally-binding international agreement on plastics.    The resolution is entitled “End plastic pollution: Towards an international legally binding instrument.”|2023-08-17-01-59-16
Global plastic pollution treaty|Timeline|" Following UNEA-5.2, The mandate specifies that the INC must begin its work by the end of 2022 with the goal of ""completing a draft global legally binding agreement by the end of 2024."" Work towards the treaty began with the meeting of an Ad Hoc Open-Ended Working Group (OEWG) in Dakar, Senegal from May 30 through June 1, 2022.   During that meeting, Member States established a timeline for subsequent meetings through the end of 2024, rules of procedure, and the initial scope of work for the first meeting of the INC. Until the end of 2024, three more meetings are planned. Kenya, Canada, and the Republic of Korea have offered to host INC-3, INC-4 and INC-5. In 2025, the treaty is to be finalized at the conference of the plenipotentiaries, with Ecuador, Peru, Rwanda, and Senegal as potential hosts"|2023-08-17-01-59-16
Global plastic pollution treaty|Content| Members agreed that the treaty will be international in scope, legally binding, and should address the full life cycle of plastics, including its design, production, and disposal.  It has been argued that chemicals contained in plastics such as additives, processing aids, and nonintentionally added substances need to be addressed, too.|2023-08-17-01-59-16
Global plastic pollution treaty|Support for the treaty| In the lead-up to UNEA-5.2, the majority of UN Member States had expressed their support for advancing a global treaty.  Other groups making public declarations about the need for a treaty include the business sector,  civil society, Indigenous Peoples, workers, trade unions,  waste pickers  and scientists.|2023-08-17-01-59-16
Hazard quotient|General| A hazard quotient is the ratio of the potential exposure to a substance and the level at which no adverse effects are expected. If the Hazard Quotient is calculated to be less than 1, then no adverse health effects are expected as a result of exposure. If the Hazard Quotient is greater than 1, then adverse health effects are possible. The Hazard Quotient cannot be translated to a probability that adverse health effects will occur, and is unlikely to be proportional to risk. It is especially important to note that a Hazard Quotient exceeding 1 does not necessarily mean that adverse effects will occur.|2021-02-02-19-18-03
Hazard quotient|References|" This article is based on material from the public domain U.S. Federal Government document ""NATA Glossary of Terms"" This environment-related article is a stub. You can help Wikipedia by expanding it."|2021-02-02-19-18-03
Haze|General|" Lists Categories Haze is traditionally an atmospheric phenomenon in which dust, smoke, and other dry particulates suspended in air obscure visibility and the clarity of the sky. The World Meteorological Organization manual of codes includes a classification of particulates causing horizontal obscuration into categories of fog, ice fog, steam fog, mist, haze, smoke, volcanic ash, dust, sand, and snow. [dead link] Sources for particles that cause haze include farming (ploughing in dry weather), traffic, industry, windy weather, volcanic activity and wildfires.
Seen from afar (e.g. an approaching airplane) and depending on the direction of view with respect to the Sun, haze may appear brownish or bluish, while mist tends to be bluish grey instead. Whereas haze often is considered a phenomenon occurring in dry air, mist formation is a phenomenon in saturated, humid air. However, haze particles may act as condensation nuclei that leads to the subsequent vapor condensation and formation of mist droplets; such forms of haze are known as ""wet haze"". In meteorological literature, the word haze is generally used to denote visibility-reducing aerosols of the wet type suspended in the atmosphere. Such aerosols commonly arise from complex chemical reactions that occur as sulfur dioxide gases emitted during combustion are converted into small droplets of sulfuric acid when exposed. The reactions are enhanced in the presence of sunlight, high relative humidity, and an absence of air flow (wind). A small component of wet-haze aerosols appear to be derived from compounds released by trees when burning, such as terpenes. For all these reasons, wet haze tends to be primarily a warm-season phenomenon. Large areas of haze covering many thousands of kilometers may be produced under extensive favorable conditions each summer."|2023-06-08-01-45-28
Haze|Air pollution| Haze often occurs when suspended dust and smoke particles accumulate in relatively dry air. When weather conditions block the dispersal of smoke and other pollutants they concentrate and form a usually low-hanging shroud that impairs visibility and may become a respiratory health threat if excessively inhaled. Industrial pollution can result in dense haze, which is known as smog. Since 1991, haze has been a particularly acute problem in Southeast Asia. The main source of the haze has been smoke from fires occurring in Sumatra and Borneo which dispersed over a wide area. In response to the 1997 Southeast Asian haze, the ASEAN countries agreed on a Regional Haze Action Plan (1997) as an attempt to reduce haze. In 2002, all ASEAN countries signed the Agreement on Transboundary Haze Pollution, but the pollution is still a problem there today. Under the agreement, the ASEAN secretariat hosts a co-ordination and support unit.  During the 2013 Southeast Asian haze, Singapore experienced a record high pollution level, with the 3-hour Pollutant Standards Index reaching a record high of 401. In the United States, the Interagency Monitoring of Protected Visual Environments (IMPROVE) program was developed as a collaborative effort between the US EPA and the National Park Service in order to establish the chemical composition of haze in National Parks and establish air pollution control measures in order to restore the visibility of the air to pre-industrial levels.   Additionally, the Clean Air Act requires that any current visibility problems be addressed and remedied, and future visibility problems be prevented, in 156 Class I Federal areas located throughout the United States.  A full list of these areas is available on EPA's website. In addition to the severe health issues caused by haze from air pollution, dust storm particles, and bush fire smoke, reduction in irradiance is the most dominant impact of these sources of haze and a growing issue for photovoltaic production as the solar industry grows.  Smog also lowers agricultural yield and it has been proposed that pollution controls could increase agricultural production in China.  These effects are negative for both sides of agrivoltaics (the combination of photovoltaic electricity production and food from agriculture).|2023-06-08-01-45-28
Haze|International disputes| Haze is no longer just a confined as a domestic problem. It has become one of the causes of international disputes among neighboring countries. Haze can migrate to adjacent countries in the path of wind and thereby pollutes other countries as well, even if haze does not first manifest there. One of the most recent problems occur in Southeast Asia which largely affects the nations of Indonesia, Malaysia and Singapore. In 2013, due to forest fires in Indonesia, Kuala Lumpur and surrounding areas became shrouded in a pall of noxious fumes dispersed from Indonesia, that brings a smell of ash and coal for more than a week, in the country's worst environmental crisis since 1997. The main sources of the haze are Indonesia's Sumatra Island, Indonesian areas of Borneo, and Riau, where farmers, plantation owners and miners have set hundreds of fires in the forests to clear land during dry weather. Winds blew most of the particulates and fumes across the narrow Strait of Malacca to Malaysia, although parts of Indonesia in the path are also affected.  The 2015 Southeast Asian haze was another major crisis of air quality, although there were occasions such as the 2006 and 2019 haze which were less impactful than the three major Southeast Asian haze of 1997, 2013 and 2015.|2023-06-08-01-45-28
Haze|Obscuration| Haze causes issues in the area of terrestrial photography and imaging, where the penetration of large amounts of dense atmosphere may be necessary to image distant subjects. This results in the visual effect of a loss of contrast in the subject, due to the effect of light scattering and reflection through the haze particles. For these reasons, sunrise and sunset colors and possibly the sun itself appear subdued on hazy days, and stars may be obscured by haze at night. In some cases, attenuation by haze is so great that, toward sunset, the sun disappears altogether before even reaching the horizon. Haze can be defined as an aerial form of the Tyndall effect therefore unlike other atmospheric effects such as cloud, mist and fog, haze is spectrally selective in accordance to the electromagnetic spectrum: shorter (blue) wavelengths are scattered more, and longer (red/infrared) wavelengths are scattered less. For this reason, many super-telephoto lenses often incorporate yellow light filters or coatings to enhance image contrast.  Infrared (IR) imaging may also be used to penetrate haze over long distances, with a combination of IR-pass optical filters and IR-sensitive detectors at the intended destination.|2023-06-08-01-45-28
Health effect|General| Health effects (or health impacts) are changes in health resulting from exposure to a source. Health effects are an important consideration in many areas, such as hygiene, pollution studies, occupational safety and health, ([nutrition]) and health sciences in general.  Some of the major environmental sources of health effects are air pollution, water pollution, soil contamination, noise pollution and over-illumination. A non-stochastic or deterministic health effect has a severity that is dependent on dose and is believed to have a threshold level for which no effect is seen. Stochastic health effects occur by chance, generally occurring without a threshold level of dose, whose probability is proportional to the dose and whose severity is independent of the dose, such as cancer and genetic effects. Occasionally, lack of exposure to certain effectors has detrimental consequences on an individual's health. Examples of such effectors include sunlight and exercise.|2023-07-18-23-12-31
Industrial Emissions Directive|General| The Industrial Emissions Directive (Directive 2010/75/EU of the European Parliament and of the Council of 24 November 2010 on industrial emissions (integrated pollution prevention and control)) is a European Union directive which commits European Union member states to control and reduce the impact of industrial emissions on the environment. The directive aims to lower emissions from industrial production through an integrated approach.  The directive uses a polluter pays to assign the cost of the updates to the plant.  The plan to lower emissions is based on Best available technology to help reach the goals of the directive. The plan allows for flexibility given the best available technology; exemptions to the directive can be granted to firms as well if the cost is greater than the benefit.|2023-08-24-20-13-03
Industrial Emissions Directive|Rationale| The European Commission undertook a 2-year review with all stakeholders to examine how the legislation on industrial emissions could be improved to offer a high level of protection for the environment and human health while simplifying the existing legislation and cutting unnecessary administrative costs. Throughout Europe there is high acceptance that industrial emissions are the leading cause of pollution in Europe.  As well there is high support for a system where the polluter will pay under a Polluter pays principle. The IED is intended to provide significant improvement on the interaction between the previous seven directives (including the Waste Incineration Directive) which it replaces. It also strengthens, in several instances, some provisions in previous directives, for example the Large Combustion Plant Directive.|2023-08-24-20-13-03
Industrial Emissions Directive|Exemptions| Certain firms are allowed to apply for exemptions when the cost of the best available technology is higher than the benefit.  They will be evaluated using Cost–benefit analysis to decide if an exemption will be granted to the firm. Bulgaria is currently seeking an exemption for their whole fleet of coal fired power plants. [needs update]|2023-08-24-20-13-03
Industrial Emissions Directive|Criticism| The exemptions have allowed for a large amount of Europe's power plants to exceed the set standards.  There is concern that if the exemptions were removed some plants will be forced to shut down due to the increased cost associated with the best available technology. The passing of stricter laws now makes it harder for some plants to receive an exemption from the directive.|2023-08-24-20-13-03
Inhalation exposure|General| Inhalation is a major route of exposure that occurs when an individual breathes in polluted air which enters the respiratory tract.   Identification of the pollutant uptake by the respiratory system can determine how the resulting exposure contributes to the  dose.  In this way, the mechanism of pollutant uptake by the respiratory system can be used to predict potential health impacts within the human population.|2023-07-11-10-32-27
Inhalation exposure|Definition| Exposure is commonly understood to be the concentration of the airborne pollutant in the air at the mouth and nose boundary. Outdoor concentrations are often measured at fixed sites or estimated with models. The fraction of this ambient concentration that is inhaled by a person depends mainly on their location (indoor or outdoor), distance to pollution sources and their minute ventilation. Traditionally exposure is estimated based on outdoor concentrations at the residential address. Trips to other locations and physical activity level are mostly neglected although some recent studies have attempted to use portable and wearable sensors. Intake dose is the mass of the pollutant that crosses the contact boundary and is inhaled by the individual.    Some of this pollutant is exhaled, and the fraction that is absorbed by the respiratory system is known as the absorbed dose.  A portion of the pollutant may also be expelled by sneezing, coughing, spitting, or swallowing.  The remaining  pollutant that is transported through the liquid layer, making contact with the respiratory tract tissues is the fraction of bioavailability, called the effective dose.|2023-07-11-10-32-27
Inhalation exposure|Major pollutants of concern| In 1970, the Clean Air Act Amendments set six criteria air pollutants which are updated periodically by the National Air Quality Standards (NAAQS) and the U.S Environmental Protection Agency (USEPA).  The six criteria pollutants were identified based on scientific knowledge of health effects caused by the pollutants.  The six criteria are the following:  particulate matter (PM), nitrogen oxide NO2, ozone O3, sulfur dioxide SO2, carbon monoxide (CO), and nonmethane hydrocarbons (NHMC).  Particulate matter (PM) is divided into two sizes, PM10 which is called inhalable PM, and  PM2.5, which is called fine PM.|2023-07-11-10-32-27
Inhalation exposure|Uptake of gaseous pollutants| The diffusion of O2 from the air in the lungs to the bloodstream, and diffusion of CO2 from the bloodstream back out to the lungs is an essential part of human respiration.  The absorption and diffusion of gases is a bidirectional process.  Once the gases are absorbed into the mucus or surfactant layer, the dissolved gases can desorb back to the air in the lungs.  Gases may diffuse in either direction depending on the concentration gradient between the two layers.  Gases may react chemically during transport into the bloodstream. Estimates of the resistance for gas mucus and tissue in the terminal bronchioles for  SO2,  O2, and CO show that  SO2 has the quickest uptake due to its high aqueous solubility and very low resistance of mucus and tissue layers.  Ozone and CO, have lower aqueous solubilities and higher resistance to mass transfer.  Ozone is the most reactive, reducing mass transfer into tissue and blood.  CO has the slowest uptake and the highest resistance into the terminal bronchioles.|2023-07-11-10-32-27
Inhalation exposure|Uptake of particulate pollutants| The deposition of particulate pollutants into the lungs is necessary before the particles can travel through the mucus into the lung tissue. There are four mechanisms of deposition:  interception, impaction, gravitational settling, and Brownian diffusion.  Interception happens when a particle is removed after brushing up against an obstacle. Impaction happens when the particle collides into the surface of the respiratory tract due to the high inertia.  Gravitational settling is influenced by the force of gravity which causes the particle to settle on the respiratory tract.  Brownian motion causes the random collision of gas molecules against the particle, until the particle goes into the respiratory tract. Prediction of the location of particle deposition into the respiratory tract depends on the size and type of particle.  Coarse particles, originating from natural sources such as dust, sand and gravel, tend to deposit in the nasal-pharyngeal region. Fine particles, derived from anthropogenic sources such as fossil fuels and smoking, typically deposit in the pulmonary region.  Most gas exchange occurs in the pulmonary region due to the alveoli, which contain a large surface area.|2023-07-11-10-32-27
Inhalation exposure|Health impacts of particulate pollutants| Scientists have identified a positive correlation between particulate matter concentrations being the causative factor of respiratory and cardiovascular disease.  Particulate matter may also be responsible for as many as 20,000 deaths annually, and exacerbation of asthma.  Quantification of dose, determining total number of particles deposited in the pulmonary region, surface area of particles, acidity of particles, and shape are important in determining health impacts.  A larger surface area will cause more toxins to be available for absorption into the mucus.  Particles such as asbestos have the ability to become permanently enlodged into the alveoli causing cancer in some cases. Soluble particulate matter can be highly detrimental to the respirator tract because of their ability to dissolve into the mucus or surfactant layer.  This can irritate tissues by changing pH, and transport into the rest of the body or gastrointestinal tract.  Insoluble PM, such as lead particles, deposit in the nasal-pharyngeal region and can be cleared by blowing, sniffling, or spitting.  However, swallowing can cause the particles to deposit into th GI tract.  Particles in the tracheobronchial region can be cleared by the cilia, which will move particles into the mucus.   Insoluble particles that enter the pulmonary region cause swelling  of the alveoli, coughing, and shortness of breath.|2023-07-11-10-32-27
Inhalation exposure|Uptake of carbon monoxide| Carbon monoxide is a relatively nonreactive gas with limited solubility.  High CO levels build up in the pulmonary region over several hours, and equilibrate with inhaled CO concentrations.  Exposure to carbon monoxide is dangerous because of its toxic, odorless nature.  Since the gas takes time to build up in the pulmonary region, an inhaled concentration of 600 ppm would cause a headache and reduce mental capacity within an hour, without any other symptoms.  Eventually, the substance would induce a coma.  Equilibrium of CO in the blood is reached between 6–8 hours of exposure to constant concentration in the air. A baseline level of carboxyhemoglobin, (COHb) is contained in the blood due to small quantities of CO as a by-product in the body.  The total amount of COHb present within the body is equivalent to the COHb baseline level in addition to the COHb exogenous level. [COHb] total = [COHb] bas + [COHb] exo|2023-07-11-10-32-27
Inhalation exposure|Sources| ^ 1. Ott, W. R., Steinemann, A. C., & Wallace, L. A. (2007). Biomarkers of exposure. In W. R. Ott, A. C. Steinemann & L. A. Wallace (Eds.), Exposure analysis (pp. 395–404). Boca Raton, FL: Taylor & Francis.|2023-07-11-10-32-27
Internal combustion engine|General| An internal combustion engine (ICE or IC engine) is a heat engine in which the combustion of a fuel occurs with an oxidizer (usually air) in a combustion chamber that is an integral part of the working fluid flow circuit. In an internal combustion engine, the expansion of the high-temperature and high-pressure gases produced by combustion applies direct force to some component of the engine. The force is typically applied to pistons (piston engine), turbine blades (gas turbine), a rotor (Wankel engine), or a nozzle (jet engine). This force moves the component over a distance, transforming chemical energy into kinetic energy which is used to propel, move or power whatever the engine is attached to. The first commercially successful internal combustion engine was created by Étienne Lenoir around 1860,  and the first modern internal combustion engine, known as the Otto engine, was created in 1876 by Nicolaus Otto. The term internal combustion engine usually refers to an engine in which combustion is intermittent, such as the more familiar two-stroke and four-stroke piston engines, along with variants, such as the six-stroke piston engine and the Wankel rotary engine. A second class of internal combustion engines use continuous combustion: gas turbines, jet engines and most rocket engines, each of which are internal combustion engines on the same principle as previously described.   Firearms are also a form of internal combustion engine,  though of a type so specialized that they are commonly treated as a separate category, along with weaponry such as mortars and anti-aircraft cannons. In contrast, in external combustion engines, such as steam or Stirling engines, energy is delivered to a working fluid not consisting of, mixed with, or contaminated by combustion products. Working fluids for external combustion engines include air, hot water, pressurized water or even boiler-heated liquid sodium. While there are many stationary applications, most ICEs are used in mobile applications and are the primary power supply for vehicles such as cars, aircraft and boats. ICEs are typically powered by hydrocarbon-based fuels like natural gas, gasoline, diesel fuel, or ethanol. Renewable fuels like biodiesel are used in compression ignition (CI) engines and bioethanol or ETBE (ethyl tert-butyl ether) produced from bioethanol in spark ignition (SI) engines. As early as 1900 the inventor of the diesel engine, Rudolf Diesel, was using peanut oil to run his engines.  Renewable fuels are commonly blended with fossil fuels.  Hydrogen, which is rarely used, can be obtained from either fossil fuels or renewable energy.|2023-09-27-11-24-56
Internal combustion engine|History|" Various scientists and engineers contributed to the development of internal combustion engines. In 1791, John Barber developed the gas turbine. In 1794 Thomas Mead patented a gas engine. Also in 1794, Robert Street patented an internal combustion engine, which was also the first to use liquid fuel, and built an engine around that time. In 1798, John Stevens built the first American internal combustion engine. In 1807, French engineers Nicéphore Niépce (who went on to invent photography) and Claude Niépce ran a prototype internal combustion engine, using controlled dust explosions, the Pyréolophore, which was granted a patent by Napoleon Bonaparte. This engine powered a boat on the Saône river in France.   In the same year, Swiss engineer François Isaac de Rivaz invented a hydrogen-based internal combustion engine and powered the engine by electric spark. In 1808, De Rivaz fitted his invention to a primitive working vehicle – ""the world's first internal combustion powered automobile"".  In 1823, Samuel Brown patented the first internal combustion engine to be applied industrially. In 1854 in the UK, the Italian inventors Eugenio Barsanti and Felice Matteucci obtained the certification: ""Obtaining Motive Power by the Explosion of Gases"". In 1857 the Great Seal Patent Office conceded them patent No.1655 for the invention of an ""Improved Apparatus for Obtaining Motive Power from Gases"".     Barsanti and Matteucci obtained other patents for the same invention in France, Belgium and Piedmont between 1857 and 1859.   In 1860, Belgian engineer Jean Joseph Etienne Lenoir produced a gas-fired internal combustion engine.  In 1864, Nicolaus Otto patented the first atmospheric gas engine. In 1872, American George Brayton invented the first commercial liquid-fueled internal combustion engine. In 1876, Nicolaus Otto began working with Gottlieb Daimler and Wilhelm Maybach, patented the compressed charge, four-cycle engine. In 1879, Karl Benz patented a reliable two-stroke gasoline engine. Later, in 1886, Benz began the first commercial production of motor vehicles with an internal combustion engine, in which a three-wheeled, four-cycle engine and chassis formed a single unit.  In 1892, Rudolf Diesel developed the first compressed charge, compression ignition engine. In 1926, Robert Goddard launched the first liquid-fueled rocket. In 1939, the Heinkel He 178 became the world's first jet aircraft."|2023-09-27-11-24-56
Internal combustion engine|Etymology|" At one time, the word engine (via Old French, from Latin ingenium, ""ability"") meant any piece of machinery—a sense that persists in expressions such as siege engine. A ""motor"" (from Latin motor, ""mover"") is any machine that produces mechanical power. Traditionally, electric motors are not referred to as ""engines""; however, combustion engines are often referred to as ""motors"". (An electric engine refers to a locomotive operated by electricity.) In boating, an internal combustion engine that is installed in the hull is referred to as an engine, but the engines that sit on the transom are referred to as motors."|2023-09-27-11-24-56
Internal combustion engine|Applications|" Reciprocating piston engines are by far the most common power source for land and water vehicles, including automobiles, motorcycles, ships and to a lesser extent, locomotives (some are electrical but most use diesel engines  ). Rotary engines of the Wankel design are used in some automobiles, aircraft and motorcycles.  These are collectively known as internal-combustion-engine vehicles (ICEV). Where high power-to-weight ratios are required, internal combustion engines appear in the form of combustion turbines, or sometimes Wankel engines. Powered aircraft typically use an ICE which may be a reciprocating engine. Airplanes can instead use jet engines and helicopters can instead employ turboshafts; both of which are types of turbines. In addition to providing propulsion, airliners may employ a separate ICE as an auxiliary power unit. Wankel engines are fitted to many unmanned aerial vehicles. ICEs drive large electric generators that power electrical grids. They are found in the form of combustion turbines with a typical electrical output in the range of some 100 MW. Combined cycle power plants use the high temperature exhaust to boil and superheat water steam to run a steam turbine. Thus, the efficiency is higher because more energy is extracted from the fuel than what could be extracted by the combustion engine alone.
Combined cycle power plants achieve efficiencies in the range of 50–60%. In a smaller scale, stationary engines like gas engines or diesel generators are used for backup or for providing electrical power to areas not connected to an electric grid. Small engines (usually 2‐stroke gasoline/petrol engines) are a common power source for lawnmowers, string trimmers, chain saws, leafblowers, pressure washers, snowmobiles, jet skis, outboard motors, mopeds, and motorcycles."|2023-09-27-11-24-56
Internal combustion engine|Classification| There are several possible ways to classify internal combustion engines.|2023-09-27-11-24-56
Internal combustion engine|Reciprocating| By number of strokes: By type of ignition: By mechanical/thermodynamic cycle (these cycles are infrequently used but are commonly found in hybrid vehicles, along with other vehicles manufactured for fuel efficiency ):|2023-09-27-11-24-56
Internal combustion engine|Structure| The base of a reciprocating internal combustion engine is the engine block, which is typically made of cast iron (due to its good wear resistance and low cost)  or aluminum. In the latter case, the cylinder liners are made of cast iron or steel,  or a coating such as nikasil or alusil. The engine block contains the cylinders. In engines with more than one cylinder they are usually arranged either in 1 row (straight engine) or 2 rows (boxer engine or V engine); 3 rows are occasionally used (W engine) in contemporary engines, and other engine configurations are possible and have been used. Single cylinder engines (or thumpers) are common for motorcycles and other small engines found in light machinery. On the outer side of the cylinder, passages that contain cooling fluid are cast into the engine block whereas, in some heavy duty engines, the passages are the types of removable cylinder sleeves which can be replaceable.  Water-cooled engines contain passages in the engine block where cooling fluid circulates (the water jacket). Some small engines are air-cooled, and instead of having a water jacket the cylinder block has fins protruding away from it to cool the engine by directly transferring heat to the air. The cylinder walls are usually finished by honing to obtain a cross hatch, which is able to retain more oil. A too rough surface would quickly harm the engine by excessive wear on the piston. The pistons are short cylindrical parts which seal one end of the cylinder from the high pressure of the compressed air and combustion products and slide continuously within it while the engine is in operation. In smaller engines, the pistons are made of aluminum; while in larger applications, they are typically made of cast iron.  The top wall of the piston is termed its crown and is typically flat or concave. Some two-stroke engines use pistons with a deflector head. Pistons are open at the bottom and hollow except for an integral reinforcement structure (the piston web). When an engine is working, the gas pressure in the combustion chamber exerts a force on the piston crown which is transferred through its web to a gudgeon pin. Each piston has rings fitted around its circumference that mostly prevent the gases from leaking into the crankcase or the oil into the combustion chamber.  A ventilation system drives the small amount of gas that escapes past the pistons during normal operation (the blow-by gases) out of the crankcase so that it does not accumulate contaminating the oil and creating corrosion.  In two-stroke gasoline engines the crankcase is part of the air–fuel path and due to the continuous flow of it, two-stroke engines do not need a separate crankcase ventilation system. The cylinder head is attached to the engine block by numerous bolts or studs. It has several functions. The cylinder head seals the cylinders on the side opposite to the pistons; it contains short ducts (the ports) for intake and exhaust and the associated intake valves that open to let the cylinder be filled with fresh air and exhaust valves that open to allow the combustion gases to escape. However, 2-stroke crankcase scavenged engines connect the gas ports directly to the cylinder wall without poppet valves; the piston controls their opening and occlusion instead. The cylinder head also holds the spark plug in the case of spark ignition engines and the injector for engines that use direct injection. All CI (compression ignition) engines use fuel injection, usually direct injection but some engines instead use indirect injection. SI (spark ignition) engines can use a carburetor or fuel injection as port injection or direct injection. Most SI engines have a single spark plug per cylinder but some have 2. A head gasket prevents the gas from leaking between the cylinder head and the engine block. The opening and closing of the valves is controlled by one or several camshafts and springs—or in some engines—a desmodromic mechanism that uses no springs. The camshaft may press directly the stem of the valve or may act upon a rocker arm, again, either directly or through a pushrod. The crankcase is sealed at the bottom with a sump that collects the falling oil during normal operation to be cycled again. The cavity created between the cylinder block and the sump houses a crankshaft that converts the reciprocating motion of the pistons to rotational motion. The crankshaft is held in place relative to the engine block by main bearings, which allow it to rotate. Bulkheads in the crankcase form a half of every main bearing; the other half is a detachable cap. In some cases a single main bearing deck is used rather than several smaller caps. A connecting rod is connected to offset sections of the crankshaft (the crankpins) in one end and to the piston in the other end through the gudgeon pin and thus transfers the force and translates the reciprocating motion of the pistons to the circular motion of the crankshaft. The end of the connecting rod attached to the gudgeon pin is called its small end, and the other end, where it is connected to the crankshaft, the big end. The big end has a detachable half to allow assembly around the crankshaft. It is kept together to the connecting rod by removable bolts. The cylinder head has an intake manifold and an exhaust manifold attached to the corresponding ports. The intake manifold connects to the air filter directly, or to a carburetor when one is present, which is then connected to the air filter. It distributes the air incoming from these devices to the individual cylinders. The exhaust manifold is the first component in the exhaust system. It collects the exhaust gases from the cylinders and drives it to the following component in the path. The exhaust system of an ICE may also include a catalytic converter and muffler. The final section in the path of the exhaust gases is the tailpipe.|2023-09-27-11-24-56
Internal combustion engine|4-stroke engines| The top dead center (TDC) of a piston is the position where it is nearest to the valves; bottom dead center (BDC) is the opposite position where it is furthest from them. A stroke is the movement of a piston from TDC to BDC or vice versa, together with the associated process. While an engine is in operation, the crankshaft rotates continuously at a nearly constant speed. In a 4-stroke ICE, each piston experiences 2 strokes per crankshaft revolution in the following order. Starting the description at TDC, these are:|2023-09-27-11-24-56
Internal combustion engine|2-stroke engines|" The defining characteristic of this kind of engine is that each piston completes a cycle every crankshaft revolution. The 4 processes of intake, compression, power and exhaust take place in only 2 strokes so that it is not possible to dedicate a stroke exclusively for each of them. Starting at TDC the cycle consists of: While a 4-stroke engine uses the piston as a positive displacement pump to accomplish scavenging taking 2 of the 4 strokes, a 2-stroke engine uses the last part of the power stroke and the first part of the compression stroke for combined intake and exhaust. The work required to displace the charge and exhaust gases comes from either the crankcase or a separate blower. For scavenging, expulsion of burned gas and entry of fresh mix, two main approaches are described: Loop scavenging, and Uniflow scavenging. SAE news published in the 2010s that 'Loop Scavenging' is better under any circumstance than Uniflow Scavenging. Some SI engines are crankcase scavenged and do not use poppet valves. Instead, the crankcase and the part of the cylinder below the piston is used as a pump. The intake port is connected to the crankcase through a reed valve or a rotary disk valve driven by the engine. For each cylinder, a transfer port connects in one end to the crankcase and in the other end to the cylinder wall. The exhaust port is connected directly to the cylinder wall. The transfer and exhaust port are opened and closed by the piston. The reed valve opens when the crankcase pressure is slightly below intake pressure, to let it be filled with a new charge; this happens when the piston is moving upwards. When the piston is moving downwards the pressure in the crankcase increases and the reed valve closes promptly, then the charge in the crankcase is compressed. When the piston is moving downwards, it also uncovers the exhaust port and the transfer port and the higher pressure of the charge in the crankcase makes it enter the cylinder through the transfer port, blowing the exhaust gases. Lubrication is accomplished by adding 2-stroke oil to the fuel in small ratios. Petroil refers to the mix of gasoline with the aforesaid oil. This kind of 2-stroke engine has a lower efficiency than comparable 4-strokes engines and releases more polluting exhaust gases for the following conditions: The main advantage of 2-stroke engines of this type is mechanical simplicity and a higher power-to-weight ratio than their 4-stroke counterparts. Despite having twice as many power strokes per cycle, less than twice the power of a comparable 4-stroke engine is attainable in practice. In the US, 2-stroke engines were banned for road vehicles due to the pollution. Off-road only motorcycles are still often 2-stroke but are rarely road legal. However, many thousands of 2-stroke lawn maintenance engines are in use.[citation needed] Using a separate blower avoids many of the shortcomings of crankcase scavenging, at the expense of increased complexity which means a higher cost and an increase in maintenance requirement. An engine of this type uses ports or valves for intake and valves for exhaust, except opposed piston engines, which may also use ports for exhaust. The blower is usually of the Roots-type but other types have been used too. This design is commonplace in CI engines, and has been occasionally used in SI engines. CI engines that use a blower typically use uniflow scavenging. In this design the cylinder wall contains several intake ports placed uniformly spaced along the circumference just above the position that the piston crown reaches when at BDC. An exhaust valve or several like that of 4-stroke engines is used. The final part of the intake manifold is an air sleeve that feeds the intake ports. The intake ports are placed at a horizontal angle to the cylinder wall (I.e: they are in plane of the piston crown) to give a swirl to the incoming charge to improve combustion. The largest reciprocating IC are low speed CI engines of this type; they are used for marine propulsion (see marine diesel engine) or electric power generation and achieve the highest thermal efficiencies among internal combustion engines of any kind. Some Diesel-electric locomotive engines operate on the 2-stroke cycle. The most powerful of them have a brake power of around 4.5 MW or 6,000 HP. The EMD SD90MAC class of locomotives are an example of such. The comparable class GE AC6000CW whose prime mover has almost the same brake power uses a 4-stroke engine. An example of this type of engine is the Wärtsilä-Sulzer RT-flex96-C turbocharged 2-stroke Diesel, used in large container ships. It is the most efficient and powerful reciprocating internal combustion engine in the world with a thermal efficiency over 50%.    For comparison, the most efficient small four-stroke engines are around 43% thermally-efficient (SAE 900648);[citation needed] size is an advantage for efficiency due to the increase in the ratio of volume to surface area. See the external links for an in-cylinder combustion video in a 2-stroke, optically accessible motorcycle engine. Dugald Clerk developed the first two-cycle engine in 1879. It used a separate cylinder which functioned as a pump in order to transfer the fuel mixture to the cylinder. In 1899 John Day simplified Clerk's design into the type of 2 cycle engine that is very widely used today. 
Day cycle engines are crankcase scavenged and port timed. The crankcase and the part of the cylinder below the exhaust port is used as a pump. The operation of the Day cycle engine begins when the crankshaft is turned so that the piston moves from BDC upward (toward the head) creating a vacuum in the crankcase/cylinder area. The carburetor then feeds the fuel mixture into the crankcase through a reed valve or a rotary disk valve (driven by the engine). There are cast in ducts from the crankcase to the port in the cylinder to provide for intake and another from the exhaust port to the exhaust pipe.  The height of the port in relationship to the length of the cylinder is called the ""port timing"". On the first upstroke of the engine there would be no fuel inducted into the cylinder as the crankcase was empty. On the downstroke, the piston now compresses the fuel mix, which has lubricated the piston in the cylinder and the bearings due to the fuel mix having oil added to it. As the piston moves downward it first uncovers the exhaust, but on the first stroke there is no burnt fuel to exhaust. As the piston moves downward further, it uncovers the intake port which has a duct that runs to the crankcase. Since the fuel mix in the crankcase is under pressure, the mix moves through the duct and into the cylinder. Because there is no obstruction in the cylinder of the fuel to move directly out of the exhaust port prior to the piston rising far enough to close the port, early engines used a high domed piston to slow down the flow of fuel. Later the fuel was ""resonated"" back into the cylinder using an expansion chamber design. When the piston rose close to TDC, a spark ignited the fuel. As the piston is driven downward with power, it first uncovers the exhaust port where the burned fuel is expelled under high pressure and then the intake port where the process has been completed and will keep repeating. Later engines used a type of porting devised by the Deutz company to improve performance. It was called the Schnurle Reverse Flow system. DKW licensed this design for all their motorcycles. Their DKW RT 125 was one of the first motor vehicles to achieve over 100 mpg as a result."|2023-09-27-11-24-56
Internal combustion engine|Ignition| Internal combustion engines require ignition of the mixture, either by spark ignition (SI) or compression ignition (CI). Before the invention of reliable electrical methods, hot tube and flame methods were used. Experimental engines with laser ignition have been built. The spark-ignition engine was a refinement of the early engines which used Hot Tube ignition. When Bosch developed the magneto it became the primary system for producing electricity to energize a spark plug.  Many small engines still use magneto ignition. Small engines are started by hand cranking using a recoil starter or hand crank. Prior to Charles F. Kettering of Delco's development of the automotive starter all gasoline engined automobiles used a hand crank. Larger engines typically power their starting motors and ignition systems using the electrical energy stored in a lead–acid battery.  The battery's charged state is maintained by an automotive alternator or (previously) a generator which uses engine power to create electrical energy storage. The battery supplies electrical power for starting when the engine has a starting motor system, and supplies electrical power when the engine is off. The battery also supplies electrical power during rare run conditions where the alternator cannot maintain more than 13.8 volts (for a common 12V automotive electrical system). As alternator voltage falls below 13.8 volts, the lead-acid storage battery increasingly picks up electrical load. During virtually all running conditions, including normal idle conditions, the alternator supplies primary electrical power. Some systems disable alternator field (rotor) power during wide-open throttle conditions. Disabling the field reduces alternator pulley mechanical loading to nearly zero, maximizing crankshaft power. In this case, the battery supplies all primary electrical power. Gasoline engines take in a mixture of air and gasoline and compress it by the movement of the piston from bottom dead center to top dead center when the fuel is at maximum compression. The reduction in the size of the swept area of the cylinder and taking into account the volume of the combustion chamber is described by a ratio. Early engines had compression ratios of 6 to 1. As compression ratios were increased, the efficiency of the engine increased as well. With early induction and ignition systems the compression ratios had to be kept low. With advances in fuel technology and combustion management, high-performance engines can run reliably at 12:1 ratio. With low octane fuel, a problem would occur as the compression ratio increased as the fuel was igniting due to the rise in temperature that resulted. Charles Kettering developed a lead additive which allowed higher compression ratios, which was progressively abandoned for automotive use from the 1970s onward, partly due to lead poisoning concerns. The fuel mixture is ignited at different progressions of the piston in the cylinder. At low rpm, the spark is timed to occur close to the piston achieving top dead center. In order to produce more power, as rpm rises the spark is advanced sooner during piston movement. The spark occurs while the fuel is still being compressed progressively more as rpm rises. The necessary high voltage, typically 10,000 volts, is supplied by an induction coil or transformer. The induction coil is a fly-back system, using interruption of electrical primary system current through some type of synchronized interrupter. The interrupter can be either contact points or a power transistor. The problem with this type of ignition is that as RPM increases the availability of electrical energy decreases. This is especially a problem, since the amount of energy needed to ignite a more dense fuel mixture is higher. The result was often a high RPM misfire. Capacitor discharge ignition was developed. It produces a rising voltage that is sent to the spark plug. CD system voltages can reach 60,000 volts.  CD ignitions use step-up transformers. The step-up transformer uses energy stored in a capacitance to generate electric spark. With either system, a mechanical or electrical control system provides a carefully timed high-voltage to the proper cylinder. This spark, via the spark plug, ignites the air-fuel mixture in the engine's cylinders. While gasoline internal combustion engines are much easier to start in cold weather than diesel engines, they can still have cold weather starting problems under extreme conditions. For years, the solution was to park the car in heated areas. In some parts of the world, the oil was actually drained and heated overnight and returned to the engine for cold starts. In the early 1950s, the gasoline Gasifier unit was developed, where, on cold weather starts, raw gasoline was diverted to the unit where part of the fuel was burned causing the other part to become a hot vapor sent directly to the intake valve manifold.  This unit was quite popular until electric engine block heaters became standard on gasoline engines sold in cold climates. For ignition, diesel, PPC and HCCI engines rely solely on the high temperature and pressure created by the engine in its compression process. The compression level that occurs is usually twice or more than a gasoline engine. Diesel engines take in air only, and shortly before peak compression, spray a small quantity of diesel fuel into the cylinder via a fuel injector that allows the fuel to instantly ignite. HCCI type engines take in both air and fuel, but continue to rely on an unaided auto-combustion process, due to higher pressures and temperature. This is also why diesel and HCCI engines are more susceptible to cold-starting issues, although they run just as well in cold weather once started. Light duty diesel engines with indirect injection in automobiles and light trucks employ glowplugs (or other pre-heating: see Cummins ISB#6BT) that pre-heat the combustion chamber just before starting to reduce no-start conditions in cold weather. Most diesels also have a battery and charging system; nevertheless, this system is secondary and is added by manufacturers as a luxury for the ease of starting, turning fuel on and off (which can also be done via a switch or mechanical apparatus), and for running auxiliary electrical components and accessories. Most new engines rely on electrical and electronic engine control units (ECU) that also adjust the combustion process to increase efficiency and reduce emissions.|2023-09-27-11-24-56
Internal combustion engine|Lubrication| Surfaces in contact and relative motion to other surfaces require lubrication to reduce wear, noise and increase efficiency by reducing the power wasting in overcoming friction, or to make the mechanism work at all. Also, the lubricant used can reduce excess heat and provide additional cooling to components. At the very least, an engine requires lubrication in the following parts: In 2-stroke crankcase scavenged engines, the interior of the crankcase, and therefore the crankshaft, connecting rod and bottom of the pistons are sprayed by the 2-stroke oil in the air-fuel-oil mixture which is then burned along with the fuel. The valve train may be contained in a compartment flooded with lubricant so that no oil pump is required. In a splash lubrication system no oil pump is used. Instead the crankshaft dips into the oil in the sump and due to its high speed, it splashes the crankshaft, connecting rods and bottom of the pistons. The connecting rod big end caps may have an attached scoop to enhance this effect. The valve train may also be sealed in a flooded compartment, or open to the crankshaft in a way that it receives splashed oil and allows it to drain back to the sump. Splash lubrication is common for small 4-stroke engines. In a forced (also called pressurized) lubrication system, lubrication is accomplished in a closed-loop which carries motor oil to the surfaces serviced by the system and then returns the oil to a reservoir. The auxiliary equipment of an engine is typically not serviced by this loop; for instance, an alternator may use ball bearings sealed with their own lubricant. The reservoir for the oil is usually the sump, and when this is the case, it is called a wet sump system. When there is a different oil reservoir the crankcase still catches it, but it is continuously drained by a dedicated pump; this is called a dry sump system. On its bottom, the sump contains an oil intake covered by a mesh filter which is connected to an oil pump then to an oil filter outside the crankcase. From there it is diverted to the crankshaft main bearings and valve train. The crankcase contains at least one oil gallery (a conduit inside a crankcase wall) to which oil is introduced from the oil filter. The main bearings contain a groove through all or half its circumference; the oil enters these grooves from channels connected to the oil gallery. The crankshaft has drillings that take oil from these grooves and deliver it to the big end bearings. All big end bearings are lubricated this way. A single main bearing may provide oil for 0, 1 or 2 big end bearings. A similar system may be used to lubricate the piston, its gudgeon pin and the small end of its connecting rod; in this system, the connecting rod big end has a groove around the crankshaft and a drilling connected to the groove which distributes oil from there to the bottom of the piston and from then to the cylinder. Other systems are also used to lubricate the cylinder and piston. The connecting rod may have a nozzle to throw an oil jet to the cylinder and bottom of the piston. That nozzle is in movement relative to the cylinder it lubricates, but always pointed towards it or the corresponding piston. Typically forced lubrication systems have a lubricant flow higher than what is required to lubricate satisfactorily, in order to assist with cooling. Specifically, the lubricant system helps to move heat from the hot engine parts to the cooling liquid (in water-cooled engines) or fins (in air-cooled engines) which then transfer it to the environment. The lubricant must be designed to be chemically stable and maintain suitable viscosities within the temperature range it encounters in the engine.|2023-09-27-11-24-56
Internal combustion engine|Cylinder configuration| Common cylinder configurations include the straight or inline configuration, the more compact V configuration, and the wider but smoother flat or boxer configuration. Aircraft engines can also adopt a radial configuration, which allows more effective cooling. More unusual configurations such as the H, U, X, and W have also been used. Multiple cylinder engines have their valve train and crankshaft configured so that pistons are at different parts of their cycle. It is desirable to have the pistons' cycles uniformly spaced (this is called even firing) especially in forced induction engines; this reduces torque pulsations  and makes inline engines with more than 3 cylinders statically balanced in its primary forces. However, some engine configurations require odd firing to achieve better balance than what is possible with even firing. For instance, a 4-stroke I2 engine has better balance when the angle between the crankpins is 180° because the pistons move in opposite directions and inertial forces partially cancel, but this gives an odd firing pattern where one cylinder fires 180° of crankshaft rotation after the other, then no cylinder fires for 540°. With an even firing pattern, the pistons would move in unison and the associated forces would add. Multiple crankshaft configurations do not necessarily need a cylinder head at all because they can instead have a piston at each end of the cylinder called an opposed piston design. Because fuel inlets and outlets are positioned at opposed ends of the cylinder, one can achieve uniflow scavenging, which, as in the four-stroke engine is efficient over a wide range of engine speeds. Thermal efficiency is improved because of a lack of cylinder heads. This design was used in the Junkers Jumo 205 diesel aircraft engine, using two crankshafts at either end of a single bank of cylinders, and most remarkably in the Napier Deltic diesel engines. These used three crankshafts to serve three banks of double-ended cylinders arranged in an equilateral triangle with the crankshafts at the corners. It was also used in single-bank locomotive engines, and is still used in marine propulsion engines and marine auxiliary generators.|2023-09-27-11-24-56
Internal combustion engine|Diesel cycle| Most truck and automotive diesel engines use a cycle reminiscent of a four-stroke cycle, but with temperature increase by compression causing ignition, rather than needing a separate ignition system. This variation is called the diesel cycle. In the diesel cycle, diesel fuel is injected directly into the cylinder so that combustion occurs at constant pressure, as the piston moves.|2023-09-27-11-24-56
Internal combustion engine|Otto cycle| The Otto cycle is the most common cycle for most cars' internal combustion engines that use gasoline as a fuel. It consists of the same major steps as described for the four-stroke engine: Intake, compression, ignition, expansion and exhaust.|2023-09-27-11-24-56
Internal combustion engine|Five-stroke engine| In 1879, Nicolaus Otto manufactured and sold a double expansion engine (the double and triple expansion principles had ample usage in steam engines), with two small cylinders at both sides of a low-pressure larger cylinder, where a second expansion of exhaust stroke gas took place; the owner returned it, alleging poor performance. In 1906, the concept was incorporated in a car built by EHV (Eisenhuth Horseless Vehicle Company);  and in the 21st century Ilmor designed and successfully tested a 5-stroke double expansion internal combustion engine, with high power output and low SFC (Specific Fuel Consumption).|2023-09-27-11-24-56
Internal combustion engine|Six-stroke engine|" The six-stroke engine was invented in 1883. Four kinds of six-stroke engines use a regular piston in a regular cylinder (Griffin six-stroke, Bajulaz six-stroke, Velozeta six-stroke and Crower six-stroke), firing every three crankshaft revolutions. These systems capture the waste heat of the four-stroke Otto cycle with an injection of air or water. The Beare Head and ""piston charger"" engines operate as opposed-piston engines, two pistons in a single cylinder, firing every two revolutions rather than every four like a four-stroke engine."|2023-09-27-11-24-56
Internal combustion engine|Other cycles|" The very first internal combustion engines did not compress the mixture. The first part of the piston downstroke drew in a fuel-air mixture, then the inlet valve closed and, in the remainder of the down-stroke, the fuel-air mixture fired. The exhaust valve opened for the piston upstroke. These attempts at imitating the principle of a steam engine were very inefficient.
There are a number of variations of these cycles, most notably the Atkinson and Miller cycles. Split-cycle engines separate the four strokes of intake, compression, combustion and exhaust into two separate but paired cylinders. The first cylinder is used for intake and compression. The compressed air is then transferred through a crossover passage from the compression cylinder into the second cylinder, where combustion and exhaust occur. A split-cycle engine is really an air compressor on one side with a combustion chamber on the other. Previous split-cycle engines have had two major problems—poor breathing (volumetric efficiency) and low thermal efficiency. However, new designs are being introduced that seek to address these problems.
The Scuderi Engine addresses the breathing problem by reducing the clearance between the piston and the cylinder head through various turbocharging techniques. The Scuderi design requires the use of outwardly opening valves that enable the piston to move very close to the cylinder head without the interference of the valves. Scuderi addresses the low thermal efficiency via firing after top dead center (ATDC). Firing ATDC can be accomplished by using high-pressure air in the transfer passage to create sonic flow and high turbulence in the power cylinder."|2023-09-27-11-24-56
Internal combustion engine|Jet engine| Jet engines use a number of rows of fan blades to compress air which then enters a combustor where it is mixed with fuel (typically JP fuel) and then ignited. The burning of the fuel raises the temperature of the air which is then exhausted out of the engine creating thrust. A modern turbofan engine can operate at as high as 48% efficiency. There are six sections to a turbofan engine:|2023-09-27-11-24-56
Internal combustion engine|Gas turbines| A gas turbine compresses air and uses it to turn a turbine. It is essentially a jet engine which directs its output to a shaft.  There are three stages to a turbine: 1) air is drawn through a compressor where the temperature rises due to compression, 2) fuel is added in the combuster, and 3) hot air is exhausted through turbine blades which rotate a shaft connected to the compressor. A gas turbine is a rotary machine similar in principle to a steam turbine and it consists of three main components: a compressor, a combustion chamber, and a turbine. The temperature of the air, after being compressed in the compressor, is increased by burning fuel in it. The heated air and the products of combustion expand in a turbine, producing work output. About 2⁄3 of the work drives the compressor: the rest (about 1⁄3) is available as useful work output. Gas turbines are among the most efficient internal combustion engines. The General Electric 7HA and 9HA turbine combined cycle electrical plants are rated at over 61% efficiency.|2023-09-27-11-24-56
Internal combustion engine|Brayton cycle| A gas turbine is a rotary machine somewhat similar in principle to a steam turbine. It consists of three main components: compressor, combustion chamber, and turbine. The air is compressed by the compressor where a temperature rise occurs. The temperature of the compressed air is further increased by combustion of injected fuel in the combustion chamber which expands the air. This energy rotates the turbine which powers the compressor via a mechanical coupling. The hot gases are then exhausted to provide thrust. Gas turbine cycle engines employ a continuous combustion system where compression, combustion, and expansion occur simultaneously at different places in the engine—giving continuous power. Notably, the combustion takes place at constant pressure, rather than with the Otto cycle, constant volume.|2023-09-27-11-24-56
Internal combustion engine|Wankel engines|" The Wankel engine (rotary engine) does not have piston strokes. It operates with the same separation of phases as the four-stroke engine with the phases taking place in separate locations in the engine. In thermodynamic terms it follows the Otto engine cycle, so may be thought of as a ""four-phase"" engine. While it is true that three power strokes typically occur per rotor revolution, due to the 3:1 revolution ratio of the rotor to the eccentric shaft, only one power stroke per shaft revolution actually occurs. The drive (eccentric) shaft rotates once during every power stroke instead of twice (crankshaft), as in the Otto cycle, giving it a greater power-to-weight ratio than piston engines. This type of engine was most notably used in the Mazda RX-8, the earlier RX-7, and other vehicle models. The engine is also used in unmanned aerial vehicles, where the small size and weight and the high power-to-weight ratio are advantageous."|2023-09-27-11-24-56
Internal combustion engine|Forced induction| Forced induction is the process of delivering compressed air to the intake of an internal combustion engine. A forced induction engine uses a gas compressor to increase the pressure, temperature and density of the air. An engine without forced induction is considered a naturally aspirated engine. Forced induction is used in the automotive and aviation industry to increase engine power and efficiency. It particularly helps aviation engines, as they need to operate at high altitude. Forced induction is achieved by a supercharger, where the compressor is directly powered from the engine shaft or, in the turbocharger, from a turbine powered by the engine exhaust.|2023-09-27-11-24-56
Internal combustion engine|Fuels and oxidizers| All internal combustion engines depend on combustion of a chemical fuel, typically with oxygen from the air (though it is possible to inject nitrous oxide to do more of the same thing and gain a power boost). The combustion process typically results in the production of a great quantity of thermal energy, as well as the production of steam and carbon dioxide and other chemicals at very high temperature; the temperature reached is determined by the chemical make up of the fuel and oxidizers (see stoichiometry), as well as by the compression and other factors.|2023-09-27-11-24-56
Internal combustion engine|Fuels|" The most common modern fuels are made up of hydrocarbons and are derived mostly from fossil fuels (petroleum). Fossil fuels include diesel fuel, gasoline and petroleum gas, and the rarer use of propane. Except for the fuel delivery components, most internal combustion engines that are designed for gasoline use can run on natural gas or liquefied petroleum gases without major modifications. Large diesels can run with air mixed with gases  and a pilot diesel fuel ignition injection. Liquid and gaseous biofuels, such as ethanol and biodiesel (a form of diesel fuel that is produced from crops that yield triglycerides such as soybean oil), can also be used. Engines with appropriate modifications can also run on hydrogen gas, wood gas, or charcoal gas, as well as from so-called producer gas made from other convenient biomass. Experiments have also been conducted using powdered solid fuels, such as the magnesium injection cycle. Presently, fuels used include: Even fluidized metal powders and explosives have seen some use. Engines that use gases for fuel are called gas engines and those that use liquid hydrocarbons are called oil engines; however, gasoline engines are also often colloquially referred to as ""gas engines"" (""petrol engines"" outside North America). The main limitations on fuels are that it must be easily transportable through the fuel system to the combustion chamber, and that the fuel releases sufficient energy in the form of heat upon combustion to make practical use of the engine. Diesel engines are generally heavier, noisier, and more powerful at lower speeds than gasoline engines. They are also more fuel-efficient in most circumstances and are used in heavy road vehicles, some automobiles (increasingly so for their increased fuel efficiency over gasoline engines), ships, railway locomotives, and light aircraft. Gasoline engines are used in most other road vehicles including most cars, motorcycles, and mopeds. In Europe, sophisticated diesel-engined cars have taken over about 45% of the market since the 1990s. There are also engines that run on hydrogen, methanol, ethanol, liquefied petroleum gas (LPG), biodiesel, paraffin and tractor vaporizing oil (TVO). Hydrogen could eventually replace conventional fossil fuels in traditional internal combustion engines. Alternatively fuel cell technology may come to deliver its promise and the use of the internal combustion engines could even be phased out. Although there are multiple ways of producing free hydrogen, those methods require converting combustible molecules into hydrogen or consuming electric energy. Unless that electricity is produced from a renewable source—and is not required for other purposes—hydrogen does not solve any energy crisis. In many situations the disadvantage of hydrogen, relative to carbon fuels, is its storage. Liquid hydrogen has extremely low density (14 times lower than water) and requires extensive insulation—whilst gaseous hydrogen requires heavy tankage. Even when liquefied, hydrogen has a higher specific energy but the volumetric energetic storage is still roughly five times lower than gasoline. However, the energy density of hydrogen is considerably higher than that of electric batteries, making it a serious contender as an energy carrier to replace fossil fuels. The 'Hydrogen on Demand' process (see direct borohydride fuel cell) creates hydrogen as needed, but has other issues, such as the high price of the sodium borohydride that is the raw material."|2023-09-27-11-24-56
Internal combustion engine|Oxidizers| Since air is plentiful at the surface of the earth, the oxidizer is typically atmospheric oxygen, which has the advantage of not being stored within the vehicle. This increases the power-to-weight and power-to-volume ratios. Other materials are used for special purposes, often to increase power output or to allow operation under water or in space.|2023-09-27-11-24-56
Internal combustion engine|Cooling| Cooling is required to remove excessive heat—high temperature can cause engine failure, usually from wear (due to high-temperature-induced failure of lubrication), cracking or warping. Two most common forms of engine cooling are air-cooled and water-cooled. Most modern automotive engines are both water and air-cooled, as the water/liquid-coolant is carried to air-cooled fins and/or fans, whereas larger engines may be singularly water-cooled as they are stationary and have a constant supply of water through water-mains or fresh-water, while most power tool engines and other small engines are air-cooled. Some engines (air or water-cooled) also have an oil cooler. In some engines, especially for turbine engine blade cooling and liquid rocket engine cooling, fuel is used as a coolant, as it is simultaneously preheated before injecting it into a combustion chamber.|2023-09-27-11-24-56
Internal combustion engine|Starting|" Internal combustion engines must have their cycles started. In reciprocating engines this is accomplished by turning the crankshaft (Wankel Rotor Shaft) which induces the cycles of intake, compression, combustion, and exhaust.  The first engines were started with a turn of their flywheels, while the first vehicle (the Daimler Reitwagen) was started with a hand crank.  All ICE engined automobiles were started with hand cranks until Charles Kettering developed the electric starter for automobiles.  This method is now the most widely used, even among non-automobiles. As diesel engines have become larger and their mechanisms heavier, air starters have come into use.  This is due to the lack of torque in electric starters. Air starters work by pumping compressed air into the cylinders of an engine to start it turning. Two-wheeled vehicles may have their engines started in one of four ways: There are also starters where a spring is compressed by a crank motion and then used to start an engine. Some small engines use a pull-rope mechanism called ""recoil starting"", as the rope rewinds itself after it has been pulled out to start the engine. This method is commonly used in  pushed lawn mowers and other settings where only a small amount of torque is needed to turn an engine over. Turbine engines are frequently started by an electric motor or by compressed air."|2023-09-27-11-24-56
Internal combustion engine|Measures of engine performance| Engine types vary greatly in a number of different ways:|2023-09-27-11-24-56
Internal combustion engine|Energy efficiency|" Once ignited and burnt, the combustion products—hot gases—have more available thermal energy than the original compressed fuel-air mixture (which had higher chemical energy). This available energy is manifested as a higher temperature and pressure that can be converted into kinetic energy by the engine. In a reciprocating engine, the high-pressure gases inside the cylinders drive the engine's pistons. Once the available energy has been removed, the remaining hot gases are vented (often by opening a valve or exposing the exhaust outlet) and this allows the piston to return to its previous position (top dead center, or TDC). The piston can then proceed to the next phase of its cycle, which varies between engines. Any thermal energy that is not translated into work is normally considered a waste product and is removed from the engine either by an air or liquid cooling system. Internal combustion engines are considered heat engines (since the release of chemical energy in combustion has the same effect as heat transfer into the engine) and as such their theoretical efficiency can be approximated by idealized thermodynamic cycles. The thermal efficiency of a theoretical cycle cannot exceed that of the Carnot cycle, whose efficiency is determined by the difference between the lower and upper operating temperatures of the engine. The upper operating temperature of an engine is limited by two main factors; the thermal operating limits of the materials, and the auto-ignition resistance of the fuel. All metals and alloys have a thermal operating limit, and there is significant research into ceramic materials that can be made with greater thermal stability and desirable structural properties. Higher thermal stability allows for a greater temperature difference between the lower (ambient) and upper operating temperatures, hence greater thermodynamic efficiency. Also, as the cylinder temperature rises, the fuel becomes more prone to auto-ignition. This is caused when the cylinder temperature nears the flash point of the charge. At this point, ignition can spontaneously occur before the spark plug fires, causing excessive cylinder pressures. Auto-ignition can be mitigated by using fuels with high auto-ignition resistance (octane rating), however it still puts an upper bound on the allowable peak cylinder temperature. The thermodynamic limits assume that the engine is operating under ideal conditions: a frictionless world, ideal gases, perfect insulators, and operation for infinite time. Real world applications introduce complexities that reduce efficiency. For example, a real engine runs best at a specific load, termed its power band. The engine in a car cruising on a highway is usually operating significantly below its ideal load, because it is designed for the higher loads required for rapid acceleration.[citation needed] In addition, factors such as wind resistance reduce overall system efficiency.  Vehicle fuel economy is measured in miles per gallon or in liters per 100 kilometers. The volume of hydrocarbon assumes a standard energy content. Even when aided with turbochargers and stock efficiency aids, most engines retain an average efficiency of about 18–20%.  However, the latest technologies in Formula One engines have seen a boost in thermal efficiency past 50%. 
There are many inventions aimed at increasing the efficiency of IC engines. In general, practical engines are always compromised by trade-offs between different properties such as efficiency, weight, power, heat, response, exhaust emissions, or noise. Sometimes economy also plays a role in not only the cost of manufacturing the engine itself, but also manufacturing and distributing the fuel. Increasing the engine's efficiency brings better fuel economy but only if the fuel cost per energy content is the same."|2023-09-27-11-24-56
Internal combustion engine|Measures of fuel efficiency and propellant efficiency| For stationary and shaft engines including propeller engines, fuel consumption is measured by calculating the brake specific fuel consumption, which measures the mass flow rate of fuel consumption divided by the power produced. For internal combustion engines in the form of jet engines, the power output varies drastically with airspeed and a less variable measure is used: thrust specific fuel consumption (TSFC), which is the mass of propellant needed to generate impulses that is measured in either pound force-hour or the grams of propellant needed to generate an impulse that measures one kilonewton-second. For rockets, TSFC can be used, but typically other equivalent measures are traditionally used, such as specific impulse and effective exhaust velocity.|2023-09-27-11-24-56
Internal combustion engine|Air and noise pollution| Lists Categories|2023-09-27-11-24-56
Internal combustion engine|Air pollution|" Internal combustion engines such as reciprocating internal combustion engines produce air pollution emissions, due to incomplete combustion of carbonaceous fuel. The main derivatives of the process are carbon dioxide CO2, water and some soot—also called particulate matter (PM). The effects of inhaling particulate matter have been studied in humans and animals and include asthma, lung cancer, cardiovascular issues, and premature death. There are, however, some additional products of the combustion process that include nitrogen oxides and sulfur and some uncombusted hydrocarbons, depending on the operating conditions and the fuel-air ratio. Carbon dioxide emissions from internal combustion engines (particularly ones using fossil fuels such as gasoline and diesel) contribute to human-induced climate change.  Increasing the engine's fuel efficiency can reduce, but not eliminate, the amount of CO2 emissions as carbon-based fuel combustion produces CO2.  Since removing CO2 from engine exhaust is impractical, there is increasing interest in alternatives.  Sustainable fuels such as biofuels, synfuels, and electric motors powered by batteries are examples. Not all of the fuel is completely consumed by the combustion process. A small amount of fuel is present after combustion, and some of it reacts to form oxygenates, such as formaldehyde or acetaldehyde, or hydrocarbons not originally present in the input fuel mixture. Incomplete combustion usually results from insufficient oxygen to achieve the perfect stoichiometric ratio. The flame is ""quenched"" by the relatively cool cylinder walls, leaving behind unreacted fuel that is expelled with the exhaust. When running at lower speeds, quenching is commonly observed in diesel (compression ignition) engines that run on natural gas. Quenching reduces efficiency and increases knocking, sometimes causing the engine to stall. Incomplete combustion also leads to the production of carbon monoxide (CO). Further chemicals released are benzene and 1,3-butadiene that are also hazardous air pollutants. Increasing the amount of air in the engine reduces emissions of incomplete combustion products, but also promotes reaction between oxygen and nitrogen in the air to produce nitrogen oxides (NOx). NOx is hazardous to both plant and animal health, and leads to the production of ozone (O3). Ozone is not emitted directly; rather, it is a secondary air pollutant, produced in the atmosphere by the reaction of NOx and volatile organic compounds in the presence of sunlight. Ground-level ozone is harmful to human health and the environment. Though the same chemical substance, ground-level ozone should not be confused with stratospheric ozone, or the ozone layer, which protects the earth from harmful ultraviolet rays. Carbon fuels containing sulfur produce sulfur monoxides (SO) and sulfur dioxide (SO2) contributing to acid rain. In the United States, nitrogen oxides, PM, carbon monoxide, sulfur dioxide, and ozone, are regulated as criteria air pollutants under the Clean Air Act to levels where human health and welfare are protected. Other pollutants, such as benzene and 1,3-butadiene, are regulated as hazardous air pollutants whose emissions must be lowered as much as possible depending on technological and practical considerations. NOx, carbon monoxide and other pollutants are frequently controlled via exhaust gas recirculation which returns some of the exhaust back into the engine intake.  Catalytic converters are used to convert exhaust chemicals to CO2 (a greenhouse gas), H2O (water vapour, also a greenhouse gas) and N2 (nitrogen). The emission standards used by many countries have special requirements for non-road engines which are used by equipment and vehicles that are not operated on the public roadways. The standards are separated from the road vehicles."|2023-09-27-11-24-56
Internal combustion engine|Noise pollution| Significant contributions to noise pollution are made by internal combustion engines. Automobile and truck traffic operating on highways and street systems produce noise, as do aircraft flights due to jet noise, particularly supersonic-capable aircraft. Rocket engines create the most intense noise.|2023-09-27-11-24-56
Internal combustion engine|Idling| Internal combustion engines continue to consume fuel and emit pollutants while idling. Idling is reduced by stop-start systems.|2023-09-27-11-24-56
Internal combustion engine|Carbon dioxide formation| A good way to estimate the mass of carbon dioxide that is released when one litre of diesel fuel (or gasoline) is combusted can be found as follows: As a good approximation the chemical formula of diesel is CnH2n. In reality diesel is a mixture of different molecules. As carbon has a molar mass of 12 g/mol and hydrogen (atomic) has a molar mass of about 1 g/mol, the fraction by weight of carbon in diesel is roughly 12⁄14. The reaction of diesel combustion is given by: 2CnH2n + 3nO2 ⇌ 2nCO2 + 2nH2O Carbon dioxide has a molar mass of 44 g/mol as it consists of 2 atoms of oxygen (16 g/mol) and 1 atom of carbon (12 g/mol). So 12 g of carbon yields 44 g of carbon dioxide. Diesel has a density of 0.838 kg per litre. Putting everything together the mass of carbon dioxide that is produced by burning 1 litre of diesel can be calculated as: The figure obtained with this estimation is close to the values found in the literature. For gasoline, with a density of 0.75 kg/L and a ratio of carbon to hydrogen atoms of about 6 to 14, the estimated value of carbon dioxide emission from burning 1 litre of gasoline is:|2023-09-27-11-24-56
Internal combustion engine|Parasitic loss| The term parasitic loss is often applied to devices that take energy from the engine in order to enhance the engine's ability to create more energy or convert energy to motion. In the internal combustion engine, almost every mechanical component, including the drivetrain, causes parasitic loss and could thus be characterized as a parasitic load.|2023-09-27-11-24-56
Internal combustion engine|Examples| Bearings, oil pumps, piston rings, valve springs, flywheels, transmissions, driveshafts, and differentials all act as parasitic loads that rob the system of power. These parasitic loads can be divided into two categories: those inherent to the working of the engine and those drivetrain losses incurred in the systems that transfer power from the engine to the road (such as the transmission, driveshaft, differentials and axles). For example, the former category (engine parasitic loads) includes the oil pump used to lubricate the engine, which is a necessary parasite that consumes power from the engine (its host). Another example of an engine parasitic load is a supercharger, which derives its power from the engine and creates more power for the engine. The power that the supercharger consumes is parasitic loss and is usually expressed in kilowatt or horsepower. While the power that the supercharger consumes in comparison to what it generates is small, it is still measurable or calculable. One of the desirable features of a turbocharger over a supercharger is the lower parasitic loss of the former. Drivetrain parasitic losses include both steady state and dynamic loads. Steady state loads occur at constant speeds and may originate in discrete components such as the torque converter, the transmission oil pump, and/or clutch drag, and in seal/bearing drag, churning of lubricant and gear windage/friction found throughout the system. Dynamic loads occur under acceleration and are caused by inertia of rotating components and/or increased friction.|2023-09-27-11-24-56
Internal combustion engine|Measurement| While rules of thumb such as a 15% power loss from drivetrain parasitic loads have been commonly repeated, the actual loss of energy due to parasitic loads varies between systems. It can be influenced by powertrain design, lubricant type and temperature and many other factors.   In automobiles, drivetrain loss can be quantified by measuring the difference between power measured by an engine dynamometer and a chassis dynamometer. However, this method is primarily useful for measuring steady state loads and may not accurately reflect losses due to dynamic loads.  More advanced methods can be used in a laboratory setting, such as measuring in-cylinder pressure measurements, flow rate and temperature at certain points, and testing of individual parts or sub-assemblies to determine friction and pumping losses. For example, in a dynamometer test by Hot Rod magazine, a Ford Mustang equipped with a modified 357ci small-block Ford V8 engine and an automatic transmission had a measured drivetrain power loss averaging 33%. In the same test, a Buick equipped with a modified 455ci V8 engine and a 4-speed manual transmission was measured to have an average drivetrain power loss of 21%. Laboratory testing of a heavy-duty diesel engine determined that 1.3% of the fuel energy input was lost to parasitic loads of engine accessories such as water and oil pumps.|2023-09-27-11-24-56
Internal combustion engine|Reduction| Automotive engineers and tuners commonly make design choices that reduce parasitic loads in order to improve efficiency and power output.  These may involve the choice of major engine components or systems, such as the use of dry sump lubrication system over a wet sump system. Alternately, this can be effected through substitution of minor components available as aftermarket modifications, such as exchanging a directly engine-driven fan for one equipped with a fan clutch or an electric fan.  Another modification to reduce parasitic loss, usually seen in track-only cars, is the replacement of an engine-driven water pump for an electrical water pump.  The reduction in parasitic loss from these changes may be due to reduced friction or many other variables that cause the design to be more efficient.[citation needed]|2023-09-27-11-24-56
Kessler syndrome|General|" The Kessler syndrome (also called the Kessler effect,   collisional cascading, or ablation cascade), proposed by NASA scientist Donald J. Kessler in 1978, is a scenario in which the density of objects in low Earth orbit (LEO) due to space pollution is numerous enough that collisions between objects could cause a cascade in which each collision generates space debris that increases the likelihood of further collisions.  In 2009 Kessler wrote that modeling results had concluded that the debris environment was already unstable, ""such that any attempt to achieve a growth-free small debris environment by eliminating sources of past debris will likely fail because fragments from future collisions will be generated faster than atmospheric drag will remove them"".  One implication is that the distribution of debris in orbit could render space activities and the use of satellites in specific orbital ranges difficult for many generations."|2023-08-27-07-25-57
Kessler syndrome|NORAD, Gabbard and Kessler|" Willy Ley predicted in 1960 that ""In time, a number of such accidentally too-lucky shots will accumulate in space and will have to be removed when the era of manned space flight arrives"".  After the launch of Sputnik 1 in 1957, the North American Aerospace Defense Command (NORAD) began compiling a database (the Space Object Catalog) of all known rocket launches and objects reaching orbit: satellites, protective shields and upper- and lower-stage booster rockets. NASA later published[when?] modified versions of the database in two-line element set,  and during the early 1980s the CelesTrak bulletin board system re-published them. The trackers who fed the database were aware of other objects in orbit, many of which were the result of in-orbit explosions.  Some were deliberately caused during the 1960s anti-satellite weapon (ASAT) testing, and others were the result of rocket stages blowing up in orbit as leftover propellant expanded and ruptured their tanks. To improve tracking, NORAD employee John Gabbard kept a separate database. Studying the explosions, Gabbard developed a technique for predicting the orbital paths of their products, and Gabbard diagrams (or plots) are now widely used. These studies were used to improve the modeling of orbital evolution and decay. When the NORAD database became publicly available during the 1970s, NASA scientist Donald J. Kessler applied the technique developed for the asteroid-belt study to the database of known objects. In June 1978, Kessler and Burton Cour-Palais co-authored ""Collision Frequency of Artificial Satellites: The Creation of a Debris Belt"",  demonstrating that the process controlling asteroid evolution would cause a similar collision process in LEO in decades rather than billions of years. They concluded that by about 2000, space debris would outpace micrometeoroids as the primary ablative risk to orbiting spacecraft. At the time, it was widely thought that drag from the upper atmosphere would de-orbit debris faster than it was created.[citation needed] However, Gabbard was aware that the number and type of objects in space were under-represented in the NORAD data and was familiar with their behavior. In an interview shortly after the publication of the 1978 paper, Gabbard coined the term Kessler syndrome to refer to the accumulation of debris;  it became widely used after its appearance in a 1982 Popular Science article,  which won the Aviation-Space Writers Association 1982 National Journalism Award."|2023-08-27-07-25-57
Kessler syndrome|Follow-up studies| The lack of hard data about space debris prompted a series of studies to better characterize the LEO environment. In October 1979, NASA provided Kessler with funding for further studies.  Several approaches were used by these studies. Optical telescopes and short-wavelength radar were used to measure the number and size of space objects, and these measurements demonstrated that the published population count was at least 50% too low.  Before this, it was believed that the NORAD database accounted for the majority of large objects in orbit. Some objects (typically, US military spacecraft) were found to be omitted from the NORAD list, and others were not included because they were considered unimportant. The list could not easily account for objects under 20 cm (8 in) in size—in particular, debris from exploding rocket stages and several 1960s anti-satellite tests. Returned spacecraft were microscopically examined for small impacts, and sections of Skylab and the Apollo Command/Service Module which were recovered were found to be pitted. Each study indicated that the debris flux was higher than expected and debris was the primary source of micrometeoroids and orbital debris collisions in space. LEO already demonstrated the Kessler syndrome. In 1978, Kessler found that 42 percent of cataloged debris was the result of 19 events, primarily explosions of spent rocket stages (especially US Delta rockets).  He discovered this by first identifying those launches that were described as having a large number of objects associated with a payload, then researching the literature to determine the rockets used in the launch.  In 1979, this finding resulted in establishment of the NASA Orbital Debris Program after a briefing to NASA senior management, overturning the previously held belief that most unknown debris was from old ASAT tests, not from US upper stage rocket explosions that could seemingly be easily managed by depleting the unused fuel from the upper stage Delta rocket following the payload injection.  Beginning in 1986, when it was discovered that other international agencies were possibly experiencing the same type of problem, NASA expanded its program to include international agencies, the first being the European Space Agency.   A number of other Delta components in orbit (Delta was a workhorse of the US space program) had not yet exploded.[citation needed]|2023-08-27-07-25-57
Kessler syndrome|A new Kessler syndrome|" During the 1980s, the United States Air Force (USAF) conducted an experimental program to determine what would happen if debris collided with satellites or other debris. The study demonstrated that the process differed from micrometeoroid collisions, with large chunks of debris created which would become collision threats. In 1991, Kessler published ""Collisional cascading: The limits of population growth in low Earth orbit""  with the best data then available. Citing the USAF conclusions about creation of debris, he wrote that although almost all debris objects (such as paint flecks) were lightweight, most of its mass was in debris about 1 kg (2 lb 3 oz) or heavier. This mass could destroy a spacecraft on impact, creating more debris in the critical-mass area.  According to the National Academy of Sciences: A 1 kg object impacting at 10 km/s, for example, is probably capable of catastrophically breaking up a 1,000 kg spacecraft if it strikes a high-density element in the spacecraft. In such a breakup, numerous fragments larger than 1 kg would be created. Kessler's analysis divided the problem into three parts. With a low-enough density, the addition of debris by impacts is slower than their decay rate and the problem is not significant. Beyond that is a critical density, where additional debris leads to additional collisions. At densities beyond this critical mass production exceeds decay, leading to a cascading chain reaction reducing the orbiting population to small objects (several centimeters in size) and increasing the hazard of space activity.  This chain reaction is known as the Kessler syndrome. In an early 2009 historical overview, Kessler summed up the situation: Aggressive space activities without adequate safeguards could significantly shorten the time between collisions and produce an intolerable hazard to future spacecraft. Some of the most environmentally dangerous activities in space include large constellations such as those initially proposed by the Strategic Defense Initiative in the mid-1980s, large structures such as those considered in the late-1970s for building solar power stations in Earth orbit, and anti-satellite warfare using systems tested by the USSR, the US, and China over the past 30 years. Such aggressive activities could set up a situation where a single satellite failure could lead to cascading failures of many satellites in a period much shorter than years."|2023-08-27-07-25-57
Kessler syndrome|Anti-satellite missile tests| In 1985, the first anti-satellite (ASAT) missile was used in the destruction of a satellite. The American 1985 ASM-135 ASAT test was carried out, in which the Solwind P78-1 satellite flying at an altitude of 555 kilometres was struck by the 14-kilogram payload at a velocity of 24,000 kilometres per hour (15,000 mph; 6.7 km/s). When NASA learned of U.S. Air Force plans for the Solwind ASAT test, they modeled the effects of the test and determined that debris produced by the collision would still be in orbit late into the 1990s. It would force NASA to enhance debris shielding for its planned space station. On 11 January 2007, China conducted an anti-satellite missile test in which one of their FY-1C weather satellites was chosen as the target. The collision occurred at an altitude of 865 kilometres, when the satellite with a mass of 750 kilograms was struck in a head-on-collision by a kinetic payload traveling with a speed of 8 km/s (18,000 mph) in the opposite direction. The resulting debris orbits the Earth with a mean altitude above 850 kilometres, and will likely remain in orbit for decades or centuries. The destruction of the Kosmos 1408 satellite by a Russian ASAT missile on November 15, 2021, has created a large debris cloud, with 1500 pieces of debris being tracked and an estimated hundreds of thousands of pieces too small to track. Since the satellite was in a polar orbit, and its debris has spread out between the altitudes of 300 km and 1000 km, it could potentially collide with any LEO satellite, including the International Space Station and the Chinese Space Station (Tiangong).|2023-08-27-07-25-57
Kessler syndrome|Debris generation and destruction| Every satellite, space probe, and crewed mission has the potential to produce space debris. The theoretical cascading Kessler syndrome becomes more likely as satellites in orbit increase in number. As of 2014, there were about 2,000 commercial and government satellites orbiting the Earth,  and as of 2021 more than 4000.  It is estimated that there are 600,000 pieces of space junk ranging from 1 to 10 cm (1⁄2 to 4 in), and 23,000 larger than that.  On average one satellite is destroyed by collision with space junk each year.   As of 2009 there had been four collisions between catalogued objects, including a collision between two satellites in 2009. Orbital decay is much slower at altitudes where atmospheric drag is insignificant. Slight atmospheric drag, lunar perturbation, and solar wind drag can gradually bring debris down to lower altitudes where fragments finally re-enter, but this process can take millennia at very high altitudes.|2023-08-27-07-25-57
Kessler syndrome|Implications| The Kessler syndrome is troublesome because of the domino effect and feedback runaway wherein impacts between objects of sizable mass spall off debris from the force of the collision. The fragments can then hit other objects, producing even more space debris: if a large enough collision or explosion were to occur, such as between a space station and a defunct satellite, or as the result of hostile actions in space, then the resulting debris cascade could make prospects for long-term viability of satellites in particular low Earth orbits extremely low.   However, even a catastrophic Kessler scenario at LEO would pose minimal risk for launches continuing past LEO, or satellites travelling at medium Earth orbit (MEO) or geosynchronous orbit (GEO). The catastrophic scenarios predict an increase in the number of collisions per year, as opposed to a physically impassable barrier to space exploration that occurs in higher orbits.[citation needed]|2023-08-27-07-25-57
Kessler syndrome|Avoidance and reduction| Designers of a new vehicle or satellite are frequently required by the ITU  to demonstrate that it can be safely disposed of at the end of its life, for example by use of a controlled atmospheric reentry system or a boost into a graveyard orbit.  For US launches or satellites that will have broadcast to US territories—in order to obtain a license to provide telecommunications services in the United States—the Federal Communications Commission (FCC) required all geostationary satellites launched after 18 March 2002 to commit to moving to a graveyard orbit at the end of their operational life.  US government regulations similarly require a plan to dispose of satellites after the end of their mission: atmospheric re-entry,[clarification needed] movement to a storage orbit, or direct retrieval. A proposed energy-efficient means of deorbiting a spacecraft from MEO is to shift it to an orbit in an unstable resonance with the Sun or Moon that speeds up orbital decay. One technology proposed to help deal with fragments from 1 to 10 cm (1⁄2 to 4 in) in size is the laser broom, a proposed multimegawatt land-based laser that could deorbit debris: the side of the debris hit by the laser would ablate and create a thrust that would change the eccentricity of the remains of the fragment until it would re-enter and be destroyed harmlessly.|2023-08-27-07-25-57
Kessler syndrome|Potential triggers| The Envisat satellite is a large, inactive satellite with a mass of 8,211 kg (18,102 lb) that orbits at 785 km (488 mi), an altitude where the debris environment is the greatest—two catalogued objects can be expected to pass within about 200 m (660 ft) of Envisat every year —and likely to increase. Don Kessler predicted in 2012 that it could easily become a major debris contributor from a collision during the next 150 years that it will remain in orbit. SpaceX's Starlink program raises concern among many experts about significantly worsening the possibility of Kessler Syndrome due to the large number of satellites the program aims to place in LEO, as the program's goal will more than double the satellites currently in LEO.  In response to these concerns, SpaceX said that a large part of Starlink satellites are launched at a lower altitude of 550 km (340 mi) to achieve lower latency (versus 1,150 km (710 mi) as originally planned), and failed satellites or debris are thus expected to deorbit within five years even without propulsion, due to atmospheric drag.|2023-08-27-07-25-57
Kids Ocean Day HK|General| Kids Ocean Day HK  was organised by Ocean Recovery Alliance to celebrate Kids Ocean Day in Hong Kong.|2023-05-09-09-59-17
Kids Ocean Day HK|Foundation| The Malibu Foundation, a California-based non-profit organisation, started Kids Ocean Day to connect children to the ocean and beaches, and to foster understanding of the environmental issues they face.  The first Kids Ocean Day Hong Kong was celebrated 9 November 2012. Over 800 students, teachers and volunteers met at Repulse Bay and helped create a piece of aerial artwork featuring a Chinese white dolphin, organised by aerial artist John Quigley of Spectral Q.  The design was based on 9-year-old Leung Man-Hin's artwork which won the drawing competition for the event.|2023-05-09-09-59-17
Kids Ocean Day HK|Goal| To raise awareness, understanding and appreciation among Hong Kong youth about the state of the ocean and the health of its ecosystem.|2023-05-09-09-59-17
List of least carbon efficient power stations|General| This is a list of least carbon efficient power stations in selected countries. Lists were created by the WWF and lists the most polluting power stations in terms of the level of carbon dioxide produced per unit of electricity generated.  In general lignite burning coal-fired power stations with subcritical boilers (in which bubbles form in contrast to the newer supercritical steam generator) emit the most.   The Chinese national carbon trading scheme may follow the European Union Emission Trading Scheme in making such power stations uneconomic to run.   However some companies such as NLC India Limited and Electricity Generation Company (Turkey) generate in countries without a carbon price. Lignite power stations built or retrofitted before 1995 often also emit local air pollution.     In early 2021 the EU carbon price rose above 50 euros per tonne, causing many of the European plants listed below to become unprofitable,  and close down.  However, because many countries outside Europe and the USA do not publish plant level emissions data it was difficult to make up to date lists. Public information from space-based measurements of carbon dioxide by Climate Trace is expected to quantify CO2 from individual large plants before the 2021 United Nations Climate Change Conference,  thus enabling large polluters to be identified.|2023-04-30-06-45-49
List of least carbon efficient power stations|2015 report - companies| In 2015 the Stranded Assets Programme at the University of Oxford’s Smith School of Enterprise and the Environment published Stranded Assets and Subcritical Coal report analyzing inter alia carbon intensity of subcritical coal-fired power stations of 100 largest companies having these power stations.|2023-04-30-06-45-49
List of least carbon efficient power stations|2018 - largest emitters| The table lists the largest emitters, regardless of their carbon efficiency.|2023-04-30-06-45-49
List of least carbon efficient power stations|Other| At over 1.34 tCO2-e/MWh Yallourn is the most carbon intense in Australia. In the very unlikely event of being built, the proposed Afşin-Elbistan C power station would become the least carbon efficient coal-fired power station.|2023-04-30-06-45-49
Legacy pollution|General| Lists Categories Legacy pollution or legacy pollutants  are persistent materials in the environment that were created through a polluting industry or process that have polluting effects after the process has finished. Frequently these include persistent organic pollutants, heavy metals or other chemicals residual in the environment long after the industrial or extraction processes that produced them.     Often these are chemicals produced by industry and polluted before there was widespread awareness of the toxic effects of the pollutants, and subsequently regulated or banned.  Notable legacy pollutants include mercury, PCBs, Dioxins and other chemicals that are widespread health and environmental effects.   Sites for legacy pollutants include mining sites, industrial parks, waterways contaminated by industry, and other dump sites. These chemicals often have outsized impact in countries jurisdictions with little or no environmental monitoring or regulation—because the chemical were often produced in new jurisdictions after they were banned in more heavily regulated jurisdictions.  Often in these countries, there is a lack of capacity in environmental regulatory, health and civic infrastructure to address the impact of the pollutants. The impact of legacy pollutants can be visible many years after the initial polluting process, and require environmental remediation.  Grassroots communities and environmental defender frequently advocate for responsibility of industry and states through environmental justice action and advocacy for recognition of human rights, such as the right to a healthy environment.|2023-08-28-15-03-28
Legacy pollution|Brownfields| Brownfield refers to land that is abandoned or underutilized due to pollution from industrial use.  The specific definition of brownfield land varies and is decided by policy makers and/or land developers within different countries.   The main difference in definitions of whether a piece of land is considered a brownfield or not depends on the presence or absence of pollution.   Overall, brownfield land is a site previously developed for industrial or commercial purposes and thus requires further development before reuse.|2023-08-28-15-03-28
Legacy pollution|Mine tailings| In mining, tailings or tails are the materials left over after the process of separating the valuable fraction from the uneconomic fraction (gangue) of an ore. Tailings are different from overburden, which is the waste rock or other material that overlies an ore or mineral body and is displaced during mining without being processed.|2023-08-28-15-03-28
Legacy pollution|Abandoned mines| An abandoned mine refers to a former mining or quarrying operation that is no longer in use and has no responsible entity to finance the cost of remediation and/or restoration of the mine feature or site. Such mines are typically left unattended and may pose safety hazards or cause environmental damage without proper maintenance. The term incorporates all types of old mines, including underground shaft mines and drift mines, and surface mines, including quarries and placer mining. Typically, the cost of addressing the mine's hazards is borne by the public/taxpayers/the government. An abandoned mine may be a hazard to health, safety or environment.|2023-08-28-15-03-28
Legacy pollution|Abandoned gas wells| Orphan, orphaned or abandoned wells are oil or gas wells that have been abandoned by fossil fuel extraction industries. These wells may have been deactivated because of economic viability, failure to transfer ownerships (especially at bankruptcy of companies), or neglect and thus no longer have legal owners responsible for their care. Decommissioning wells effectively can be expensive, costing millions of dollars,  and economic incentives for businesses generally encourage abandonment. This process leaves the wells the burden of  government agencies or landowners when a business entity can no longer be held responsible. As climate change mitigation reduces demand and usage of oil and gas, its expected that more wells will be abandoned as stranded assets. Orphan wells are a potent contributor of greenhouse gas emissions, such as methane emissions, causing climate change. Much of this leakage can be attributed to broken plugs, or failure to plug properly. A 2020 estimate of US abandoned wells alone was that methane emissions released from abandoned wells produced greenhouse gas impacts equivalent of 3 weeks of US oil consumption each year.  The scale of leaking abandoned wells are well understood in the US and Canada because of public data and regulation; however, a Reuters investigation in 2020 could not find good estimates for Russia, Saudi Arabia and China—the next biggest oil and gas producers.  However, they estimate there are 29 million abandoned wells internationally.|2023-08-28-15-03-28
Legacy pollution|International policy| The Stockholm Convention on Persistent Organic Pollutants is one of the main international mechanisms for supporting the elimination of legacy persistent organic pollutants such as PCBs.|2023-08-28-15-03-28
Light pollution|General|" Lists Categories Light pollution is the presence of unwanted, inappropriate, or excessive artificial lighting.   In a descriptive sense, the term light pollution refers to the effects of any poorly implemented lighting, during the day or night. Light pollution can be understood not only as a phenomenon resulting from a specific source or kind of pollution, but also as a contributor to the wider, collective impact of various sources of pollution. Although this type of pollution can exist throughout the day, its effects are magnified during the night with the contrast of darkness. It has been estimated that 83 percent of the world's people live under light-polluted skies and that 23 percent of the world's land area is affected by skyglow.  
The area affected by artificial illumination continues to increase. 
A major side-effect of urbanization, light pollution is blamed for compromising health, disrupting ecosystems, and spoiling aesthetic environments. Globally, it has increased by at least 49% from 1992 to 2017. Solutions to light pollution are often easy steps like adjusting light fixtures or using more appropriate lightbulbs. However, because it is a manmade phenomenon, addressing its impacts on humans and the environment has political, social, and economic considerations."|2023-09-23-20-01-28
Light pollution|Definitions|" Light pollution is the presence of anthropogenic artificial light in otherwise dark conditions. The term is most commonly used in relation to in the outdoor environment and surrounding, but is also used to refer to artificial light indoors. Adverse consequences are multiple; some of them may not be known yet. Light pollution competes with starlight in the night sky for urban residents, interferes with astronomical observatories,  and, like any other form of pollution, disrupts ecosystems and has adverse health effects.
  
 
Light pollution is a side-effect of industrial civilization. Its sources include building exterior and interior lighting, advertising, outdoor area lighting (such as car parks), offices, factories, streetlights, and illuminated sporting venues. It is most severe in highly industrialized, densely populated areas of North America, Europe, and Asia and in major cities in the Middle East and North Africa like Tehran and Cairo, but even relatively small amounts of light can be noticed and create problems. Awareness of the deleterious effects of light pollution began in the second half of the 19th century,  but efforts to address its effects did not begin until the 1950s.  In the 1980s a global dark-sky movement emerged with the founding of the International Dark-Sky Association (IDA). There are now such educational and advocacy organizations in many countries worldwide. About 83% of people, including 99% of Europeans and Americans, live under light-polluted skies that are more than 10% brighter than natural darkness.  80% of North Americans cannot see the Milky Way galaxy."|2023-09-23-20-01-28
Light pollution|Remediation| Energy conservation advocates contend that light pollution must be addressed by changing the habits of society,  so that lighting is used more efficiently, with less waste and less creation of unwanted or unneeded illumination.[citation needed] Several industry groups[which?] also recognize light pollution as an important issue. For example, the Institution of Lighting Engineers in the United Kingdom provides its members with information about light pollution, the problems it causes, and how to reduce its impact.  Although, recent research  point that the energy efficiency is not enough to reduce the light pollution because of the rebound effect. Since people may disagree over whether any particular lighting source is irritating or how important its effects on non-human life are, it is common for one person to consider as light pollution something that another finds desirable. One example is found in advertising, when an advertiser wishes for particular lights to be bright and visible while others find them annoying. Other types of light pollution are less disputed. For instance, light that accidentally crosses a property boundary and annoys a neighbour is generally considered wasted and pollutive. For this reason and others, decisions about how to manage artificial light are often marked by disputes. Differences of opinion over what light is reasonable and who should have authority and responsibility sometimes make it necessary for parties to negotiate. Where it is desired that such decisions be supported by objective data, light levels can be quantified by field measurement or mathematical modeling, the results of which are typically rendered in isophote maps or light contour maps. To deal with light pollution, authorities have taken a variety of measures depending on the interests, beliefs, and understandings of the society involved.[citation needed] These measures range from doing nothing at all to implementing strict laws and regulations specifying how lights may be installed and used.|2023-09-23-20-01-28
Light pollution|Types| Light pollution is caused by inefficient or unnecessary use of artificial light. Specific categories of light pollution include light trespass, over-illumination, glare, light clutter, and skyglow. A single offending light source often falls into more than one of these categories.|2023-09-23-20-01-28
Light pollution|Light trespass| Light trespass occurs when unwanted light enters one's property, for instance, by shining over a neighbour's fence. A common light trespass problem occurs when a strong light enters the window of one's home from the outside, causing problems such as sleep deprivation. A number of cities in the U.S. have developed standards for outdoor lighting to protect the rights of their citizens against light trespass. To assist them, the International Dark-Sky Association has developed a set of model lighting ordinances. The Dark-Sky Association was started to reduce the light going up into the sky which reduces the visibility of stars (see Skyglow below). This is any light that is emitted more than 90° above nadir. By limiting light at this 90° mark they have also reduced the light output in the 80–90° range which creates most of the light trespass issues. U.S. federal agencies may also enforce standards and process complaints within their areas of jurisdiction. For instance, in the case of light trespass by white strobe lighting from communication towers in excess of FAA minimum lighting requirements  the Federal Communications Commission maintains an Antenna Structure Registration database  information which citizens may use to identify offending structures and provides a mechanism for processing citizen inquiries and complaints.  The U.S. Green Building Council (USGBC) has also incorporated a credit for reducing the amount of light trespass and sky glow into their environmentally friendly building standard known as LEED. Light trespass can be reduced by selecting light fixtures that limit the amount of light emitted more than 80° above the nadir. The IESNA definitions include full cutoff (0%), cutoff (10%), and semi-cutoff (20%). (These definitions also include limits on light emitted above 90° to reduce sky glow.)|2023-09-23-20-01-28
Light pollution|Over-illumination|" Over-illumination is the excessive use of light.[citation needed] In the USA commercial building lighting consumes in excess of 81.68 terawatt-hours (1999 data) of electricity per year,  according to the DOE. Even among developed countries there are large differences in patterns of light use. American cities emit three to five times more light to space per capita compared to German cities. Over-illumination stems from several factors: Most of these issues can be readily corrected with available, inexpensive technology, and with the resolution of landlord/tenant practices that create barriers to rapid correction of these matters. Most importantly, public awareness would need to improve for industrialized countries to realize the large payoff in reducing over-illumination. In certain cases, an over-illumination lighting technique may be needed. For example, indirect lighting is often used to obtain a ""softer"" look, since hard direct lighting is generally found less desirable for certain surfaces, such as skin. The indirect lighting method is perceived as cozier and suits bars, restaurants, and living quarters. It is also possible to block the direct lighting effect by adding softening filters or other solutions, though intensity will be reduced."|2023-09-23-20-01-28
Light pollution|Glare|" Glare can be categorized into different types. One such classification is described in a book by Bob Mizon, coordinator for the British Astronomical Association's Campaign for Dark Skies, as follows: According to Mario Motta, president of the Massachusetts Medical Society, ""... glare from bad lighting is a public-health hazard—especially the older you become. Glare light scattering in the eye causes loss of contrast and leads to unsafe driving conditions, much like the glare on a dirty windshield from low-angle sunlight or the high beams from an oncoming car.""  In essence bright and/or badly shielded lights around roads can partially blind drivers or pedestrians and contribute to accidents. The blinding effect is caused in large part by reduced contrast due to light scattering in the eye by excessive brightness, or to the reflection of light from dark areas in the field of vision, with luminance similar to the background luminance. This kind of glare is a particular instance of disability glare, called veiling glare. (This is not the same as loss of accommodation of night vision which is caused by the direct effect of the light itself on the eye.)"|2023-09-23-20-01-28
Light pollution|Light clutter| Light clutter refers to excessive groupings of lights. Groupings of lights may generate confusion, distract from obstacles (including those that they may be intended to illuminate), and potentially cause accidents. Clutter is particularly noticeable on roads where the street lights are badly designed, or where brightly lit advertisements surround the roadways. Depending on the motives of the person or organization that installed the lights, their placement and design can even be intended to distract drivers, and can contribute to accidents.|2023-09-23-20-01-28
Light pollution|From satellites| Another source of light pollution are artificial satellites. With future increase in numbers of satellite constellations such as OneWeb and Starlink, it is feared especially by the astronomical community, such as the IAU that light pollution will increase significantly, beside other problems of satellite overcrowding. Public discourse surrounding the anticipated growth of satellite constellation, like OneWeb and Starlink, includes multiple petitions by astronomers and citizen scientists,   and has raised questions about which regulatory bodies hold jurisdiction over human actions that obscure starlight.|2023-09-23-20-01-28
Light pollution|Issues to measuring light pollution| Measuring the effect of sky glow on a global scale is a complex procedure. The natural atmosphere is not completely dark, even in the absence of terrestrial sources of light and illumination from the Moon. This is caused by two main sources: airglow and scattered light. At high altitudes, primarily above the mesosphere, there is enough UV radiation from the sun at very short wavelengths to cause ionization. When the ions collide with electrically neutral particles they recombine and emit photons in the process, causing airglow. The degree of ionization is sufficiently large to allow a constant emission of radiation even during the night when the upper atmosphere is in the Earth's shadow. Lower in the atmosphere all the solar photons with energies above the ionization potential of N2 and O2 have already been absorbed by the higher layers and thus no appreciable ionization occurs. Apart from emitting light, the sky also scatters incoming light, primarily from distant stars and the Milky Way, but also the zodiacal light, sunlight that is reflected and backscattered from interplanetary dust particles. The amount of airglow and zodiacal light is quite varied (depending, amongst other things on sunspot activity and the Solar cycle) but given optimal conditions, the darkest possible sky has a brightness of about 22 magnitude/square arc second. If a full moon is present, the sky brightness increases to about 18 magnitude/sq. arcsecond depending on local atmospheric transparency, 40 times brighter than the darkest sky. In densely populated areas a sky brightness of 17 magnitude/sq. an arcsecond is not uncommon, or as much as 100 times brighter than is natural.|2023-09-23-20-01-28
Light pollution|Satellite imagery measuring| To precisely measure how bright the sky gets, night time satellite imagery of the earth is used as raw input for the number and intensity of light sources. These are put into a physical model  of scattering due to air molecules and aerosoles to calculate cumulative sky brightness. Maps that show the enhanced sky brightness have been prepared for the entire world.|2023-09-23-20-01-28
Light pollution|Bortle scale|" The Bortle scale is a nine-level measuring system used to track how much light pollution there is in the sky. A Bortle scale of five or less is required to see the Milky Way whilst one is ""pristine"", the darkest possible."|2023-09-23-20-01-28
Light pollution|Europe| Inspection of the area surrounding Madrid reveals that the effects of light pollution caused by a single large conglomeration can be felt up to 100 km (62 mi) away from the center. Global effects of light pollution are also made obvious. Research in the late 1990s showed that the entire area consisting of southern England, Netherlands, Belgium, West Germany, and northern France have a sky brightness of at least two to four times normal.  The only places in continental Europe where the sky can attain its natural darkness are in northern Scandinavia and in islands far from the continent.[citation needed] The growth of light pollution on the green band has been 11% from 2012-2013 to 2014-2020, and 24% on the blue band.|2023-09-23-20-01-28
Light pollution|North America| In North America the situation is comparable. There is a significant problem with light pollution ranging from the Canadian Maritime Provinces to the American Southwest.  The International Dark-Sky Association works to designate areas that have high-quality night skies. These areas are supported by communities and organizations that are dedicated to reducing light pollution (e.g. Dark-sky preserve). The National Park Service Natural Sounds and Night Skies Division has measured night sky quality in national park units across the U.S. Sky quality in the U.S. ranges from pristine (Capitol Reef National Park and Big Bend National Park) to severely degraded (Santa Monica Mountains National Recreation Area and Biscayne National Park).  The National Park Service Night Sky Program monitoring database is available online (2015).|2023-09-23-20-01-28
Light pollution|East Asia| Light pollution in Hong Kong was declared the 'worst on the planet' in March 2013. In June 2016, it was estimated that one third of the world's population could no longer see the Milky Way, including 80% of Americans and 60% of Europeans. Singapore was found to be the most light-polluted country in the world. Over the past 21 years, China's provincial capital cities have seen a major increase in light pollution, with hotspots along the eastern coastline region.|2023-09-23-20-01-28
Light pollution|Public health impact|" Medical research on the effects of excessive light on the human body suggests that a variety of adverse health effects may be caused by light pollution or excessive light exposure, and some lighting design textbooks  use human health as an explicit criterion for proper interior lighting. Health effects of over-illumination or improper spectral composition of light may include: increased headache incidence, worker fatigue, medically defined stress, decrease in sexual function and increase in anxiety.     Likewise, animal models have been studied demonstrating unavoidable light to produce adverse effect on mood and anxiety.  For those who need to be awake at night, light at night also has an acute effect on alertness and mood. Outdoor artificial light at night – exposure to contemporary types such as current types of street lighting – has been linked to risks for obesity,  mental disorders,  diabetes,  and potentially other health issues  by preliminary studies. In 2007, ""shift work that involves circadian disruption"" was listed as a probable carcinogen by the World Health Organization's International Agency for Research on Cancer. (IARC Press release No. 180).   Multiple studies have documented a correlation between night shift work and the increased incidence of breast and prostate cancer.       One study which examined the link between exposure to artificial light at night (ALAN) and levels of breast cancer in South Korea found that regions which had the highest levels of ALAN reported the highest number of cases of breast cancer. Seoul, which had the highest levels of light pollution, had 34.4% more cases of breast cancer than Ganwon-do, which had the lowest levels of light pollution. This suggested a high correlation between ALAN and the prevalence of breast cancer. It was also found that there was no correlation between other types of cancer such as cervical or lung cancer and ALAN levels. A more recent discussion (2009), written by Professor Steven Lockley, Harvard Medical School, can be found in the CfDS handbook ""Blinded by the Light?"".  Chapter 4, ""Human health implications of light pollution"" states that ""... light intrusion, even if dim, is likely to have measurable effects on sleep disruption and melatonin suppression. Even if these effects are relatively small from night to night, continuous chronic circadian, sleep and hormonal disruption may have longer-term health risks"". The New York Academy of Sciences hosted a meeting in 2009 on Circadian Disruption and Cancer.  Red light suppresses melatonin the least. In June 2009, the American Medical Association developed a policy in support of control of light pollution. News about the decision emphasized glare as a public health hazard leading to unsafe driving conditions. Especially in the elderly, glare produces loss of contrast, obscuring night vision. A new 2021 study published in the Southern Economic Journal indicates that light pollution may increase by 13% in preterm births before 23 weeks of gestation."|2023-09-23-20-01-28
Light pollution|Ecological impact|" While light at night can be beneficial, neutral, or damaging for individual species, its presence invariably disturbs ecosystems. For example, some species of spiders avoid lit areas, while other species are happy to build their webs directly on lamp posts. Since lamp posts attract many flying insects, the spiders that tolerate the light gain an advantage over the spiders that avoid it. This is a simple example of the way in which species frequencies and food webs can be disturbed by the introduction of light at night. Light pollution poses a serious threat in particular to nocturnal wildlife, having negative impacts on plant and animal physiology.  It can confuse animal navigation, alter competitive interactions, change predator-prey relations,  and cause physiological harm.  The rhythm of life is orchestrated by the natural diurnal patterns of light and dark, so disruption to these patterns impacts the ecological dynamics.  Many species of marine plankton, such as Calanus copepods, can detect light levels as low as 0.1 μWm−2;  using this as a threshold a global atlas of marine Artificial Light at Night has been generated,  showing its global widespread nature. Studies suggest that light pollution around lakes prevents zooplankton, such as Daphnia, from eating surface algae, causing algal blooms that can kill off the lakes' plants and lower water quality.  Light pollution may also affect ecosystems in other ways. For example, entomologists have documented that nighttime light may interfere with the ability of moths and other nocturnal insects to navigate.  It can also negative impact on insect development and reproduction.  Night-blooming flowers that depend on moths for pollination may be affected by night lighting, as there is no replacement pollinator that would not be affected by the artificial light. This can lead to species decline of plants that are unable to reproduce, and change an area's longterm ecology.  Among nocturnal insects, fireflies (Coleoptera: Lampyridae, Phengodidae and Elateridae) are especially interesting study objects for light pollution, once they depend on their own light to reproduce and, consequently, are very sensitive to environmental levels of light.    Fireflies are well known and interesting to the general public (unlike many other insects)  and are easily spotted by non-experts, and, due to their sensibility and rapid response to environmental changes, good bioindicators for artificial night lighting.  Significant declines in some insect populations have been suggested as being at least partially mediated by artificial lights at night. A 2009 study  also suggests deleterious impacts on animals and ecosystems because of perturbation of polarized light or artificial polarization of light (even during the day, because direction of natural polarization of sun light and its reflection is a source of information for a lot of animals). This form of pollution is named polarized light pollution (PLP). Unnatural polarized light sources can trigger maladaptive behaviors in polarization-sensitive taxa and alter ecological interactions. Lights on tall structures can disorient migrating birds. Estimates by the U.S. Fish and Wildlife Service of the number of birds killed after being attracted to tall towers range from four to five million per year to an order of magnitude higher.  The Fatal Light Awareness Program (FLAP) works with building owners in Toronto, Ontario, Canada and other cities to reduce mortality of birds by turning out lights during migration periods. Another study has found that the lights produced by the Post Tower has affected 25 bird species. As a result, they discovered that decreasing the use of excessive lights increased the survival rate of bird species. Similar disorientation has also been noted for bird species migrating close to offshore production and drilling facilities. Studies carried out by Nederlandse Aardolie Maatschappij b.v. (NAM) and Shell have led to the development and trial of new lighting technologies in the North Sea. In early 2007, the lights were installed on the Shell production platform L15. The experiment proved a great success since the number of birds circling the platform declined by 50 to 90%. Birds migrate at night for several reasons. Save water from dehydration in hot day flying and part of the bird's navigation system works with stars in some way. With city light outshining the night sky, birds (and also about mammals) no longer navigate by stars. Sea turtle hatchlings emerging from nests on beaches are another casualty of light pollution. It is a common misconception that hatchling sea turtles are attracted to the moon. Rather, they find the ocean by moving away from the dark silhouette of dunes and their vegetation, a behavior with which artificial lights interfere.  The breeding activity and reproductive phenology of toads, however, are cued by moonlight.  Juvenile seabirds are also disoriented by lights as they leave their nests and fly out to sea, causing events of high mortality.     Amphibians and reptiles are also affected by light pollution. Introduced light sources during normally dark periods can disrupt levels of melatonin production. Melatonin is a hormone that regulates photoperiodic physiology and behaviour. Some species of frogs and salamanders utilize a light-dependent ""compass"" to orient their migratory behaviour to breeding sites. Introduced light can also cause developmental irregularities, such as retinal damage, reduced juvenile growth, premature metamorphosis,  reduced sperm production, and genetic mutation.       Close to global coastal megacities (e.g. Tokyo, Shanghai), the natural illumination cycles provided by the moon in the marine environment are considerably disrupted by light pollution, with only nights around the full moon providing greater radiances, and over a given month lunar dosages may be a factor of 6 less than light pollution dosages. In September 2009, the 9th European Dark-Sky Symposium in Armagh, Northern Ireland had a session on the environmental effects of light at night (LAN). It dealt with bats, turtles, the ""hidden"" harms of LAN, and many other topics.  The environmental effects of LAN were mentioned as early as 1897, in a Los Angeles Times article. The following is an excerpt from that article, called ""Electricity and English songbirds"": An English journal has become alarmed at the relation of electricity to songbirds, which it maintains is closer than that of cats and fodder crops. How many of us, it asks, foresee that electricity may extirpate the songbird? ... With the exception of the finches, all the English songbirds may be said to be insectivorous, and their diet consists chiefly of vast numbers of very small insects which they collect from the grass and herbs before the dew is dry. As the electric light is finding its way for street illumination into the country parts of England, these poor winged atoms are slain by thousands at each light every warm summer evening. ... The fear is expressed, that when England is lighted from one end to the other with electricity the songbirds will die out from the failure of their food supply."|2023-09-23-20-01-28
Light pollution|Effect on astronomy|" Astronomy is very sensitive to light pollution. The night sky viewed from a city bears no resemblance to what can be seen from dark skies.  Skyglow (the scattering of light in the atmosphere at night) reduces the contrast between stars and galaxies and the sky itself, making it much harder to see fainter objects. This is one factor that has caused newer telescopes to be built in increasingly remote areas. Even at apparent clear night skies, there can be a lot of stray light that becomes visible at longer exposure times in astrophotography. By means of software, the stray light can be reduced, but at the same time, object detail will be lost in the image. The following picture of the area around the Pinwheel Galaxy (Messier 101) with the apparent magnitude of 7.5m with all stars down to an apparent magnitude of 10m was taken in Berlin in a direction close to the zenith with a fast lens (f-number 1.2) and an exposure time of five seconds at an exposure index of ISO 12800: Original shot: lower edge Alkaid, right of center the double star Mizar with Alcor and right edge Alioth; the Pinwheel Galaxy is a small diffuse dot in the center of the image. Black level compensation: the darkest point in the digital picture was set to zero luminance, in order to reduce the visible stray light. However, blue light caused by Rayleigh scattering is visible in the center of the image. 50 percent of stray light removed: the darker half of the stray light was set to zero luminance. The darker part of the blue light caused by Rayleigh scattering is still visible in the center of the image. Complete elimination of stray light: all pixels showing stray light have been set to zero luminance, the faint and two-dimensional Pinwheel Galaxy is no longer visible, too. Some astronomers use narrow-band ""nebula filters"", which allow only specific wavelengths of light commonly seen in nebulae, or broad-band ""light pollution filters"", which are designed to reduce (but not eliminate) the effects of light pollution by filtering out spectral lines commonly emitted by sodium- and mercury-vapor lamps, thus enhancing contrast and improving the view of dim objects such as galaxies and nebulae.  Unfortunately, these light pollution reduction (LPR) filters are not a cure for light pollution. LPR filters reduce the brightness of the object under study and this limits the use of higher magnifications. LPR filters work by blocking light of certain wavelengths, which alters the color of the object, often creating a pronounced green cast. Furthermore, LPR filters work only on certain object types (mainly emission nebulae) and are of little use on galaxies and stars. No filter can match the effectiveness of a dark sky for visual or photographic purposes. Light pollution affects the visibility of diffuse sky objects like nebulae and galaxies more than stars, due to their low surface brightness. Most such objects are rendered invisible in heavily light-polluted skies above major cities. A simple method for estimating the darkness of a location is to look for the Milky Way, which from truly dark skies appears bright enough to cast a shadow. In addition to skyglow, light trespass can impact observations when artificial light directly enters the tube of the telescope and is reflected from non-optical surfaces until it eventually reaches the eyepiece. This direct form of light pollution causes a glow across the field of view, which reduces contrast. Light trespass also makes it hard for a visual observer to become sufficiently adapted to the dark. The usual measures to reduce this glare, if reducing the light directly is not an option, include flocking the telescope tube and accessories to reduce reflection, and putting a light shield (also usable as a dew shield) on the telescope to reduce light entering from angles other than those near the target. Under these conditions, some astronomers prefer to observe under a black cloth to ensure maximum adaptation to the dark."|2023-09-23-20-01-28
Light pollution|Increase in atmospheric pollution| A study presented at the American Geophysical Union meeting in San Francisco found that light pollution destroys nitrate radicals thus preventing the normal night time reduction of atmospheric smog produced by fumes emitted from cars and factories.   The study was presented by Harald Stark from the National Oceanic and Atmospheric Administration.|2023-09-23-20-01-28
Light pollution|Reduction of natural sky polarization|" In the night, the polarization of the moonlit sky is very strongly reduced in the presence of urban light pollution, because scattered urban light is not strongly polarized.  Polarized moonlight cannot be seen by humans, but is believed to be used by many animals for navigation. Economic Impact Research surrounding light pollution focuses on the quality of lighting and reducing our ability to clearly view the sky at night. However, light pollution has many root causes and effects across the spectrum of life. Since the time of the Industrial Revolution grew out of England and spread across the globe, major changes have been made in the way we live. Technological innovation is moving at a rapid pace. It is not uncommon to find 24-hour business, such as gas stations, convenience stores, and pharmacies. Hospitals and other healthcare facilities must be staffed 24 hours per day, seven days per week. With the rise of Amazon, many factories and shipping companies now operate 24x7 shifts to keep up with the demand of the new global consumer. These industries all require light, both inside and outside their facilities to ensure the safety of their workers as they move about their jobs and when the enter and depart the facilities. As a result, ""40% of the United States and almost 20% of the European Union population has lost the ability to view the night sky…in other words, it is as if they never really experience nighttime."" With a focus on shift work and the continued need for 24-hour operations of specific sectors of the economy, researchers are looking at the impact of light pollution on this group of workers. In 2007 the International Agency for Research on Cancer (IARC) sought to bring notice to the risk from shift work as a probable risk for developing cancers. This move was the result of numerous studies that found increased risks of cancers in groups of shift workers. The 1998 Nurses Health Study found a link between breast cancer and nurses who had worked more than 30 years on rotating night shifts. However, it is not possible to halt shift work in these industries. Hospitals must be staffed around the clock. Research suggests that, like other environmental issues, light pollution is primarily a problem caused by industrialized nations. Research by Galloway, et al. (2010) examined numerous economic indicators to get a better sense of where light pollution was occurring around the globe. Galloway's research found that countries with paved roads, a factor in a developed infrastructure, often had increased light pollution (2010). Similarly, countries with a high rate of resource extraction also have high rates of light pollution . Finally, Galloway found that countries with the highest GDP and high surface area described as urban and suburban also had the highest rates of light pollution. China is an emerging leader in industrial and economic growth. A recent study of light pollution using the Defense Meteorological Satellite Program Operational Linescan System (DMSL/OLS) found that light pollution is increasing over the eastern coastal cities but decreasing over the industrial and mineral extraction cities. Specifically, urban areas around the Yangtze River delta, Pearl River delta, and Beijing-Tianjin area are specific light pollution areas of concern. Examining China as a whole, Jiang found that light pollution in the East and North was much higher than the West. This is consistent with major industrial factories located in the East and North while resource extraction dominates the West. In 2010, following the United Nations declaration of The Year of Astronomy researchers urged a better understanding of artificial light and the role it plays in social, economic, and environmental issues. The researchers argues that the continued unfettered use of artificial light in urban and rural areas would cause a global shift with unpredictable outcomes. Holker argued that focusing on the economic impact of increased energy consumption in light bulbs, or the move to energy efficiency of lighting, was not enough. Rather, the broader focus should be on the socio-economic, ecologic, and physiologic impacts of light pollution. In essence, getting your package from Amazon in less than 48 hours is not a viable reason for increased light pollution. Humans require some artificial night light for shift work, manufacturing, street safety, and nighttime driving and research has shown that artificial light disrupts the lives of animals. However, a recent article suggests that we may be able to find a happy medium. A 2021 article examined seasonal light changes and its effect on all animals, but specifically mollusks. The authors noted that light research primarily focuses on length of exposure to light. Based on their research they suggest that further research should examine the lowest quantifying the least amount of light, in terms of duration and intensity, that would allow both humans and animals to continue safely. To collect as much data as possible, scientists are recruiting the public to act as citizen scientists from various locations around the globe and enter their findings in apps and websites. By collecting and uploading sky images, star counts, agricultural data, and bird and butterfly statistics, scientists gain access to volumes of data reflecting how light pollution is affecting the world around us. Hopefully scientists can predict and recommend responses to problems before changes become permanent."|2023-09-23-20-01-28
Light pollution|Reduction| Reducing light pollution implies many things, such as reducing sky glow, reducing glare, reducing light trespass, and reducing clutter. The method for best reducing light pollution, therefore, depends on exactly what the problem is in any given instance. Possible solutions include:|2023-09-23-20-01-28
Light pollution|Improving lighting fixtures|" The use of full cutoff lighting fixtures, as much as possible, is advocated by most campaigners for the reduction of light pollution. It is also commonly recommended that lights be spaced appropriately for maximum efficiency, and that number of luminaires being used as well as the wattage of each luminaire match the needs of the particular application (based on local lighting design standards). Full cutoff fixtures first became available in 1959 with the introduction of General Electric's M100 fixture. A full cutoff fixture, when correctly installed, reduces the chance for light to escape above the plane of the horizontal. Light released above the horizontal may sometimes be lighting an intended target, but often serves no purpose. When it enters into the atmosphere, light contributes to sky glow. Some governments and organizations are now considering, or have already implemented, full cutoff fixtures in street lamps and stadium lighting. The use of full cutoff fixtures helps to reduce sky glow by preventing light from escaping above the horizontal. Full cutoff typically reduces the visibility of the lamp and reflector within a luminaire, so the effects of glare are also reduced. Campaigners also commonly argue that full cutoff fixtures are more efficient than other fixtures, since light that would otherwise have escaped into the atmosphere may instead be directed towards the ground. However, full cutoff fixtures may also trap more light in the fixture than other types of luminaires, corresponding to lower luminaire efficiency, suggesting a re-design of some luminaires may be necessary. The use of full cutoff fixtures can allow for lower wattage lamps to be used in the fixtures, producing the same or sometimes a better effect, due to being more carefully controlled. In every lighting system, some sky glow also results from light reflected from the ground. This reflection can be reduced, however, by being careful to use only the lowest wattage necessary for the lamp, and setting spacing between lights appropriately.  Assuring luminaire setback is greater than 90° from highly reflective surfaces also diminishes reflectance. A common criticism of full cutoff lighting fixtures is that they are sometimes not as aesthetically pleasing to look at. This is most likely because historically there has not been a large market specifically for full cutoff fixtures, and because people typically like to see the source of illumination. Due to the specificity with their direction of light, full cutoff fixtures sometimes also require expertise to install for maximum effect. The effectiveness of using full cutoff roadway lights to combat light pollution has also been called into question. According to design investigations, luminaires with full cutoff distributions (as opposed to cutoff or semi cutoff, compared here)  have to be closer together to meet the same light level, uniformity and glare requirements specified by the IESNA. These simulations optimized the height and spacing of the lights while constraining the overall design to meet the IESNA requirements, and then compared total uplight and energy consumption of different luminaire designs and powers. Cutoff designs performed better than full cutoff designs, and semi-cutoff performed better than either cutoff or full cutoff. This indicates that, in roadway installations, over-illumination or poor uniformity produced by full cutoff fixtures may be more detrimental than direct uplight created by fewer cutoff or semi-cutoff fixtures. Therefore, the overall performance of existing systems could be improved more by reducing the number of luminaires than by switching to full cutoff designs. However, using the definition of ""light pollution"" from some Italian regional bills (i.e., ""every irradiance of artificial light outside competence areas and particularly upward the sky"") only full cutoff design prevents light pollution. The Italian Lombardy region, where only full cutoff design is allowed (Lombardy act no. 17/2000, promoted by Cielobuio-coordination for the protection of the night sky), in 2007 had the lowest per capita energy consumption for public lighting in Italy. The same legislation also imposes a minimum distance between street lamps of about four times their height, so full cut-off street lamps are the best solution to reduce both light pollution and electrical power usage. This kind of LED droplight could reduce unnecessary light pollution in building interiors. A flat-lens cobra luminaire, which is a full-cutoff fixture, is very effective in reducing light pollution. It ensures that light is directed only below the horizontal, which means less light is wasted by directing it outwards and upwards. This drop-lens cobra luminaire allows light to escape sideways and upwards, where it may cause problems. The majority of Italian regions require ""zero upward light"", which usually implies the use of overall full cut-off lamps for new luminaires, but violations are common."|2023-09-23-20-01-28
Light pollution|Adjusting types of light sources| Several different types of light sources exist, each having a variety of properties that determine their appropriateness for different tasks. Particularly notable characteristics are efficiency and spectral power distribution. It is often the case that inappropriate light sources have been selected for a task, either due to ignorance or because more appropriate lighting technology was unavailable at the time of installation. Therefore, poorly chosen light sources often contribute unnecessarily to light pollution and energy waste. By updating light sources appropriately, it is often possible to reduce energy use and pollutive effects while simultaneously improving efficiency and visibility. Some types of light sources are listed in order of energy efficiency in the table below (figures are approximate maintained values), and include their visual skyglow impact, relative to LPS lighting. Many astronomers request that nearby communities use low-pressure sodium lights or amber Aluminium gallium indium phosphide LED as much as possible because the principal wavelength emitted is comparably easy to work around or in rare cases filter out.  The low cost of operating sodium lights is another feature. In 1980, for example, San Jose, California, replaced all street lamps with low pressure sodium lamps, whose light is easier for nearby Lick Observatory to filter out. Similar programs are now in place in Arizona and Hawaii. Such yellow light sources also have significantly less visual skyglow impact,  so reduce visual sky brightness and improve star visibility for everyone. Disadvantages of low-pressure sodium lighting are that fixtures must usually be larger than competing fixtures, and that color cannot be distinguished, due to its emitting principally a single wavelength of light (see security lighting). Due to the substantial size of the lamp, particularly in higher wattages such as 135 W and 180 W, control of light emissions from low-pressure sodium luminaires is more difficult. For applications requiring more precise direction of light (such as narrow roadways) the native lamp efficacy advantage of this lamp type is decreased and may be entirely lost compared to high pressure sodium lamps. Allegations that this also leads to higher amounts of light pollution from luminaires running these lamps arise principally because of older luminaires with poor shielding, still widely in use in the UK and in some other locations. Modern low-pressure sodium fixtures with better optics and full shielding, and the decreased skyglow impacts of yellow light preserve the luminous efficacy advantage of low-pressure sodium and result in most cases is less energy consumption and less visible light pollution. Unfortunately, due to continued lack of accurate information,  many lighting professionals continue to disparage low-pressure sodium, contributing to its decreased acceptance and specification in lighting standards and therefore its use. According to Narisada and Schrueder (2004), another disadvantage of low-pressure sodium lamps is that some research has found that many people find the characteristic yellow light to be less pleasing aesthetically, although they caution that this research isn't thorough enough to draw conclusions from. Because of the increased sensitivity of the human eye to blue and green wavelengths when viewing low-luminances (the Purkinje effect) in the night sky, different sources produce dramatically different amounts of visible skyglow from the same amount of light sent into the atmosphere.|2023-09-23-20-01-28
Light pollution|Re-designing lighting plans|" In some cases, evaluation of existing plans has determined that more efficient lighting plans are possible. For instance, light pollution can be reduced by turning off unneeded outdoor lights, and lighting stadiums only when there are people inside. Timers are especially valuable for this purpose. One of the world's first coordinated legislative efforts to reduce the adverse effect of this pollution on the environment began in Flagstaff, Arizona, in the U.S. There, more than three decades of ordinance development has taken place, with the full support of the population,  often with government support,  with community advocates,  and with the help of major local observatories,  including the United States Naval Observatory Flagstaff Station. Each component helps to educate, protect and enforce the imperatives to intelligently reduce detrimental light pollution. One example of a lighting plan assessment can be seen in a report originally commissioned by the Office of the Deputy Prime Minister in the United Kingdom, and now available through the Department for Communities and Local Government.  The report details a plan to be implemented throughout the UK, for designing lighting schemes in the countryside, with a particular focus on preserving the environment. In another example, the city of Calgary has recently replaced most residential street lights with models that are comparably energy efficient.  The motivation is primarily operation cost and environmental conservation. The costs of installation are expected to be regained through energy savings within six to seven years. The Swiss Agency for Energy Efficiency (SAFE) uses a concept that promises to be of great use in the diagnosis and design of road lighting, ""consommation électrique spécifique (CES)"", which can be translated into English as ""specific electric power consumption (SEC)"".  Thus, based on observed lighting levels in a wide range of Swiss towns, SAFE has defined target values for electric power consumption per metre for roads of various categories. Thus, SAFE currently recommends an SEC of two to three watts per meter for roads less than ten metres wide (four to six for wider roads). Such a measure provides an easily applicable environmental protection constraint on conventional ""norms"", which usually are based on the recommendations of lighting manufacturing interests, who may not take into account environmental criteria. In view of ongoing progress in lighting technology, target SEC values will need to be periodically revised downwards. A newer method for predicting and measuring various aspects of light pollution was described in the journal Lighting Research & Technology (September 2008). Scientists at Rensselaer Polytechnic Institute's Lighting Research Center have developed a comprehensive method called Outdoor Site-Lighting Performance (OSP), which allows users to quantify, and thus optimize, the performance of existing and planned lighting designs and applications to minimize excessive or obtrusive light leaving the boundaries of a property. OSP can be used by lighting engineers immediately, particularly for the investigation of glow and trespass (glare analyses are more complex to perform and current commercial software does not readily allow them), and can help users compare several lighting design alternatives for the same site. In the effort to reduce light pollution, researchers have developed a ""Unified System of Photometry"", which is a way to measure how much or what kind of street lighting is needed. The Unified System of Photometry allows light fixtures to be designed to reduce energy use while maintaining or improving perceptions of visibility, safety, and security.  There was a need to create a new system of light measurement at night because the biological way in which the eye's rods and cones process light is different in nighttime conditions versus daytime conditions. Using this new system of photometry, results from recent studies have indicated that replacing traditional, yellowish, high-pressure sodium (HPS) lights with ""cool"" white light sources, such as induction, fluorescent, ceramic metal halide, or LEDs can actually reduce the amount of electric power used for lighting while maintaining or improving visibility in nighttime conditions. The International Commission on Illumination, also known as the CIE from its French title, la Commission Internationale de l'Eclairage, will soon be releasing its own form of unified photometry for outdoor lighting."|2023-09-23-20-01-28
Light pollution|Dark sky reserves| In 2001 International Dark Sky Places Program was founded in order to encourage communities, parks and protected areas around the world to preserve and protect dark sites through responsible lighting policies and public education. As of January 2022, there are 195 certified International Dark Sky Places in the world.  For example, in 2016 China launched its first dark sky reserve in the Tibet Autonomous Region's Ngari Prefecture which covers an area of 2,500 square kilometers. Such areas are important for astronomical observation.|2023-09-23-20-01-28
Light pollution|Gallery| This time exposure photo of New York City at night shows skyglow. A comparison of the view of the night sky from a small rural town (top) and a metropolitan area (bottom). Light pollution dramatically reduces the visibility of stars. Impact of light pollution on a starry night, as seen from a 4200 m altitude on Mount Damavand in Iran. Impact of LED lighting at the skyglow at night, as seen at Tiberias, Israel. Before the moving to LED lighting at 2017, the skyglow above the city was very little.|2023-09-23-20-01-28
Light pollution|Videos| This nighttime look at Earth, dubbed the Black Marble, provides researchers with a unique perspective of human activities around the globe.|2023-09-23-20-01-28
Light pollution|See also| Events Organisations Other pollution Geographical locations Literature Wikiversity|2023-09-23-20-01-28
Light pollution|Further reading| Introductory Astronomy Energy Environment and ecology General Handbooks Industrialisation Reading lists UK|2023-09-23-20-01-28
Light pollution|Campaigns and research organizations| Oceanica Europe North America|2023-09-23-20-01-28
List of environmental podcasts|General| The following is a list of environmental podcasts that focus on environmentalism, sustainability, climate change, and pollution.|2023-06-16-15-21-25
List of space debris fall incidents|General| Space debris usually burns up in the atmosphere, but larger debris objects can reach the ground intact. According to NASA, an average of one cataloged piece of debris has fallen back to Earth each day for the past 50 years. Despite their size, there has been no significant property damage from the debris.  Burning up in the atmosphere may also contribute to atmospheric pollution.  Numerous small cylindrical tanks from space objects have been found, designed to hold fuel or gasses. Notable examples of space debris falling to Earth and impacting human life include:|2023-08-20-23-15-19
Marginal abatement cost|General|" Abatement cost is the cost of reducing environmental negatives such as pollution. Marginal cost is an economic concept that measures the cost of an additional unit. The marginal abatement cost, in general, measures the cost of reducing one more unit of pollution. Marginal abatement costs are also called the ""marginal cost"" of reducing such environmental negatives. Although marginal abatement costs can be negative, such as when the low carbon option is cheaper than the business-as-usual option, marginal abatement costs often rise steeply as more pollution is reduced. In other words, it becomes more expensive [technology or infrastructure changes] to reduce pollution past a certain point. Marginal abatement costs are typically used on a marginal abatement cost curve, which shows the marginal cost of additional reductions in pollution."|2023-09-21-14-40-28
Marginal abatement cost|Usage| Carbon traders use marginal abatement cost curves to derive the supply function for modelling carbon price fundamentals. Power companies may employ marginal abatement cost curves to guide their decisions about long-term capital investment strategies to select among a variety of efficiency and generation options. Economists have used marginal abatement cost curves to explain the economics of interregional carbon trading.  Policy-makers use marginal abatement cost curves as merit order curves, to analyze how much abatement can be done in an economy at what cost, and where policy should be directed to achieve the emission reductions. However, marginal abatement cost curves should not be used as abatement supply curves[why?] (or merit order curves) to decide which measures to implement in order to achieve a given emission-reduction target. Indeed, the options they list would take decades to implement, and it may be optimal to implement expensive but high-potential measures before introducing cheaper measures.|2023-09-21-14-40-28
Marginal abatement cost|Criticism| The way that marginal abatement cost curves are usually built has been criticized for lack of transparency and the poor treatment it makes of uncertainty, inter-temporal dynamics, interactions between sectors and ancillary benefits.  There is also concern regarding the biased ranking that occurs if some included options have negative costs.|2023-09-21-14-40-28
Marginal abatement cost|Examples of existing marginal abatement cost curves| Worldwide, marginal abatement cost studies show that improving the energy efficiency of buildings and replacing fossil fuelled power plants with renewables are usually the most cost effective ways of reducing carbon emissions. Various economists, research organizations, and consultancies have produced marginal abatement cost curves. Bloomberg New Energy Finance  and McKinsey & Company  have produced economy wide analyses on greenhouse gas emissions reductions for the United States. ICF International  produced a California specific curve following the Global Warming Solutions Act of 2006 legislation as have Sweeney and Weyant. The Wuppertal Institute for Climate, Environment and Energy produced several marginal abatement cost curves for Germany (also called Cost Potential Curves), depending on the perspective (end-user, utilities, society). The US Environmental Protection Agency has done work on a marginal abatement cost curve for non-carbon dioxide emissions such as methane, N2O, and hydrofluorocarbons.  Enerdata and Laboratoire d'Economie de la Production et de l'Intégration-Le Centre national de la recherche scientifique (France) produce marginal abatement cost curves with the Prospective Outlook on Long-term Energy Systems (POLES) model for the 6 Kyoto Protocol gases.  These curves have been used for various public and private actors either to assess carbon policies  or through the use of a carbon market analysis tool. The World Bank 2013 low-carbon energy development plan for Nigeria,  prepared jointly with the World Bank, utilizes marginal abatement cost curves created in Analytica.|2023-09-21-14-40-28
Measures of pollutant concentration|General| Measures of pollutant concentration are used to determine risk assessment in public health. Industry is continually synthesizing new chemicals, the regulation of which requires evaluation of the potential danger for human health and the environment. Risk assessment is nowadays considered essential for making these decisions on a scientifically sound basis. Measures or defined limits include:|2022-12-02-01-57-42
Measures of pollutant concentration|No-effect concentration| No-effect concentration (NEC) is a risk assessment parameter that represents the concentration of a pollutant that will not harm the species involved, with respect to the effect that is studied. It is often the starting point for environmental policy. There is not much debate on the existence of an NEC,  but the assignment of a value is another matter. Current practice consists of the use of standard tests. In the standard tests groups of animals are exposed to different concentrations of chemicals and different effects such as survival, growth or reproduction are monitored. These toxicity tests typically result in a no-observed-effect concentration (NOEC, also called a no-observed-effect level, or NOEL). This NOEC has been severely criticized on statistical grounds by several authors  and it was concluded that the NOEC should be abandoned.|2022-12-02-01-57-42
Measures of pollutant concentration|ECx| A proposed alternative is the use of so-called ECx – the concentration(s) showing x% effect (e.g. an EC50 in a survival experiment indicates the concentration where 50% of the test animals would die in that experiment). ECx concentrations also have their problems in applying them to risk assessment. Any other value for x other than zero may give the impression that an effect is accepted, and this is in conflict with the aim of maximally protecting the environment.  In addition ECx values do depend on the exposure time.  ECx values for survival decrease for increasing exposure time, until equilibrium has been established. This is because effects depend on internal concentrations,  and that it takes time for the compound to penetrate the body of test organisms. However, sub-lethal endpoints (e.g., body size, reproductive output) may reveal less predictable effect patterns in time. The shape of the effect patterns over time depends on properties of the test compound, properties of the organism, the endpoint considered and the dimensions in which the endpoint is expressed (e.g., body size or body weight; reproduction rate or cumulative reproduction).|2022-12-02-01-57-42
Measures of pollutant concentration|Biology-based| Biology-based methods not only aim to describe observed effects, but also to understand them in terms of underlying processes such as toxicokinetics, mortality, feeding, growth and reproduction (Kooijman 1997). This type of approach starts with the description of the uptake and elimination of a compound by an organism, as an effect can only be expected if the compound is inside the organism, and where the no-effect-concentration is one of the modeling parameters. As the approach is biologically based it is also possible by using the dynamic energy budget theory  to incorporate multiple stressors (e.g. effects of food restriction, temperature, etc.)  and processes that are active under field conditions (e.g. adaptation, population dynamics, species interactions, life cycle phenomena, etc.).  The effects of these multiple stressors are excluded in the standard test procedures by keeping the local environment in the test constant. It is also possible to use these parameter values to predict effects at longer exposure times, or effects when the concentration in the medium is not constant. If the observed effects include those on survival and reproduction of individuals, these parameters can also be used to predict effects on growing populations in the field.|2022-12-02-01-57-42
Naval Radiological Defense Laboratory|General| The United States Naval Radiological Defense Laboratory (NRDL) was an early military lab created to study the effects of radiation and nuclear weapons. The facility was based at the Hunter's Point Naval Shipyard in San Francisco, California.|2022-12-20-06-08-22
Naval Radiological Defense Laboratory|History| The NRDL was formed in 1946 to manage testing, decontamination, and disposition of US Navy ships contaminated by the Operation Crossroads nuclear tests in the Pacific.  A number of ships that survived the atomic detonations were towed to Hunter's Point for detailed study and decontamination. Some of the ships were cleaned and sold for scrap. The aircraft carrier USS Independence, which had been heavily damaged and contaminated with nuclear fallout by Operation Crossroads explosions in July 1946, was brought to the NRDL for study.   After years of trying in vain to decontaminate the ship enough that it could be safely sold for scrap, the Navy ultimately packed the ship full of nuclear waste and scuttled the radioactive hulk off California near the Farallon Islands in January 1951. The ship's wreck was discovered resting upright under 790 m of water in 2009. The NRDL used several buildings at the Hunter's Point shipyard from 1946 to 1969. Working with the newly formed US Atomic Energy Commission (predecessor to the U.S. Nuclear Regulatory Commission established in 1974), the Navy conducted a wide variety of radiation experiments on materials and animals at the lab,  including the construction of a cyclotron on the site for use in radiation experiments and storage for various nuclear materials.|2022-12-20-06-08-22
Naval Radiological Defense Laboratory|Activities| An article published 2 May 2001 in SF Weekly detailed various aspects of nuclear testing at NRDL from declassified records: The NRDL often experimented with and disposed of nuclear material with little apparent concern that it was operating in the middle of a major metropolitan area. Among other things, historical documents show, scientists at the NRDL:|2022-12-20-06-08-22
Naval Radiological Defense Laboratory|Contamination|" The first use of radioactive materials at NRDL predated the issuing of licenses by the Atomic Energy Commission, but the AEC later issued licenses for a broad spectrum of radioactive materials to be used in research at the NRDL. Radioactive materials specific to nuclear weapon testing were exempted from AEC licensing. For closure of the NRDL in 1969, the AEC issued licenses for decommissioning activities. AEC licenses for the shipyard and NRDL were terminated in the 1970s. The NRDL testing and decontamination activities caused significant contamination of the shipyard site.   The NRDL and the military radiation training school at nearby Naval Station Treasure Island loaded the nuclear waste left from experiments into steel barrels and sent weekly barges to dump them offshore near the Gulf of the Farallons, which is a US National Wildlife Refuge and a major commercial fishery. Between 1946 and 1970, records estimate the lab and naval station dumped an estimated 47,000 drums  of nuclear waste in the Pacific Ocean 30 miles west of San Francisco, creating the first and largest offshore nuclear waste dump in the United States.  The USGS states the barrels contain only ""low-level radioactive waste,""  but this is disputed by historical records and experts. The US Navy completed a Historical Radiological Assessment of the Hunter's Point Shipyard in 2004, including the known NRDL facilities on the property, years after the SF Weekly article cited declassified documents showing that many sites and buildings used by NRDL were not included in the Navy's list of sites with potential for radiological contamination.  Many of the buildings formerly used by NRDL had been razed by that point. The former shipyard site is still being decontaminated, and has been split into multiple parcels to allow the Navy to declare them clean and safe for redevelopment separately.  While Lennar has built and sold hundreds of new condominium units on the site of the former Hunters Point Naval Shipyard,  regulators, activists, and cleanup workers have claimed that the site is still heavily contaminated and that the company contracted to handle the cleanup and testing, Tetra Tech, has repeatedly violated established cleanup protocols,  deliberately falsified radiation test results at the site to falsely show that there is little remaining radiation,   and fired employees who attempted to force workers to perform radiation tests as required."|2022-12-20-06-08-22
Naval Radiological Defense Laboratory|External links| 37°43′32.18″N 122°22′8.19″W﻿ / ﻿37.7256056°N 122.3689417°W﻿ / 37.7256056; -122.3689417|2022-12-20-06-08-22
Neuroplastic effects of pollution|General| Research indicates that living in areas of high pollution has serious long term health effects. Living in these areas during childhood and adolescence can lead to diminished mental capacity and an increased risk of brain damage. People of all ages who live in high pollution areas for extended periods place themselves at increased risk of various neurological disorders. Both air pollution and heavy metal pollution have been implicated as having negative effects on central nervous system (CNS) functionality. The ability of pollutants to affect the neurophysiology of individuals after the structure of the CNS has become mostly stabilized is an example of negative neuroplasticity.|2023-09-11-23-02-38
Neuroplastic effects of pollution|Air pollution|" Air pollution is known to affect small and large blood vessels throughout the body.
  
High levels of air pollution are associated with increased risk of strokes and heart attacks.  By permanently affecting vascular structures in the brain, air pollution can have serious effects on neural functioning and neural matter. In dogs, air pollution has been shown to cause damage to the CNS by altering the blood–brain barrier, causing neurons in the cerebral cortex to degenerate, destroying glial cells found in white matter, and causing neurofibrillary tangles.   These changes can permanently alter brain structure and chemistry, resulting in various impairments and disorders. Sometimes, the effects of neural remodeling do not manifest themselves for a prolonged period of time."|2023-09-11-23-02-38
Neuroplastic effects of pollution|Effects in adolescents and canines| A study from 2008 compared children and dogs raised in Mexico City (a location known for high pollution levels) with children and dogs raised in Polotitlán, Mexico (a city whose pollution levels meet the current US National Ambient Air Quality Standards).  According to this study, children raised in areas of higher pollution scored lower in intelligence (i.e. on IQ tests), and showed signs of lesions in MRI scanning of the brain. In contrast, children from the low pollution area scored as expected on IQ tests, and did not show any significant sign of the risk of brain lesions. This correlation was found to be statistically significant, and shows that pollution levels may be related to, and contribute to, brain lesion formation and IQ scores, which, in turn, manifests as impaired intellectual capacity and/or performance. Living in high pollution areas thus places adolescents at risk of premature brain degeneration and improper neural development—these findings could have significant implications for future generations. With regard to traffic related air pollution, children of mothers exposed to higher levels during the first trimester of pregnancy were at increased risk of allergic sensitization at age 1 year.|2023-09-11-23-02-38
Neuroplastic effects of pollution|Effects in adults|" There are indications that the effects of physical activity and air pollution on neuroplasticity counteract. Physical activity is known for its health-enhancing benefits, particularly on the cardiovascular system, and has also demonstrated benefits for brain plasticity processes, cognition and mental health. The neurotrophine, brain-derived neurotrophic factor (BDNF) is thought to play a key role in exercise-induced cognitive improvements. Brief bouts of physical activity have been shown to increase serum levels of BDNF, but this increase may be offset by increased exposure to traffic-related air pollution. 
Over longer periods of physical exercise, the cognitive improvements which were demonstrated in rural joggers were found to be absent in urban joggers who were partaking in the same 12-week start-2-run training programme."|2023-09-11-23-02-38
Neuroplastic effects of pollution|Epilepsy| Researchers in Chile found statistically-significant correlations between multiple air pollutants and the risk of epilepsy using a 95% confidence interval.  The air pollutants that the researchers attempted to correlate with increased incidence of epilepsy included carbon monoxide, ozone, sulfur dioxide, nitrogen dioxide, large particulate matter, and fine particulate matter. The researchers tested these pollutants across seven cities and, in all but one case, a correlation was found between pollutant levels and the occurrence of epilepsy. All of the correlations found were shown to be statistically significant. The researchers hypothesized that air pollutants increase epilepsy risk by increasing inflammatory mediators, and by providing a source of oxidative stress. They believe that these changes eventually alter the functioning of the blood–brain barrier, causing brain inflammation. Brain inflammation is known to be a risk factor for epilepsy; thus, the sequence of events provides a plausible mechanism by which pollution may increase epilepsy risk in individuals who are genetically vulnerable to the disease.|2023-09-11-23-02-38
Neuroplastic effects of pollution|Dioxin poisoning| Organohalogen compounds, such as dioxins, are commonly found in pesticides or created as by-products of pesticide manufacture or degradation. These compounds can have a significant impact on the neurobiology of exposed organisms. Some observed effects of exposure to dioxins are altered astroglial intracellular calcium ion (Ca2+), decreased glutathione levels, modified neurotransmitter function in the CNS, and loss of pH maintenance.  A study of 350 chemical plant employees exposed to a dioxin precursor for herbicide synthesis between 1965 and 1968 showed that 80 of the employees displayed signs of dioxin poisoning.  Of these 350 employees, 15 were contacted again in 2004 to submit to neurological tests to assess whether the dioxin poisoning had any long-term effects on neurological capabilities. The amount of time that had passed made it difficult to assemble a larger cohort, but the results of the tests indicated that eight of the 15 subjects exhibited some central nervous system impairment, nine showed signs of polyneuropathy, and electroencephalography (EEG) showed various degrees of structural abnormalities. This study suggested that the effects of dioxins were not limited to initial toxicity. Dioxins, through neuroplastic effects, can cause long-term damage that may not manifest itself for years or even decades.|2023-09-11-23-02-38
Neuroplastic effects of pollution|Metal exposure| Heavy metal exposure can result in an increased risk of various neurological diseases. Research indicates that the two most neurotoxic heavy metals are mercury and lead. The impact that these two metals will have is highly dependent upon the individual due to genetic variations. Mercury and lead are particularly neurotoxic for many reasons: they easily cross cell membranes, have oxidative effects on cells, react with sulfur in the body (leading to disturbances in the many functions that rely upon sulfhydryl groups), and reduce glutathione levels inside cells. Methylmercury, in particular, has an extremely high affinity for sulfhydryl groups.  Organomercury is a particularly damaging form of mercury because of its high absorbability  Lead also mimics calcium, a very important mineral in the CNS, and this mimicry leads to many adverse effects.  Mercury's neuroplastic mechanisms work by affecting protein production. Elevated mercury levels increase glutathione levels by affecting gene expression, and this in turn affects two proteins (MT1 and MT2) that are contained in astrocytes and neurons.  Lead's ability to imitate calcium allows it to cross the blood–brain barrier. Lead also upregulates glutathione.|2023-09-11-23-02-38
Neuroplastic effects of pollution|Autism| Heavy metal exposure, when combined with certain genetic predispositions, can place individuals at increased risk for developing autism. Many examples of CNS pathophysiology, such as oxidative stress, neuroinflammation, and mitochondrial dysfunction, could be by-products of environmental stressors such as pollution, as found in a 2010 study.  There have been reports of autism outbreaks occurring in specific locations.  Since these cases of autism are related to geographic location, the implication is that something in the environment is complementing an at-risk genotype to cause autism in these vulnerable individuals. Mercury and lead both contribute to inflammation, leading scientists to speculate that these heavy metals could play a role in autism. These findings are controversial, however, with some researchers mentioned in a 2002 study that increasing rates of autism are mostly a consequence of more accurate screening and diagnostic methods, and are not due to any sort of environmental factor.|2023-09-11-23-02-38
Neuroplastic effects of pollution|Accelerated neural aging| Neuroinflammation is associated with increased rates of neurodegeneration.  Inflammation tends to increase naturally with age. By facilitating inflammation, pollutants such as air particulates and heavy metals cause the CNS to age more quickly. Many late-onset diseases are caused by neurodegeneration. Multiple sclerosis, Parkinson's disease, amyotrophic lateral sclerosis (ALS), and Alzheimer's disease are all believed to be exacerbated by inflammatory processes, resulting in individuals displaying signs of these diseases at an earlier age than is typically expected. Multiple sclerosis occurs when chronic inflammation leads to the compromise of oligodendrocytes, which in turn leads to the destruction of the myelin sheath. Then axons begin exhibiting signs of damage, which in turn leads to neuron death. Multiple sclerosis has been correlated to living in areas with high particulate matter levels in the air. In Parkinson's disease, inflammation leading to depletion of antioxidant stores will ultimately lead to dopaminergic neuron degeneration, causing a shortage of dopamine and contributing to the formation of Parkinson's disease. Chronic glial activation as a result of inflammation causes motor neuron death and compromises astrocytes, these factors leading to the symptoms of amyotrophic lateral sclerosis (ALS, aka Lou Gehrig's disease). In the case of Alzheimer's disease, inflammatory processes lead to neuron death by inhibiting growth at axons and activating astrocytes that produce proteoglycans.  This product can only be deposited in the hippocampus and cortex, indicating that this may be the reason these two areas show the highest levels of degeneration in Alzheimer's disease.  Airborne metal particulates (e.g. manganese) have been shown to directly access and affect the brain through olfactory pathways, which allows a large amount of particulate matter to reach the blood–brain barrier. These facts, coupled with air pollution's link to neurofibrillary tangles and the observed subcortical vascular changes observed in dogs, imply that the negative neuroplastic effects of pollution could result in increased risk for Alzheimer's disease, and could also implicate pollution as a cause of early-onset Alzheimer's disease through multiple mechanisms. The general effect of pollution is increased levels of inflammation. As a result, pollution can significantly contribute to various neurological disorders that are caused by inflammatory processes.|2023-09-11-23-02-38
Noise pollution|General| Lists Categories Noise pollution, or sound pollution, is the propagation of noise or sound with ranging impacts on the activity of human or animal life, most of which are harmful to a degree. The source of outdoor noise worldwide is mainly caused by machines, transport and propagation systems.    Poor urban planning may give rise to noise disintegration or pollution, side-by-side industrial and residential buildings can result in noise pollution in the residential areas. Some of the main sources of noise in residential areas include loud music, transportation (traffic, rail, airplanes, etc.), lawn care maintenance, construction, electrical generators, wind turbines, explosions and people. Documented problems associated with noise in urban environments go back as far as ancient Rome.  Research suggests that noise pollution in the United States is the highest in low-income and racial minority neighborhoods,  and noise pollution associated with household electricity generators is an emerging environmental degradation in many developing nations. High noise levels can contribute to cardiovascular effects in humans and an increased incidence of coronary artery disease.   In animals, noise can increase the risk of death by altering predator or prey detection and avoidance, interfere with reproduction and navigation, and contribute to permanent hearing loss.  A substantial amount of the noise that humans produce occurs in the ocean. Up until recently, most research on noise impacts has been focused on marine mammals, and to a lesser degree, fish.   In the past few years, scientists have shifted to conducting studies on invertebrates and their responses to anthropogenic sounds in the marine environment. This research is essential, especially considering that invertebrates make up 75% of marine species, and thus compose a large percentage of ocean food webs.  Of the studies that have been conducted, a sizable variety in families of invertebrates have been represented in the research. A variation in the complexity of their sensory systems exists, which allows scientists to study a range of characteristics and develop a better understanding of anthropogenic noise impacts on living organisms. Because the local civic noise environment can impact the perceived value of real estate, often the largest equity held by a home owner, personal stakes in the noise environment and the civic politics surrounding the noise environment can run extremely high.|2023-08-18-08-33-02
Noise pollution|Human health| Noise pollution affects both health and behavior. Unwanted sound (noise) can damage physiological health. Noise pollution is associated with several health conditions, including cardiovascular disorders, hypertension, high stress levels, tinnitus, hearing loss, sleep disturbances, and other harmful and disturbing effects.      According to a 2019 review of the existing literature, noise pollution was associated with faster cognitive decline. Across Europe, according to the European Environment Agency, it estimated 113 million people are affected by road traffic noise levels above 55 decibels, the threshold at which noise becomes harmful to human health by the WHO's definition. Sound becomes unwanted when it either interferes with normal activities such as sleep or conversation, or disrupts or diminishes one's quality of life.  Noise-induced hearing loss can be caused by prolonged exposure to noise levels above 85 A-weighted decibels.  A comparison of Maaban tribesmen, who were insignificantly exposed to transportation or industrial noise, to a typical U.S. population showed that chronic exposure to moderately high levels of environmental noise contributes to hearing loss. Noise exposure in the workplace can also contribute to noise-induced hearing loss and other health issues. Occupational hearing loss is one of the most common work-related illnesses in the U.S. and worldwide. It is less clear how humans adapt to noise subjectively. Tolerance for noise is frequently independent of decibel levels. Murray Schafer's soundscape research was groundbreaking in this regard. In his work, he makes compelling arguments about how humans relate to noise on a subjective level, and how such subjectivity is conditioned by culture.  Schafer notes that sound is an expression of power, and as such, material culture (e.g., fast cars or Harley Davidson motorcycles with aftermarket pipes) tend to have louder engines not only for safety reasons, but for expressions of power by dominating the soundscape with a particular sound. Other key research in this area can be seen in Fong's comparative analysis of soundscape differences between Bangkok, Thailand, and Los Angeles, California, US. Based on Schafer's research, Fong's study showed how soundscapes differ based on the level of urban development in the area. He found that cities in the periphery have different soundscapes than inner city areas. Fong's findings tie not only soundscape appreciation to subjective views of sound, but also demonstrates how different sounds of the soundscape are indicative of class differences in urban environments. Noise pollution can have negative affects on adults and children on the autistic spectrum.  Those with Autism Spectrum Disorder (ASD) can have hyperacusis, which is an abnormal sensitivity to sound.  People with ASD who experience hyperacusis may have unpleasant emotions, such as fear and anxiety, and uncomfortable physical sensations in noisy environments with loud sounds.  This can cause individuals with ASD to avoid environments with noise pollution, which in turn can result in isolation and negatively affect their quality of life. Sudden explosive noises typical of high-performance car exhausts and car alarms are types of noise pollution that can affect people with ASD. While the elderly may have cardiac problems due to noise, according to the World Health Organization, children are especially vulnerable to noise, and the effects that noise has on children may be permanent.  Noise poses a serious threat to a child's physical and psychological health, and may negatively interfere with a child's learning and behavior.  Exposure to persistent noise pollution shows how important maintaining environmental health is in keeping children and elderly healthy.|2023-08-18-08-33-02
Noise pollution|Wildlife|" Noise generated by traffic, ships, vehicles, and aircraft can affect the survivability of wildlife species and can reach undisturbed habitats.  Although sounds are commonly present in the environment, anthropogenic noises are distinguishable due to differences in frequency and amplitude.  Many animals use sounds to communicate with others of their species, whether that is for reproduction purposes, navigation, or to notify others of prey or predators. However, anthropogenic noises inhibit species from detecting these sounds, affecting overall communication within the population.  Species such as birds, amphibians, reptiles, fishes, mammals, and invertebrates are examples of biological groups that are impacted by noise pollution.   If animals cannot communicate with one another, this would result in reproduction to decline (not able to find mates), and higher mortality (lack of communication for predator detection). European robins living in urban environments are more likely to sing at night in places with high levels of noise pollution during the day, suggesting that they sing at night because it is quieter, and their message can propagate through the environment more clearly.  The same study showed that daytime noise was a stronger predictor of nocturnal singing than night-time light pollution, to which the phenomenon often is attributed. Anthropogenic noise reduced the species richness of birds found in Neotropical urban parks. Zebra finches become less faithful to their partners when exposed to traffic noise. This could alter a population's evolutionary trajectory by selecting traits, sapping resources normally devoted to other activities and thus leading to profound genetic and evolutionary consequences. Several reasons have been identified relating to hypersensitivity in invertebrates when exposed to anthropogenic noise. Invertebrates have evolved to pick up sound, and a large portion of their physiology is adapted for the purpose of detecting environmental vibrations.  Antennae or hairs on the organism pick up particle motion.  Anthropogenic noise created in the marine environment, such as pile driving and shipping, are picked up through particle motion; these activities exemplify near-field stimuli. The ability to detect vibration through mechanosensory structures is most important in invertebrates and fish. Mammals, also, depend on pressure detector ears to perceive the noise around them.  Therefore, it is suggested that marine invertebrates are likely perceiving the effects of noise differently than marine mammals. It is reported that invertebrates can detect a large range of sounds, but noise sensitivity varies substantially between each species. Generally, however, invertebrates depend on frequencies under 10 kHz. This is the frequency at which a great deal of ocean noise occurs. Therefore, not only does anthropogenic noise often mask invertebrate communication, but it also negatively impacts other biological system functions through noise-induced stress. 
Another one of the leading causes of noise effects in invertebrates is because sound is used in multiple behavioral contexts by many groups. This includes regularly sound produced or perceived in the context of aggression or predator avoidance. Invertebrates also utilize sound to attract or locate mates, and often employ sound in the courtship process. Many of the studies that were conducted on invertebrate exposure to noise found that a physiological or behavioral response was triggered. Most of the time, this related to stress, and provided concrete evidence that marine invertebrates detect and respond to noise. Some of the most informative studies in this category focus on hermit crabs. In one study, it was found that the behavior of the hermit crab Pagurus bernhardus, when attempting to choose a shell, was modified when subjected to noise. Proper selection of hermit crab shells strongly contributes to their ability to survive. Shells offer protection against predators, high salinity and desiccation.  However, researchers determined that approach to shell, investigation of shell, and habitation of shell, occurred over a shorter time duration with anthropogenic noise as a factor. This indicated that assessment and decision-making processes of the hermit crab were both altered, even though hermit crabs are not known to evaluate shells using any auditory or mechanoreception mechanisms. In another study that focused on Pagurus bernhardus and the blue mussel, (Mytilus edulis) physical behaviors exhibited a stress response to noise. When the hermit crab and mussel were exposed to different types of noise, significant variation in the valve gape occurred in the blue mussel.  The hermit crab responded to the noise by lifting the shell off of the ground multiple times, then vacating the shell to examine it before returning inside.  The results from the hermit crab trials were ambiguous with respect to causation; more studies must be conducted in order to determine whether the behavior of the hermit crab can be attributed to the noise produced. Another study that demonstrates a stress response in invertebrates was conducted on the squid species Doryteuthis pealeii. The squid was exposed to sounds of construction known as pile driving, which impacts the sea bed directly and produces intense substrate-borne and water-borne vibrations.  The squid reacted by jetting, inking, pattern change and other startle responses.  Since the responses recorded are similar to those identified when faced with a predator, it is implied that the squid initially viewed the sounds as a threat. However, it was also noted that the alarm responses decreased over a period of time, signifying that the squid had likely acclimated to the noise.  Regardless, it is apparent that stress occurred in the squid, and although further investigation has not been pursued, researchers suspect that other implications exist that may alter the squid's survival habits. An additional study examined the impact noise exposure had on the Indo-Pacific humpbacked dolphin (Sousa chinensis). The dolphins were exposed to elevated noise levels due to construction in the Pearl River Estuary in China, specifically caused by the world's largest vibration hammer—the OCTA-KONG.  The study suggested that while the dolphin's clicks were not affected, their whistles were because of susceptibility to auditory masking.  The noise from the OCTA-KONG was found to have been detectable by the dolphins up to 3.5 km away from the original source, and while the noise was not found to be life-threatening it was indicated that prolonged exposure to this noise could be responsible for auditory damage."|2023-08-18-08-33-02
Noise pollution|Marine life|" Noise pollution is common in marine ecosystems, affecting at least 55 marine species.  For many marine populations, sound is their primary sense used for their survival; able to detect sound hundreds to thousands kilometers away from a source, while vision is limited to tens of meters underwater.  As anthropogenic noises continue to increase, doubling every decade, this compromises the survivability of marine species.  One study discovered that as seismic noises and naval sonar increases in marine ecosystems, cetacean, such as whales and dolphins, diversity decreases.  Noise pollution has also impaired fish hearing, killed and isolated whale populations, intensified stress response in marine species, and changed species' physiology. Because marine species are sensitive to noise, most marine wildlife are located in undisturbed habitats or areas not exposed to significant anthropogenic noise, limiting suitable habitats to forage and mate. Whales have changed their migration route to avoid anthropogenic noise, as well as altering their calls. For many marine organisms, sound is the primary means of learning about their environments. For example, many species of marine mammals and fish use sound as their primary means of navigating, communicating, and foraging.  Anthropogenic noise can have a detrimental effect on animals, increasing the risk of death by changing the delicate balance in predator or prey detection  and avoidance, and interfering with the use of the sounds in communication, especially in relation to reproduction, and in navigation and echolocation.  These effects then may alter more interactions within a community through indirect (""domino"") effects.  Acoustic overexposure can lead to temporary or permanent loss of hearing. Noise pollution may have caused the death of certain species of whales that beached themselves after being exposed to the loud sound of military sonar.  (see also Marine mammals and sonar) Even marine invertebrates, such as crabs (Carcinus maenas), have been shown to be negatively affected by ship noise.   Larger crabs were noted to be negatively affected more by the sounds than smaller crabs. Repeated exposure to the sounds did lead to acclimatization. Underwater noise pollution due to human activities is also prevalent in the sea, and given that sound travels faster through water than through air, is a major source of disruption of marine ecosystems and does significant harm to sea life, including marine mammals, fish and invertebrates.   The once-calm sea environment is now noisy and chaotic due to ships, oil drilling, sonar equipment, and seismic testing.  The principal anthropogenic noise sources come from merchant ships, naval sonar operations, underwater explosions (nuclear), and seismic exploration by oil and gas industries. Cargo ships generate high levels of noise due to propellers and diesel engines.   This noise pollution significantly raises the low-frequency ambient noise levels above those caused by wind.   Animals such as whales that depend on sound for communication can be affected by this noise in various ways. Higher ambient noise levels also cause animals to vocalize more loudly, which is called the Lombard effect. Researchers have found that humpback whales' song lengths were longer when low-frequency sonar was active nearby. Underwater noise pollution is not only limited to oceans, and can occur in freshwater environments as well. Noise pollution has been detected in the Yangzte River, and has resulted in the endangerment of Yangtze finless porpoises.  A study conducted on noise pollution in the Yangzte River suggested that the elevated levels of noise pollution altered the temporal hearing threshold of the finless porpoises and posed a significant threat to their survival."|2023-08-18-08-33-02
Noise pollution|Impacts on communication| Terrestrial anthropogenic noise affects the acoustic communications in grasshoppers while producing sound to attract a mate. The fitness and reproductive success of a grasshopper is dependent on its ability to attract a mating partner. Male Corthippus biguttulus grasshoppers attract females by using stridulation to produce courtship songs.  The females produce acoustic signals that are shorter and primarily low frequency and amplitude, in response to the male's song. Research has found that this species of grasshopper changes its mating call in response to loud traffic noise. Lampe and Schmoll (2012) found that male grasshoppers from quiet habitats have a local frequency maximum of about 7319 Hz. In contrast, male grasshoppers exposed to loud traffic noise can create signals with a higher local frequency maximum of 7622 Hz. The higher frequencies are produced by the grasshoppers to prevent background noise from drowning out their signals. This information reveals that anthropogenic noise disturbs the acoustic signals produced by insects for communication.  Similar processes of behavior perturbation, behavioral plasticity, and population level shifts in response to noise likely occur in sound-producing marine invertebrates, but more experimental research is needed.|2023-08-18-08-33-02
Noise pollution|Impacts on development| Boat-noise has been shown to affect the embryonic development and fitness of the sea hare Stylocheilus striatus.  Anthropogenic noise can alter conditions in the environment that have a negative effect on invertebrate survival. Although embryos can adapt to normal changes in their environment, evidence suggests they are not well adapted to endure the negative effects of noise pollution. Studies have been conducted on the sea hare to determine the effects of boat noise on the early stages of life and the development of embryos. Researchers have studied sea hares from the lagoon of Moorea Island, French Polynesia. In the study, recordings of boat noise were made by using a hydrophone.  In addition, recordings of ambient noise were made that did not contain boat noise. In contrast to ambient noise playbacks, mollusks exposed to boat noise playbacks had a 21% reduction in embryonic development. Additionally, newly hatched larvae experienced an increased mortality rate of 22% when exposed to boat noise playbacks.|2023-08-18-08-33-02
Noise pollution|Impacts on ecosystem| Anthropogenic noise can have negative effects on invertebrates that aid in controlling environmental processes that are crucial to the ecosystem. There are a variety of natural underwater sounds produced by waves in coastal and shelf habitats, and biotic communication signals that do not negatively impact the ecosystem. The changes in behavior of invertebrates vary depending on the type of anthropogenic noise and is similar to natural noisescapes. Experiments have examined the behavior and physiology of the clam (Ruditapes philippinarum), the decapod (Nephrops norvegicus), and the brittlestar (Amphiura filiformis) that are affected by sounds resembling shipping and building noises.  The three invertebrates in the experiment were exposed to continuous broadband noise and impulsive broadband noise. The anthropogenic noise impeded the bioirrigation and burying behavior of Nephrops norvegicus. In addition, the decapod exhibited a reduction in movement. Ruditapes philippinarum experienced stress which caused a reduction in surface relocation.  The anthropogenic noise caused the clams to close their valves and relocate to an area above the interface of the sediment-water. This response inhibits the clam from mixing the top layer of the sediment profile and hinders suspension feeding. Sound causes Amphiura filiformis to experience changes in physiological processes which results in irregularity of bioturbation behavior. These invertebrates play an important role in transporting substances for benthic nutrient cycling.  As a result, ecosystems are negatively impacted when species cannot perform natural behaviors in their environment. Locations with shipping lanes, dredging, or commercial harbors are known as continuous broadband sound. Pile-driving, and construction are sources that exhibit impulsive broadband noise. The different types of broadband noise have different effects on the varying species of invertebrates and how they behave in their environment. Another study found that the valve closures in the Pacific oyster Magallana gigas was a behavioral response to varying degrees of acoustic amplitude levels and noise frequencies.  Oysters perceive near-field sound vibrations by utilizing statocysts. In addition, they have superficial receptors that detect variations in water pressure. Sound pressure waves from shipping can be produced below 200Hz. Pile driving generates noise between 20 and 1000 Hz. In addition, large explosions can create frequencies ranging from 10 to 200Hz. M. gigas can detect these noise sources because their sensory system can detect sound in the 10 to < 1000Hz range. The anthropogenic noise produced by human activity has been shown to negatively impact oysters.  Studies have revealed that wide and relaxed valves are indicative of healthy oysters. The oysters are stressed when they do not open their valves as frequently in response to environmental noise. This provides support that the oysters detect noise at low acoustic energy levels.  While we generally understand that marine noise pollution influences charismatic megafauna like whales and dolphins, understanding  how invertebrates like oysters perceive and respond to human generated sound can provide further insight about the effects of anthropogenic noise on the larger ecosystem.  The aquatic ecosystems are known to use sound to navigate, find food, and protect themselves. In 2020, one of the worst mass stranding of whales occurred in Australia. Experts suggest that noise pollution plays a major role in the mass stranding of whales. Noise pollution has also altered avian communities and diversity. Anthropogenic noises have a similar effect on bird population as seen in marine ecosystems, where noises reduce reproductive success; cannot detect predators due to interferences of anthropogenic noises, minimize nesting areas, increase stress response, and species abundances and richness declining.   Certain avian species are more sensitive to noises compared to others, resulting in highly-sensitive birds migrating to less disturbed habitats. There has also been evidence of indirect positive effects of anthropogenic noises on avian populations. It was found that nesting bird predators, such as the western scrub-jay (Aphelocoma californica), were uncommon in noisy environments (western scrub-jay are sensitive to noise). Therefore, reproductive success for nesting prey communities was higher due to the lack of predators.  Noise pollution can alter the distribution and abundance of prey species, which can then impact predator populations.|2023-08-18-08-33-02
Noise pollution|Metrics of noise|" Researchers measure noise in terms of pressure, intensity, and frequency. Sound pressure level (SPL) represents the amount of pressure relative to atmospheric pressure during sound wave propagation that can vary with time; this is also known as the sum of the amplitudes of a wave.  Sound intensity, measured in Watts per meters-squared, represents the flow of sound over a particular area. Although sound pressure and intensity differ, both can describe the level of loudness by comparing the current state to the threshold of hearing; this results in decibel units on the logarithmic scale.   The logarithmic scale accommodates the vast range of sound heard by the human ear. Frequency, or pitch, is measured in Hertz (Hz) and reflects the number of sound waves propagated through the air per second.   The range of frequencies heard by the human ear range from 20 Hz to 20,000 Hz; however, sensitivity to hearing higher frequencies decreases with age.  Some organisms, such as elephants,  can register frequencies between 0 and 20 Hz (infrasound), and others, such as bats, can recognize frequencies above 20,000 Hz (ultrasound) to echolocate. Researchers use different weights to account for noise frequency with intensity, as humans do not perceive sound at the same loudness level.  The most commonly used weighted levels are A-weighting, C-weighting, and Z-weighting. A-weighting mirrors the range of hearing, with frequencies of 20 Hz to 20,000 Hz.  This gives more weight to higher frequencies and less weight to lower frequencies.   C-weighting has been used to measure peak sound pressure or impulse noise, similar to loud short-lived noises from machinery in occupational settings.   Z-weighting, also known as zero-weighting, represents noise levels without any frequency weights. Understanding sound pressure levels is key to assessing measurements of noise pollution. Several metrics describing noise exposure include: Researchers with the US National Park Service found that human activity doubles the background-noise levels in 63 percent of protected spaces like national parks, and increases them tenfold in 21 percent. In the latter places, ""if you could have heard something 100 feet away, now you can only hear it 10 feet away,"""|2023-08-18-08-33-02
Noise pollution|Instrumentation| Sound can be measured in the air using a sound level meter, a device consisting of a microphone, an amplifier, and a time meter.  Sound level meters can measure noise at different frequencies (usually A- and C-weighted levels).  There are two settings for response time constants, fast (time constant = 0.125 seconds, similar to human hearing) or slow (1 second, used for calculating averages over widely varying sound levels).  Sound level meters meet the required standards set by the International Electrotechnical Commission (IEC)  and in the United States, the American National Standards Institute as type 0, 1, or 2 instruments. Type 0 devices are not required to meet the same criteria expected of types 1 and 2 since scientists use these as laboratory reference standards.  Type 1 (precision) instruments are to study the precision of capturing sound measurements, while type 2 instruments are for general field use.  Type 1 devices acceptable by the standards have a margin of error of ±1.5 dB, while type 2 instruments meet a margin of error of ±2.3 dB. Sound can also be measured using a noise dosimeter, a device similar to a sound level meter. Individuals have used dosimeters to measure personal exposure levels in occupational settings given their smaller, more portable size. Unlike many sound level meters, a dosimeter microphone attaches to the worker and monitors levels throughout a work shift.  Additionally, dosimeters can calculate the percent dose or time-weighted average (TWA). In recent years, scientists and audio engineers have been developing smartphone apps to conduct sound measurements, similar to the standalone sound level meters and dosimeters. In 2014, the National Institute for Occupational Safety and Health (NIOSH) within the Centers for Disease Control and Prevention (CDC) published a study examining the efficacy of 192 sound measurement apps on Apple and Android smartphones. The authors found that only 10 apps, all of which were on the App Store, met all acceptability criteria. Of these 10 apps, only 4 apps met accuracy criteria within 2 dB(A) from the reference standard.   As a result of this study, they created the NIOSH Sound Level Meter App to increase accessibility and decrease costs of monitoring noise using crowdsourcing data with a tested and highly accurate application.   The app is compliant with ANSI S1.4 and IEC 61672 requirements. The app calculates the following measures: total run time, instantaneous sound level, A-weighted equivalent sound level (LAeq), maximum level (LAmax), C-weighted peak sound level, time-weighted average (TWA), dose, and projected dose.  Dose and projected dose are based on sound level and duration of noise exposure in relation to the NIOSH recommended exposure limit of 85 dB(A) for an eight-hour work shift. Using the phone's internal microphone (or an attached external microphone), the NIOSH Sound Level Meter measures instantaneous sound levels in realtime and converts sound into electrical energy to calculate measurements in A-, C-, or Z-weighted decibels. App users are able to generate, save, and e-mail measurement reports. The NIOSH Sound Level Meter is currently only available on Apple iOS devices.|2023-08-18-08-33-02
Noise pollution|Noise control| The Hierarchy of Controls concept is often used to reduce noise in the environment or the workplace. Engineering noise controls can be used to reduce noise propagation and protect individuals from overexposure. When noise controls are not feasible or adequate, individuals can also take steps to protect themselves from the harmful effects of noise pollution. If people must be around loud sounds, they can protect their ears with hearing protection (e.g., ear plugs or ear muffs). Buy Quiet programs and initiatives have arisen in an effort to combat occupational noise exposures. These programs promote the purchase of quieter tools and equipment and encourage manufacturers to design quieter equipment. Noise from roadways and other urban factors can be mitigated by urban planning and better design of roads. Roadway noise can be reduced by the use of noise barriers, limitation of vehicle speeds, alteration of roadway surface texture, limitation of heavy vehicles, use of traffic controls that smooth vehicle flow to reduce braking and acceleration, and tyre design. An important factor in applying these strategies is a computer model for roadway noise, that is capable of addressing local topography, meteorology, traffic operations, and hypothetical mitigation. Costs of building-in mitigation can be modest, provided these solutions are sought in the planning stage of a roadway project. Aircraft noise can be reduced by using quieter jet engines. Altering flight paths and time of day runway has benefited residents near airports.|2023-08-18-08-33-02
Noise pollution|Country-specific regulations|" Up until the 1970s governments tended to view noise as a ""nuisance"" rather than an environmental problem. Many conflicts over noise pollution are handled by negotiation between the emitter and the receiver. Escalation procedures vary by country, and may include action in conjunction with local authorities, in particular the police. In 2007, the Egyptian National Research Center found that the average noise level in central Cairo was 90 decibels and that the noise never fell below 70 decibels. Noise limits set by law in 1994 are not enforced.  In 2018, the World Hearing Index declared Cairo to be the world's second-noisiest city. Noise pollution is a major problem in India.  The government of India has rules and regulations against firecrackers and loudspeakers, but enforcement is extremely lax.  Awaaz Foundation is a non-governmental organization in India working to control noise pollution from various sources through advocacy, public interest litigation, awareness, and educational campaigns since 2003.   Despite increased enforcement and stringency of laws now being practiced in urban areas, rural areas are still affected. The Supreme Court of India had banned playing of music on loudspeakers after 10pm. In 2015, The National Green Tribunal directed authorities in Delhi to ensure strict adherence to guidelines on noise pollution, saying noise is more than just a nuisance as it can produce serious psychological stress. However, implementation of the law remains poor. How noise emissions should be reduced, without the industry being hit too hard, is a major problem in environmental care in Sweden today. The Swedish Work Environment Authority has set an input value of 80 dB for maximum sound exposure for eight hours. In workplaces where there is a need to be able to converse comfortably the background noise level should not exceed 40 dB.  The government of Sweden has taken soundproofing and acoustic absorbing actions, such as noise barriers and active noise control. Figures compiled by rockwool, the mineral wool insulation manufacturer, based on responses from local authorities to a Freedom of Information Act (FOI) request reveal in the period April 2008 – 2009 UK councils received 315,838 complaints about noise pollution from private residences. This resulted in environmental health officers across the UK serving 8,069 noise abatement notices or citations under the terms of the Anti-Social Behavior (Scotland) Act. In the last 12 months, 524 confiscations of equipment have been authorized involving the removal of powerful speakers, stereos and televisions. Westminster City Council has received more complaints per head of population than any other district in the UK with 9,814 grievances about noise, which equates to 42.32 complaints per thousand residents. Eight of the top 10 councils ranked by complaints per 1,000 residents are located in London. The Noise Control Act of 1972 established a U.S. national policy to promote an environment for all Americans free from noise that jeopardizes their health and welfare. In the past, Environmental Protection Agency coordinated all federal noise control activities through its Office of Noise Abatement and Control. The EPA phased out the office's funding in 1982 as part of a shift in federal noise control policy to transfer the primary responsibility of regulating noise to state and local governments. However, the Noise Control Act of 1972 and the Quiet Communities Act of 1978 were never rescinded by Congress and remain in effect today, although essentially unfunded. The National Institute for Occupational Safety and Health (NIOSH) at the Centers for Disease Control and Prevention (CDC) researches noise exposure in occupational settings and recommends a Recommended Exposure Limit (REL) for an 8-hour time-weighted average (TWA) or work shift of 85 dB(A) and for impulse noise (instant events such as bangs or crashes) of 140 dB(A).   The agency published this recommendation along with its origin, noise measurement devices, hearing loss prevention programs, and research needs in 1972 (later revised June 1998) as an approach in preventing occupational noise-related hearing loss. The Occupational Safety and Health Administration (OSHA) within the Department of Labor issues enforceable standards to protect workers from occupational noise hazards. The permissible exposure limit (PEL) for noise is a TWA of 90 dB(A) for an eight-hour work day.   However, in manufacturing and service industries, if the TWA is greater than 85 dB(A), employers must implement a Hearing Conservation Program. The Federal Aviation Administration (FAA) regulates aircraft noise by specifying the maximum noise level that individual civil aircraft can emit through requiring aircraft to meet certain noise certification standards. These standards designate changes in maximum noise level requirements by ""stage"" designation. The U.S. noise standards are defined in the Code of Federal Regulations (CFR) Title 14 Part 36 – Noise Standards: Aircraft Type and Airworthiness Certification (14 CFR Part 36).  The FAA also pursues a program of aircraft noise control in cooperation with the aviation community.  The FAA has set up a process to report  for anyone who may be impacted by aircraft noise. The Federal Highway Administration (FHWA) developed noise regulations to control highway noise as required by the Federal-Aid Highway Act of 1970. The regulations requires promulgation of traffic noise-level criteria for various land use activities, and describe procedures for the abatement of highway traffic noise and construction noise. The Department of Housing and Urban Development (HUD) noise standards as described in 24 CFR part 51, Subpart B provides minimum national standards applicable to HUD programs to protect citizen against excessive noise in their communities and places of residence. For instance, all sites whose environmental or community noise exposure exceeds the day night average sound level (DNL) of 65 (dB) are considered noise-impacted areas, it defines ""Normally Unacceptable"" noise zones where community noise levels are between 65 and 75 dB, for such locations, noise abatement and noise attenuation features must be implemented. Locations where the DNL is above 75 dB are considered ""Unacceptable"" and require approval by the Assistant Secretary for Community Planning and Development. The Department of Transportation's Bureau of Transportation Statistics has created a  to provide access to comprehensive aircraft and road noise data on national and county levels.  The map aims to assist city planners, elected officials, scholars, and residents to gain access to up-to-date aviation and Interstate highway noise information. States and local governments typically have very specific statutes on building codes, urban planning, and roadway development. Noise laws and ordinances vary widely among municipalities and indeed do not even exist in some cities. An ordinance may contain a general prohibition against making noise that is a nuisance, or it may set out specific guidelines for the level of noise allowable at certain times of the day and for certain activities.  Noise laws classify sound into three categories. First is ambient noise, which refers to sound pressure of all-encompassing noise associated with a given environment. The second is continuous noise, which could be steady or fluctuating, but continues for more than an hour. The third is cyclically varying noise, which could be steady or fluctuating, but occurs repetitively at reasonably uniform intervals of time. New York City instituted the first comprehensive noise code in 1985. The Portland Noise Code includes potential fines of up to $5000 per infraction and is the basis for other major U.S. and Canadian city noise ordinances."|2023-08-18-08-33-02
Noise pollution|World Health Organization| In 1995, the World Health Organization (WHO) European Region released guidelines on regulating community noise.  The WHO European Region subsequently released other versions of the guidelines, with the most recent version circulated in 2018.  The guidelines provide the most up-to-date evidence from research conducted in Europe and other parts of the world on non-occupational noise exposure and its relationship to physical and mental health outcomes. The guidelines provide recommendations for limits and preventive  measures regarding various noise sources (road traffic, railway, aircraft, wind turbine) for day-evening-night average and nighttime average levels. Recommendations for leisure noise in 2018 were conditional and based on the equivalent sound pressure level during an average 24-hour period in a year without weights for nighttime noise (LAeq, 24 hrs); WHO set the recommended limit to 70 dB(A). Day-Evening-Night Average Level (Lden) Nighttime Average Noise (Lnight)|2023-08-18-08-33-02
Non-exhaust emissions|General| Non-exhaust emissions come from wearing down motor vehicle brake pads, tires, roads themselves, and unsettling of particles on the road.       This particulate matter is made up of micrometre-sized particles and causes negative health effects, including respiratory disease and cancer.   Very fine particulate matter has been linked to cardiovascular disease.  Multiple epidemiological studies have demonstrates that particulate matter exposure is associated with acute respiratory infections, lung cancer, and chronic respiratory and cardiovascular disease.   Researchers have also found correlations between exposure to fine particulate matter and fatality rates in previous coronavirus epidemics. Studies have shown that non-exhaust emissions of particles from vehicles can be greater than particles due to exhaust.|2023-06-07-21-58-14
Non-exhaust emissions|Ways of reducing emissions| More comprehensive regulation of tires has been suggested.  Lighter vehicles pollute less   and reducing vehicle kilometers traveled is another method of mitigating non-exhaust emissions. Reducing demand for private vehicle travel can be accomplished by a variety of measures that increase the relative attractiveness of public transport and non-motorized modes relative to private vehicles. These measures can consist of disincentives for private vehicle ownership and use, i.e. measures that raise their costs and/or inconvenience, as well as incentives for alternative modes (e.g. public transit, walking, and biking).|2023-06-07-21-58-14
Non-exhaust emissions|Electric and hybrid vehicles| Electric vehicles and hybrid vehicles with regenerative braking still unsettle particles on the roadway and give off rubber and road pollution, and do so at a higher rate than lighter internal combustion vehicles,  but do not emit the same level of brake wear compared to vehicles of the same type.|2023-06-07-21-58-14
Non-exhaust emissions|Regulatory agencies and policies that target exhaust emissions| Very few agencies are charged with implementing exhaust emission standards for non-exhaust emissions.   Most policies target exhaust emissions and do not regulate non-exhaust particulate matter emissions.  As of February 2023 Euro 7 standards are still being argued about.|2023-06-07-21-58-14
Ozone|General| Ozone (/ˈoʊzoʊn/) (or trioxygen) is an inorganic molecule with the chemical formula O3. It is a pale blue gas with a distinctively pungent smell. It is an allotrope of oxygen that is much less stable than the diatomic allotrope O2, breaking down in the lower atmosphere to O2 (dioxygen). Ozone is formed from dioxygen by the action of ultraviolet (UV) light and electrical discharges within the Earth's atmosphere. It is present in very low concentrations throughout the latter, with its highest concentration high in the ozone layer of the stratosphere, which absorbs most of the Sun's ultraviolet (UV) radiation. Ozone's odor is reminiscent of chlorine, and detectable by many people at concentrations of as little as 0.1 ppm in air. Ozone's O3 structure was determined in 1865. The molecule was later proven to have a bent structure and to be weakly diamagnetic. In standard conditions, ozone is a pale blue gas that condenses at cryogenic temperatures to a dark blue liquid and finally a violet-black solid. Ozone's instability with regard to more common dioxygen is such that both concentrated gas and liquid ozone may decompose explosively at elevated temperatures, physical shock, or fast warming to the boiling point.   It is therefore used commercially only in low concentrations. Ozone is a powerful oxidant (far more so than dioxygen) and has many industrial and consumer applications related to oxidation. This same high oxidizing potential, however, causes ozone to damage mucous and respiratory tissues in animals, and also tissues in plants, above concentrations of about 0.1 ppm. While this makes ozone a potent respiratory hazard and pollutant near ground level, a higher concentration in the ozone layer (from two to eight ppm) is beneficial, preventing damaging UV light from reaching the Earth's surface.|2023-09-19-19-37-35
Ozone|Nomenclature| The trivial name ozone is the most commonly used and preferred IUPAC name. The systematic names 2λ4-trioxidiene[dubious  – discuss] and catena-trioxygen, valid IUPAC names, are constructed according to the substitutive and additive nomenclatures, respectively. The name ozone derives from ozein (ὄζειν), the Greek neuter present participle for smell,  referring to ozone's distinctive smell. In appropriate contexts, ozone can be viewed as trioxidane with two hydrogen atoms removed, and as such, trioxidanylidene may be used as a systematic name, according to substitutive nomenclature. By default, these names pay no regard to the radicality of the ozone molecule. In an even more specific context, this can also name the non-radical singlet ground state, whereas the diradical state is named trioxidanediyl. Trioxidanediyl (or ozonide) is used, non-systematically, to refer to the substituent group (-OOO-). Care should be taken to avoid confusing the name of the group for the context-specific name for the ozone given above.|2023-09-19-19-37-35
Ozone|History|" In 1785, Dutch chemist Martinus van Marum was conducting experiments involving electrical sparking above water when he noticed an unusual smell, which he attributed to the electrical reactions, failing to realize that he had in fact created ozone. A half century later, Christian Friedrich Schönbein noticed the same pungent odour and recognized it as the smell often following a bolt of lightning. In 1839, he succeeded in isolating the gaseous chemical and named it ""ozone"", from the Greek word ozein (ὄζειν) meaning ""to smell"".  
For this reason, Schönbein is generally credited with the discovery of ozone.     He also noted the similarity of ozone smell to the smell of phosphorus, and in 1844 proved that the product of reaction of white phosphorus with air is identical.  A subsequent effort to call ozone ""electrified oxygen"" he ridiculed by proposing to call the ozone from white phosphorus ""phosphorized oxygen"".  The formula for ozone, O3, was not determined until 1865 by Jacques-Louis Soret  and confirmed by Schönbein in 1867. For much of the second half of the 19th century and well into the 20th, ozone was considered a healthy component of the environment by naturalists and health-seekers. Beaumont, California had as its official slogan ""Beaumont: Zone of Ozone"", as evidenced on postcards and Chamber of Commerce letterhead.  Naturalists working outdoors often considered the higher elevations beneficial because of their ozone content. ""There is quite a different atmosphere [at higher elevation] with enough ozone to sustain the necessary energy [to work]"", wrote naturalist Henry Henshaw, working in Hawaii.  Seaside air was considered to be healthy because of its believed ozone content. The smell giving rise to this belief is in fact that of halogenated seaweed metabolites  and dimethyl sulfide. Much of ozone's appeal seems to have resulted from its ""fresh"" smell, which evoked associations with purifying properties. Scientists noted its harmful effects. In 1873 James Dewar and John Gray McKendrick documented that frogs grew sluggish, birds gasped for breath, and rabbits' blood showed decreased levels of oxygen after exposure to ""ozonized air"", which ""exercised a destructive action"".   Schönbein himself reported that chest pains, irritation of the mucous membranes and difficulty breathing occurred as a result of inhaling ozone, and small mammals died.  In 1911, Leonard Hill and Martin Flack stated in the Proceedings of the Royal Society B that ozone's healthful effects ""have, by mere iteration, become part and parcel of common belief; and yet exact physiological evidence in favour of its good effects has been hitherto almost entirely wanting ... The only thoroughly well-ascertained knowledge concerning the physiological effect of ozone, so far attained, is that it causes irritation and œdema of the lungs, and death if inhaled in relatively strong concentration for any time."" During World War I, ozone was tested at Queen Alexandra Military Hospital in London as a possible disinfectant for wounds. The gas was applied directly to wounds for as long as 15 minutes. This resulted in damage to both bacterial cells and human tissue. Other sanitizing techniques, such as irrigation with antiseptics, were found preferable. Until the 1920s, it was not certain whether small amounts of oxozone, O4, were also present in ozone samples due to the difficulty of applying analytical chemistry techniques to the explosive concentrated chemical.   In 1923, Georg-Maria Schwab (working for his doctoral thesis under Ernst Hermann Riesenfeld) was the first to successfully solidify ozone and perform accurate analysis which conclusively refuted the oxozone hypothesis.   Further hitherto unmeasured physical properties of pure concentrated ozone were determined by the Riesenfeld group in the 1920s."|2023-09-19-19-37-35
Ozone|Physical properties|" Ozone is a colourless or pale blue gas, slightly soluble in water and much more soluble in inert non-polar solvents such as carbon tetrachloride or fluorocarbons, in which it forms a blue solution. At 161 K (−112 °C; −170 °F), it condenses to form a dark blue liquid. It is dangerous to allow this liquid to warm to its boiling point, because both concentrated gaseous ozone and liquid ozone can detonate. At temperatures below 80 K (−193.2 °C; −315.7 °F), it forms a violet-black solid. Most people can detect about 0.01 μmol/mol of ozone in air where it has a very specific sharp odour somewhat resembling chlorine bleach. Exposure of 0.1 to 1 μmol/mol produces headaches, burning eyes and irritation to the respiratory passages. 
Even low concentrations of ozone in air are very destructive to organic materials such as latex, plastics and animal lung tissue. Ozone is weakly diamagnetic."|2023-09-19-19-37-35
Ozone|Structure| According to experimental evidence from microwave spectroscopy, ozone is a bent molecule, with C2v symmetry (similar to the water molecule).  The O–O distances are 127.2 pm (1.272 Å). The O–O–O angle is 116.78°.  The central atom is sp² hybridized with one lone pair. Ozone is a polar molecule with a dipole moment of 0.53 D.  The molecule can be represented as a resonance hybrid with two contributing structures, each with a single bond on one side and double bond on the other. The arrangement possesses an overall bond order of 1.5 for both sides. It is isoelectronic with the nitrite anion. Naturally occurring ozone can be composed of substituted isotopes (16O, 17O, 18O). A cyclic form has been predicted but not observed.|2023-09-19-19-37-35
Ozone|Reactions| Ozone is among the most powerful oxidizing agents known, far stronger than O2. It is also unstable at high concentrations, decaying into ordinary diatomic oxygen. Its half-life varies with atmospheric conditions such as temperature, humidity, and air movement. Under laboratory conditions, the half-life will average ~1500 minutes (25 hours) in still air at room temperature (24 °C), zero humidity with zero air changes per hour. This reaction proceeds more rapidly with increasing temperature. Deflagration of ozone can be triggered by a spark and can occur in ozone concentrations of 10 wt% or higher. Ozone can also be produced from oxygen at the anode of an electrochemical cell. This reaction can create smaller quantities of ozone for research purposes. This can be observed as an unwanted reaction in a Hoffman gas apparatus during the electrolysis of water when the voltage is set above the necessary voltage.|2023-09-19-19-37-35
Ozone|With metals| Ozone will oxidize most metals (except gold, platinum, and iridium) to oxides of the metals in their highest oxidation state. For example:|2023-09-19-19-37-35
Ozone|With nitrogen and carbon compounds| Ozone also oxidizes nitric oxide to nitrogen dioxide: This reaction is accompanied by chemiluminescence. The NO2 can be further oxidized to nitrate radical: The NO3 formed can react with NO2 to form dinitrogen pentoxide (N2O5). Solid nitronium perchlorate can be made from NO2, ClO2, and O3 gases: Ozone does not react with ammonium salts, but it oxidizes ammonia to ammonium nitrate: Ozone reacts with carbon to form carbon dioxide, even at room temperature:|2023-09-19-19-37-35
Ozone|With sulfur compounds| Ozone oxidizes sulfides to sulfates. For example, lead(II) sulfide is oxidized to lead(II) sulfate: Sulfuric acid can be produced from ozone, water and either elemental sulfur or sulfur dioxide: In the gas phase, ozone reacts with hydrogen sulfide to form sulfur dioxide: In an aqueous solution, however, two competing simultaneous reactions occur, one to produce elemental sulfur, and one to produce sulfuric acid:|2023-09-19-19-37-35
Ozone|With alkenes and alkynes| Alkenes can be oxidatively cleaved by ozone, in a process called ozonolysis, giving alcohols, aldehydes, ketones, and carboxylic acids, depending on the second step of the workup. Ozone can also cleave alkynes to form an acid anhydride or diketone product.  If the reaction is performed in the presence of water, the anhydride hydrolyzes to give two carboxylic acids. Usually ozonolysis is carried out in a solution of dichloromethane, at a temperature of −78 °C. After a sequence of cleavage and rearrangement, an organic ozonide is formed. With reductive workup (e.g. zinc in acetic acid or dimethyl sulfide), ketones and aldehydes will be formed, with oxidative workup (e.g. aqueous or alcoholic hydrogen peroxide), carboxylic acids will be formed.|2023-09-19-19-37-35
Ozone|Other substrates| All three atoms of ozone may also react, as in the reaction of tin(II) chloride with hydrochloric acid and ozone: Iodine perchlorate can be made by treating iodine dissolved in cold anhydrous perchloric acid with ozone: Ozone could also react with potassium iodide to give oxygen and iodine gas that can be titrated for quantitative determination :|2023-09-19-19-37-35
Ozone|Combustion| Ozone can be used for combustion reactions and combustible gases; ozone provides higher temperatures than burning in dioxygen (O2). The following is a reaction for the combustion of carbon subnitride which can also cause higher temperatures: Ozone can react at cryogenic temperatures. At 77 K (−196.2 °C; −321.1 °F), atomic hydrogen reacts with liquid ozone to form a hydrogen superoxide radical, which dimerizes:|2023-09-19-19-37-35
Ozone|Ozone decomposition| Ozone is a toxic substance,   commonly found or generated in human environments (aircraft cabins, offices with photocopiers, laser printers, sterilizers...) and its catalytic decomposition is very important to reduce pollution. This type of decomposition is the most widely used, especially with solid catalysts, and it has many advantages such as a higher conversion with a lower temperature. Furthermore, the product and the catalyst can be instantaneously separated, and this way the catalyst can be easily recovered without using any separation operation. Moreover, the most used materials in the catalytic decomposition of ozone in the gas phase are noble metals like Pt, Rh or Pd and transition metals such as Mn, Co, Cu, Fe, Ni or Ag. There are two other possibilities for the ozone decomposition in gas phase: The first one is a thermal decomposition where the ozone can be decomposed using only the action of heat. The problem is that this type of decomposition is very slow with temperatures below 250 °C. However, the decomposition rate can be increased working with higher temperatures but this would involve a high energy cost. The second one is a photochemical decomposition, which consists of radiating ozone with ultraviolet radiation (UV) and it gives rise to oxygen and radical peroxide. The process of ozone decomposition is a complex reaction involving two elementary reactions that finally lead to molecular oxygen, and this means that the reaction order and the rate law cannot be determined by the stoichiometry of the fitted equation. Overall reaction: Rate law (observed): It has been determined that the ozone decomposition follows a first order kinetics, and from the rate law above it can be determined that the partial order respect to molecular oxygen is -1 and respect to ozone is 2, therefore the global reaction order is 1. The ozone decomposition consists of two elementary steps: The first one corresponds to a unimolecular reaction because one only molecule of ozone decomposes into two products (molecular oxygen and oxygen). Then, the oxygen from the first step is an intermediate because it participates as a reactant in the second step, which is a bimolecular reaction because there are two different reactants (ozone and oxygen) that give rise to one product, that corresponds to molecular oxygen in the gas phase. Step 1: Unimolecular reaction Step 2: Bimolecular reaction These two steps have different reaction rates, the first one is reversible and faster than the second reaction, which is slower, so this means that the determining step is the second reaction and this is used to determine the observed reaction rate. The reaction rate laws for every step are the ones that follow: The following mechanism allows to explain the rate law of the ozone decomposition observed experimentally, and also it allows to determine the reaction orders with respect to ozone and oxygen, with which the overall reaction order will be determined. The slower step, the bimolecular reaction, is the one that determines the rate of product formation, and considering that this step gives rise to two oxygen molecules the rate law has this form: However, this equation depends on the concentration of oxygen (intermediate), which can be determined considering the first step. Since the first step is faster and reversible and the second step is slower, the reactants and products from the first step are in equilibrium, so the concentration of the intermediate can be determined as follows: Then using these equations, the formation rate of molecular oxygen is as shown below: Finally, the mechanism presented allows to establish the rate observed experimentally, with a rate constant (Kobs) and corresponding to a first order kinetics, as follows: where|2023-09-19-19-37-35
Ozone|Reduction to ozonides| Reduction of ozone gives the ozonide anion, O−3. Derivatives of this anion are explosive and must be stored at cryogenic temperatures. Ozonides for all the alkali metals are known. KO3, RbO3, and CsO3 can be prepared from their respective superoxides: Although KO3 can be formed as above, it can also be formed from potassium hydroxide and ozone: NaO3 and LiO3 must be prepared by action of CsO3 in liquid NH3 on an ion-exchange resin containing Na+ or Li+ ions: A solution of calcium in ammonia reacts with ozone to give ammonium ozonide and not calcium ozonide:|2023-09-19-19-37-35
Ozone|Applications| Ozone can be used to remove iron and manganese from water, forming a precipitate which can be filtered: Ozone will also oxidize dissolved hydrogen sulfide in water to sulfurous acid: These three reactions are central in the use of ozone-based well water treatment. Ozone will also detoxify cyanides by converting them to cyanates. Ozone will also completely decompose urea:|2023-09-19-19-37-35
Ozone|Spectroscopic properties| Ozone is a bent triatomic molecule with three vibrational modes: the symmetric stretch (1103.157 cm−1), bend (701.42 cm−1) and antisymmetric stretch (1042.096 cm−1).  The symmetric stretch and bend are weak absorbers, but the antisymmetric stretch is strong and responsible for ozone being an important minor greenhouse gas. This IR band is also used to detect ambient and atmospheric ozone although UV-based measurements are more common. The electromagnetic spectrum of ozone is quite complex. An overview can be seen at the MPI Mainz UV/VIS Spectral Atlas of Gaseous Molecules of Atmospheric Interest. All of the bands are dissociative, meaning that the molecule falls apart to O + O2 after absorbing a photon. The most important absorption is the Hartley band, extending from slightly above 300 nm down to slightly above 200 nm. It is this band that is responsible for absorbing UV C in the stratosphere. On the high wavelength side, the Hartley band transitions to the so-called Huggins band, which falls off rapidly until disappearing by ~360 nm. Above 400 nm, extending well out into the NIR, are the Chappius and Wulf bands. There, unstructured absorption bands are useful for detecting high ambient concentrations of ozone, but are so weak that they do not have much practical effect. There are additional absorption bands in the far UV, which increase slowly from 200 nm down to reaching a maximum at ~120 nm.|2023-09-19-19-37-35
Ozone|Ozone in Earth's atmosphere| The standard way to express total ozone levels (the amount of ozone in a given vertical column) in the atmosphere is by using Dobson units. Point measurements are reported as mole fractions in nmol/mol (parts per billion, ppb) or as concentrations in μg/m3. The study of ozone concentration in the atmosphere started in the 1920s.|2023-09-19-19-37-35
Ozone|Ozone layer|" The highest levels of ozone in the atmosphere are in the stratosphere, in a region also known as the ozone layer between about 10 and 50 km above the surface (or between about 6 and 31 miles). However, even in this ""layer"", the ozone concentrations are only two to eight parts per million, so most of the oxygen there is dioxygen, O2, at about 210,000 parts per million by volume. Ozone in the stratosphere is mostly produced from short-wave ultraviolet rays between 240 and 160 nm. Oxygen starts to absorb weakly at 240 nm in the Herzberg bands, but most of the oxygen is dissociated by absorption in the strong Schumann–Runge bands between 200 and 160 nm where ozone does not absorb. While shorter wavelength light, extending to even the X-Ray limit, is energetic enough to dissociate molecular oxygen, there is relatively little of it, and, the strong solar emission at Lyman-alpha, 121 nm, falls at a point where molecular oxygen absorption is a minimum. The process of ozone creation and destruction is called the Chapman cycle and starts with the photolysis of molecular oxygen followed by reaction of the oxygen atom with another molecule of oxygen to form ozone. where ""M"" denotes the third body that carries off the excess energy of the reaction. The ozone molecule can then absorb a UV-C photon and dissociate The excess kinetic energy heats the stratosphere when the O atoms and the molecular oxygen fly apart and collide with other molecules. This conversion of UV light into kinetic energy warms the stratosphere. The oxygen atoms produced in the photolysis of ozone then react back with other oxygen molecule as in the previous step to form more ozone. In the clear atmosphere, with only nitrogen and oxygen, ozone can react with the atomic oxygen to form two molecules of O2: An estimate of the rate of this termination step to the cycling of atomic oxygen back to ozone can be found simply by taking the ratios of the concentration of O2 to O3. The termination reaction is catalysed by the presence of certain free radicals, of which the most important are hydroxyl (OH), nitric oxide (NO) and atomic chlorine (Cl) and bromine (Br). In the second half of the 20th century, the amount of ozone in the stratosphere was discovered to be declining, mostly because of increasing concentrations of chlorofluorocarbons (CFC) and similar chlorinated and brominated organic molecules. The concern over the health effects of the decline led to the 1987 Montreal Protocol, the ban on the production of many ozone depleting chemicals and in the first and second decade of the 21st century the beginning of the recovery of stratospheric ozone concentrations. Ozone in the ozone layer filters out sunlight wavelengths from about 200 nm UV rays to 315 nm, with ozone peak absorption at about 250 nm.  This ozone UV absorption is important to life, since it extends the absorption of UV by ordinary oxygen and nitrogen in air (which absorb all wavelengths < 200 nm) through the lower UV-C (200–280 nm) and the entire UV-B band (280–315 nm). The small unabsorbed part that remains of UV-B after passage through ozone causes sunburn in humans, and direct DNA damage in living tissues in both plants and animals. Ozone's effect on mid-range UV-B rays is illustrated by its effect on UV-B at 290 nm, which has a radiation intensity 350 million times as powerful at the top of the atmosphere as at the surface. Nevertheless, enough of UV-B radiation at similar frequency reaches the ground to cause some sunburn, and these same wavelengths are also among those responsible for the production of vitamin D in humans. The ozone layer has little effect on the longer UV wavelengths called UV-A (315–400 nm), but this radiation does not cause sunburn or direct DNA damage, and while it probably does cause long-term skin damage in certain humans, it is not as dangerous to plants and to the health of surface-dwelling organisms on Earth in general (see ultraviolet for more information on near ultraviolet)."|2023-09-19-19-37-35
Ozone|Low level ozone|" Lists Categories Low level ozone (or tropospheric ozone) is an atmospheric pollutant.  It is not emitted directly by car engines or by industrial operations, but formed by the reaction of sunlight on air containing hydrocarbons and nitrogen oxides that react to form ozone directly at the source of the pollution or many kilometers downwind. Ozone reacts directly with some hydrocarbons such as aldehydes and thus begins their removal from the air, but the products are themselves key components of smog. Ozone photolysis by UV light leads to production of the hydroxyl radical HO• and this plays a part in the removal of hydrocarbons from the air, but is also the first step in the creation of components of smog such as peroxyacyl nitrates, which can be powerful eye irritants. The atmospheric lifetime of tropospheric ozone is about 22 days; its main removal mechanisms are being deposited to the ground, the above-mentioned reaction giving HO•, and by reactions with OH and the peroxy radical HO2•. There is evidence of significant reduction in agricultural yields because of increased ground-level ozone and pollution which interferes with photosynthesis and stunts overall growth of some plant species.   The United States Environmental Protection Agency (EPA) has proposed a secondary regulation to reduce crop damage, in addition to the primary regulation designed for the protection of human health. Certain examples of cities with elevated ozone readings are Denver, Colorado; Houston, Texas; and Mexico City, Mexico. Houston has a reading of around 41 nmol/mol, while Mexico City is far more hazardous, with a reading of about 125 nmol/mol. Low level ozone, or tropospheric ozone, is the most concerning type of ozone pollution in urban areas and is increasing in general.  Ozone pollution in urban areas affects denser populations, and is worsened by high populations of vehicles, which emit pollutants NO2 and VOCs, the main contributors to problematic ozone levels.  Ozone pollution in urban areas is especially concerning with increasing temperatures, raising heat-related mortality during heat waves.  During heat waves in urban areas, ground level ozone pollution can be 20% higher than usual.  Ozone pollution in urban areas reaches higher levels of exceedance in the summer and autumn, which may be explained by weather patterns and traffic patterns.  People experiencing poverty are more affected by pollution in general, even though these populations are less likely to be contributing to pollution levels. As mentioned above, Denver, Colorado, is one of the many cities in the U.S. that have high amounts of ozone. According to the American Lung Association, the Denver–Aurora area is the 14th most ozone-polluted area in the U.S.  The problem of high ozone levels is not new to this area. In 2004, the EPA allotted the Denver Metro/North Front Range  as non-attainment areas per 1997's 8-hour ozone standard,  but later deferred this status until 2007. The non-attainment standard indicates that an area does not meet the EPA's air quality standards. The Colorado Ozone Action Plan was created in response, and numerous changes were implemented from this plan. The first major change was that car emission testing was expanded across the state to more counties that did not previously mandate emissions testing, like areas of Larimer and Weld County. There have also been changes made to decrease Nitrogen Oxides (NOx) and Volatile Organic Compound (VOC) emissions, which should help lower ozone levels. One large contributor to high ozone levels in the area is the oil and natural gas industry situated in the Denver-Julesburg Basin (DJB) which overlaps with a majority of Colorado's metropolitan areas. Ozone is created naturally in the Earth's stratosphere, but is also created in the troposphere from human efforts. Briefly mentioned above, NOx and VOCs react with sunlight to create ozone through a process called photochemistry. One hour elevated ozone events (<75 ppb) ""occur during June–August indicating that elevated ozone levels are driven by regional photochemistry"".  According to an article from the University of Colorado-Boulder, ""Oil and natural gas VOC emission have a major role in ozone production and bear the potential to contribute to elevated O3 levels in the Northern Colorado Front Range (NCFR)"".   Using complex analyses to research wind patterns and emissions from large oil and natural gas operations, the authors concluded that ""elevated O3 levels in the NCFR are predominantly correlated with air transport from N– ESE, which are the upwind sectors where the O&NG operations in the Wattenberg Field area of the DJB are located"". Contained in the Colorado Ozone Action Plan, created in 2008, plans exist to evaluate ""emission controls for large industrial sources of NOx"" and ""statewide control requirements for new oil and gas condensate tanks and pneumatic valves"".  In 2011, the Regional Haze Plan was released that included a more specific plan to help decrease NOx emissions. These efforts are increasingly difficult to implement and take many years to come to pass. Of course there are also other reasons that ozone levels remain high. These include: a growing population meaning more car emissions, and the mountains along the NCFR that can trap emissions. If interested, daily air quality readings can be found at the Colorado Department of Public Health and Environment's website.  As noted earlier, Denver continues to experience high levels of ozone to this day. It will take many years and a systems-thinking approach to combat this issue of high ozone levels in the Front Range of Colorado. Ozone gas attacks any polymer possessing olefinic or double bonds within its chain structure, such as natural rubber, nitrile rubber, and styrene-butadiene rubber. Products made using these polymers are especially susceptible to attack, which causes cracks to grow longer and deeper with time, the rate of crack growth depending on the load carried by the rubber component and the concentration of ozone in the atmosphere. Such materials can be protected by adding antiozonants, such as waxes, which bond to the surface to create a protective film or blend with the material and provide long term protection. Ozone cracking used to be a serious problem in car tires,  for example, but it is not an issue with modern tires. On the other hand, many critical products, like gaskets and O-rings, may be attacked by ozone produced within compressed air systems. Fuel lines made of reinforced rubber are also susceptible to attack, especially within the engine compartment, where some ozone is produced by electrical components. Storing rubber products in close proximity to a DC electric motor can accelerate ozone cracking. The commutator of the motor generates sparks which in turn produce ozone."|2023-09-19-19-37-35
Ozone|Ozone as a greenhouse gas| Although ozone was present at ground level before the Industrial Revolution, peak concentrations are now far higher than the pre-industrial levels, and even background concentrations well away from sources of pollution are substantially higher.   Ozone acts as a greenhouse gas, absorbing some of the infrared energy emitted by the earth. Quantifying the greenhouse gas potency of ozone is difficult because it is not present in uniform concentrations across the globe. However, the most widely accepted scientific assessments relating to climate change (e.g. the Intergovernmental Panel on Climate Change Third Assessment Report)  suggest that the radiative forcing of tropospheric ozone is about 25% that of carbon dioxide. The annual global warming potential of tropospheric ozone is between 918 and 1022 tons carbon dioxide equivalent/tons tropospheric ozone. This means on a per-molecule basis, ozone in the troposphere has a radiative forcing effect roughly 1,000 times as strong as carbon dioxide. However, tropospheric ozone is a short-lived greenhouse gas, which decays in the atmosphere much more quickly than carbon dioxide. This means that over a 20-year span, the global warming potential of tropospheric ozone is much less, roughly 62 to 69 tons carbon dioxide equivalent / ton tropospheric ozone. Because of its short-lived nature, tropospheric ozone does not have strong global effects, but has very strong radiative forcing effects on regional scales. In fact, there are regions of the world where tropospheric ozone has a radiative forcing up to 150% of carbon dioxide.  For example, ozone increase in the troposphere is shown to be responsible for ~30% of upper Southern Ocean interior warming between 1955 and 2000.|2023-09-19-19-37-35
Ozone|Health effects| For the last few decades, scientists studied the effects of acute and chronic ozone exposure on human health. Hundreds of studies suggest that ozone is harmful to people at levels currently found in urban areas.   Ozone has been shown to affect the respiratory, cardiovascular and central nervous system. Early death and problems in reproductive health and development are also shown to be associated with ozone exposure.|2023-09-19-19-37-35
Ozone|Vulnerable populations| The American Lung Association has identified five populations who are especially vulnerable to the effects of breathing ozone: Additional evidence suggests that women, those with obesity and low-income populations may also face higher risk from ozone, although more research is needed.|2023-09-19-19-37-35
Ozone|Acute ozone exposure|" Acute ozone exposure ranges from hours to a few days. Because ozone is a gas, it directly affects the lungs and the entire respiratory system. Inhaled ozone causes inflammation and acute—but reversible—changes in lung function, as well as airway hyperresponsiveness.  These changes lead to shortness of breath, wheezing, and coughing which may exacerbate lung diseases, like asthma or chronic obstructive pulmonary disease (COPD) resulting in the need to receive medical treatment.   Acute and chronic exposure to ozone has been shown to cause an increased risk of respiratory infections, due to the following mechanism. Multiple studies have been conducted to determine the mechanism behind ozone's harmful effects, particularly in the lungs. These studies have shown that exposure to ozone causes changes in the immune response within the lung tissue, resulting in disruption of both the innate and adaptive immune response, as well as altering the protective function of lung epithelial cells.  It is thought that these changes in immune response and the related inflammatory response are factors that likely contribute to the increased risk of lung infections, and worsening or triggering of asthma and reactive airways after exposure to ground-level ozone pollution. The innate (cellular) immune system consists of various chemical signals and cell types that work broadly and against multiple pathogen types, typically bacteria or foreign bodies/substances in the host.   The cells of the innate system include phagocytes, neutrophils,  both thought to contribute to the mechanism of ozone pathology in the lungs, as the functioning of these cell types have been shown to change after exposure to ozone.  Macrophages, cells that serve the purpose of eliminating pathogens or foreign material through the process of ""phagocytosis"",  have been shown to change the level of inflammatory signals they release in response to ozone, either up-regulating and resulting in an inflammatory response in the lung, or down-regulating and reducing immune protection.  Neutrophils, another important cell type of the innate immune system that primarily targets bacterial pathogens,  are found to be present in the airways within 6 hours of exposure to high ozone levels. Despite high levels in the lung tissues, however, their ability to clear bacteria appears impaired by exposure to ozone. The adaptive immune system is the branch of immunity that provides long-term protection via the development of antibodies targeting specific pathogens and is also impacted by high ozone exposure.   Lymphocytes, a cellular component of the adaptive immune response, produce an increased amount of inflammatory chemicals called ""cytokines"" after exposure to ozone, which may contribute to airway hyperreactivity and worsening asthma symptoms. The airway epithelial cells also play an important role in protecting individuals from pathogens. In normal tissue, the epithelial layer forms a protective barrier, and also contains specialized ciliary structures that work to clear foreign bodies, mucus and pathogens from the lungs. When exposed to ozone, the cilia become damaged and mucociliary clearance of pathogens is reduced. Furthermore, the epithelial barrier becomes weakened, allowing pathogens to cross the barrier, proliferate and spread into deeper tissues. Together, these changes in the epithelial barrier help make individuals more susceptible to pulmonary infections. Inhaling ozone not only affects the immune system and lungs, but it may also affect the heart as well. Ozone causes short-term autonomic imbalance leading to changes in heart rate and reduction in heart rate variability;  and high levels exposure for as little as one-hour results in a supraventricular arrhythmia in the elderly,  both increase the risk of premature death and stroke. Ozone may also lead to vasoconstriction resulting in increased systemic arterial pressure contributing to increased risk of cardiac morbidity and mortality in patients with pre-existing cardiac diseases."|2023-09-19-19-37-35
Ozone|Chronic ozone exposure| Breathing ozone for periods longer than eight hours at a time for weeks, months or years defines chronic exposure. Numerous studies suggest a serious impact on the health of various populations from this exposure. One study finds significant positive associations between chronic ozone and all-cause, circulatory, and respiratory mortality with 2%, 3%, and 12% increases in risk per 10 ppb  and report an association (95% CI) of annual ozone and all-cause mortality with a hazard ratio of 1.02 (1.01–1.04), and with cardiovascular mortality of 1.03 (1.01–1.05). A similar study finds similar associations with all-cause mortality and even larger effects for cardiovascular mortality.  An increased risk of mortality from respiratory causes is associated with long-term chronic exposure to ozone. Chronic ozone has detrimental effects on children, especially those with asthma. The risk for hospitalization in children with asthma increases with chronic exposure to ozone; younger children and those with low-income status are even at greater risk. Adults suffering from respiratory diseases (asthma,  COPD,  lung cancer ) are at a higher risk of mortality and morbidity and critically ill patients have an increased risk of developing acute respiratory distress syndrome with chronic ozone exposure as well.|2023-09-19-19-37-35
Ozone|Ozone produced by air cleaners|" Ozone generators sold as air cleaners intentionally produce the gas ozone.  These are often marketed to control indoor air pollution, and use misleading terms to describe ozone. Some examples are describing it as ""energized oxygen"" or ""pure air"", suggesting that ozone is a healthy or ""better"" kind of oxygen.  However, according to the EPA, ""There is evidence to show that at concentrations that do not exceed public health standards, ozone is not effective at removing many odor-causing chemicals."" and ""If used at concentrations that do not exceed public health standards, ozone applied to indoor air does not effectively remove viruses, bacteria, mold, or other biological pollutants."".  Furthermore, another report states that ""results of some controlled studies show that concentrations of ozone considerably higher than these [human safety] standards are possible even when a user follows the manufacturer's operating instructions"". The California Air Resources Board has a page listing air cleaners (many with ionizers) meeting their indoor ozone limit of 0.050 parts per million.  From that article: All portable indoor air cleaning devices sold in California must be certified by the California Air Resources Board (CARB). To be certified, air cleaners must be tested for electrical safety and ozone emissions, and meet an ozone emission concentration limit of 0.050 parts per million. For more information about the regulation, visit the air cleaner regulation."|2023-09-19-19-37-35
Ozone|Ozone air pollution|" Ozone precursors are a group of pollutants, predominantly those emitted during the combustion of fossil fuels. Ground-level ozone pollution (tropospheric ozone) is created near the Earth's surface by the action of daylight UV rays on these precursors. The ozone at ground level is primarily from fossil fuel precursors, but methane is a natural precursor, and the very low natural background level of ozone at ground level is considered safe. This section examines the health impacts of fossil fuel burning, which raises ground level ozone far above background levels. There is a great deal of evidence to show that ground-level ozone can harm lung function and irritate the respiratory system.   Exposure to ozone (and the pollutants that produce it) is linked to premature death, asthma, bronchitis, heart attack, and other cardiopulmonary problems. Long-term exposure to ozone has been shown to increase risk of death from respiratory illness.  A study of 450,000 people living in U.S. cities saw a significant correlation between ozone levels and respiratory illness over the 18-year follow-up period. The study revealed that people living in cities with high ozone levels, such as Houston or Los Angeles, had an over 30% increased risk of dying from lung disease. Air quality guidelines such as those from the World Health Organization, the U.S. Environmental Protection Agency (EPA), and the European Union are based on detailed studies designed to identify the levels that can cause measurable ill health effects. According to scientists with the EPA, susceptible people can be adversely affected by ozone levels as low as 40 nmol/mol.    In the EU, the current target value for ozone concentrations is 120 µg/m3 which is about 60 nmol/mol. This target applies to all member states in accordance with Directive 2008/50/EC.  Ozone concentration is measured as a maximum daily mean of 8 hour averages and the target should not be exceeded on more than 25 calendar days per year, starting from January 2010. Whilst the directive requires in the future a strict compliance with 120 µg/m3 limit (i.e. mean ozone concentration not to be exceeded on any day of the year), there is no date set for this requirement and this is treated as a long-term objective. In the US, the Clean Air Act directs the EPA to set National Ambient Air Quality Standards for several pollutants, including ground-level ozone, and counties out of compliance with these standards are required to take steps to reduce their levels. In May 2008, under a court order, the EPA lowered its ozone standard from 80 nmol/mol to 75 nmol/mol. The move proved controversial, since the Agency's own scientists and advisory board had recommended lowering the standard to 60 nmol/mol.  Many public health and environmental groups also supported the 60 nmol/mol standard,  and the World Health Organization recommends 100 µg/m3 (51 nmol/mol). On January 7, 2010, the U.S. Environmental Protection Agency (EPA) announced proposed revisions to the National Ambient Air Quality Standard (NAAQS) for the pollutant ozone, the principal component of smog: ... EPA proposes that the level of the 8-hour primary standard, which was set at 0.075 μmol/mol in the 2008 final rule, should instead be set at a lower level within the range of 0.060 to 0.070 μmol/mol, to provide increased protection for children and other at risk populations against an array of O3 – related adverse health effects that range from decreased lung function and increased respiratory symptoms to serious indicators of respiratory morbidity including emergency department visits and hospital admissions for respiratory causes, and possibly cardiovascular-related morbidity as well as total non- accidental and cardiopulmonary mortality ... On October 26, 2015, the EPA published a final rule with an effective date of December 28, 2015 that revised the 8-hour primary NAAQS from 0.075 ppm to 0.070 ppm. The EPA has developed an air quality index (AQI) to help explain air pollution levels to the general public. Under the current standards, eight-hour average ozone mole fractions of 85 to 104 nmol/mol are described as ""unhealthy for sensitive groups"", 105 nmol/mol to 124 nmol/mol as ""unhealthy"", and 125 nmol/mol to 404 nmol/mol as ""very unhealthy"". Ozone can also be present in indoor air pollution, partly as a result of electronic equipment such as photocopiers. A connection has also been known to exist between the increased pollen, fungal spores, and ozone caused by thunderstorms and hospital admissions of asthma sufferers. In the Victorian era, one British folk myth held that the smell of the sea was caused by ozone. In fact, the characteristic ""smell of the sea"" is caused by dimethyl sulfide, a chemical generated by phytoplankton. Victorian Britons considered the resulting smell ""bracing"". An investigation to assess the joint mortality effects of ozone and heat during the European heat waves in 2003, concluded that these appear to be additive."|2023-09-19-19-37-35
Ozone|Physiology| Ozone, along with reactive forms of oxygen such as superoxide, singlet oxygen, hydrogen peroxide, and hypochlorite ions, is produced by white blood cells and other biological systems (such as the roots of marigolds) as a means of destroying foreign bodies. Ozone reacts directly with organic double bonds. Also, when ozone breaks down to dioxygen it gives rise to oxygen free radicals, which are highly reactive and capable of damaging many organic molecules. Moreover, it is believed that the powerful oxidizing properties of ozone may be a contributing factor of inflammation. The cause-and-effect relationship of how the ozone is created in the body and what it does is still under consideration and still subject to various interpretations, since other body chemical processes can trigger some of the same reactions. There is evidence linking the antibody-catalyzed water-oxidation pathway of the human immune response to the production of ozone. In this system, ozone is produced by antibody-catalyzed production of trioxidane from water and neutrophil-produced singlet oxygen. When inhaled, ozone reacts with compounds lining the lungs to form specific, cholesterol-derived metabolites that are thought to facilitate the build-up and pathogenesis of atherosclerotic plaques (a form of heart disease). These metabolites have been confirmed as naturally occurring in human atherosclerotic arteries and are categorized into a class of secosterols termed atheronals, generated by ozonolysis of cholesterol's double bond to form a 5,6 secosterol  as well as a secondary condensation product via aldolization.|2023-09-19-19-37-35
Ozone|Impact on plant growth and crop yields|" Ozone has been implicated to have an adverse effect on plant growth: ""... ozone reduced total chlorophylls, carotenoid and carbohydrate concentration, and increased 1-aminocyclopropane-1-carboxylic acid (ACC) content and ethylene production. In treated plants, the ascorbate leaf pool was decreased, while lipid peroxidation and solute leakage were significantly higher than in ozone-free controls. The data indicated that ozone triggered protective mechanisms against oxidative stress in citrus.""  Studies that have used pepper plants as a model have shown that ozone decreased fruit yield and changed fruit quality.   Furthermore, it was also observed a decrease in chlorophylls levels and antioxidant defences on the leaves, as well as increased the reactive oxygen species (ROS) levels and lipid and protein damages. A 2022 study concludes that East Asia loses 63 billion dollars in crops per year due to ozone pollution, a byproduct of fossil fuel combustion. China loses about one third of its potential wheat production and one fourth of its rice production."|2023-09-19-19-37-35
Ozone|Safety regulations|" Because of the strongly oxidizing properties of ozone, ozone is a primary irritant, affecting especially the eyes and respiratory systems and can be hazardous at even low concentrations. The Canadian Centre for Occupation Safety and Health reports that: Even very low concentrations of ozone can be harmful to the upper respiratory tract and the lungs. The severity of injury depends on both the concentration of ozone and the duration of exposure. Severe and permanent lung injury or death could result from even a very short-term exposure to relatively low concentrations."" To protect workers potentially exposed to ozone, U.S. Occupational Safety and Health Administration has established a permissible exposure limit (PEL) of 0.1 μmol/mol (29 CFR 1910.1000 table Z-1), calculated as an 8-hour time weighted average. Higher concentrations are especially hazardous and NIOSH has established an Immediately Dangerous to Life and Health Limit (IDLH) of 5 μmol/mol.  Work environments where ozone is used or where it is likely to be produced should have adequate ventilation and it is prudent to have a monitor for ozone that will alarm if the concentration exceeds the OSHA PEL. Continuous monitors for ozone are available from several suppliers. Elevated ozone exposure can occur on passenger aircraft, with levels depending on altitude and atmospheric turbulence.  U.S. Federal Aviation Administration regulations set a limit of 250 nmol/mol with a maximum four-hour average of 100 nmol/mol.  Some planes are equipped with ozone converters in the ventilation system to reduce passenger exposure."|2023-09-19-19-37-35
Ozone|Production| Ozone generators, or ozonators,  are used to produce ozone for cleaning air or removing smoke odours in unoccupied rooms. These ozone generators can produce over 3 g of ozone per hour. Ozone often forms in nature under conditions where O2 will not react.  Ozone used in industry is measured in μmol/mol (ppm, parts per million), nmol/mol (ppb, parts per billion), μg/m3, mg/h (milligrams per hour) or weight percent. The regime of applied concentrations ranges from 1% to 5% (in air) and from 6% to 14% (in oxygen) for older generation methods. New electrolytic methods can achieve up 20% to 30% dissolved ozone concentrations in output water. Temperature and humidity play a large role in how much ozone is being produced using traditional generation methods (such as corona discharge and ultraviolet light). Old generation methods will produce less than 50% of nominal capacity if operated with humid ambient air, as opposed to very dry air. New generators, using electrolytic methods, can achieve higher purity and dissolution through using water molecules as the source of ozone production.|2023-09-19-19-37-35
Ozone|Coronal discharge method|" This is the most common type of ozone generator for most industrial and personal uses. While variations of the ""hot spark"" coronal discharge method of ozone production exist, including medical grade and industrial grade ozone generators, these units usually work by means of a corona discharge tube or ozone plate.   They are typically cost-effective and do not require an oxygen source other than the ambient air to produce ozone concentrations of 3–6%. Fluctuations in ambient air, due to weather or other environmental conditions, cause variability in ozone production. However, they also produce nitrogen oxides as a by-product. Use of an air dryer can reduce or eliminate nitric acid formation by removing water vapor and increase ozone production. At room temperature, nitric acid will form into a vapour that is hazardous if inhaled. Symptoms can include chest pain, shortness of breath, headaches and a dry nose and throat causing a burning sensation. Use of an oxygen concentrator can further increase the ozone production and further reduce the risk of nitric acid formation by removing not only the water vapor, but also the bulk of the nitrogen."|2023-09-19-19-37-35
Ozone|Ultraviolet light| UV ozone generators, or vacuum-ultraviolet (VUV) ozone generators, employ a light source that generates a narrow-band ultraviolet light, a subset of that produced by the Sun. The Sun's UV sustains the ozone layer in the stratosphere of Earth. UV ozone generators use ambient air for ozone production, no air prep systems are used (air dryer or oxygen concentrator), therefore these generators tend to be less expensive. However, UV ozone generators usually produce ozone with a concentration of about 0.5% or lower which limits the potential ozone production rate. Another disadvantage of this method is that it requires the ambient air (oxygen) to be exposed to the UV source for a longer amount of time, and any gas that is not exposed to the UV source will not be treated. This makes UV generators impractical for use in situations that deal with rapidly moving air or water streams (in-duct air sterilization, for example). Production of ozone is one of the potential dangers of ultraviolet germicidal irradiation. VUV ozone generators are used in swimming pools and spa applications ranging to millions of gallons of water. VUV ozone generators, unlike corona discharge generators, do not produce harmful nitrogen by-products and also unlike corona discharge systems, VUV ozone generators work extremely well in humid air environments. There is also not normally a need for expensive off-gas mechanisms, and no need for air driers or oxygen concentrators which require extra costs and maintenance.|2023-09-19-19-37-35
Ozone|Cold plasma|" In the cold plasma method, pure oxygen gas is exposed to a plasma created by DBD. The diatomic oxygen is split into single atoms, which then recombine in triplets to form ozone.
It is common in the industry to mislabel some DBD ozone generators as CD Corona Discharge generators. Typically all solid flat metal electrode ozone generators produce ozone using the dielectric barrier discharge method. Cold plasma machines use pure oxygen as the input source and produce a maximum concentration of about 24% ozone. They produce far greater quantities of ozone in a given time compared to ultraviolet production that has about 2% efficiency. The discharges manifest as filamentary transfer of electrons (micro discharges) in a gap between two electrodes. In order to evenly distribute the micro discharges, a dielectric insulator must be used to separate the metallic electrodes and to prevent arcing."|2023-09-19-19-37-35
Ozone|Electrolytic|" Electrolytic ozone generation (EOG) splits water molecules into H2, O2, and O3.
In most EOG methods, the hydrogen gas will be removed to leave oxygen and ozone as the only reaction products. Therefore, EOG can achieve higher dissolution in water without other competing gases found in corona discharge method, such as nitrogen gases present in ambient air. This method of generation can achieve concentrations of 20–30% and is independent of air quality because water is used as the source material. Production of ozone electrolytically is typically unfavorable because of the high overpotential required to produce ozone as compared to oxygen. This is why ozone is not produced during typical water electrolysis. However, it is possible to increase the overpotential of oxygen by careful catalyst selection such that ozone is preferentially produced under electrolysis. Catalysts typically chosen for this approach are lead dioxide  or boron-doped diamond. The ozone to oxygen ratio is improved by increasing current density at the anode, cooling the electrolyte around the anode close to 0 °C, using an acidic electrolyte (such as dilute sulfuric acid) instead of a basic solution, and by applying pulsed current instead of DC."|2023-09-19-19-37-35
Ozone|Special considerations| Ozone cannot be stored and transported like other industrial gases (because it quickly decays into diatomic oxygen) and must therefore be produced on site. Available ozone generators vary in the arrangement and design of the high-voltage electrodes. At production capacities higher than 20 kg per hour, a gas/water tube heat-exchanger may be utilized as ground electrode and assembled with tubular high-voltage electrodes on the gas-side. The regime of typical gas pressures is around 2 bars (200 kPa) absolute in oxygen and 3 bars (300 kPa) absolute in air. Several megawatts of electrical power may be installed in large facilities, applied as single phase AC current at 50 to 8000 Hz and peak voltages between 3,000 and 20,000 volts. Applied voltage is usually inversely related to the applied frequency. The dominating parameter influencing ozone generation efficiency is the gas temperature, which is controlled by cooling water temperature and/or gas velocity. The cooler the water, the better the ozone synthesis. The lower the gas velocity, the higher the concentration (but the lower the net ozone produced). At typical industrial conditions, almost 90% of the effective power is dissipated as heat and needs to be removed by a sufficient cooling water flow. Because of the high reactivity of ozone, only a few materials may be used like stainless steel (quality 316L), titanium, aluminium (as long as no moisture is present), glass, polytetrafluorethylene, or polyvinylidene fluoride. Viton may be used with the restriction of constant mechanical forces and absence of humidity (humidity limitations apply depending on the formulation). Hypalon may be used with the restriction that no water comes in contact with it, except for normal atmospheric levels. Embrittlement or shrinkage is the common mode of failure of elastomers with exposure to ozone. Ozone cracking is the common mode of failure of elastomer seals like O-rings. Silicone rubbers are usually adequate for use as gaskets in ozone concentrations below 1 wt%, such as in equipment for accelerated aging of rubber samples.|2023-09-19-19-37-35
Ozone|Incidental production|" Ozone may be formed from O2 by electrical discharges and by action of high energy electromagnetic radiation. Unsuppressed arcing in electrical contacts, motor brushes, or mechanical switches breaks down the chemical bonds of the atmospheric oxygen surrounding the contacts [O2 → 2O]. Free radicals of oxygen in and around the arc recombine to create ozone [O3].  Certain electrical equipment generate significant levels of ozone. This is especially true of devices using high voltages, such as ionic air purifiers, laser printers, photocopiers, tasers and arc welders. Electric motors using brushes can generate ozone from repeated sparking inside the unit. Large motors that use brushes, such as those used by elevators or hydraulic pumps, will generate more ozone than smaller motors. Ozone is similarly formed in the Catatumbo lightning storms phenomenon on the Catatumbo River in Venezuela, though ozone's instability makes it dubious that it has any effect on the ozonosphere. 
It is the world's largest single natural generator of ozone, lending calls for it to be designated a UNESCO World Heritage Site."|2023-09-19-19-37-35
Ozone|Laboratory production| In the laboratory, ozone can be produced by electrolysis using a 9 volt battery, a pencil graphite rod cathode, a platinum wire anode and a 3 molar sulfuric acid electrolyte.  The half cell reactions taking place are: where E° represents the standard electrode potential. In the net reaction, three equivalents of water are converted into one equivalent of ozone and three equivalents of hydrogen. Oxygen formation is a competing reaction. It can also be generated by a high voltage arc. In its simplest form, high voltage AC, such as the output of a neon-sign transformer is connected to two metal rods with the ends placed sufficiently close to each other to allow an arc. The resulting arc will convert atmospheric oxygen to ozone. It is often desirable to contain the ozone. This can be done with an apparatus consisting of two concentric glass tubes sealed together at the top with gas ports at the top and bottom of the outer tube. The inner core should have a length of metal foil inserted into it connected to one side of the power source. The other side of the power source should be connected to another piece of foil wrapped around the outer tube. A source of dry O2 is applied to the bottom port. When high voltage is applied to the foil leads, electricity will discharge between the dry dioxygen in the middle and form O3 and O2 which will flow out the top port. This is called a Siemen's ozoniser. The reaction can be summarized as follows:|2023-09-19-19-37-35
Ozone|Industry| The largest use of ozone is in the preparation of pharmaceuticals, synthetic lubricants, and many other commercially useful organic compounds, where it is used to sever carbon-carbon bonds.  It can also be used for bleaching substances and for killing microorganisms in air and water sources.  Many municipal drinking water systems kill bacteria with ozone instead of the more common chlorine.  Ozone has a very high oxidation potential.  Ozone does not form organochlorine compounds, nor does it remain in the water after treatment. Ozone can form the suspected carcinogen bromate in source water with high bromide concentrations. The U.S. Safe Drinking Water Act mandates that these systems introduce an amount of chlorine to maintain a minimum of 0.2 μmol/mol residual free chlorine in the pipes, based on results of regular testing. Where electrical power is abundant, ozone is a cost-effective method of treating water, since it is produced on demand and does not require transportation and storage of hazardous chemicals. Once it has decayed, it leaves no taste or odour in drinking water. Although low levels of ozone have been advertised to be of some disinfectant use in residential homes, the concentration of ozone in dry air required to have a rapid, substantial effect on airborne pathogens exceeds safe levels recommended by the U.S. Occupational Safety and Health Administration and Environmental Protection Agency. Humidity control can vastly improve both the killing power of the ozone and the rate at which it decays back to oxygen (more humidity allows more effectiveness). Spore forms of most pathogens are very tolerant of atmospheric ozone in concentrations at which asthma patients start to have issues. In 1908 artificial ozonisation the Central Line of the London Underground was introduced as an aerial disinfectant. The process was found to be worthwhile, but was phased out by 1956. However the beneficial effect was maintained by the ozone created incidentally from the electrical discharges of the train motors (see above: Incidental production). Ozone generators were made available to schools and universities in Wales for the Autumn term 2021, to disinfect classrooms after COVID-19 outbreaks. Industrially, ozone is used to: Ozone is a reagent in many organic reactions in the laboratory and in industry. Ozonolysis is the cleavage of an alkene to carbonyl compounds. Many hospitals around the world use large ozone generators to decontaminate operating rooms between surgeries. The rooms are cleaned and then sealed airtight before being filled with ozone which effectively kills or neutralizes all remaining bacteria. Ozone is used as an alternative to chlorine or chlorine dioxide in the bleaching of wood pulp.  It is often used in conjunction with oxygen and hydrogen peroxide to eliminate the need for chlorine-containing compounds in the manufacture of high-quality, white paper. Ozone can be used to detoxify cyanide wastes (for example from gold and silver mining) by oxidizing cyanide to cyanate and eventually to carbon dioxide.|2023-09-19-19-37-35
Ozone|Water disinfection| Since the invention of Dielectric Barrier Discharge (DBD) plasma reactors, it has been employed for water treatment with ozone.  However, with cheaper alternative disinfectants like chlorine, such applications of DBD ozone water decontamination have been limited by high power consumption and bulky equipment.   Despite this, with research revealing the negative impacts of common disinfectants like chlorine with respect to toxic residuals and ineffectiveness in killing certain micro-organisms,  DBD plasma-based ozone decontamination is of interest in current available technologies. Although ozonation of water with a high concentration of bromide does lead to the formation of undesirable brominated disinfection byproducts, unless drinking water is produced by desalination, ozonation can generally be applied without concern for these byproducts.     Advantages of ozone include high thermodynamic oxidation potential, less sensitivity to organic material and better tolerance for pH variations while retaining the ability to kill bacteria, fungi, viruses, as well as spores and cysts.    Although, ozone has been widely accepted in Europe for decades, it is sparingly used for decontamination in the U.S due to limitations of high-power consumption, bulky installation and stigma attached with ozone toxicity.   Considering this, recent research efforts have been directed towards the study of effective ozone water treatment systems  Researchers have looked into lightweight and compact low power surface DBD reactors,   energy efficient volume DBD reactors  and low power micro-scale DBD reactors.   Such studies can help pave the path to re-acceptance of DBD plasma-based ozone decontamination of water, especially in the U.S.|2023-09-19-19-37-35
Ozone|Consumers|" Devices generating high levels of ozone, some of which use ionization, are used to sanitize and deodorize uninhabited buildings, rooms, ductwork, woodsheds, boats and other vehicles. Ozonated water is used to launder clothes and to sanitize food, drinking water, and surfaces in the home. According to the U.S. Food and Drug Administration (FDA), it is ""amending the food additive regulations to provide for the safe use of ozone in gaseous and aqueous phases as an antimicrobial agent on food, including meat and poultry."" Studies at California Polytechnic University demonstrated that 0.3 μmol/mol levels of ozone dissolved in filtered tapwater can produce a reduction of more than 99.99% in such food-borne microorganisms as salmonella, E. coli 0157:H7 and Campylobacter. This quantity is 20,000 times the WHO-recommended limits stated above.  
Ozone can be used to remove pesticide residues from fruits and vegetables. Ozone is used in homes and hot tubs to kill bacteria in the water and to reduce the amount of chlorine or bromine required by reactivating them to their free state. Since ozone does not remain in the water long enough, ozone by itself is ineffective at preventing cross-contamination among bathers and must be used in conjunction with halogens. Gaseous ozone created by ultraviolet light or by corona discharge is injected into the water. Ozone is also widely used in the treatment of water in aquariums and fishponds. Its use can minimize bacterial growth, control parasites, eliminate transmission of some diseases, and reduce or eliminate ""yellowing"" of the water. Ozone must not come in contact with fishes' gill structures. Natural saltwater (with life forms) provides enough ""instantaneous demand"" that controlled amounts of ozone activate bromide ions to hypobromous acid, and the ozone entirely decays in a few seconds to minutes. If oxygen-fed ozone is used, the water will be higher in dissolved oxygen and fishes' gill structures will atrophy, making them dependent on oxygen-enriched water."|2023-09-19-19-37-35
Ozone|Aquaculture| Ozonation – a process of infusing water with ozone – can be used in aquaculture to facilitate organic breakdown. Ozone is also added to recirculating systems to reduce nitrite levels  through conversion into nitrate. If nitrite levels in the water are high, nitrites will also accumulate in the blood and tissues of fish, where it interferes with oxygen transport (it causes oxidation of the heme-group of haemoglobin from ferrous (Fe2+) to ferric (Fe3+), making haemoglobin unable to bind O2).  Despite these apparent positive effects, ozone use in recirculation systems has been linked to reducing the level of bioavailable iodine in salt water systems, resulting in iodine deficiency symptoms such as goitre and decreased growth in Senegalese sole (Solea senegalensis) larvae. Ozonate seawater is used for surface disinfection of haddock and Atlantic halibut eggs against nodavirus. Nodavirus is a lethal and vertically transmitted virus which causes severe mortality in fish. Haddock eggs should not be treated with high ozone level as eggs so treated did not hatch and died after 3–4 days.|2023-09-19-19-37-35
Ozone|Agriculture| Ozone application on freshly cut pineapple and banana shows increase in flavonoids and total phenol contents when exposure is up to 20 minutes. Decrease in ascorbic acid (one form of vitamin C) content is observed but the positive effect on total phenol content and flavonoids can overcome the negative effect.  Tomatoes upon treatment with ozone show an increase in β-carotene, lutein and lycopene.  However, ozone application on strawberries in pre-harvest period shows decrease in ascorbic acid content. Ozone facilitates the extraction of some heavy metals from soil using EDTA. EDTA forms strong, water-soluble coordination compounds with some heavy metals (Pb, Zn) thereby making it possible to dissolve them out from contaminated soil. If contaminated soil is pre-treated with ozone, the extraction efficacy of Pb, Am and Pu increases by 11.0–28.9%,  43.5%  and 50.7%  respectively.|2023-09-19-19-37-35
Ozone|Unintentional Environmental Effect on Pollinators| Crop pollination is an essential part of an ecosystem. Ozone can have detrimental effects on plant-pollinator interactions.  Pollinators carry pollen from one plant to another. This is an essential cycle inside of an ecosystem. Causing changes in certain atmospheric conditions around pollination sites or with xenobiotics could cause unknown changes to the natural cycles of pollinators and flowering plants. In a study conducted in North-Western Europe, crop pollinators were negatively affected more when ozone levels were higher.|2023-09-19-19-37-35
Ozone|Alternative medicine| The use of ozone for the treatment of medical conditions is not supported by high quality evidence, and is generally considered alternative medicine.|2023-09-19-19-37-35
Ozone|References| Footnotes Citations|2023-09-19-19-37-35
Ozone|External links| Nascent oxygen  O Dioxygen (singlet and triplet)O2 Trioxygen (ozone and cyclic ozone)  O3 Tetraoxygen  O4 Octaoxygen  O8|2023-09-19-19-37-35
Ozone depletion|General| Lists Categories Ozone depletion consists of two related events observed since the late 1970s: a steady lowering of about four percent in the total amount of ozone in Earth's atmosphere, and a much larger springtime decrease in stratospheric ozone (the ozone layer) around Earth's polar regions.  The latter phenomenon is referred to as the ozone hole. There are also springtime polar tropospheric ozone depletion events in addition to these stratospheric events. The main causes of ozone depletion and the ozone hole are manufactured chemicals, especially manufactured halocarbon refrigerants, solvents, propellants, and foam-blowing agents (chlorofluorocarbons (CFCs), HCFCs, halons), referred to as ozone-depleting substances (ODS).  These compounds are transported into the stratosphere by turbulent mixing after being emitted from the surface, mixing much faster than the molecules can settle.  Once in the stratosphere, they release atoms from the halogen group through photodissociation, which catalyze the breakdown of ozone (O3) into oxygen (O2).  Both types of ozone depletion were observed to increase as emissions of halocarbons increased. Ozone depletion and the ozone hole have generated worldwide concern over increased cancer risks and other negative effects. The ozone layer prevents harmful wavelengths of ultraviolet (UVB) light from passing through the Earth's atmosphere. These wavelengths cause skin cancer, sunburn, permanent blindness, and cataracts,  which were projected to increase dramatically as a result of thinning ozone, as well as harming plants and animals. These concerns led to the adoption of the Montreal Protocol in 1987, which bans the production of CFCs, halons, and other ozone-depleting chemicals.  Currently,[when?] scientists plan to develop new refrigerants to replace older ones. The ban came into effect in 1989. Ozone levels stabilized by the mid-1990s and began to recover in the 2000s, as the shifting of the jet stream in the southern hemisphere towards the south pole has stopped and might even be reversing.  Recovery is projected to continue over the next century, and the ozone hole was expected to reach pre-1980 levels by around 2075.  In 2019, NASA reported that the ozone hole was the smallest ever since it was first discovered in 1982. The Montreal Protocol is considered the most successful international environmental agreement to date.   Following the bans on ozone-depleting chemicals, the UN projects that under the current regulations the ozone layer will completely regenerate by 2045, thirty years earlier than previously predicted.|2023-09-27-07-29-43
Ozone depletion|Ozone cycle overview| Three forms (or allotropes) of oxygen are involved in the ozone-oxygen cycle: oxygen atoms (O or atomic oxygen), oxygen gas (O2 or diatomic oxygen), and ozone gas (O3 or triatomic oxygen).  Ozone is formed in the stratosphere when oxygen gas molecules photodissociate after absorbing UVC photons. This converts a single O2 into two atomic oxygen radicals. The atomic oxygen radicals then combine with separate O2 molecules to create two O3 molecules. These ozone molecules absorb UVB light, following which ozone splits into a molecule of O2 and an oxygen atom. The oxygen atom then joins up with an oxygen molecule to regenerate ozone. This is a continuing process that terminates when an oxygen atom recombines with an ozone molecule to make two O2 molecules. It is worth noting that ozone is the only atmospheric gas that absorbs UVB light. The total amount of ozone in the stratosphere is determined by a balance between photochemical production and recombination. Ozone can be destroyed by a number of free radical catalysts; the most important are the hydroxyl radical (OH·), nitric oxide radical (NO·), chlorine radical (Cl·) and bromine radical (Br·). The dot is a notation to indicate that each species has an unpaired electron and is thus extremely reactive. All of these have both natural and man-made sources; at present, most of the OH· and NO· in the stratosphere is naturally occurring, but human activity has drastically increased the levels of chlorine and bromine.  These elements are found in stable organic compounds, especially chlorofluorocarbons, which can travel to the stratosphere without being destroyed in the troposphere due to their low reactivity. Once in the stratosphere, the Cl and Br atoms are released from the parent compounds by the action of ultraviolet light, e.g. Ozone is a highly reactive molecule that easily reduces to the more stable oxygen form with the assistance of a catalyst. Cl and Br atoms destroy ozone molecules through a variety of catalytic cycles. In the simplest example of such a cycle,  a chlorine atom reacts with an ozone molecule (O3), taking an oxygen atom to form chlorine monoxide (ClO) and leaving an oxygen molecule (O2). The ClO can react with a second molecule of ozone, releasing the chlorine atom and yielding two molecules of oxygen. The chemical shorthand for these gas-phase reactions is: The overall effect is a decrease in the amount of ozone, though the rate of these processes can be decreased by the effects of null cycles. More complicated mechanisms have also been discovered that lead to ozone destruction in the lower stratosphere. A single chlorine atom would continuously destroy ozone (thus a catalyst) for up to two years (the time scale for transport back down to the troposphere) except for reactions that remove it from this cycle by forming reservoir species such as hydrogen chloride (HCl) and chlorine nitrate (ClONO2). Bromine is even more efficient than chlorine at destroying ozone on a per-atom basis, but there is much less bromine in the atmosphere at present. Both chlorine and bromine contribute significantly to overall ozone depletion. Laboratory studies have also shown that fluorine and iodine atoms participate in analogous catalytic cycles. However, fluorine atoms react rapidly with water vapour, methane and hydrogen to form strongly bound hydrogen fluoride (HF) in the Earth's stratosphere,  while organic molecules containing iodine react so rapidly in the lower atmosphere that they do not reach the stratosphere in significant quantities. A single chlorine atom is able to react with an average of 100,000 ozone molecules before it is removed from the catalytic cycle. This fact plus the amount of chlorine released into the atmosphere yearly by chlorofluorocarbons (CFCs) and hydrochlorofluorocarbons (HCFCs) demonstrates the danger of CFCs and HCFCs to the environment.|2023-09-27-07-29-43
Ozone depletion|Observations on ozone layer depletion|" The ozone hole is usually measured by reduction in the total column ozone above a point on the Earth's surface. This is normally expressed in Dobson units; abbreviated as ""DU"". The most prominent decrease in ozone has been in the lower stratosphere. Marked decreases in column ozone in the Antarctic spring and early summer compared to the early 1970s and before have been observed using instruments such as the Total Ozone Mapping Spectrometer (TOMS). Reductions of up to 70 percent in the ozone column observed in the austral (southern hemispheric) spring over Antarctica and first reported in 1985 (Farman et al.) are continuing. Antarctic total column ozone in September and October have continued to be 40–50 percent lower than pre-ozone-hole values since the 1990s.  A gradual trend toward ""healing"" was reported in 2016.  In 2017, NASA announced that the ozone hole was the weakest since 1988 because of warm stratospheric conditions. It is expected to recover around 2070. The amount lost is more variable year-to-year in the Arctic than in the Antarctic. The greatest Arctic declines are in the winter and spring, reaching up to 30 percent when the stratosphere is coldest. Reactions that take place on polar stratospheric clouds (PSCs) play an important role in enhancing ozone depletion.  PSCs form more readily in the extreme cold of the Arctic and Antarctic stratosphere. This is why ozone holes first formed, and are deeper, over Antarctica. Early models failed to take PSCs into account and predicted a gradual global depletion, which is why the sudden Antarctic ozone hole was such a surprise to many scientists. It is more accurate to speak of ozone depletion in middle latitudes rather than holes. Total column ozone declined below pre-1980 values between 1980 and 1996 for mid-latitudes. In the northern mid-latitudes, it then increased from the minimum value by about two percent from 1996 to 2009 as regulations took effect and the amount of chlorine in the stratosphere decreased. In the Southern Hemisphere's mid-latitudes, total ozone remained constant over that time period. There are no significant trends in the tropics, largely because halogen-containing compounds have not had time to break down and release chlorine and bromine atoms at tropical latitudes. Large volcanic eruptions have been shown to have substantial albeit uneven ozone-depleting effects, as observed with the 1991 eruption of Mt. Pinatubo in the Philippines. Ozone depletion also explains much of the observed reduction in stratospheric and upper tropospheric temperatures.   The source of the warmth of the stratosphere is the absorption of UV radiation by ozone, hence reduced ozone leads to cooling. Some stratospheric cooling is also predicted from increases in greenhouse gases such as CO2 and CFCs themselves; however, the ozone-induced cooling appears to be dominant. Predictions of ozone levels remain difficult, but the precision of models' predictions of observed values and the agreement among different modeling techniques have increased steadily.  The World Meteorological Organization Global Ozone Research and Monitoring Project—Report No. 44 comes out strongly in favor of the Montreal Protocol, but notes that a UNEP 1994 Assessment overestimated ozone loss for the 1994–1997 period."|2023-09-27-07-29-43
Ozone depletion|Compounds in the atmosphere| Chlorofluorocarbons (CFCs) and other halogenated ozone-depleting substances (ODS) are mainly responsible for man-made chemical ozone depletion. The total amount of effective halogens (chlorine and bromine) in the stratosphere can be calculated and are known as the equivalent effective stratospheric chlorine (EESC). CFCs as refrigerants were invented by Thomas Midgley, Jr. in the 1930s.  They were used in air conditioning and cooling units, as aerosol spray propellants prior to the 1970s, and in the cleaning processes of delicate electronic equipment. They also occur as by-products of some chemical processes. No significant natural sources have ever been identified for these compounds—their presence in the atmosphere is due almost entirely to human manufacture. As mentioned above, when such ozone-depleting chemicals reach the stratosphere, they are dissociated by ultraviolet light to release chlorine atoms. The chlorine atoms act as a catalyst, and each can break down tens of thousands of ozone molecules before being removed from the stratosphere. Given the longevity of CFC molecules, recovery times are measured in decades. It is calculated that a CFC molecule takes an average of about five to seven years to go from the ground level up to the upper atmosphere, and it can stay there for about a century, destroying up to one hundred thousand ozone molecules during that time. [verification needed] 1,1,1-Trichloro-2,2,2-trifluoroethane, also known as CFC-113a, is one of four man-made chemicals newly discovered in the atmosphere by a team at the University of East Anglia. CFC-113a is the only known CFC whose abundance in the atmosphere is still growing. Its source remains a mystery, but illegal manufacturing is suspected by some.  CFC-113a seems to have been accumulating unabated since 1960. Between 2012 and 2017, concentrations of the gas jumped by 40 percent. A study by an international team of researchers published in Nature found that since 2013 emissions that are predominately from north-eastern China have released large quantities of the banned chemical Chlorofluorocarbon-11 (CFC-11) into the atmosphere. Scientists estimate that without action, these CFC-11 emissions will delay the recovery of the planet's ozone hole by a decade.|2023-09-27-07-29-43
Ozone depletion|Computer modeling| Scientists have attributed ozone depletion to the increase of man-made (anthropogenic) halogen compounds from CFCs by combining observational data with computer models. These complex chemistry transport models (e.g. SLIMCAT, CLaMS—Chemical Lagrangian Model of the Stratosphere) work by combining measurements of chemicals and meteorological fields with chemical reaction rate constants. They identify key chemical reactions and transport processes that bring CFC photolysis products into contact with ozone.|2023-09-27-07-29-43
Ozone depletion|Ozone hole and its causes|" The Antarctic ozone hole is an area of the Antarctic stratosphere in which the recent ozone levels have dropped to as low as 33 percent of their pre-1975 values.  The ozone hole occurs during the Antarctic spring, from September to early December, as strong westerly winds start to circulate around the continent and create an atmospheric container. Within this polar vortex, over 50 percent of the lower stratospheric ozone is destroyed during the Antarctic spring. As explained above, the primary cause of ozone depletion is the presence of chlorine-containing source gases (primarily CFCs and related halocarbons). In the presence of UV light, these gases dissociate, releasing chlorine atoms, which then go on to catalyze ozone destruction. The Cl-catalyzed ozone depletion can take place in the gas phase, but it is dramatically enhanced in the presence of polar stratospheric clouds (PSCs). These polar stratospheric clouds form during winter, in the extreme cold. Polar winters are dark, consisting of three months without solar radiation (sunlight). The lack of sunlight contributes to a decrease in temperature and the polar vortex traps and chills the air. Temperatures hover around or below −80 °C. These low temperatures form cloud particles. There are three types of PSC clouds—nitric acid trihydrate clouds, slowly cooling water-ice clouds, and rapid cooling water-ice (nacreous) clouds—provide surfaces for chemical reactions whose products will, in the spring lead to ozone destruction. The photochemical processes involved are complex but well understood. The key observation is that, ordinarily, most of the chlorine in the stratosphere resides in ""reservoir"" compounds, primarily chlorine nitrate (ClONO2) as well as stable end products such as HCl. The formation of end products essentially removes Cl from the ozone depletion process. The former sequester Cl, which can be later made available via absorption of light at shorter wavelengths than 400 nm.  During the Antarctic winter and spring, however, reactions on the surface of the polar stratospheric cloud particles convert these ""reservoir"" compounds into reactive free radicals (Cl and ClO). Denitrification is the process by which the clouds remove NO2 from the stratosphere by converting it to nitric acid in PSC particles, which then are lost by sedimentation. This prevents newly formed ClO from being converted back into ClONO2. The role of sunlight in ozone depletion is the reason why the Antarctic ozone depletion is greatest during spring. During winter, even though PSCs are at their most abundant, there is no light over the pole to drive chemical reactions. During the spring, however, sunlight returns and provides energy to drive photochemical reactions and melt the polar stratospheric clouds, releasing considerable ClO, which drives the hole mechanism. Further warming temperatures near the end of spring break up the vortex around mid-December. As warm, ozone and NO2-rich air flows in from lower latitudes, the PSCs are destroyed, the enhanced ozone depletion process shuts down, and the ozone hole closes. Most of the ozone that is destroyed is in the lower stratosphere, in contrast to the much smaller ozone depletion through homogeneous gas-phase reactions, which occurs primarily in the upper stratosphere."|2023-09-27-07-29-43
Ozone depletion|Interest in ozone layer depletion|" Public misconceptions and misunderstandings of complex issues like ozone depletion are common. The limited scientific knowledge of the public led to confusion about global warming  or the perception of global warming as a subset of the ""ozone hole"". 
In the beginning, classical green NGOs refrained from using CFC depletion for campaigning, as they assumed the topic was too complicated.  They became active much later, e.g. in Greenpeace's support for a CFC-free fridge produced by the former East German company VEB dkk Scharfenstein. The metaphors used in the CFC discussion (ozone shield, ozone hole) are not ""exact"" in the scientific sense. The ""ozone hole"" is more of a depression, less ""a hole in the windshield"". The ozone does not disappear through the layer, nor is there a uniform ""thinning"" of the ozone layer. However, they resonated better with non-scientists and their concerns.  The ozone hole was seen as a ""hot issue"" and imminent risk  as laypeople feared severe personal consequences such as skin cancer, cataracts, damage to plants, and reduction of plankton populations in the ocean's photic zone. Not only on the policy level, ozone regulation compared to climate change fared much better in public opinion. Americans voluntarily switched away from aerosol sprays before legislation was enforced, while climate change failed to achieve comparable concern and public action.  The sudden identification in 1985 that there was a substantial ""hole"" was widely reported in the press. The especially rapid ozone depletion in Antarctica had previously been dismissed as a measurement error.  Scientific consensus was established after regulation. While the Antarctic ozone hole has a relatively small effect on global ozone, the hole has generated a great deal of public interest because:"|2023-09-27-07-29-43
Ozone depletion|Effects| Since the ozone layer absorbs UVB ultraviolet light from the sun, ozone layer depletion increases surface UVB levels (all else equal), which could lead to damage, including an increase in skin cancer. This was the reason for the Montreal Protocol. Although decreases in stratospheric ozone are well-tied to CFCs and increases in surface UVB, there is no direct observational evidence linking ozone depletion to higher incidence of skin cancer and eye damage in human beings. This is partly because UVA, which has also been implicated in some forms of skin cancer, is not absorbed by ozone, and because it is nearly impossible to control statistics for lifestyle changes over time. Ozone depletion may also influence wind patterns.|2023-09-27-07-29-43
Ozone depletion|Increased UV| Ozone, while a minority constituent in Earth's atmosphere, is responsible for most of the absorption of UVB radiation. The amount of UVB radiation that penetrates through the ozone layer decreases exponentially with the slant-path thickness and density of the layer.  When stratospheric ozone levels decrease, higher levels of UVB reach the Earth's surface.   UV-driven phenolic formation in tree rings has dated the start of ozone depletion in northern latitudes to the late 1700s. In October 2008, the Ecuadorian Space Agency published a report called HIPERION. The study used ground instruments in Ecuador and the last 28 years' data from 12 satellites of several countries, and found that the UV radiation reaching equatorial latitudes was far greater than expected, with the UV Index climbing as high as 24 in Quito; the WHO considers 11 as an extreme index and a great risk to health. The report concluded that depleted ozone levels around the mid-latitudes of the planet are already endangering large populations in these areas.  Later, the CONIDA, the Peruvian Space Agency, published its own study, which yielded almost the same findings as the Ecuadorian study.|2023-09-27-07-29-43
Ozone depletion|Biological effects|" The main public concern regarding the ozone hole has been the effects of increased surface UV radiation on human health. So far, ozone depletion in most locations has been typically a few percent and, as noted above, no direct evidence of health damage is available in most latitudes. If the high levels of depletion seen in the ozone hole were to be common across the globe, the effects could be substantially more dramatic. As the ozone hole over Antarctica has in some instances grown so large as to affect parts of Australia, New Zealand, Chile, Argentina, and South Africa, environmentalists have been concerned that the increase in surface UV could be significant.  Excessive ultraviolet radiation (UVR) has reducing effects on the rates of photosynthesis and growth of benthic diatom communities (microalgae species that increase water quality and are pollution resistant) that are present in shallow freshwater. Ozone depletion would magnify all of the effects of UV on human health, both positive (including production of vitamin D) and negative (including sunburn, skin cancer, and cataracts). In addition, increased surface UV leads to increased tropospheric ozone, which is a health risk to humans. The most common forms of skin cancer in humans, basal and squamous cell carcinomas, have been strongly linked to UV-B exposure. The mechanism by which UVB induces these cancers is well understood—absorption of UV-B radiation causes the pyrimidine bases in the DNA molecule to form dimers, resulting in transcription errors when the DNA replicates. These cancers are relatively mild and rarely fatal, although the treatment of squamous cell carcinoma sometimes requires extensive reconstructive surgery. By combining epidemiological data with results of animal studies, scientists have estimated that every one percent decrease in long-term stratospheric ozone would increase the incidence of these cancers by 2%. Another form of skin cancer, malignant melanoma, is much less common but far more dangerous, being lethal in about 15–20 percent of the cases diagnosed. The relationship between malignant melanoma and ultraviolet exposure is not yet fully understood, but it appears that both UV-B and UV-A are involved. Because of this uncertainty, it is difficult to estimate the effect of ozone depletion on melanoma incidence. One study showed that a 10 percent increase in UV-B radiation was associated with a 19 percent increase in melanomas for men and 16 percent for women.  A study of people in Punta Arenas, at the southern tip of Chile, showed a 56 percent increase in melanoma and a 46 percent increase in nonmelanoma skin cancer over a period of seven years, along with decreased ozone and increased UVB levels. Epidemiological studies suggest an association between ocular cortical cataracts and UV-B exposure, using crude approximations of exposure and various cataract assessment techniques. A detailed assessment of ocular exposure to UV-B was carried out in a study on Chesapeake Bay Watermen, where increases in average annual ocular exposure were associated with increasing risk of cortical opacity.  In this highly exposed group of predominantly white males, the evidence linking cortical opacities to sunlight exposure was the strongest to date. Based on these results, ozone depletion is predicted to cause hundreds of thousands of additional cataracts by 2050. Increased surface UV leads to increased tropospheric ozone. Ground-level ozone is generally recognized to be a health risk, as ozone is toxic due to its strong oxidant properties. The risks are particularly high for young children, the elderly, and those with asthma or other respiratory difficulties. At this time, ozone at ground level is produced mainly by the action of UV radiation on combustion gases from vehicle exhausts. Vitamin D is produced in the skin by ultraviolet light. Thus, higher UVB exposure raises human vitamin D in those deficient in it.  Recent research (primarily since the Montreal Protocol) shows that many humans have less than optimal vitamin D levels. In particular, in the U.S. population, the lowest quarter of vitamin D (<17.8 ng/ml) were found using information from the National Health and Nutrition Examination Survey to be associated with an increase in all-cause mortality in the general population.  While blood level of vitamin D in excess of 100 ng/ml appear to raise blood calcium excessively and to be associated with higher mortality, the body has mechanisms that prevent sunlight from producing vitamin D in excess of the body's requirements. A November 2011 report by scientists at the Institute of Zoology in London found that whales off the coast of California have shown a sharp rise in sun damage, and these scientists ""fear that the thinning ozone layer is to blame"".  The study photographed and took skin biopsies from over 150 whales in the Gulf of California and found ""widespread evidence of epidermal damage commonly associated with acute and severe sunburn"", having cells that form when the DNA is damaged by UV radiation. The findings suggest ""rising UV levels as a result of ozone depletion are to blame for the observed skin damage, in the same way that human skin cancer rates have been on the increase in recent decades.""  Apart from whales many other animals such as dogs, cats, sheep and terrestrial ecosystems also suffer the negative effects of increased UV-B radiations. An increase of UV radiation would be expected to affect crops. A number of economically important species of plants, such as rice, depend on cyanobacteria residing on their roots for the retention of nitrogen. Cyanobacteria are sensitive to UV radiation and would be affected by its increase.  ""Despite mechanisms to reduce or repair the effects of increased ultraviolet radiation, plants have a limited ability to adapt to increased levels of UVB, therefore plant growth can be directly affected by UVB radiation."" Over the years, the Arctic ozone layer has depleted severely. As a consequence species that live above the snow cover or in areas where snow has melted abundantly, due to hot temperatures, are negatively impacted due to UV radiation that reaches the ground.  Depletion of the ozone layer and allowing excess UVB radiation would initially be assumed to increase damage done to plant DNA. Reports have found that when plants are exposed to UVB radiation similar to stratospheric ozone depletion, there was no significant change in plant height or leaf mass, but showed a response in shoot biomass and leaf area with a small decrease.  However, UVB radiation has been shown to decrease quantum yield of photosystem II.  UVB damage only occurs under extreme exposure, and most plants also have UVB absorbing flavonoids which allow them to acclimatize to the radiation present. Plants experience different levels of UV radiation throughout the day. It is known that they are able to shift the levels and types of UV sunscreens (i.e. flavonoids), that they contain, throughout the day. This allows them to increase their protection against UV radiation.  Plants that have been affected by radiation throughout development are more affected by the inability to intercept light with a larger leaf area than having photosynthetic systems compromised.  Damage from UVB radiation is more likely to be significant on species interactions than on plants themselves. Another significant impact of ozone depletion on plant life is the stress experienced by plants when exposed to UV radiation. This can cause a decrease in plant growth and an increase in oxidative stress, due to the production of nitric oxide and hydrogen peroxide.  In areas where substantial ozone depletion has occurred, increased UV-B radiation reduces terrestrial plant productivity (and likewise carbon sequestration) by about 6%. Moreover, if plants are exposed to high levels of UV radiation, it can elicit the production of harmful volatile organic compounds, like isoprenes. The emission of isoprenes into the air, by plants, can severely impact the environment by adding to air pollution and increasing the amount of carbon in the atmosphere, ultimately contributing to climate change."|2023-09-27-07-29-43
Ozone depletion|Public policy|" The full extent of the damage that CFCs have caused to the ozone layer is not known and will not be known for decades; however, marked decreases in column ozone have already been observed. The Montreal and Vienna conventions were installed long before a scientific consensus was established or important uncertainties in the science field were being resolved.  The ozone case was understood comparably well by lay persons as e.g. Ozone shield or ozone hole were useful ""easy-to-understand bridging metaphors"".  Americans voluntarily switched away from aerosol sprays, resulting in a 50 percent sales loss even before legislation was enforced. After a 1976 report by the United States National Academy of Sciences concluded that credible scientific evidence supported the ozone depletion hypothesis  a few countries, including the United States, Canada, Sweden, Denmark, and Norway, moved to eliminate the use of CFCs in aerosol spray cans.  At the time this was widely regarded as a first step towards a more comprehensive regulation policy, but progress in this direction slowed in subsequent years, due to a combination of political factors (continued resistance from the halocarbon industry and a general change in attitude towards environmental regulation during the first two years of the Reagan administration) and scientific developments (subsequent National Academy assessments that indicated that the first estimates of the magnitude of ozone depletion had been overly large).
A critical DuPont manufacturing patent for Freon was set to expire in 1979. The United States banned the use of CFCs in aerosol cans in 1978.  The European Community rejected proposals to ban CFCs in aerosol sprays, and in the U.S., CFCs continued to be used as refrigerants and for cleaning circuit boards. Worldwide CFC production fell sharply after the U.S. aerosol ban, but by 1986 had returned nearly to its 1976 level.  In 1993, DuPont Canada closed its CFC facility. The U.S. government's attitude began to change again in 1983, when William Ruckelshaus replaced Anne M. Burford as Administrator of the United States Environmental Protection Agency. Under Ruckelshaus and his successor, Lee Thomas, the EPA pushed for an international approach to halocarbon regulations. In 1985 twenty nations, including most of the major CFC producers, signed the Vienna Convention for the Protection of the Ozone Layer, which established a framework for negotiating international regulations on ozone-depleting substances. That same year, the discovery of the Antarctic ozone hole was announced, causing a revival in public attention to the issue. In 1987, representatives from 43 nations signed the Montreal Protocol. Meanwhile, the halocarbon industry shifted its position and started supporting a protocol to limit CFC production. However, this shift was uneven with DuPont acting more quickly than its European counterparts. DuPont may have feared court action related to increased skin cancer, especially as the EPA had published a study in 1986 claiming that an additional 40 million cases and 800,000 cancer deaths were to be expected in the U.S. in the next 88 years.  The EU shifted its position as well after Germany gave up its defence of the CFC industry and started supporting moves towards regulation. Government and industry in France and the UK tried to defend their CFC producing industries even after the Montreal Protocol had been signed. At Montreal, the participants agreed to freeze production of CFCs at 1986 levels and to reduce production by 50 percent by 1999.  After a series of scientific expeditions to the Antarctic produced convincing evidence that the ozone hole was indeed caused by chlorine and bromine from manmade organohalogens, the Montreal Protocol was strengthened at a 1990 meeting in London. The participants agreed to phase out CFCs and halons entirely (aside from a very small amount marked for certain ""essential"" uses, such as asthma inhalers) by 2000 in non-Article 5 countries and by 2010 in Article 5 (less developed) signatories.  At a 1992 meeting in Copenhagen, the phase-out date was moved up to 1996.  At the same meeting, methyl bromide (MeBr), a fumigant used primarily in agricultural production, was added to the list of controlled substances. For all substances controlled under the protocol, phaseout schedules were delayed for less developed ('Article 5(1)') countries, and phaseout in these countries was supported by transfers of expertise, technology, and money from non-Article 5(1) Parties to the Protocol. Additionally, exemptions from the agreed schedules could be applied for under the Essential Use Exemption (EUE) process for substances other than methyl bromide and under the Critical Use Exemption (CUE) process for methyl bromide. Civil society including especially NGOs, played critical roles at all stages of policy development leading up to the Vienna Conference, the Montreal Protocol, and in assessing compliance afterwards.     The major companies claimed that no alternatives to HFC existed.   An ozone-safe hydrocarbon refrigerant was developed at a Hamburg technological institute in Germany, consisting of a mixture of the hydrocarbon gases propane and butane, and in 1992 came to the attention of the non-governmental organization (NGO) Greenpeace.  Greenpeace called it ""Greenfreeze"".    The NGO then worked successfully first with a small and struggling company to market an appliance beginning in Europe, then Asia and later Latin America, receiving a 1997 UNEP award.   By 1995, Germany had already made CFC refrigerators illegal.   Since 2004, corporations like Coca-Cola, Carlsberg, and IKEA have been forming a coalition to promote the ozone-safe Greenfreeze units. Production spread to companies like Electrolux, Bosch, and LG, with sales reaching some 300 million refrigerators by 2008.    In Latin America, a domestic Argentinian company began Greenfreeze production in 2003, while the giant Bosch in Brazil began a year later.   By 2013 it was being used by some 700 million refrigerators, making up about 40 percent of the market.   In the U.S., however, change has been much slower.  To some extent, CFCs were being replaced by the less damaging hydrochlorofluorocarbons (HCFCs), although concerns remain regarding HCFCs also. In some applications, hydrofluorocarbons (HFCs) were being used to replace CFCs. HFCs, which contain no chlorine or bromine, do not contribute at all to ozone depletion although they are potent greenhouse gases. The best known of these compounds is probably HFC-134a (R-134a), which in the United States has largely replaced CFC-12 (R-12) in automobile air conditioners. In laboratory analytics (a former ""essential"" use) the ozone depleting substances can be replaced with various other solvents.   Chemical companies like Du Pont, whose representatives even disparaged Greenfreeze as ""that German technology,"" maneuvered the EPA to block the technology in the U.S. until 2011.      Ben & Jerry's of Unilever and General Electric, spurred by Greenpeace, had expressed formal interest in 2008 which figured in the EPA's final approval. The EU recast its Ozone Regulation in 2009. The law bans ozone-depleting substances with the goal of protecting the ozone layer.  The list of ODS that are subject to the regulation is the same as those under the Montreal Protocol, with some additions. More recently, policy experts have advocated for efforts to link ozone protection efforts to climate protection efforts.   Many ODS are also greenhouse gases, some thousands of times more powerful agents of radiative forcing than carbon dioxide over the short and medium term. Thus policies protecting the ozone layer have had benefits in mitigating climate change. In fact, the reduction of the radiative forcing due to ODS probably masked the true level of climate change effects of other greenhouse gases, and was responsible for the ""slow down"" of global warming from the mid-90s. [additional citation(s) needed] Policy decisions in one arena affect the costs and effectiveness of environmental improvements in the other."|2023-09-27-07-29-43
Ozone depletion|ODS requirements in the marine industry| The IMO has amended MARPOL Annex VI Regulation 12 regarding ozone depleting substances. As from July 1, 2010, all vessels where MARPOL Annex VI is applicable should have a list of equipment using ozone depleting substances. The list should include name of ODS, type and location of equipment, quantity in kg and date. All changes since that date should be recorded in an ODS Record book on board recording all intended or unintended releases to the atmosphere. Furthermore, new ODS supply or landing to shore facilities should be recorded as well.|2023-09-27-07-29-43
Ozone depletion|Prospects of ozone depletion|" Since the adoption and strengthening of the Montreal Protocol has led to reductions in the emissions of CFCs, atmospheric concentrations of the most-significant compounds have been declining. These substances are being gradually removed from the atmosphere; since peaking in 1994, the Effective Equivalent Chlorine (EECl) level in the atmosphere had dropped about 10 percent by 2008. The decrease in ozone-depleting chemicals has also been significantly affected by a decrease in bromine-containing chemicals. The data suggest that substantial natural sources exist for atmospheric methyl bromide (CH3Br).  The phase-out of CFCs means that nitrous oxide (N2O), which is not covered by the Montreal Protocol, has become the most highly emitted ozone-depleting substance and is expected to remain so throughout the 21st century. According to the IPCC Sixth Assessment Report, global stratospheric ozone levels experienced rapid decline in the 1970s and 1980s and have since been increasing, but have not reached preindustrial levels. Although considerable variability is expected from year to year, including in polar regions where depletion is largest, the ozone layer is expected to continue recovering in coming decades due to declining ozone-depleting substance concentrations, assuming full compliance with the Montreal Protocol. The Antarctic ozone hole is expected to continue for decades. Ozone concentrations in the lower stratosphere over Antarctica increased by 5–10 percent by 2020 and will return to pre-1980 levels by about 2060–2075. This is 10–25 years later than predicted in earlier assessments, because of revised estimates of atmospheric concentrations of ozone-depleting substances, including a larger predicted future usage in developing countries. Another factor that may prolong ozone depletion is the drawdown of nitrogen oxides from above the stratosphere due to changing wind patterns.  A gradual trend toward ""healing"" was reported in 2016.  In 2019, the ozone hole was at its smallest in the previous thirty years, due to the warmer polar stratosphere weakening the polar vortex."|2023-09-27-07-29-43
Ozone depletion|Research history|" The basic physical and chemical processes that lead to the formation of an ozone layer in the Earth's stratosphere were discovered by Sydney Chapman in 1930. Short-wavelength UV radiation splits an oxygen (O2) molecule into two oxygen (O) atoms, which then combine with other oxygen molecules to form ozone. Ozone is removed when an oxygen atom and an ozone molecule ""recombine"" to form two oxygen molecules, i.e. O + O3 → 2O2. In the 1950s, David Bates and Marcel Nicolet presented evidence that various free radicals, in particular hydroxyl (OH) and nitric oxide (NO), could catalyze this recombination reaction, reducing the overall amount of ozone. These free radicals were known to be present in the stratosphere, and so were regarded as part of the natural balance—it was estimated that in their absence, the ozone layer would be about twice as thick as it currently is. In 1970 Paul Crutzen pointed out that emissions of nitrous oxide (N2O), a stable, long-lived gas produced by soil bacteria, from the Earth's surface could affect the amount of nitric oxide (NO) in the stratosphere. Crutzen showed that nitrous oxide lives long enough to reach the stratosphere, where it is converted into NO. Crutzen then noted that increasing use of fertilizers might have led to an increase in nitrous oxide emissions over the natural background, which would in turn result in an increase in the amount of NO in the stratosphere. Thus human activity could affect the stratospheric ozone layer. In the following year, Crutzen and (independently) Harold Johnston suggested that NO emissions from supersonic passenger aircraft, which would fly in the lower stratosphere, could also deplete the ozone layer. However, more recent analysis in 1995 by David W. Fahey, an atmospheric scientist at the National Oceanic and Atmospheric Administration, found that the drop in ozone would be from 1–2 percent if a fleet of 500 supersonic passenger aircraft were operated.  This, Fahey expressed, would not be a showstopper for advanced supersonic passenger aircraft development."|2023-09-27-07-29-43
Ozone depletion|Rowland–Molina hypothesis|" In 1974 Frank Sherwood Rowland, Chemistry Professor at the University of California at Irvine, and his postdoctoral associate Mario J. Molina suggested that long-lived organic halogen compounds, such as CFCs, might behave in a similar fashion as Crutzen had proposed for nitrous oxide. James Lovelock had recently discovered, during a cruise in the South Atlantic in 1971, that almost all of the CFC compounds manufactured since their invention in 1930 were still present in the atmosphere. Molina and Rowland concluded that, like N2O, the CFCs would reach the stratosphere where they would be dissociated by UV light, releasing chlorine atoms. A year earlier, Richard Stolarski and Ralph Cicerone at the University of Michigan had shown that Cl is even more efficient than NO at catalyzing the destruction of ozone. Similar conclusions were reached by Michael McElroy and Steven Wofsy at Harvard University. Neither group, however, had realized that CFCs were a potentially large source of stratospheric chlorine—instead, they had been investigating the possible effects of HCl emissions from the Space Shuttle, which are very much smaller. The Rowland–Molina hypothesis was strongly disputed by representatives of the aerosol and halocarbon industries. The Chair of the Board of DuPont was quoted as saying that ozone depletion theory is ""a science fiction tale … a load of rubbish … utter nonsense"".  Robert Abplanalp, the President of Precision Valve Corporation (and inventor of the first practical aerosol spray can valve), wrote to the Chancellor of UC Irvine to complain about Rowland's public statements.  Nevertheless, within three years most of the basic assumptions made by Rowland and Molina were confirmed by laboratory measurements and by direct observation in the stratosphere. The concentrations of the source gases (CFCs and related compounds) and the chlorine reservoir species (HCl and ClONO2) were measured throughout the stratosphere, and demonstrated that CFCs were indeed the major source of stratospheric chlorine, and that nearly all of the CFCs emitted would eventually reach the stratosphere. Even more convincing was the measurement, by James G. Anderson and collaborators, of chlorine monoxide (ClO) in the stratosphere. ClO is produced by the reaction of Cl with ozone—its observation thus demonstrated that Cl radicals not only were present in the stratosphere but also were actually involved in destroying ozone. McElroy and Wofsy extended the work of Rowland and Molina by showing that bromine atoms were even more effective catalysts for ozone loss than chlorine atoms and argued that the brominated organic compounds known as halons, widely used in fire extinguishers, were a potentially large source of stratospheric bromine. In 1976 the United States National Academy of Sciences released a report concluding that the ozone depletion hypothesis was strongly supported by the scientific evidence.  In response the United States, Canada and Norway banned the use of CFCs in aerosol spray cans in 1978. Early estimates were that, if CFC production continued at 1977 levels, the total atmospheric ozone would after a century or so reach a steady state, 15 to 18 percent below normal levels. By 1984, when better evidence on the speed of critical reactions was available, this estimate was changed to 5 to 9 percent steady-state depletion. Crutzen, Molina, and Rowland were awarded the 1995 Nobel Prize in Chemistry for their work on stratospheric ozone."|2023-09-27-07-29-43
Ozone depletion|Antarctic ozone hole|" The discovery of the Antarctic ""ozone hole"" by British Antarctic Survey scientists Farman, Gardiner and Shanklin (first reported in a paper in Nature in May 1985 ) came as a shock to the scientific community, because the observed decline in polar ozone was far larger than anyone had anticipated.  Satellite measurements (TOMS onboard Nimbus 7) showing massive depletion of ozone around the south pole were becoming available at the same time.  However, these were initially rejected as unreasonable by data quality control algorithms (they were filtered out as errors since the values were unexpectedly low); the ozone hole was detected only in satellite data when the raw data was reprocessed following evidence of ozone depletion in in situ observations.  When the software was rerun without the flags, the ozone hole was seen as far back as 1976. Susan Solomon, an atmospheric chemist at the National Oceanic and Atmospheric Administration (NOAA), proposed that chemical reactions on polar stratospheric clouds (PSCs) in the cold Antarctic stratosphere caused a massive, though localized and seasonal, increase in the amount of chlorine present in active, ozone-destroying forms. The polar stratospheric clouds in Antarctica are only formed when there are very low temperatures, as low as −80 °C, and early spring conditions. In such conditions the ice crystals of the cloud provide a suitable surface for conversion of unreactive chlorine compounds into reactive chlorine compounds, which can deplete ozone easily. Moreover, the polar vortex formed over Antarctica is very tight and the reaction occurring on the surface of the cloud crystals is far different from when it occurs in atmosphere. These conditions have led to ozone hole formation in Antarctica. This hypothesis was decisively confirmed, first by laboratory measurements and subsequently by direct measurements, from the ground and from high-altitude airplanes, of very high concentrations of chlorine monoxide (ClO) in the Antarctic stratosphere. Alternative hypotheses, which had attributed the ozone hole to variations in solar UV radiation or to changes in atmospheric circulation patterns, were also tested and shown to be untenable. Meanwhile, analysis of ozone measurements from the worldwide network of ground-based Dobson spectrophotometers led an international panel to conclude that the ozone layer was in fact being depleted, at all latitudes outside of the tropics.  These trends were confirmed by satellite measurements. As a consequence, the major halocarbon-producing nations agreed to phase out production of CFCs, halons, and related compounds, a process that was completed in 1996. Since 1981 the United Nations Environment Programme, under the auspices of the World Meteorological Organization, has sponsored a series of technical reports on the Scientific Assessment of Ozone Depletion, based on satellite measurements. The 2007 report showed that the hole in the ozone layer was recovering and the smallest it had been for about a decade. 
The 2010 report found, ""Over the past decade, global ozone and ozone in the Arctic and Antarctic regions is no longer decreasing but is not yet increasing. The ozone layer outside the Polar regions is projected to recover to its pre-1980 levels some time before the middle of this century. In contrast, the springtime ozone hole over the Antarctic is expected to recover much later."" 
In 2012, NOAA and NASA reported ""Warmer air temperatures high above the Antarctic led to the second smallest season ozone hole in 20 years averaging 17.9 million square kilometres. The hole reached its maximum size for the season on Sept 22, stretching to 21.2 million square kilometres.""  A gradual trend toward ""healing"" was reported in 2016  and then in 2017.  It is reported that the recovery signal is evident even in the ozone loss saturation altitudes. The hole in the Earth's ozone layer over the South Pole has affected atmospheric circulation in the Southern Hemisphere all the way to the equator.  The ozone hole has influenced atmospheric circulation all the way to the tropics and increased rainfall at low, subtropical latitudes in the Southern Hemisphere."|2023-09-27-07-29-43
Ozone depletion|"Arctic ozone ""mini-hole"""|" On March 3, 2005, the journal Nature  published an article linking 2004's unusually large Arctic ozone hole to solar wind activity. On March 15, 2011, a record ozone layer loss was observed, with about half of the ozone present over the Arctic having been destroyed.    The change was attributed to increasingly cold winters in the Arctic stratosphere at an altitude of approximately 20 km (12 mi), a change associated with global warming in a relationship that is still under investigation.  By March 25, the ozone loss had become the largest compared to that observed in all previous winters with the possibility that it would become an ozone hole.  This would require that the quantities of ozone to fall below 200 Dobson units, from the 250 recorded over central Siberia.  It is predicted that the thinning layer would affect parts of Scandinavia and Eastern Europe on March 30–31. On October 2, 2011, a study was published in the journal Nature, which said that between December 2010 and March 2011 up to 80 percent of the ozone in the atmosphere at about 20 kilometres (12 mi) above the surface was destroyed.  The level of ozone depletion was severe enough that scientists said it could be compared to the ozone hole that forms over Antarctica every winter.  According to the study, ""for the first time, sufficient loss occurred to reasonably be described as an Arctic ozone hole.""  The study analyzed data from the Aura and CALIPSO satellites, and determined that the larger-than-normal ozone loss was due to an unusually long period of cold weather in the Arctic, some 30 days more than typical, which allowed for more ozone-destroying chlorine compounds to be created.  According to Lamont Poole, a co-author of the study, cloud and aerosol particles on which the chlorine compounds are found ""were abundant in the Arctic until mid March 2011—much later than usual—with average amounts at some altitudes similar to those observed in the Antarctic, and dramatically larger than the near-zero values seen in March in most Arctic winters"". In 2013, researchers analyzed the data and found the 2010–11 Arctic event did not reach the ozone depletion levels to classify as a true hole. A hole in the ozone is generally classified as 220 Dobson units or lower;  the Arctic hole did not approach that low level.   It has since been classified as a ""mini-hole."" Following the ozone depletion in 1997 and 2011, a 90% drop in ozone was measured by weather balloons over the Arctic in March 2020, as they normally recorded 3.5 parts per million of ozone, compared to only around 0.3 parts per million lastly, due to cold temperatures ever recorded since 1979, and a strong polar vortex which allowed chemicals, including chlorine and bromine, to gnaw away. A rare hole, the result of unusually low temperatures in the atmosphere above the North Pole, was studied in 2020."|2023-09-27-07-29-43
Ozone depletion|Tibet ozone hole| As winters that are colder are more affected, at times there is an ozone hole over Tibet. In 2006, a 2.5 million square kilometer ozone hole was detected over Tibet.  Also again in 2011 an ozone hole appeared over mountainous regions of Tibet, Xinjiang, Qinghai and the Hindu Kush, along with an unprecedented hole over the Arctic, though the Tibet one is far less intense than the ones over the Arctic or Antarctic.|2023-09-27-07-29-43
Ozone depletion|Potential depletion by storm clouds| Research in 2012 showed that the same process that produces the ozone hole over Antarctica occurs over summer storm clouds in the United States, and thus may be destroying ozone there as well.|2023-09-27-07-29-43
Ozone depletion|Ozone hole over tropics|" Physicist Qing-Bin Lu, of the University of Waterloo, claimed to have discovered a large, all-season ozone hole in the lower stratosphere over the tropics in July 2022.  However, other researchers in the field refuted this claim, stating that the research was riddled with ""serious errors and unsubstantiated assertions.""  According to Dr Paul Young, a lead author of the 2022 WMO/UNEP Scientific Assessment of Ozone Depletion, ""The author's identification of a ‘tropical ozone hole’ is down to him looking at percentage changes in ozone, rather than absolute changes, with the latter being much more relevant for damaging UV reaching the surface."" Specifically, Lu's work defines ""ozone hole"" as ""an area with O3 loss in percent larger than 25%, with respect to the undisturbed O3 value when there were no significant CFCs in the stratosphere (~ in the 1960s)""  instead of the general definition of 220 Dobson units or lower. Dr Marta Abalos Alvarez has added ""Ozone depletion in the tropics is nothing new and is mainly due to the acceleration of the Brewer-Dobson circulation."""|2023-09-27-07-29-43
Ozone depletion|Ozone depletion and global warming| Among others, Robert Watson had a role in the science assessment and in the regulation efforts of ozone depletion and global warming.  Prior to the 1980s, the EU, NASA, NAS, UNEP, WMO and the British government had dissenting scientific reports and Watson played a role in the process of unified assessments. Based on the experience with the ozone case, the IPCC started to work on a unified reporting and science assessment   to reach a consensus to provide the IPCC Summary for Policymakers. There are various areas of linkage between ozone depletion and global warming science: In 2019, NASA reported that there was no significant relation between size of the ozone hole and the climate change.|2023-09-27-07-29-43
Ozone depletion|CFC weight| Since CFC molecules are heavier than air (nitrogen or oxygen), it is commonly believed that the CFC molecules cannot reach the stratosphere in significant amounts.  However, atmospheric gases are not sorted by weight; the forces of wind can fully mix the gases in the atmosphere. Lighter CFCs are evenly distributed throughout the turbosphere and reach the upper atmosphere,  although some of the heavier CFCs are not evenly distributed.|2023-09-27-07-29-43
Ozone depletion|Percentage of man-made chlorine|" Another misconception is that ""it is generally accepted that natural sources of tropospheric chlorine are four to five times larger than man-made ones."" While this statement is strictly true, tropospheric chlorine is irrelevant; it is stratospheric chlorine that affects ozone depletion. Chlorine from ocean spray is soluble and thus is washed by rainfall before it reaches the stratosphere. CFCs, in contrast, are insoluble and long-lived, allowing them to reach the stratosphere. In the lower atmosphere, there is much more chlorine from CFCs and related haloalkanes than there is in HCl from salt spray, and in the stratosphere halocarbons are dominant.  Only methyl chloride, which is one of these halocarbons, has a mainly natural source,  and it is responsible for about 20 percent of the chlorine in the stratosphere; the remaining 80 percent comes from manmade sources. Very violent volcanic eruptions can inject HCl into the stratosphere, but researchers  have shown that the contribution is not significant compared to that from CFCs.
A similar erroneous assertion is that soluble halogen compounds from the volcanic plume of Mount Erebus on Ross Island, Antarctica are a major contributor to the Antarctic ozone hole. Nevertheless, a 2015 study  showed that the role of Mount Erebus volcano in the Antarctic ozone depletion was probably underestimated. Based on the NCEP/NCAR reanalysis data over the last 35 years and by using the NOAA HYSPLIT trajectory model, researchers showed that Erebus volcano gas emissions (including hydrogen chloride (HCl)) can reach the Antarctic stratosphere via high-latitude cyclones and then the polar vortex. Depending on Erebus volcano activity, the additional annual HCl mass entering the stratosphere from Erebus varies from 1.0 to 14.3 kt."|2023-09-27-07-29-43
Ozone depletion|First observation| G.M.B. Dobson mentioned that when springtime ozone levels in the Antarctic over Halley Bay were first measured in 1956, he was surprised to find that they were ~320 DU, or about 150 DU below spring Arctic levels of ~450 DU. These were at that time the only known Antarctic ozone values available. What Dobson describes is essentially the baseline from which the ozone hole is measured: actual ozone hole values are in the 150–100 DU range. The discrepancy between the Arctic and Antarctic noted by Dobson was primarily a matter of timing: during the Arctic spring ozone levels rose smoothly, peaking in April, whereas in the Antarctic they stayed approximately constant during early spring, rising abruptly in November when the polar vortex broke down. The behavior seen in the Antarctic ozone hole is completely different. Instead of staying constant, early springtime ozone levels suddenly drop from their already low winter values, by as much as 50 percent, and normal values are not reached again until December.|2023-09-27-07-29-43
Ozone depletion|Location of hole|" Some people thought that the ozone hole should be above the sources of CFCs. However, CFCs are well mixed globally in the troposphere and stratosphere. The reason for occurrence of the ozone hole above Antarctica is not because there are more CFCs concentrated but because the low temperatures help form polar stratospheric clouds.  In fact, there are findings of significant and localized ""ozone holes"" above other parts of the earth, like above Central Asia."|2023-09-27-07-29-43
Ozone depletion|World Ozone Day|" In 1994, the United Nations General Assembly voted to designate September 16 as the International Day for the Preservation of the Ozone Layer, or ""World Ozone Day"".  The designation commemorates the signing of the Montreal Protocol  on that date in 1987."|2023-09-27-07-29-43
Particulate pollution|General| Particulate pollution is pollution of an environment that consists of particles suspended in some medium. There are three primary forms: atmospheric particulate matter,  marine debris,  and space debris.  Some particles are released directly from a specific source, while others form in chemical reactions in the atmosphere. Particulate pollution can be derived from either natural sources or anthropogenic processes.|2023-08-15-23-16-15
Particulate pollution|Atmospheric particulate matter| Atmospheric particulate matter, also known as particulate matter, or PM, describes solids and/or liquid particles suspended in a gas, most commonly the Earth's atmosphere.  Particles in the atmosphere can be divided into two types, depending on the way they are emitted. Primary particles, such as mineral dust, are emitted into the atmosphere.  Secondary particles, such as ammonium nitrate, are formed in the atmosphere through gas-to-particle conversion.|2023-08-15-23-16-15
Particulate pollution|Sources| Some particulates occur naturally, originating from volcanoes, dust storms, forest and grassland fires, living vegetation and sea spray. Human activities, such as the burning of fossil fuels in vehicles,  wood burning,      stubble burning, power plants, road dust, wet cooling towers in cooling systems and various industrial processes, also generate significant amounts of particulates. Coal combustion in developing countries is the primary method for heating homes and supplying energy. Because salt spray over the oceans is the overwhelmingly most common form of particulate in the atmosphere, anthropogenic aerosols—those made by human activities—currently account for about 10 percent of the total mass of aerosols in our atmosphere. Domestic combustion pollution is mainly composed of burning fuel including wood, gas, and charcoal in activities of heating, cooking, agriculture, and wildfires.   Major domestic pollutants contain 17% of carbon dioxide, 13% of carbon monoxide, 6% of nitrogen monoxide, polycyclic aromatic hydrocarbons, and fine and ultrafine particles. In the United Kingdom domestic combustion is the largest single source of PM2.5 annually.   In some towns and cities in New South Wales wood smoke may be responsible for 60% of fine particle air pollution in the winter.  Research conducted about biomass burning in 2015, estimated that 38% of European total particulate pollution emissions are composed of domestic wood burning. The particulate pollutant is often in microscopic size that enables it to infiltrate into interior space even if windows and doors are closed.[citation needed] The main component of woodsmoke, black carbon significantly appears in the indoor environment compared to other ambient pollutants.[citation needed] If the room is sealed tight enough to prevent woodsmoke transmission, it will also prevent oxygen exchange from indoors to outdoor.[citation needed] The regular dusk mask also can help little with particulate pollutants since they are designed to filter out larger particles.  Musk with HEPA filter can filter out microscopic pollutants but cause difficulty of breathing to the population with lung disease. Living under high concentrations of pollutants can lead to headaches, fatigue, lung disease, asthma, and throat and eye irritation.  One of the most common diseases among those living among pollutants is chronic obstructive pulmonary disease (COPD).  Exposure to wood and charcoal smoke is significantly associated with COPD diagnoses among those living in developing and developed countries.  Exposure to woodsmoke intensifies the respiratory systems and increases the risk of hospital admissions.|2023-08-15-23-16-15
Particulate pollution|Marine debris| Marine debris and marine aerosols refer to particulates suspended in a liquid, usually water on the Earth's surface. Particulates in water are a kind of water pollution measured as total suspended solids, a water quality measurement listed as a conventional pollutant in the U.S. Clean Water Act, a water quality law.  Notably, some of the same kinds of particles can be suspended both in air and water, and pollutants specifically may be carried in the air and deposited in water, or fall to the ground as acid rain.  The majority of marine aerosols are created through the bubble bursting of breaking waves and capillary action on the ocean surface due to the stress exerted from surface winds.  Among common marine aerosols, pure sea salt aerosols are the major component of marine aerosols with an annual global emission between 2,000-10,000 teragrams annually.  Through interactions with water, many marine aerosols help to scatter light, and aid in cloud condensation and ice nuclei (IN); thus, affecting the atmospheric radiation budget.  When they interact with anthropogenic pollution, marine aerosols can affect biogeochemical cycles through the depletion of acids such as nitric acid and halogens.|2023-08-15-23-16-15
Particulate pollution|Space debris|" Space debris describes particulates in the vacuum of outer space, specifically particles originating from human activity that remain in geocentric orbit around the Earth. The International Association of Astronauts define space debris as ""any man-made Earth orbiting object which is non-functional with no reasonable expectation of assuming or resuming its intended function or any other function for which it is or can be expected to be authorized, including fragments and parts thereof"". Space debris is classified by size and operational purpose, and divided into four main subsets: inactive payloads, operational debris, fragmentation debris and microparticulate matter.  Inactive payloads refer to any launched space objects that have lost the capability to reconnect to its corresponding space operator; thus, preventing a return to Earth.  In contrast, operational debris describes the matter associated with the propulsion of a larger entity into space, which may include upper rocket stages and ejected nose cones.  Fragmentation debris refers to any object in space that has become dissociated from a larger entity by means of explosion, collision or deterioration.  Microparticulate matter describes space matter that typically cannot be seen singly with the naked eye, including particles, gases, and spaceglow. In response to research that concluded that impacts from Earth orbital debris could lead to greater hazards to spacecraft than the natural meteoroid environment, NASA began the orbital debris program in 1979, initiated by the Space Sciences branch at Johnson Space Center (JSC).  Beginning with an initial budget of $70,000, the NASA orbital debris program began with the initial goals of characterizing hazards induced by space debris and creating mitigation standards that would minimize the growth of the orbital debris environment.  By 1990, the NASA orbital debris program created a debris monitoring program, which included mechanisms to sample the low Earth orbit (LEO) environment for debris as small as 6mm using the Haystack X-band ground radar."|2023-08-15-23-16-15
Particulate pollution|Epidemiology| Particulate pollution is observed around the globe in varying sizes and compositions and is the focus of many epidemiological studies.  Particulate matter (PM) is generally classified into two main size categories: PM10 and PM2.5. PM10, also known as coarse particulate matter, consists of particles 10 micrometers (μm) and smaller, while PM2.5, also called fine particulate matter, consists of particles 2.5 μm and smaller.  Particles 2.5 μm or smaller in size are especially notable as they can be inhaled into the lower respiratory system, and with enough exposure, absorbed into the bloodstream. Particulate pollution can occur directly or indirectly from a number of sources including, but not limited to: agriculture, automobiles, construction, forest fires, chemical pollutants, and power plants. Exposure to particulates of any size and composition may occur acutely over a short duration, or chronically over a long duration.  Particulate exposure has been associated with adverse respiratory symptoms ranging from irritation of the airways, aggravated asthma, coughing, and difficulty breathing from acute exposure to symptoms such as irregular heartbeat, lung cancer, kidney disease, chronic bronchitis, and premature death in individuals who suffer from pre-existing cardiovascular or lung diseases due to chronic exposure.  The severity of health effects generally depends upon the size of the particles as well as the health status of the individual exposed; older adults, children, pregnant women, and immunocompromised populations are at the greatest risk for adverse health outcomes.  Short-term exposure to particulate pollution has been linked to adverse health impacts. As a result, the US Environmental Protection Agency (EPA) and various health agencies around the world have established thresholds for concentrations of PM2.5 and PM10 that are determined to be acceptable. However, there is no known safe level of exposure and thus, any exposure to particulate pollution is likely to increase an individual's risk of adverse health effects.  In European countries, air quality at or above 10 micrograms per cubic meter of air (μg/m3) for PM2.5 increases the all-causes daily mortality rate by 0.2-0.6% and the cardiopulmonary mortality rate by 6-13%. Worldwide, PM10 concentrations of 70μg/m3 and PM2.5 concentrations of 35μg/m3 have been shown to increase long-term mortality by 15%.  More so, approximately 4.2 million of all premature deaths observed in 2016 occurred due to airborne particulate pollution, 91% of which occurred in countries with low to middle socioeconomic status. Of these premature deaths, 58% were attributed to strokes and ischaemic heart diseases, 8% attributed to COPD (Chronic Obstructive Pulmonary Disease), and 6% to lung cancer. In 2006, the EPA conducted air quality designations in all 50 states, denoting areas of high pollution based on criteria such as air quality monitoring data, recommendations submitted by the states, and other technical information; and reduced the National Ambient Air Quality Standard for daily exposure to particulates in the 2.5 micrometers and smaller category from 15μg/m3 to 12μg/m3 in 2012.  As a result, U.S. annual PM2.5 averages have decreased from 13.5 µg/m3 to 8.02 µg/m3, between 2000 and 2017.|2023-08-15-23-16-15
Particulates|General| Lists Categories Particulates or atmospheric particulate matter (see below for other names) are microscopic particles of solid or liquid matter suspended in the air. The term aerosol commonly refers to the particulate/air mixture, as opposed to the particulate matter alone.  Sources of particulate matter can be natural or anthropogenic.  They have impacts on climate and precipitation that adversely affect human health, in ways additional to direct inhalation. Types of atmospheric particles include suspended particulate matter; thoracic and respirable particles;  inhalable coarse particles, designated PM10, which are coarse particles with a diameter of 10 micrometers (μm) or less; fine particles, designated PM2.5, with a diameter of 2.5 μm or less;  ultrafine particles, with a diameter of 100 nm or less; and soot. The IARC and WHO designate airborne particulates as a Group 1 carcinogen.  Particulates are the most harmful form (other than ultra-fines) of air pollution  due to their ability to penetrate deep into the lungs and brain from blood streams, causing health problems such as heart disease, lung disease, and premature death.  In 2013, a study involving 312,944 people in nine European countries revealed that there was no safe level of particulates and that for every increase of 10 μg/m3 in PM10, the lung cancer rate rose 22% (95% CI [1.03–1.45]). The smaller PM2.5, which can penetrate deeper into the lungs, were associated with an 18% increase in lung cancer per 5 μg/m3; however, this study did not show statistical significance for this association (95% CI [0.96–1.46]).  Worldwide, exposure to PM2.5 contributed to 4.1 million deaths from heart disease, stroke, lung cancer, chronic lung disease, and respiratory infections in 2016.  Overall, ambient particulate matter ranks as the sixth leading risk factor for premature death globally.|2023-09-27-12-47-55
Particulates|Atmospheric sources| Some particulates occur naturally, originating from volcanoes, dust storms, forest and grassland fires, living vegetation and sea spray. Human activities also generate significant amounts of particulates. For example, In 2010 it was estimated that human-made (anthropogenic) aerosols account for about 10 percent of the total mass of aerosols in the atmosphere.|2023-09-27-12-47-55
Particulates|Domestic combustion and wood smoke| In the United Kingdom domestic combustion is the largest single source of PM2.5 and PM10 annually, with domestic wood burning in both closed stoves and open fires responsible for 38% of PM2.5 in 2019.    To tackle the problem some new laws were introduced since 2021. In some towns and cities in New South Wales wood smoke may be responsible for 60% of fine particle air pollution in the winter. There are a few ways to reduce wood smoke, e.g, buying the right wood heater and maintaining it well,  choosing the right firewood  and burning it the right way.  There are also regulations in some countries where people can report smoke pollution to the local council.|2023-09-27-12-47-55
Particulates|Composition|" The composition and toxicity of aerosols, including particles, depends on their source and atmospheric chemistry and varies widely.
Wind-blown mineral dust  tends to be made of mineral oxides and other material blown from the Earth's crust; this particulate is light-absorbing.  Sea salt  is considered the second-largest contributor in the global aerosol budget, and consists mainly of sodium chloride originated from sea spray; other constituents of atmospheric sea salt reflect the composition of sea water, and thus include magnesium, sulfate, calcium, potassium, and others. In addition, sea spray aerosols may contain organic compounds like fatty acids and sugars, which influence their chemistry. Some secondary particles derive from the oxidation of primary gases such as sulfur and nitrogen oxides into sulfuric acid (liquid) and nitric acid (gaseous) or from biogenic emissions. The precursors for these aerosols—i.e. the gases from which they originate—may have an anthropogenic origin (from biomass and fossil fuel combustion) as well as a natural biogenic origin. In the presence of ammonia, secondary aerosols often take the form of ammonium salts; i.e. ammonium sulfate and ammonium nitrate (both can be dry or in aqueous solution); in the absence of ammonia, secondary compounds take an acidic form as sulfuric acid (liquid aerosol droplets) and nitric acid (atmospheric gas), all of which probably contribute to the health effects of particulates. Secondary sulfate and nitrate aerosols are strong light-scatterers.  This is mainly because the presence of sulfate and nitrate causes the aerosols to increase to a size that scatters light effectively. Organic matter (OM) found in aerosols can be either primary or secondary, the latter part deriving from the oxidation of volatile organic compounds (VOCs); organic material in the atmosphere may either be biogenic or anthropogenic. Organic matter influences the atmospheric radiation field by both scattering and absorption. Some aerosols are predicted to include strongly light-absorbing material and are thought to yield large positive radiative forcing. Some secondary organic aerosols (SOAs) resulting from combustion products of internal combustion engines, have been identified as a danger to health.  Particulate toxicity has been found to vary by region and source contribution which affects the particles chemical composition. The chemical composition of the aerosol directly affects how it interacts with solar radiation. The chemical constituents within the aerosol change the overall refractive index. The refractive index will determine how much light is scattered and absorbed. The composition of particulate matter that generally causes visual effects, haze, consists of sulfur dioxide, nitrogen oxides, carbon monoxide, mineral dust, and organic matter. The particles are hygroscopic due to the presence of sulfur, and SO2 is converted to sulfate when high humidity and low temperatures are present. This causes reduced visibility and yellow color."|2023-09-27-12-47-55
Particulates|Size distribution| Human-produced aerosols such as particle pollution tend to have a smaller radius than aerosol particles of natural origin (such as windblown dust). The false-color maps in the map of distribution of aerosol particles on the right show where there are natural aerosols, human pollution, or a mixture of both, monthly. Among the most obvious patterns that the size distribution time series shows is that in the planet's most southerly latitudes, nearly all the aerosols are large, but in the high northern latitudes, smaller aerosols are very abundant. Most of the Southern Hemisphere is covered by the ocean, where the largest source of aerosols is natural sea salt from dried sea spray. Because the land is concentrated in the Northern Hemisphere, the amount of small aerosols from fires and human activities is greater there than in the Southern Hemisphere. Overland, patches of large-radius aerosols appear over deserts and arid regions, most prominently, the Sahara Desert in North Africa and the Arabian Peninsula, where dust storms are common. Places where human-triggered or natural fire activity is common (land-clearing fires in the Amazon from August–October, for example, or lightning-triggered fires in the forests of northern Canada in Northern Hemisphere summer) are dominated by smaller aerosols. Human-produced (fossil fuel) pollution is largely responsible for the areas of small aerosols over developed areas such as the eastern United States and Europe, especially in their summer. [better source needed] Satellite measurements of aerosols, called aerosol optical thickness, are based on the fact that the particles change the way the atmosphere reflects and absorbs visible and infrared light. As shown in this page, an optical thickness of less than 0.1 (palest yellow) indicates a crystal clear sky with maximum visibility, whereas a value of 1 (reddish-brown) indicates very hazy conditions.[better source needed]|2023-09-27-12-47-55
Particulates|Deposition processes|" In general, the smaller and lighter a particle is, the longer it will stay in the air. Larger particles (greater than 10 micrometers in diameter) tend to settle to the ground by gravity in a matter of hours whereas the smallest particles (less than 1 micrometer) can stay in the atmosphere for weeks and are mostly removed by precipitation. There are also evidence that it is not uncommon for aerosols to ""travel across the ocean"". For example, in September 2017 wildfires burning across the western United States and Canada, and the smoke was found to have arrived over the United Kingdom and northern France in three days, as shown by satellite images.  Diesel particulate matter is highest near the source of emission.  Any information regarding DPM and the atmosphere, flora, height, and distance from major sources is useful to determine health effects."|2023-09-27-12-47-55
Particulates|Controlling technologies and measures|" Particulate matter emissions are highly regulated in most industrialized countries. Due to environmental concerns, most industries are required to operate some kind of dust collection system.  These systems include inertial collectors (cyclonic separators), fabric filter collectors (baghouses), electrostatic filters used in facemasks,  wet scrubbers, and electrostatic precipitators. Cyclonic separators are useful for removing large, coarse particles and are often employed as a first step or ""pre-cleaner"" to other more efficient collectors. Well-designed cyclonic separators can be very efficient in removing even fine particulates,  and may be operated continuously without requiring frequent shutdowns for maintenance.[citation needed] Fabric filters or baghouses are the most commonly employed in general industry.  They work by forcing dust-laden air through a bag-shaped fabric filter leaving the particulate to collect on the outer surface of the bag and allowing the now clean air to pass through to either be exhausted into the atmosphere or in some cases recirculated into the facility. Common fabrics include polyester and fiberglass and common fabric coatings include PTFE (commonly known as Teflon). The excess dust buildup is then cleaned from the bags and removed from the collector. Wet scrubbers pass the dirty air through a scrubbing solution (usually a mixture of water and other compounds) allowing the particulate to attach to the liquid molecules.  Electrostatic precipitators electrically charge the dirty air as it passes through. The now charged air then passes through large electrostatic plates which attract the charged particle in the airstream collecting them and leaving the now clean air to be exhausted or recirculated. For general building construction, some places that have acknowledged the possible health risks of construction dust for decades legally require the relevant contractor to adopt effective dust control measures, although inspections, fines and imprisonments are rare in recent years (for example, two prosecutions with a total fines of HKD$6000 in Hong Kong in the year 2021). Some of the mandatory dust control measures include     load, unload, handle, transfer, store or dispose of cement or dry pulverized fuel ash in a completely enclosed system or facility, and fit any vent or exhaust with an effective fabric filter or equivalent air pollution control system or equipment, enclose the scaffolding of the building with dust screens, use impervious sheeting to enclose both material hoist and debris chute, wet debris with water before it is dumped into a debris chute, have water sprayed on the facade surface before and during grinding work, use grinder equipped with vacuum cleaner for facade grinding work, spray water continuously on the surface for any pneumatic or power-driven drilling, cutting, polishing or other mechanical breaking operation that causes dust emission, unless there is the operation of an effective dust extraction and filtering device, provide hoarding of not less than 2.4 m in height along the whole length of the site boundary, have hard paving on open area and wash every vehicle that leaves the construction sites. Use of automatic sprinkler equipment, automatic carwash equipment and installation of video surveillance system for the pollution control facilities and retain the videos for one month for future inspections. Besides removing particulates from the source of pollution, it may also be cleaned in the open air (e.g. smog tower, moss wall
  and anti-smog gun ), while other control measures employ the use of barriers."|2023-09-27-12-47-55
Particulates|Measurement| Particulates have been measured in increasingly sophisticated ways since air pollution was first systematically studied in the early 20th century. The earliest methods included relatively crude Ringelmann charts, which were grey-shaded cards against which emissions from smokestacks could be visually compared, and deposit gauges, which collected the soot deposited in a particular location so it could be weighed. Automated, modern methods of measuring particulates include optical photodetectors, tapered element oscillating microbalances, and Aethalometers.  Besides measuring the total mass of particles per unit volume of air (particle mass concentration), sometimes it is more useful to measure the total number of particles per unit volume of air (particle number concentration). This can be done by using a condensation particle counter (CPC). To measure the atomic composition of particulate samples, techniques such as X-ray spectrometry can be used.|2023-09-27-12-47-55
Particulates|Climate effects| Atmospheric aerosols affect the climate of the earth by changing the amount of incoming solar radiation and outgoing terrestrial longwave radiation retained in the earth's system. This occurs through several distinct mechanisms which are split into direct, indirect   and semi-direct aerosol effects. The aerosol climate effects are the biggest source of uncertainty in future climate predictions.  The Intergovernmental Panel on Climate Change (IPCC), Third Assessment Report, says: While the radiative forcing due to greenhouse gases may be determined to a reasonably high degree of accuracy... the uncertainties relating to aerosol radiative forcings remain large, and rely to a large extent on the estimates from global modeling studies that are difficult to verify at the present time.|2023-09-27-12-47-55
Particulates|Aerosol radiative| The direct aerosol effect consists of any direct interaction of radiation with atmospheric aerosols, such as absorption or scattering. It affects both short and longwave radiation to produce a net negative radiative forcing.  The magnitude of the resultant radiative forcing due to the direct effect of an aerosol is dependent on the albedo of the underlying surface, as this affects the net amount of radiation absorbed or scattered to space. For example, if a highly scattering aerosol is above a surface of low albedo it has a greater radiative forcing than if it was above a surface of high albedo. The converse is true of absorbing aerosol, with the greatest radiative forcing arising from a highly absorbing aerosol over a surface of high albedo.  The direct aerosol effect is a first-order effect and is therefore classified as a radiative forcing by the IPCC.  The interaction of an aerosol with radiation is quantified by the single-scattering albedo (SSA), the ratio of scattering alone to scattering plus absorption (extinction) of radiation by a particle. The SSA tends to unity if scattering dominates, with relatively little absorption, and decreases as absorption increases, becoming zero for infinite absorption. For example, the sea-salt aerosol has an SSA of 1, as a sea-salt particle only scatters, whereas soot has an SSA of 0.23, showing that it is a major atmospheric aerosol absorber.[citation needed] The Indirect aerosol effect consists of any change to the earth's radiative budget due to the modification of clouds by atmospheric aerosols and consists of several distinct effects. Cloud droplets form onto pre-existing aerosol particles, known as cloud condensation nuclei (CCN). Droplets condensing around human-produced aerosols such as found in particulate pollution tend to be smaller and more numerous than those forming around aerosol particles of natural origin (such as windblown dust). For any given meteorological conditions, an increase in CCN leads to an increase in the number of cloud droplets. This leads to more scattering of shortwave radiation i.e. an increase in the albedo of the cloud, known as the cloud albedo effect, First indirect effect or Twomey effect.  Evidence supporting the cloud albedo effect has been observed from the effects of ship exhaust plumes  and biomass burning  on cloud albedo compared to ambient clouds. The Cloud albedo aerosol effect is a first order effect and therefore classified as a radiative forcing by the IPCC. An increase in cloud droplet number due to the introduction of aerosol acts to reduce the cloud droplet size, as the same amount of water is divided into more droplets. This has the effect of suppressing precipitation, increasing the cloud lifetime, known as the cloud lifetime aerosol effect, second indirect effect or Albrecht effect.  This has been observed as the suppression of drizzle in ship exhaust plume compared to ambient clouds,  and inhibited precipitation in biomass burning plumes.  This cloud lifetime effect is classified as a climate feedback (rather than a radiative forcing) by the IPCC due to the interdependence between it and the hydrological cycle.  However, it has previously been classified as a negative radiative forcing. The Semi-direct effect concerns any radiative effect caused by absorbing atmospheric aerosol such as soot, apart from direct scattering and absorption, which is classified as the direct effect. It encompasses many individual mechanisms, and in general is more poorly defined and understood than the direct and indirect aerosol effects. For instance, if absorbing aerosols are present in a layer aloft in the atmosphere, they can heat surrounding air which inhibits the condensation of water vapour, resulting in less cloud formation.  Additionally, heating a layer of the atmosphere relative to the surface results in a more stable atmosphere due to the inhibition of atmospheric convection. This inhibits the convective uplift of moisture,  which in turn reduces cloud formation. The heating of the atmosphere aloft also leads to a cooling of the surface, resulting in less evaporation of surface water. The effects described here all lead to a reduction in cloud cover i.e. an increase in planetary albedo. The semi-direct effect classified as a climate feedback) by the IPCC due to the interdependence between it and the hydrological cycle.  However, it has previously been classified as a negative radiative forcing.|2023-09-27-12-47-55
Particulates|Specific aerosol roles|" Sulfate aerosols are mostly inorganic sulfur compounds like (SO42-),HSO4- and H2SO4-,  which are mainly produced when sulfur dioxide reacts with water vapor to form gaseous sulfuric acid and various salts (often through an oxidation reaction in the clouds), which are then thought to experience hygroscopic growth and coagulation and then shrink through evaporation.   Some of them are biogenic (typically produced via atmospheric chemical reactions with dimethyl sulfide from mostly marine plankton ) or geological via volcanoes or weather-driven from wildfires and other natural combustion events,  but in the recent decades anthropogenic sulfate aerosols produced through combustion of fossil fuels with a high sulfur content, primarily coal and certain less-refined fuels, like aviation and bunker fuel, had dominated.  By 1990, global human-caused emissions of sulfur into the atmosphere became ""at least as large"" as all natural emissions of sulfur-containing compounds combined, and were at least 10 times more numerous than the natural aerosols in the most polluted regions of Europe and North America,  where they accounted for 25% or more of all air pollution.  This led to acid rain,   and also contributed to heart and lung conditions   and even the risk of preterm birth and low birth weight.  Sulfate pollution also has a complex relationship with NOx pollution and ozone, reducing the also harmful ground-level ozone, yet capable of damaging the stratospheric ozone layer as well. Once the problem became clear, the efforts to remove this pollution through flue-gas desulfurization measures and other pollution controls were largely successful,   reducing their prevalence by 53% and causing healthcare savings valued at $50 billion annually in the United States alone.    Yet, around the same time, research had shown that sulfate aerosols were affecting both the visible light received by the Earth and its surface temperature,  and as the so-called global dimming) began to reverse in the 1990s in line with the reduced anthropogenic sulfate pollution,    climate change accelerated.  As of 2021, state-of-the-art CMIP6 models estimate that total cooling from the currently present aerosols is between 0.1 °C (0.18 °F) to 0.7 °C (1.3 °F);  the IPCC Sixth Assessment Report uses the best estimate of 0.5 °C (0.90 °F),  with the uncertainty mainly caused by contradictory research on the impacts of aerosols of clouds.       Some are certain that they cool the planet, though, and this led to solar geoengineering proposals known as stratospheric aerosol injection, which seeks to replicate and enhance the cooling from sulfate pollution while minimizing the negative effects on health through deploying in the stratosphere, where only a fraction of the current sulfur pollution would be needed to avoid multiple degrees of warming,  but the assessment of costs and benefits remains incomplete,  even with hundreds of studies into the subject completed by the early 2020s. Black carbon (BC), or carbon black, or elemental carbon (EC), often called soot, is composed of pure carbon clusters, skeleton balls and fullerenes, and is one of the most important absorbing aerosol species in the atmosphere. It should be distinguished from organic carbon (OC): clustered or aggregated organic molecules on their own or permeating an EC buckyball. Black carbon from fossil fuels is estimated by the IPCC in the Fourth Assessment Report of the IPCC, 4AR, to contribute a global mean radiative forcing of +0.2 W/m2 (was +0.1 W/m2 in the Second Assessment Report of the IPCC, SAR), with a range +0.1 to +0.4 W/m2. A study published in 2013 however, states that ""the best estimate for the industrial-era (1750 to 2005) direct radiative forcing of atmospheric black carbon is +0.71 W/m2 with 90% uncertainty bounds of (+0.08, +1.27) W/m2"" with ""total direct forcing by all-black carbon sources, without subtracting the preindustrial background, is estimated as +0.88 (+0.17, +1.48) W/m2""."|2023-09-27-12-47-55
Particulates|Instances| Volcanoes are a large natural source of aerosol and have been linked to changes in the earth's climate often with consequences for the human population. Eruptions linked to changes in climate include the 1600 eruption of Huaynaputina which was linked to the Russian famine of 1601–1603,    leading to the deaths of two million, and the 1991 eruption of Mount Pinatubo which caused a global cooling of approximately 0.5 °C lasting several years.   Research tracking the effect of light-scattering aerosols in the stratosphere during 2000 and 2010 and comparing its pattern to volcanic activity show a close correlation. Simulations of the effect of anthropogenic particles showed little influence at present levels. Aerosols are also thought to affect weather and climate on a regional scale. The failure of the Indian monsoon has been linked to the suppression of evaporation of water from the Indian Ocean due to the semi-direct effect of anthropogenic aerosol. Recent studies of the Sahel drought  and major increases since 1967 in rainfall in Australia over the Northern Territory, Kimberley, Pilbara and around the Nullarbor Plain have led some scientists to conclude that the aerosol haze over South and East Asia has been steadily shifting tropical rainfall in both hemispheres southward.|2023-09-27-12-47-55
Particulates|Size, shape, and solubility matter| Particle size is the main determinant of where in the respiratory tract it will come to rest when inhaled. Larger particles are generally filtered in the nose and throat via cilia and mucus, but particulate matter smaller than about 10 micrometers can settle in the bronchi and lungs and cause health problems. The 10-micrometer size does not represent a strict boundary between respirable and non-respirable particles but has been agreed upon for monitoring of airborne PM by most regulatory agencies. Because of their small size, particles on the order of 10 micrometers or less (coarse particulate matter, PM10) can penetrate the deepest part of the lungs such as the bronchioles or alveoli.  When asthmatics are exposed to these conditions it can trigger bronchoconstriction. Similarly, fine particulate matter (PM2.5) tends to penetrate into the gas exchange regions of the lung (alveoli), and very small particles (ultrafine particulate matter PM0.1) may pass through the lungs to affect other organs. Penetration of particles is not wholly dependent on their size; shape and chemical composition also play a part. To avoid this complication, simple nomenclature is used to indicate the different degrees of relative penetration of a PM particle into the cardiovascular system. Inhalable particles penetrate no further than the bronchi as they are filtered out by the cilia. Thoracic particles can penetrate right into terminal bronchioles whereas PM0.1, which can penetrate to alveoli, the gas exchange area, and hence the circulatory system, are termed respirable particles.[citation needed] In analogy, the inhalable dust fraction is the fraction of dust entering the nose and mouth which may be deposited anywhere in the respiratory tract. The thoracic fraction is the fraction that enters the thorax and is deposited within the lung's airways. The respirable fraction is what is deposited in the gas exchange regions (alveoli). The smallest particles, nanoparticles, which are less than 180 nanometers in size, may be even more damaging to the cardiovascular system.   Nanoparticles can pass through cell membranes and migrate into other organs, including the brain. Particles emitted from modern diesel engines (commonly referred to as Diesel Particulate Matter, or DPM) are typically in the size range of 100 nanometers (0.1 micrometers). These soot particles also carry carcinogens like benzopyrenes adsorbed on their surface. Particulate mass is not a proper measure of the health hazard. A particle of 10 μm diameter has approximately the same mass as 1 million particles of 100 nm diameter, but is much less hazardous, as it is unlikely to enter the alveoli. Legislative limits for engine emissions based on mass are therefore not protective. Proposals for new regulations exist in some countries,[which?] with suggestions to limit the particle surface area or the particle count (numerical quantity) / particle number concentration (PNC) instead. The site and extent of absorption of inhaled gases and vapors are determined by their solubility in water. Absorption is also dependent upon air flow rates and the partial pressure of the gases in the inspired air. The fate of a specific contaminant is dependent upon the form in which it exists (aerosol or particulate). Inhalation also depends upon the breathing rate of the subject. Another complexity not entirely documented is how the shape of PM can affect health, except for the needle-like shape of asbestos fibres which can lodge in the lungs. Geometrically angular shapes have more surface area than rounder shapes, which in turn affects the binding capacity of the particle to other, possibly more dangerous substances.[citation needed] The table below lists the colours and shapes of some common atmospheric particulates: Scanning electron microscopy of glass powder originated from glass bottles Scanning electron microscopy of cement Scanning electron microscopy of mortar glass powder (10%) which seems to have fibre-like structure Scanning electron microscopy of white asbesto with needle-like shape fibre|2023-09-27-12-47-55
Particulates|Composition, quantity, and duration are important|" Composition of particles can vary greatly depending on their sources and how they are produced. For example, dust emitted from the burning of living and dead vegetation would be different from those emitted from the burning of joss paper or construction wastes. Particles emitted from fuel combustion are not the same as those emitted from waste combustion. The particulate matter generated from the fire of a recycling yard  or a ship full of scrap metal   may contain more toxic substances than other types of burning. Different types of building refurbishment activities produce different kinds of dust too. The composition of PM generated from cutting or mixing concrete made with Portland Cement would be different from those generated from cutting or mixing concrete made with different types of slag (e.g. GGBFS, EAF slag ), fly ash or even EAF dust (EAFD),  while EFAD, slag and fly ash are likely to be more toxic as they contain heavy metals. Besides slag cement that is sold and used as an environmental friendly product,    fake (adulterated) cement, where different types of slag, fly ash or other unknown substances are added, is also very common in some places   due to the much lower production cost.  To address to the quality  and toxicity problems, some places are starting to ban the use of EAF slag in cement used in buildings.  Composition of welding fumes varies a lot as well and it depends on the metals in the material being welded, the composition of the coatings, electrode, etc, and hence a lot of health problems (e.g., lead poisoning, metal Fume Fever, cancers, nausea, irritation, kidney and liver damage, central nervous system problems, asthma, pneumonia, etc.) can be resulted from the different types of toxic emissions. Besides composition, quantity and duration of exposure are also important, since they would affect the triggering and severity of a disease. Particles that get into indoor would directly affect indoor air quality. Possible secondary contaminations   like what happened in third-hand smoke are also of concern. In a nutshell, while background concentration is important, only ""improvement in air quality"" or ""decrease in ambient concentration of PM"" do not necessarily mean better health. The health impact mainly depends on the toxicity (or source ) of the particulate matter a person is exposed to, the amount he is exposed to and for how long, and also the size, shape, and solubility of the PM. Since construction and refurbishment projects are prominent sources of particulate matter, it implicates that such projects, which are very common in some places,   should be avoided in health facilities that already commenced and under operation as far as possible. For inevitable projects, better plannings and mitigation measures regarding PM emission should be introduced. Use of power tools, heavy equipments, diesel fuels and potentially toxic building materials (e.g. concrete, metals, solder, paint, etc.) should be strictly monitored to ensure that patients who are there seeking for disease treatments or chances to survive are not adversely affected."|2023-09-27-12-47-55
Particulates|Health problems|" The effects of inhaling particulate matter that have been widely studied in humans and animals include COVID-19,      asthma, lung cancer, respiratory diseases like silicosis,
 
  cardiovascular disease, premature delivery, birth defects, low birth weight, developmental disorders,     neurodegenerative disorders   mental disorders,    and premature death. Outdoor fine particulates with diameter less than 2.5 microns accounts for 4.2 million annual deaths worldwide, and more than 103 million disability-adjusted life-years lost, making it the fifth leading risk factor for death. Air pollution has also been linked to a range of other psychosocial problems.  Particulates may cause tissue damage by entering organs directly, or indirectly by systemic inflammation. Adverse impacts may obtain even at exposure levels lower than published air quality standards deemed safe. Increased levels of fine particles in the air as a result of anthropogenic particulate air pollution ""is consistently and independently related to the most serious effects, including lung cancer  and other cardiopulmonary mortality.""  The association between a large number of deaths  and other health problems and particulate pollution was first demonstrated in the early 1970s  and has been reproduced many times since. PM pollution is estimated to cause 22,000–52,000 deaths per year in the United States (from 2000)  contributed to ~370,000 premature deaths in Europe during 2005.  and 3.22 million deaths globally in 2010 per the global burden of disease collaboration.  A study by the European Environment Agency estimates that 307,000 people have died prematurely in 2019 due to fine particle pollution in the 27 EU member states. A study in 2000 conducted in the U.S. explored how fine particulate matter may be more harmful than coarse particulate matter. The study was based on six different cities. They found that deaths and hospital visits that were caused by particulate matter in the air were primarily due to fine particulate matter.  Similarly, a 1987 study of American air pollution data found that fine particles and sulfates, as opposed to coarser particles, most consistently and significantly correlated to total annual mortality rates in standard metropolitan statistical areas. A study published in 2022 in GeoHealth concluded that eliminating energy-related fossil fuel emissions in the United States would prevent 46,900–59,400 premature deaths each year and provide $537–$678 billion in benefits from avoided PM2.5-related illness and death. Higher rates of infertility have been correlated with exposure to particulates. In addition, inhalation of PM2.5 – PM10 is associated with elevated risk of adverse pregnancy outcomes, such as low birth weight.  Maternal PM2.5 exposure during pregnancy is also associated with high blood pressure in children.  Exposure to PM2.5 has been associated with greater reductions in birth weight than exposure to PM10.  PM exposure can cause inflammation, oxidative stress, endocrine disruption, and impaired oxygen transport access to the placenta,  all of which are mechanisms for heightening the risk of low birth weight.  Overall epidemiologic and toxicological evidence suggests that a causal relationship exists between long-term exposures to PM2.5 and developmental outcomes (i.e. low birth weight).  However, studies investigating the significance of trimester-specific exposure have proven to be inconclusive,  and results of international studies have been inconsistent in drawing associations of prenatal particulate matter exposure and low birth weight.  As perinatal outcomes have been associated with lifelong health   and exposure to particulate matter is widespread, this issue is of critical public health importance and additional research will be essential to inform public policy on the matter. A 2002 study indicated that PM2.5 leads to high plaque deposits in arteries, causing vascular inflammation and atherosclerosis – a hardening of the arteries that reduces elasticity, which can lead to heart attacks and other cardiovascular problems.  A 2014 meta analysis reported that long term exposure to particulate matter is linked to coronary events. The study included 11 cohorts participating in the European Study of Cohorts for Air Pollution Effects (ESCAPE) with 100,166 participants, followed for an average of 11.5 years. An increase in estimated annual exposure to PM 2.5 of just 5 μg/m3 was linked with a 13% increased risk of heart attacks.  In 2017, a study revealed that PM not only affects human cells and tissues, but also impacts bacteria which cause disease in humans.  This study concluded that biofilm formation, antibiotic tolerance, and colonisation of both Staphylococcus aureus and Streptococcus pneumoniae was altered by black carbon exposure. The largest US study on acute health effects of coarse particle pollution between 2.5 and 10 micrometers in diameter was published 2008 and found an association with hospital admissions for cardiovascular diseases but no evidence of an association with the number of hospital admissions for respiratory diseases.  After taking into account fine particle levels (PM2.5 and less), the association with coarse particles remained but was no longer statistically significant, which means the effect is due to the subsection of fine particles. The Mongolian government agency recorded a 45% increase in the rate of respiratory illness in the past five years (reported in 2011).  Bronchial asthma, chronic obstructive pulmonary disease, and interstitial pneumonia were the most common ailments treated by area hospitals. Levels of premature death, chronic bronchitis, and cardiovascular disease are increasing at a rapid rate. The effects of air pollution and particulate matter on cognitive performance has become an active area of research.  A recent longitudinal study in China comparing air pollution and particulate exposure with verbal and mathematics test scores found that accumulative exposure impeded verbal scores significantly more than math scores. The negative impact in verbal reasoning as a result of particulate exposure was more pronounced as people aged and affected men more than women. Level of cognitive decline in verbal reasoning scores was more pronounced in the less educated (middle school diploma or lower).  Short term PM exposure has been linked to short term cognitive decline in otherwise healthy adults. Air pollution, particulate matter and wood smoke may also cause brain damage     and increase the risk of developmental disorders (e.g., autism),     neurodegenerative disorders,   mental disorders,    and suicide,    although studies on the link between depression and some air pollutants are not consistent.  At least one study has identified ""the abundant presence in the human brain of magnetite nanoparticles that match precisely the high-temperature magnetite nanospheres, formed by combustion and/or friction-derived heating, which are prolific in urban, airborne particulate matter (PM)."" Particulates also appear to have a role in the pathogenesis of Alzheimer's disease and premature brain aging. There is increasing evidence to suggest a correlation between PM2.5 exposure and the prevalence of neurodegenerative diseases such as Alzheimer's. Several epidemiological studies have suggested a link between PM2.5 exposure and cognitive decline, particularly in the development of neurodegenerative diseases such as Alzheimer's. While the exact mechanisms behind the link between PM2.5 exposure and cognitive decline are not fully understood, research suggests that the fine particles may be able to enter the brain through the olfactory nerve and cause inflammation and oxidative stress, which can damage brain cells and contribute to the development of neurodegenerative diseases. The World Health Organization (WHO) estimated in 2005 that ""... fine particulate air pollution (PM(2.5)), causes about 3% of mortality from cardiopulmonary disease, about 5% of mortality from cancer of the trachea, bronchus, and lung, and about 1% of mortality from acute respiratory infections in children under 5 years, worldwide.""  A 2011 study concluded that traffic exhaust is the single most serious preventable cause of heart attack in the general public, the cause of 7.4% of all attacks. Particulate matter studies in Bangkok, Thailand from 2008 indicated a 1.9% increased risk of dying from cardiovascular disease, and 1.0% risk of all disease for every 10 micrograms per cubic meter. Levels averaged 65 in 1996, 68 in 2002, and 52 in 2004. Decreasing levels may be attributed to conversions of diesel to natural gas combustion as well as improved regulations. There have been many studies linking race to increased proximity to particulate matter, and thus susceptibility to adverse health effects that go in tandem with long term exposure. In a study analyzing the effects of air pollution on racially segregated neighborhoods in the United States, results show that ""the proportions of Black residents in a tract was linked to higher asthma rates"".  Many scholars link this disproportionality to racial housing segregation and their respective inequalities in ""toxic exposures"".  This reality is made worse by the finding that ""health care occurs in the context of broader historic and contemporary social and economic inequality and persistent racial and ethnic discrimination in many sectors of American life"".  Residential proximity to particulate emitting facilities increases exposure to PM 2.5 which is linked to increased morbidity and mortality rates.  Multiple studies confirm the burden of PM emissions is higher among non-White and poverty ridden populations,  though some say that income does not drive these differences.  This correlation between race and housing related health repercussions stems from a longstanding environmental justice problem linked to the practice of historic redlining. An example of these factors contextualized is an area of Southeastern Louisiana, colloquially dubbed 'Cancer Alley' for its high concentration of cancer related deaths due to neighboring chemical plants.  Cancer Alley being a majority African American community, with the neighborhood nearest to the plant being 90% Black,  perpetuates the scientific narrative that Black populations are located disproportionately closer to areas of high PM output than White populations. A 2020 article relates the long term health effects of living in high PM concentrations to increased risk, spread, and mortality rates from the SARS-CoV-2 or COVID-19, and faults a history of racism for this outcome. There is an increased risk of particulate exposure in regions where wildfires are persistent. Smoke from wildfires may impact sensitive groups such as the elderly, children, pregnant women, and people with lung, and cardiovascular disease.  A study found that in the 2008 wildfire season in California, the particulate matter was much more toxic to human lungs, as increased neutrophil infiltrate, cell influx and edema was observed versus particulate matter from ambient air.  Furthermore, particulate matter from wildfires have been linked to be a triggering factor of acute coronary events such as ischemic heart disease.  Wildfires also have been associated with increased emergency department visits due to particulate matter exposure, as well as an increased risk of asthma related events.   Furthermore, a link between PM2.5 from wildfires and increased risk of hospitalizations for cardiopulmonary diseases has been discovered.  Various lines of evidence also suggest wildfire smoke reduces mental performance."|2023-09-27-12-47-55
Particulates|Energy industry knowledge and response to adverse health effects|" Major energy companies understood at least since the 1960s that use of their products causes widespread adverse health effects and death but continued aggressive political lobbying in the United States and elsewhere against clean air regulation and launched major corporate propaganda campaigns to sow doubt regarding the causative link between the burning of fossil fuels and major risks to human life. Internal company memoranda reveal that energy industry scientists and executives knew that air pollutants created by fossil fuels lodge deep in human lung tissue, and cause birth defects in children of oil industry workers. The industry memos acknowledge that automobiles ""are by far the greatest sources of air pollution"" and also that air pollution causes adverse health effects and lodges toxins, including carcinogens, ""deep into the lungs which would otherwise be removed in the throat"". In response to mounting public concern, the industry eventually created the Global Climate Coalition, an industry lobby group, to derail governments' attempts to regulate air pollution and to create confusion in the public mind about the necessity of such regulation. Similar lobbying and corporate public relations efforts were undertaken by the American Petroleum Institute, a trade association of the oil and gas industry, and the climate change denier private think tank, The Heartland Institute. ""The response from fossil-fuel interests has been from the same playbook – first they know, then they scheme, then they deny and then they delay. They've fallen back on delay, subtle forms of propaganda and the undermining of regulation,"" said Geoffrey Supran, a Harvard University researcher of the history of fossil-fuel companies and climate change. These efforts have been compared, by policy analysts such as Carroll Muffett of the Center for International Environmental Law, to the tobacco industry strategy of lobbying and corporate propaganda campaigns to create doubt regarding the causal connection between cigarette smoking and cancer and to forestall its regulation. In addition, industry-funded advocates, when appointed to senior government positions in the United States, have revised scientific findings showing the deadly effects of air pollution and have rolled back its regulation."|2023-09-27-12-47-55
Particulates|Effects on vegetation| Particulate matter can clog stomatal openings of plants and interfere with photosynthesis functions.  In this manner, high particulate matter concentrations in the atmosphere can lead to growth stunting or mortality in some plant species.[citation needed]|2023-09-27-12-47-55
Particulates|Regulation| Most governments have created regulations both for the emissions allowed from certain types of pollution sources (motor vehicles, industrial emissions etc.) and for the ambient concentration of particulates. The IARC and WHO designate particulates a Group 1 carcinogen. Particulates are the deadliest form of air pollution due to their ability to penetrate deep into the lungs and blood streams unfiltered, causing respiratory diseases, heart attacks, and premature death.  In 2013, the ESCAPE study involving 312,944 people in nine European countries revealed that there was no safe level of particulates and that for every increase of 10 μg/m3 in PM10, the lung cancer rate rose 22%. For PM2.5 there was a 36% increase in lung cancer per 10 μg/m3.  In a 2014 meta-analysis of 18 studies globally including the ESCAPE data, for every increase of 10 μg/m3 in PM2.5, the lung cancer rate rose 9%.|2023-09-27-12-47-55
Particulates|Canada| In Canada the standard for particulate matter is set nationally by the federal-provincial Canadian Council of Ministers of the Environment (CCME). Jurisdictions (provinces and territories) may set more stringent standards. The CCME standard for particulate matter 2.5 (PM2.5) as of 2015 is 28 μg/m3 (calculated using the 3-year average of the annual 98th percentile of the daily 24-hr average concentrations) and 10 μg/m3 (3-year average of annual mean). PM2.5 standards will increase in stringency in 2020.|2023-09-27-12-47-55
Particulates|European Union| The European Union has established the European emission standards, which include limits for particulates in the air:|2023-09-27-12-47-55
Particulates|United Kingdom|" To mitigate the problem of wood burning, starting from May 2021, traditional house coal (bituminous coal) and wet wood, two of the most polluting fuels, can no longer be sold. Wood sold in volumes of less than 2m3 must be certified as 'Ready to Burn', which means it has a moisture content of 20% or less. Manufactured solid fuels must also be certified as 'Ready to Burn' to ensure they meet sulphur and smoke emission limits.  Starting from January 2022, all new wood burning stoves have to meet new EcoDesign standards (Ecodesign stoves produce 450 times more toxic air pollution than gas central heating. Older stoves, which are now banned from sale, produce 3,700 times more). In 2023, the amount of smoke that burners in ""smoke control areas"" - most England's towns and cities - can emit per hour is reduced from 5g to 3g. Violation will result in an on-the-spot fine of up to £300. Those who do not comply may even get a criminal record."|2023-09-27-12-47-55
Particulates|United States| The United States Environmental Protection Agency (EPA) has set standards for PM10 and PM2.5 concentrations.  (See National Ambient Air Quality Standards.) In October 2008, the Department of Toxic Substances Control (DTSC), within the California Environmental Protection Agency, announced its intent to request information regarding analytical test methods, fate and transport in the environment, and other relevant information from manufacturers of carbon nanotubes.  DTSC is exercising its authority under the California Health and Safety Code, Chapter 699, sections 57018-57020.  These sections were added as a result of the adoption of Assembly Bill AB 289 (2006).  They are intended to make information on the fate and transport, detection and analysis, and other information on chemicals more available. The law places the responsibility to provide this information to the Department on those who manufacture or import the chemicals. On 22 January 2009, a formal information request letter  was sent to manufacturers who produce or import carbon nanotubes in California, or who may export carbon nanotubes into the State.  This letter constitutes the first formal implementation of the authorities placed into statute by AB 289 and is directed to manufacturers of carbon nanotubes, both industry, and academia within the State, and to manufacturers outside California who export carbon nanotubes to California. This request for information must be met by the manufacturers within one year. DTSC is waiting for the upcoming 22 January 2010 deadline for responses to the data call-in. The California Nano Industry Network and DTSC hosted a full-day symposium on 16 November 2009 in Sacramento, California. This symposium provided an opportunity to hear from nanotechnology industry experts and discuss future regulatory considerations in California. DTSC is expanding the Specific Chemical Information Call-in to members of the nanometal oxides, the latest information can be found on their website. Key points in the Colorado Plan include reducing emission levels and solutions by sector. Agriculture, transportation, green electricity, and renewable energy research are the main concepts and goals in this plan. Political programs such as mandatory vehicle emissions testing and the prohibition of smoking indoors are actions taken by local government to create public awareness and participation in cleaner air. The location of Denver next to the Rocky Mountains and wide expanse of plains makes the metro area of Colorado's capital city a likely place for smog and visible air pollution.[citation needed]|2023-09-27-12-47-55
Particulates|Affected areas| To see the air pollution trend, 480 cities around the world (Ukraine excluded) was mapped by the air experts at HouseFresh  to calculate the average PM2.5 level of the first nine months of 2019 against that of 2022, as reported by the Forbes magazines.  Average levels of PM2.5 were measured using aqicn.org's World Air Quality Index data, and a formula developed by AirNow was used to convert the PM2.5 figure into micrograms per cubic meter of air (μg/m3) values. Among the 70 capital cities investigated, Baghdad, Iraq is the worst performing one, with PM2.5 levels going up +31.6 μg/m3. Ulan Bator (Ulaanbaatar), the capital city of Mongolia, is performing the best, with PM2.5 levels dropping by -23.4 μg/m3. Previously it was as one of the most polluted capital cities in the world. An air quality improvement plan in 2017 appears to be showing positive results. Out of the 480 cities, Dammam in Saudi Arabia is performing the worst with PM2.5 levels going up +111.1 μg/m3. The city is a significant center for the Saudi oil industry and home to both the largest airport in the world and the largest port in the Persian Gulf. It is currently the most polluted city surveyed. In Europe, the worst performing cities are located in Spain. They are Salamanca and Palma, with PM2.5 levels increase by +5.1 μg/m3 and +3.7 μg/m3 respectively. The best performing city is Skopje, the capital city of North Macedonia, with PM2.5 levels dropping by -12.4 μg/m3. It was once the most polluted capital city in Europe and still has a long way to go to achieve clean air. In the U.S., Salt Lake City, Utah and Miami, Florida are the two cities with the highest PM2.5 level increases (+1.8 μg/m3). Salt Lake City suffers from a weather event known as 'inversion'. Located in a valley, cooler, polluted air is trapped close to ground level under the warmer air above when inversion occurs. On the other hand, Omaha, Nebraska is performing the best and has a decrease of -1.1 μg/m3 in PM2.5 levels. The cleanest city in this report is Zürich, Switzerland with PM2.5 levels of just 0.5 μg/m3, placed first in both 2019 and 2022. The second cleanest city is Perth, with 1.7 μg/m3 and PM2.5 levels dropping by -6.2 μg/m3 since 2019. Of the top ten cleanest cities, five are from Australia. They are Hobart, Wollongong, Launceston, Sydney and Perth. Honolulu is the only U.S. city in the top ten list, ranking tenth with levels of 4 μg/m3, with a tiny increase since 2019. Almost all of the top ten most polluted cities are in the Middle East and Asia. The worst is Dammam in Saudi Arabia with a PM2.5 level of 155 μg/m3. Lahore in Pakistan is the second worst with 98.1 μg/m3. The third is Dubai, home to the world's tallest building. In the bottom ten are three cities from India, Muzaffarnagar, Delhi and New Delhi. Here is a list of the 30 most polluted cities by PM2.5, Jan to Sep 2022: There are limits to the above survey. For example, not every city in the world is covered, and that the number of monitoring stations for each city would not be the same. So the data is for reference only.|2023-09-27-12-47-55
Particulates|Australia| PM10 pollution in coal mining areas in Australia such as the Latrobe Valley in Victoria and the Hunter Region in New South Wales significantly increased during 2004 to 2014. Although the increase did not significantly add to non-attainment statistics the rate of increase has risen each year during 2010 to 2014.|2023-09-27-12-47-55
Particulates|China| Some cities in Northern China and South Asia have had concentrations above 200 μg/m3.  The PM levels in Chinese cities were extreme between 2010 - 2014, reaching an all-time high in Beijing on 12 January 2013, of 993 μg/m3,  but has been improving thanks to clean air actions. To monitor the air quality of south China, the U.S. Consulate Guangzhou set a PM2.5 and PM10 monitor on Shamian Island in Guangzhou and displays readings on its official website and social platforms.|2023-09-27-12-47-55
Particulates|South Korea| As of 2017, South Korea has the worst air pollution among the developed nations in the OECD (Organization for Economic Cooperation and Development).  According to a study conducted by NASA and NIER, 52% of PM2.5 measured in Olympic Park, Seoul in May and June 2016 came from local emissions. The rest was trans-boundary pollution coming from China's Shandong Province (22%), North Korea (9%), Beijing (7%), Shanghai (5%), and a combined 5% from China's Liaoning Province, Japan and the West Sea.  In December 2017, the environmental ministers from South Korea and China signed the China-Korea Environmental Cooperation Plan (2018-22), a five-year plan to jointly solve issues in the air, water, soil and waste. An environmental cooperation centre was also launched in 2018 to aid cooperation.|2023-09-27-12-47-55
Particulates|Thailand|" Air quality of Thailand is getting worse in 2023, which is described as a ""post-COVID back-to-normal situation"". In addition to the capital Bangkok, air quality in Chiang Mai, a popular tourist destination, is also deteriorating. Chiang Mai was listed as the most polluted city in a live ranking by the Swiss air quality company IQAir on 27 March 2023. The ranking includes data from about 100 world cities for which measured PM2.5 data is available."|2023-09-27-12-47-55
Particulates|Ulaanbaatar|" Mongolia's capital city Ulaanbaatar has an annual average mean temperature of about 0 °C, making it the world's coldest capital city. About 40% of the population lives in apartments, 80% of which are supplied with central heating systems from three combined heat and power plants. In 2007, the power plants consumed almost 3.4 million tons of coal. The pollution control technology is in poor condition.[citation needed] The other 60% of the population reside in shantytowns (Ger districts), which have developed due to the country's new market economy and the very cold winter seasons. The poor in these districts cook and heat their wood houses with indoor stoves fueled by wood or coal. The resulting air pollution is characterized by raised sulfur dioxide and nitrogen oxide levels and very high concentrations of airborne particles and particulate matter (PM). 
Annual seasonal average particulate matter concentrations have been recorded as high as 279 μg/m3 (micrograms per cubic meter).[citation needed] The World Health Organization's recommended annual mean PM10 level is 20 μg/m3,  which means that Ulaanbaatar's PM10 annual mean levels are 14 times higher than recommended.[citation needed] During the winter months, in particular, the air pollution obscures the air, affecting the visibility in the city to such an extent that airplanes on some occasions are prevented from landing at the airport. In addition to stack emissions, another source unaccounted for in the emission inventory is fly ash from ash ponds, the final disposal place for fly ash that has been collected in settling tanks. Ash ponds are continually eroded by wind during the winter."|2023-09-27-12-47-55
Particulates|United States|" From the ""State of Air 2022"" report compiled by the American Lung Association using data from the U.S. Environmental Protection Agency from 2018-2020,  California cities are the most polluted cities (by PM2.5) in the U.S. while the East Coast is cleaner. However, another study has come up with a very different conclusion. According to Forbes, a travel insurance comparison site InsureMyTrip conducted a survey of 50 U.S. cities in 2020 and ranked them by cleanliness with criteria like hand sanitizer demand, cleanliness of restaurants, quantity of recycling collectors, satisfaction of garbage disposal, electric vehicle market share and pollution.  On their top ten cleanest cities list, seven are from California, including Long Beach (No. 1), San Diego (No. 2), Sacramento (No. 3), San Jose (No. 6), Oakland (No. 7), Bakersfield (No. 9), and San Francisco (No. 10). The discrepancies maybe due to the differences in data choice, calculation methods, definitions of ""cleanliness"" and a large variation of air quality across the same state, etc. This again shows that one need to be very careful when drawing conclusions from the many air quality rankings available on the internet. In mid-2023, air quality in eastern U.S. lowered significantly as particulates from Canada’s wildfires blew down. According to NASA, some of the fires were ignited by lightning."|2023-09-27-12-47-55
Particulates|See also| Health-related:|2023-09-27-12-47-55
Pea soup fog|General|" Pea soup fog (also known as a pea souper, black fog or killer fog) is a very thick and often yellowish, greenish or blackish fog caused by air pollution that contains soot particulates and the poisonous gas sulphur dioxide. This very thick smog occurs in cities and is derived from the smoke given off by the burning of soft coal for home heating and in industrial processes. Smog of this intensity is often lethal to vulnerable people such as the elderly, the very young (infants) and those with respiratory problems. The result of these phenomena was commonly known as a London particular or London fog; in a reversal of the idiom, ""London particular"" became the name for a thick pea and ham soup."|2023-07-18-02-46-36
Pea soup fog|Historical observations|" From as early as the 13th century,   air pollution became increasingly prevalent, and a predominant perception in the 13th century was that sea-coal  smoke would affect one's health.   From the mid-17th century, in British cities, especially London, the incidence of ill-health was attributed to coal smoke from both domestic and industrial chimneys  combining with the mists and fogs of the Thames Valley.  Luke Howard, a pioneer in urban climate studies, published The Climate of London in 1818–1820, in which he uses the term ""city fog"" and describes the heat island effect which concentrated the accumulation of smog over the city. In 1880, Francis Albert Rollo Russell, son of the former Prime Minister Lord John Russell, published a leaflet that blamed home hearth smoke, rather than factories' smoke, for damaging the city's important buildings, depriving vegetation of sunlight, and increasing the expense and effort of laundering clothes. Furthermore, he charged the ""perpetually present"" sulphurous smoke with increasing bronchitis and other respiratory diseases. More than 2,000 Londoners had ""literally choked to death"", he wrote, on account of ""a want of carefulness in preventing smoke in our domestic fires"" which emitted coal smoke from ""more than a million chimneys"" that, when combined with the prolonged fogs of late January and early February 1880, fatally aggravated pre-existing lung conditions and was ""more fatal than the slaughter of many a great battle"". The difficulties of driving through the fog were vividly described in the Autocar magazine, with an otherwise straightforward 45 mile car journey on the night of 12 December 1946 taking over eight hours to complete. At times, the passenger had to get out and walk alongside the car to see the kerb and operate the steering through the side window while the driver operated the pedals. The most lethal incidence of this smog in London occurred in 1952 and resulted in the Clean Air Act 1956 and Clean Air Act 1968, both now repealed and consolidated into the Clean Air Act 1993, which were effective in largely removing sulphur dioxide and coal smoke, the causes of pea soup fog, though these have been replaced by less visible pollutants that derive from vehicles in urban areas."|2023-07-18-02-46-36
Pea soup fog|Origins of the term|" Reference to the sources of smog, along with the earliest extant use of ""pea-soup"" as a descriptor, is found in a report by John Sartain published in 1820 on life as a young artist, recounting what it was like to slink home through a fog as thick and as yellow as the pea-soup of the eating house; return to your painting room ... having opened your window at going out, to find the stink of the paint rendered worse, if possible, by the entrance of the fog, which, being a compound from the effusions of gas pipes, tan yards, chimneys, dyers, blanket scourers, breweries, sugar bakers, and soap boilers, may easily be imagined not to improve the smell of a painting room! An 1871 New York Times article refers to ""London, particularly, where the population are periodically submerged in a fog of the consistency of pea soup"". The fogs caused large numbers of deaths from respiratory problems."|2023-07-18-02-46-36
Pea soup fog|Remediation|" King Edward I of England banned the burning of sea-coal by proclamation in London in 1272, after its smoke became a problem.   By the 17th century London's pollution had become a serious problem, still due, in particular, to the burning of cheap, readily available sea coal.  John Evelyn, advisor to Charles II of England, defined the problem in his pamphlet, Fumifugium: Or, the Inconvenience of the Aer and Smoak of London Dissipated    published in 1661, blaming coal, a ""subterrany fuel"" that had ""a kind of virulent or arsenical vapour arising from it"" for killing many. He proposed the relocation of industry out of the city and the planting of massive gardens of ""odiferous flowers"" to ""tinge the air"" and thus mask the pollution."|2023-07-18-02-46-36
Pea soup fog|Clean Air Act| The worst recorded instance was the Great Smog of 1952, when 4,000 deaths were reported in the city over a couple of days, and a subsequent 8,000 related deaths, leading to the passage of the Clean Air Act 1956, which banned the use of coal for domestic fires in some urban areas.  The overall death toll from that incident is now believed to be around 12,000.|2023-07-18-02-46-36
Persistent organic pollutant|General|" Lists Categories Persistent organic pollutants (POPs) are organic compounds that are resistant to degradation through chemical, biological, and photolytic processes.  They are toxic chemicals that adversely affect human health and the environment around the world. Because they can be transported by wind and water, most POPs generated in one country can and do affect people and wildlife far from where they are used and released. The effect of POPs on human and environmental health was discussed, with intention to eliminate or severely restrict their production, by the international community at the Stockholm Convention on Persistent Organic Pollutants in 2001. Most POPs are pesticides or insecticides, and some are also solvents, pharmaceuticals, and industrial chemicals.  Although some POPs arise naturally (e.g. from volcanoes), most are man-made.  The ""dirty dozen"" POPs identified by the Stockholm Convention include aldrin, chlordane, dieldrin, endrin, heptachlor, HCB, mirex, toxaphene, PCBs, DDT, dioxins, and polychlorinated dibenzofurans. However, there have since been many new POPs added, for example PFASs."|2023-09-21-13-07-41
Persistent organic pollutant|Consequences of persistence| POPs typically are halogenated organic compounds (see lists below) and as such exhibit high lipid solubility.  For this reason, they bioaccumulate in fatty tissues. Halogenated compounds also exhibit great stability reflecting the nonreactivity of C-Cl bonds toward hydrolysis and photolytic degradation.  The stability and lipophilicity of organic compounds often correlates with their halogen content, thus polyhalogenated organic compounds are of particular concern.  They exert their negative effects on the environment through two processes, long range transport, which allows them to travel far from their source, and bioaccumulation, which reconcentrates these chemical compounds to potentially dangerous levels.  Compounds that make up POPs are also classed as PBTs (persistent, bioaccumulative and toxic) or TOMPs (toxic organic micro pollutants).|2023-09-21-13-07-41
Persistent organic pollutant|Long-range transport| POPs enter the gas phase under certain environmental temperatures and volatilize from soils, vegetation, and bodies of water into the atmosphere, resisting breakdown reactions in the air, to travel long distances before being re-deposited.  This results in accumulation of POPs in areas far from where they were used or emitted, specifically environments where POPs have never been introduced such as Antarctica, and the Arctic circle.  POPs can be present as vapors in the atmosphere or bound to the surface of solid particles (aerosols). A determining factor for the long-range transport is the fraction of a POP that is adsorbed on aerosols. In adsorbed form it is – as opposed to the gas phase – protected from photo-oxidation, i.e. direct photolysis as well as oxidation by OH radicals or ozone. POPs have low solubility in water but are easily captured by solid particles, and are soluble in organic fluids (oils, fats, and liquid fuels). POPs are not easily degraded in the environment due to their stability and low decomposition rates. Due to this capacity for long-range transport, POP environmental contamination is extensive, even in areas where POPs have never been used, and will remain in these environments years after restrictions implemented due to their resistance to degradation.|2023-09-21-13-07-41
Persistent organic pollutant|Bioaccumulation| Bioaccumulation of POPs is typically associated with the compounds high lipid solubility and ability to accumulate in the fatty tissues of living organisms for long periods of time.   Persistent chemicals tend to have higher concentrations and are eliminated more slowly. Dietary accumulation or bioaccumulation is another hallmark characteristic of POPs, as POPs move up the food chain, they increase in concentration as they are processed and metabolized in certain tissues of organisms. The natural capacity for animals gastrointestinal tract to concentrate ingested chemicals, along with poorly metabolized and hydrophobic nature of POPs, makes such compounds highly susceptible to bioaccumulation.  Thus POPs not only persist in the environment, but also as they are taken in by animals they bioaccumulate, increasing their concentration and toxicity in the environment.   This increase in concentration is called biomagnification, which is where organisms higher up in the food chain have a greater accumulation of POPs.  Bioaccumulation and long-range transport are the reason why POPs can accumulate in organisms like whales, even in remote areas like Antarctica.|2023-09-21-13-07-41
Persistent organic pollutant|Stockholm Convention on Persistent Organic Pollutants|" The Stockholm Convention was adopted and put into practice by the United Nations Environment Programme (UNEP) on May 22, 2001. The UNEP decided that POP regulation needed to be addressed globally for the future. The purpose statement of the agreement is ""to protect human health and the environment from persistent organic pollutants."" As of 2014, there are 179 countries in compliance with the Stockholm convention. The convention and its participants have recognized the potential human and environmental toxicity of POPs. They recognize that POPs have the potential for long range transport and bioaccumulation and biomagnification. The convention seeks to study and then judge whether or not a number of chemicals that have been developed with advances in technology and science can be categorized as POPs or not. The initial meeting in 2001 made a preliminary list, termed the ""dirty dozen"", of chemicals that are classified as POPs. As of 2022, the United States has signed the Stockholm Convention but has not ratified it. There are a handful of other countries that have not ratified the convention but most countries in the world have ratified the convention."|2023-09-21-13-07-41
Persistent organic pollutant|Compounds on the Stockholm Convention list| In May 1995, the UNEP Governing Council investigated POPs.  Initially the Convention recognized only twelve POPs for their adverse effects on human health and the environment, placing a global ban on these particularly harmful and toxic compounds and requiring its parties to take measures to eliminate or reduce the release of POPs in the environment.|2023-09-21-13-07-41
Persistent organic pollutant|New POPs on the Stockholm Convention list| Since 2001, this list has been expanded to include some polycyclic aromatic hydrocarbons (PAHs), brominated flame retardants, and other compounds. Additions to the initial 2001 Stockholm Convention list are the following POPs:|2023-09-21-13-07-41
Persistent organic pollutant|Health effects| POP exposure may cause developmental defects, chronic illnesses, and death. Some are carcinogens per IARC, possibly including breast cancer.  Many POPs are capable of endocrine disruption within the reproductive system, the central nervous system, or the immune system. People and animals are exposed to POPs mostly through their diet, occupationally, or while growing in the womb.   For humans not exposed to POPs through accidental or occupational means, over 90% of exposure comes from animal product foods due to bioaccumulation in fat tissues and bioaccumulate through the food chain. In general, POP serum levels increase with age and tend to be higher in females than males. Studies have investigated the correlation between low level exposure of POPs and various diseases. In order to assess disease risk due to POPs in a particular location, government agencies may produce a human health risk assessment which takes into account the pollutants' bioavailability and their dose-response relationships.|2023-09-21-13-07-41
Persistent organic pollutant|Endocrine disruption| The majority of POPs are known to disrupt normal functioning of the endocrine system. Low level exposure to POPs during critical developmental periods of fetus, newborn and child can have a lasting effect throughout their lifespan. A 2002 study  summarizes data on endocrine disruption and health complications from exposure to POPs during critical developmental stages in an organism's lifespan. The study aimed to answer the question whether or not chronic, low level exposure to POPs can have a health impact on the endocrine system and development of organisms from different species. The study found that exposure of POPs during a critical developmental time frame can produce a permanent changes in the organisms path of development. Exposure of POPs during non-critical developmental time frames may not lead to detectable diseases and health complications later in their life. In wildlife, the critical development time frames are in utero, in ovo, and during reproductive periods. In humans, the critical development timeframe is during fetal development.|2023-09-21-13-07-41
Persistent organic pollutant|Reproductive system|" The same study in 2002  with evidence of a link from POPs to endocrine disruption also linked low dose exposure of POPs to reproductive health effects. The study stated that POP exposure can lead to negative health effects especially in the male reproductive system, such as decreased sperm quality and quantity, altered sex ratio and early puberty onset. For females exposed to POPs, altered reproductive tissues and pregnancy outcomes as well as endometriosis have been reported. A Greek study from 2014 investigated the link between maternal weight gain during pregnancy, their PCB-exposure level and PCB level in their newborn infants, their birth weight, gestational age, and head circumference. The lower the birth weight and head circumference of the infants was, the higher POP levels during prenatal development had been, but only if mothers had either excessive or inadequate weight gain during pregnancy. No correlation between POP exposure and gestational age was found. 
A 2013 case-control study conducted 2009 in Indian mothers and their offspring showed prenatal exposure of two types of organochlorine pesticides (HCH, DDT and DDE) impaired the growth of the fetus, reduced the birth weight, length, head circumference and chest circumference."|2023-09-21-13-07-41
Persistent organic pollutant|Health effects of PFAS| Hormone-disrupting chemicals, including PFASs, are linked with rapid declines in human fertility.  In a meta-analysis for associations between PFASs and human clinical biomarkers for liver injury, authors considered both PFAS effects on liver biomarkers and histological data from rodent experimental studies and concluded that evidence exists showing that PFOA, perfluorohexanesulfonic acid (PFHxS), and perfluorononanoic acid (PFNA) are hepatotoxic to humans.|2023-09-21-13-07-41
Persistent organic pollutant|Additive and synergistic effects| Evaluation of the effects of POPs on health is very challenging in the laboratory setting. For example, for organisms exposed to a mixture of POPs, the effects are assumed to be additive.  Mixtures of POPs can in principle produce synergistic effects. With synergistic effects, the toxicity of each compound is enhanced (or depressed) by the presence of other compounds in the mixture. When put together, the effects can far exceed the approximated additive effects of the POP compound mixture.|2023-09-21-13-07-41
Persistent organic pollutant|In urban areas and indoor environments| Traditionally it was thought that human exposure to POPs occurred primarily through food, however indoor pollution patterns that characterize certain POPs have challenged this notion. Recent studies of indoor dust and air have implicated indoor environments as a major sources for human exposure via inhalation and ingestion.  Furthermore, significant indoor POP pollution must be a major route of human POP exposure, considering the modern trend in spending larger proportions of life indoors. Several studies have shown that indoor (air and dust) POP levels to exceed outdoor (air and soil) POP concentrations.|2023-09-21-13-07-41
Persistent organic pollutant|In cosmetics and personal care products| Per- and polyfluoroalkyl substances (PFAS) are a class of about 9,000 synthetic organofluorine compounds that have multiple highly toxic fluorine atoms attached to an alkyl chain. PFAS are used in the manufacture of a wide range of products such as food packaging and clothing. They are also used by major companies of the cosmetics industry in a wide range of cosmetics, including lipstick, eye liner, mascara, foundation, concealer, lip balm, blush, nail polish and other such products. A 2021 study tested 231 makeup and personal care products and found organic fluorine, an indicator of PFAS, in more than half of the samples. High levels of fluorine were most commonly identified in waterproof mascara (82% of brands tested), foundations (63%), and liquid lipstick (62%). Since PFAS compounds are highly mobile, they are readily absorbed through human skin and through tear ducts, and such products on lips are often unwittingly ingested. Manufacturers often fail to label their products as containing PFAS, which makes it difficult for cosmetics consumers to avoid products containing PFAS.|2023-09-21-13-07-41
Persistent organic pollutant|Control and removal in the environment| Current studies aimed at minimizing POPs in the environment are investigating their behavior in photocatalytic oxidation reactions.  POPs that are found in humans and in aquatic environments the most are the main subjects of these experiments. Aromatic and aliphatic degradation products have been identified in these reactions. Photochemical degradation is negligible compared to photocatalytic degradation.  A method of removal of POPs from marine environments that has been explored is adsorption. It occurs when an absorbable solute comes into contact with a solid with a porous surface structure. This technique was investigated by Mohamed Nageeb Rashed of Aswan University, Egypt.  Current efforts are more focused on banning the use and production of POPs worldwide rather than removal of POPs.|2023-09-21-13-07-41
Plastic pollution|General| Lists Categories Plastic pollution is the accumulation of plastic objects and particles (e.g. plastic bottles, bags and microbeads) in the Earth's environment that adversely affects humans, wildlife and their habitat.   Plastics that act as pollutants are categorized by size into micro-, meso-, or macro debris.  Plastics are inexpensive and durable, making them very adaptable for different uses; as a result, manufacturers choose to use plastic over other materials.  However, the chemical structure of most plastics renders them resistant to many natural processes of degradation and as a result they are slow to degrade.  Together, these two factors allow large volumes of plastic to enter the environment as mismanaged waste which persists in the ecosystem and travels throughout food webs. Plastic pollution can afflict land, waterways and oceans. It is estimated that 1.1 to 8.8 million tonnes of plastic waste enters the ocean from coastal communities each year.  It is estimated that there is a stock of 86 million tons of plastic marine debris in the worldwide ocean as of the end of 2013, with an assumption that 1.4% of global plastics produced from 1950 to 2013 has entered the ocean and has accumulated there.  Some researchers suggest that by 2050 there could be more plastic than fish in the oceans by weight.  Living organisms, particularly marine animals, can be harmed either by mechanical effects such as entanglement in plastic objects, problems related to ingestion of plastic waste, or through exposure to chemicals within plastics that interfere with their physiology. Degraded plastic waste can directly affect humans through direct consumption (i.e. in tap water), indirect consumption (by eating plants and animals), and disruption of various hormonal mechanisms. As of 2019, 368 million tonnes of plastic is produced each year; 51% in Asia, where China is the world's largest producer.  From the 1950s up to 2018, an estimated 6.3 billion tonnes of plastic has been produced worldwide, of which an estimated 9% has been recycled and another 12% has been incinerated.  This large amount of plastic waste enters the environment and causes problems throughout the ecosystem; for example, studies suggest that the bodies of 90% of seabirds contain plastic debris.   In some areas there have been significant efforts to reduce the prominence of free range plastic pollution, through reducing plastic consumption, litter cleanup, and promoting plastic recycling. As of 2020, the global mass of produced plastic exceeds the biomass of all land and marine animals combined.  A May 2019 amendment to the Basel Convention regulates the exportation/importation of plastic waste, largely intended to prevent the shipping of plastic waste from developed countries to developing countries. Nearly all countries have joined this agreement.     On 2 March 2022 in Nairobi, 175 countries pledged to create a legally binding agreement by the end of the year 2024 with a goal to end plastic pollution. The amount of plastic waste produced increased during the COVID-19 pandemic due to increased demand for protective equipment and packaging materials.  Higher amounts of plastic ended up in the ocean, especially plastic from medical waste and masks.   Several news reports point to a plastic industry trying to take advantage of the health concerns and desire for disposable masks and packaging to increase production of single use plastic.|2023-09-26-20-47-14
Plastic pollution|Causes|" There are differing estimates of how much plastic waste has been produced in the last century. By one estimate, one billion tons of plastic waste have been discarded since the 1950s.  Others estimate a cumulative human production of 8.3 billion tons of plastic, of which 6.3 billion tons is waste, with only 9% getting recycled. It is estimated that this waste is made up of 81% polymer resin, 13% polymer fibres and 32% additives. In 2018 more than 343 million tonnes of plastic waste were generated, 90% of which was composed of post-consumer plastic waste (industrial, agricultural, commercial and municipal plastic waste). The rest was pre-consumer waste from resin production and manufacturing of plastic products (e.g. materials rejected due to unsuitable colour, hardness, or processing characteristics). A large proportion of post-consumer plastic waste consists of plastic packaging. In the United States plastic packaging has been estimated to make up 5% of MSW. This packaging includes plastic bottles, pots, tubs and trays, plastic films shopping bags, rubbish bags, bubble wrap, and plastic or stretch wrap and plastic foams e.g. expanded polystyrene (EPS). Plastic waste is generated in sectors including agriculture (e.g. irrigation pipes, greenhouse covers, fencing, pellets, mulch; construction (e.g. pipes, paints, flooring and roofing, insulants and sealants); transport (e.g. abraded tyres, road surfaces and road markings); electronic and electric equipment (e-waste); and pharmaceuticals and healthcare. The total amounts of plastic waste generated by these sectors is uncertain. Several studies have attempted to quantify plastic leakage into the environment at both national and global levels which have highlight the difficulty of determining the sources and amounts of all plastic leakage. One global study has estimated that between 60 and 99 million tonnes of mismanaged plastic waste were produced in 2015. Borrelle et al. 2020 has estimated that 19–23 million tonnes of plastic waste entered aquatic ecosystems in 2016. while the Pew Charitable Trusts and SYSTEMIQ (2020) have estimated that 9–14 million tonnes of plastic waste ended up in the oceans the same year. Despite global efforts to reduce the generation of plastic waste, losses to the environment are predicted to increase. Modelling indicates that, without major interventions, between 23 and 37 million tonnes per year of plastic waste could enter the oceans by 2040 and between 155 and 265 million tonnes per year could be discharged into the environment by 2060. Under a business as usual scenario, such increases would likely be attributable to a continuing rise in production of plastic products, driven by consumer demand, accompanied by insufficient improvements in waste management. As the plastic waste released into the environment already has a significant impact on ecosystems, an increase of this magnitude could have dramatic consequences. The trade in plastic waste has been identified as ""a main culprit"" of marine litter.  Countries importing the waste plastics often lack the capacity to process all the material. As a result, the United Nations has imposed a ban on waste plastic trade unless it meets certain criteria."|2023-09-26-20-47-14
Plastic pollution|Types of plastic debris| There are three major forms of plastic that contribute to plastic pollution: micro-, macro-, and mega-plastics. Mega- and micro plastics have accumulated in highest densities in the Northern Hemisphere, concentrated around urban centers and water fronts. Plastic can be found off the coast of some islands because of currents carrying the debris. Both mega- and macro-plastics are found in packaging, footwear, and other domestic items that have been washed off of ships or discarded in landfills. Fishing-related items are more likely to be found around remote islands.   These may also be referred to as micro-, meso-, and macro debris. Plastic debris is categorized as either primary or secondary. Primary plastics are in their original form when collected. Examples of these would be bottle caps, cigarette butts, and microbeads.  Secondary plastics, on the other hand, account for smaller plastics that have resulted from the degradation of primary plastics.|2023-09-26-20-47-14
Plastic pollution|Microdebris|" Microdebris are plastic pieces between 2 mm and 5 mm in size.  Plastic debris that starts off as meso- or macrodebris can become microdebris through degradation and collisions that break it down into smaller pieces.  Microdebris is more commonly referred to as nurdles.  Nurdles are recycled to make new plastic items, but they easily end up released into the environment during production because of their small size. They often end up in ocean waters through rivers and streams.  Microdebris that come from cleaning and cosmetic products are also referred to as scrubbers. Because microdebris and scrubbers are so small in size, filter-feeding organisms often consume them. Nurdles enter the ocean by means of spills during transportation or from land based sources. The Ocean Conservancy reported that China, Indonesia, Philippines, Thailand, and Vietnam dump more plastic in the sea than all other countries combined.  It is estimated that 10% of the plastics in the ocean are nurdles, making them one of the most common types of plastic pollution, along with plastic bags and food containers.   These micro-plastics can accumulate in the oceans and allow for the accumulation of Persistent Bio-accumulating Toxins such as bisphenol A, polystyrene, DDT, and PCB's which are hydrophobic in nature and can cause adverse health affects. A 2004 study by Richard Thompson from the University of Plymouth, UK, found a great amount of microdebris on beaches and in waters in Europe, the Americas, Australia, Africa, and Antarctica.  Thompson and his associates found that plastic pellets from both domestic and industrial sources were being broken down into much smaller plastic pieces, some having a diameter smaller than human hair.  If not ingested, this microdebris floats instead of being absorbed into the marine environment. Thompson predicts there may be 300,000 plastic items per square kilometre of sea surface and 100,000 plastic particles per square kilometre of seabed.  International Pellet Watch collected samples of polythene pellets from 30 beaches in 17 countries which were analysed for organic micro-pollutants. It was found that pellets found on beaches in the US, Vietnam and southern Africa contained compounds from pesticides suggesting a high use of pesticides in the areas.  In 2020 scientists created what may be the first scientific estimate of how much microplastic currently resides in Earth's seafloor, after investigating six areas of ~3 km depth ~300 km off the Australian coast. They found the highly variable microplastic counts to be proportionate to plastic on the surface and the angle of the seafloor slope. By averaging the microplastic mass per cm3, they estimated that Earth's seafloor contains ~14 million tons of microplastic – about double the amount they estimated based on data from earlier studies – despite calling both estimates ""conservative"" as coastal areas are known to contain much more microplastic. These estimates are about one to two times the amount of plastic thought – per Jambeck et al., 2015 – to currently enter the oceans annually."|2023-09-26-20-47-14
Plastic pollution|Macrodebris| Plastic debris is categorized as macrodebris when it is larger than 20 mm. These include items such as plastic grocery bags.  Macrodebris are often found in ocean waters, and can have a serious impact on the native organisms. Fishing nets have been prime pollutants. Even after they have been abandoned, they continue to trap marine organisms and other plastic debris. Eventually, these abandoned nets become too difficult to remove from the water because they become too heavy, having grown in weight up to 6 tonnes.|2023-09-26-20-47-14
Plastic pollution|Plastic production| 9.2 billion tonnes of plastic are estimated to have been made between 1950 and 2017. More than half this plastic has been produced since 2004. Of all the plastic discarded so far, 14% has been incinerated and less than 10% has been recycled.|2023-09-26-20-47-14
Plastic pollution|Decomposition of plastics| Plastics themselves contribute to approximately 10% of discarded waste. Many kinds of plastics exist depending on their precursors and the method for their polymerization. Depending on their chemical composition, plastics and resins have varying properties related to contaminant absorption and adsorption. Polymer degradation takes much longer as a result of saline environments and the cooling effect of the sea. These factors contribute to the persistence of plastic debris in certain environments.  Recent studies have shown that plastics in the ocean decompose faster than was once thought, due to exposure to sun, rain, and other environmental conditions, resulting in the release of toxic chemicals such as bisphenol A. However, due to the increased volume of plastics in the ocean, decomposition has slowed down.  The Marine Conservancy has predicted the decomposition rates of several plastic products. It is estimated that a foam plastic cup will take 50 years, a plastic beverage holder will take 400 years, a disposable nappy will take 450 years, and fishing line will take 600 years to degrade.|2023-09-26-20-47-14
Plastic pollution|Persistent organic pollutants| It was estimated that global production of plastics is approximately 250 mt/yr. Their abundance has been found to transport persistent organic pollutants, also known as POPs. These pollutants have been linked to an increased distribution of algae associated with red tides.|2023-09-26-20-47-14
Plastic pollution|Commercial pollutants|" In 2019, the group Break Free From Plastic organized over 70,000 volunteers in 51 countries to collect and identify plastic waste. These volunteers collected over ""59,000 plastic bags, 53,000 sachets and 29,000 plastic bottles,"" as reported by The Guardian. Nearly half of the items were identifiable by consumer brands. The most common brands were Coca-Cola, Nestlé, and Pepsico.   According to the global campaign coordinator for the project Emma Priestland in 2020, the only way to solve the problem is stopping production of single use plastic and using reusable products instead.   China is the biggest consumer of single-use plastics. Coca-Cola answered that ""more than 20% of our portfolio comes in refillable or fountain packaging"", they are decreasing the amount of plastic in secondary packaging. Nestlé responded that 87% of their packaging and 66% of their plastic packaging can be reused or recycled and by 2025 they want to make it 100%. By that year they want to reduce the consumption of virgin plastic by one third.[citation needed] Pepsico responded that they want to decrease ""virgin plastic in our beverage business by 35% by 2025"" and also expanding reuse and refill practices what should prevent 67 billion single use bottles by 2025."|2023-09-26-20-47-14
Plastic pollution|Plastic waste generation| The United States is the world leader in generating plastic waste, producing an annual 42 million metric tons of plastic waste.   Per capita generation of plastic waste in the United States is higher than in any other country, with the average American producing 130.09 kilograms of plastic waste per year. Other high-income countries, such as those of the EU-28 (annual per capita generation 58.56 kg), also have a high per capita plastic waste generation rate. Some high-income countries, such as Japan (annual per capital generation 38.44 kg), produce far less plastic waste per capita.|2023-09-26-20-47-14
Plastic pollution|Plastic pollution| The United States National Academy of Sciences estimated in 2022 that the worldwide entry of plastic into the ocean was 8 million metric tons of plastic per year.   A 2021 study by The Ocean Cleanup estimated that rivers convey between 0.8 and 2.7 million metric tons of plastic into the ocean, and ranked these river's countries.  The top ten were, from the most to the least: Philippines, India, Malaysia, China, Indonesia, Myanmar, Brazil, Vietnam, Bangladesh, and Thailand.|2023-09-26-20-47-14
Plastic pollution|Mismanaged plastic waste polluters| Top 12 mismanaged plastic waste polluters In 2018 approximately 513 million tonnes of plastics wind up in the oceans every year out of which the 83,1% is from the following 20 countries: China is the most mismanaged plastic waste polluter leaving in the sea the 27.7% of the world total, second Indonesia with the 10.1%, third Philippines with 5.9%, fourth Vietnam with 5.8%, fifth Sri Lanka 5.0%, sixth Thailand with 3.2%, seventh Egypt with 3.0%, eighth Malaysia with 2.9%, ninth Nigeria with 2.7%, tenth Bangladesh with 2.5%, eleventh South Africa with 2.0%, twelfth India with 1.9%, thirteenth Algeria with 1.6%, fourteenth Turkey with 1.5%, fifteenth Pakistan with 1.5%, sixteenth Brazil with 1.5%, seventeenth Myanmar with 1.4%, eighteenth Morocco with 1.0%, nineteenth North Korea with 1.0%, twentieth United States with 0.9%. The rest of world's countries combined wind up the 16.9% of the mismanaged plastic waste in the oceans, according to a study published by Science in 2015. All the European Union countries combined would rank eighteenth on the list. In 2020, a study revised the potential 2016 U.S. contribution to mismanaged plastic;  It estimated that U.S.-generated plastic might place the U.S. behind Indonesia and India in oceanic pollution, or it might place the U.S. behind Indonesia, India, Thailand, China, Brazil, Philippines, Egypt, Japan, Russia, and Vietnam.  In 2022, it was estimated all OECD countries (North America, Chile, Colombia, Europe, Israel, Japan, S. Korea) may contribute 5% of oceanic plastic pollution, with the rest of the world polluting 95%.   Since 2016 China ceased importing plastics for recycling and since 2019 international treaties signed by 187 countries restricted the export of plastics for recycling. A 2019 study calculated the mismanaged plastic waste, in millions of metric tonnes (Mt) per year:|2023-09-26-20-47-14
Plastic pollution|Total plastic waste polluters|" Around 275 million tonnes of plastic waste is generated each year around the world; between 4.8 million and 12.7 million tonnes is dumped into the sea.  About 60% of the plastic waste in the ocean comes from the top 5 countries: China, Indonesia, the Philippines, Thailand and Vietnam.  The table below list the top 20 plastic waste polluting countries in 2010 according to a study published by Science, Jambeck et al (2015). All the European Union countries combined would rank eighteenth on the list. In a study published by Environmental Science & Technology, Schmidt et al (2017) calculated that 10 rivers: two in Africa (the Nile and the Niger) and eight in Asia (the Ganges, Indus, Yellow, Yangtze, Hai He, Pearl, Mekong and Amur) ""transport 88–95% of the global plastics load into the sea."". The Caribbean Islands are the biggest plastic polluters per capita in the world. Trinidad and Tobago produces 1.5 kilograms of waste per capita per day, is the biggest plastic polluter per capita in the world. At least 0.19 kg per person per day of Trinidad and Tobago's plastic debris end up in the ocean, or for example Saint Lucia which generates more than four times the amount of plastic waste per capita as China and is responsible for 1.2 times more improperly disposed plastic waste per capita than China. Of the top thirty global polluters per capita, ten are from the Caribbean region. These are Trinidad and Tobago, Antigua and Barbuda, Saint Kitts and Nevis, Guyana, Barbados, Saint Lucia, Bahamas, Grenada, Anguilla and Aruba, according to a set of studies summarized by Forbes (2019)."|2023-09-26-20-47-14
Plastic pollution|Effects on the environment|" The distribution of plastic debris is highly variable as a result of certain factors such as wind and ocean currents, coastline geography, urban areas, and trade routes. Human population in certain areas also plays a large role in this. Plastics are more likely to be found in enclosed regions such as the Caribbean. It serves as a means of distribution of organisms to remote coasts that are not their native environments. This could potentially increase the variability and dispersal of organisms in specific areas that are less biologically diverse. Plastics can also be used as vectors for chemical contaminants such as persistent organic pollutants and heavy metals. Plastic pollution has also greatly negatively affected our environment. ""The pollution is significant and widespread, with plastic debris found on even the most remote coastal areas and in every marine habitat"".  This information tells us about how much of a consequential change plastic pollution has made on the ocean and even the coasts. In January 2022 a group of scientists defined a planetary boundary for ""novel entities"" (pollution, including plastic pollution) and found it has already been exceeded. According to co-author Patricia Villarubia-Gómez from the Stockholm Resilience Centre, ""There has been a 50-fold increase in the production of chemicals since 1950. This is projected to triple again by 2050"". There are at least 350,000 artificial chemicals in the world. They have mostly ""negative effects on planetary health"". Plastic alone contain more than 10,000 chemicals and create large problems. The researchers are calling for limit on chemical production and shift to circular economy, meaning to products that can be reused and recycled. The problem of ocean plastic debris is ubiquitous. It is estimated that 1.5–4% of global plastics production ends up in the oceans every year, mainly as a result of poor waste management infrastructure and practices combined with irresponsible attitudes to the use and disposal of plastics. The weathering of plastic debris causes its fragmentation into particles that even small marine invertebrates may ingest hence contaminating the food chain. Their small size renders them untraceable to their source and extremely difficult to remove from open ocean environments.  In the marine environment, plastic pollution causes ""Entanglement, toxicological effects via ingestion of plastics, suffocation, starvation, dispersal, and rafting of organisms, provision of new habitats, and introduction of invasive species are significant ecological effects with growing threats to biodiversity and trophic relationships. Degradation (changes in the ecosystem state) and modifications of marine systems are associated with loss of ecosystem services and values. Consequently, this emerging contaminant affects the socio-economic aspects through negative impacts on tourism, fishery, shipping, and human health"". In 2019 a new report ""Plastic and Climate"" was published. According to the report, in 2019, production and incineration of plastic will contribute greenhouse gases in the equivalent of 850 million tonnes of carbon dioxide (CO2) to the atmosphere. In current trend, annual emissions from these sources will grow to 1.34 billion tonnes by 2030. By 2050 plastic could emit 56 billion tonnes of greenhouse gas emissions, as much as 14 percent of the earth's remaining carbon budget.  By 2100 it will emit 260 billion tonnes, more than half of the carbon budget. Those are emission from production, transportation, incineration, but there are also releases of methane and effects on phytoplankton."|2023-09-26-20-47-14
Plastic pollution|Effects of plastic on land|" Plastic pollution on land poses a threat to the plants and animals – including humans who are based on the land.  Estimates of the amount of plastic concentration on land are between four and twenty three times that of the ocean. The amount of plastic poised on the land is greater and more concentrated than that in the water.  Mismanaged plastic waste ranges from 60 percent in East Asia and Pacific to one percent in North America. The percentage of mismanaged plastic waste reaching the ocean annually and thus becoming plastic marine debris is between one third and one half the total mismanaged waste for that year. In 2021 a report conducted by the Food and Agriculture Organization stated that plastic is often used in agriculture. There is more plastic in the soil than  in the oceans. The presence of plastic in the environment hurt ecosystems and human health and pose a threat to food safety. 
Chlorinated plastic can release harmful chemicals into the surrounding soil, which can then seep into groundwater or other surrounding water sources and also the ecosystem of the world.  This can cause serious harm to the species that drink the water. Plastic waste can clog storm drains, and such clogging can increase flood damage, particularly in urban areas.  A buildup of plastic garbage at trash cans raises the water level upstream and may enhance the risk of urban flooding.  For example, in Bangkok flood risk increases substantially because of plastic waste clogging the already overburdened sewer system. A 2017 study found that 83% of tap water samples taken around the world contained plastic pollutants.   This was the first study to focus on global drinking water pollution with plastics,  and showed that with a contamination rate of 94%, tap water in the United States was the most polluted, followed by Lebanon and India. European countries such as the United Kingdom, Germany and France had the lowest contamination rate, though still as high as 72%.  This means that people may be ingesting between 3,000 and 4,000 microparticles of plastic from tap water per year.  The analysis found particles of more than 2.5 microns in size, which is 2500 times bigger than a nanometer. It is currently unclear if this contamination is affecting human health, but if the water is also found to contain nano-particle pollutants, there could be adverse impacts on human well-being, according to scientists associated with the study. However, plastic tap water pollution remains under-studied, as are the links of how pollution transfers between humans, air, water, and soil. Mismanaged plastic waste leads to plastic directly or indirectly entering terrestrial ecosystems.  There has been a significant increase of microplastic pollution due to the poor handling and disposal of plastic materials.  In particular, plastic pollution in the form of microplastics now can be found extensively in soil. It enters the soil by settling on the surface and eventually making its way into subsoils.  These microplastics find their way into plants and animals. Effluent and sludge of wastewater contain large amounts of plastics. Wastewater treatment plants don't have a treatment process to remove microplastics which results in plastics being transferred into water and soil when effluent and sludge are applied to land for agricultural purposes.  Several researchers have found plastic microfibers that are released when fleece and other polyester textiles are cleaned in washing machines.  These fibers can be transferred through effluent to land which pollutes soil environments. The increase in plastic and microplastic pollution in soils can cause adverse impacts on plants and microorganisms in the soil, which can in turn affect soil fertility. Microplastics affect soil ecosystems that are important for plant growth. Plants are important for the environment and ecosystems so the plastics are damaging to plants and organisms living in these ecosystems. Microplastics alter soil biophysical properties which affect the quality of the soil. This affects soil biological activity, biodiversity and plant health. Microplastics in the soil alter a plant's growth. It decreases seedling germination, affects the number of leaves, stem diameter and chlorophyll content in these plants. Microplastics in the soil are a risk not only to soil biodiversity but also food safety and human health. Soil biodiversity is important for plant growth in agricultural industries. Agricultural activities such as plastic mulching and application of municipal wastes contribute to the microplastic pollution in the soil. Human-modified soils are commonly used to improve crop productivity but the effects are more damaging than helpful. Plastics also release toxic chemicals into the environment and cause physical, chemical harm and biological damage to organisms. Ingestion of plastic doesn't only lead to death in animals through intestinal blockage but it can also travel up the food chain which affects humans."|2023-09-26-20-47-14
Plastic pollution|Effects of plastic on oceans and seabirds|" Marine plastic pollution (or plastic pollution in the ocean) is a type of marine pollution by plastics, ranging in size from large original material such as bottles and bags, down to microplastics formed from the fragmentation of plastic material. Marine debris is mainly discarded human rubbish which floats on, or is suspended in the ocean. Eighty percent of marine debris is plastic.   Microplastics and nanoplastics result from the breakdown or photodegradation of plastic waste in surface waters, rivers or oceans. Recently, scientists have uncovered nanoplastics in heavy snow, more specifically about 3,000 tons that cover Switzerland yearly. It is estimated that there is a stock of 86 million tons of plastic marine debris in the worldwide ocean as of the end of 2013, assuming that 1.4% of global plastics produced from 1950 to 2013 has entered the ocean and has accumulated there.  Global ""consumption"" of plastics is estimated to be 300 million tonnes per year as of 2022, with around 8 million tonnes ending up in the oceans as macroplastics.   Approximately 1.5 million tonnes of primary microplastics end up in the seas. Around 98% of this volume is created by land-based activities, with the remaining 2% being generated by sea-based activities.    It is estimated that 19–23 million tonnes of plastic leaks into aquatic ecosystems annually.  The 2017 United Nations Ocean Conference estimated that the oceans might contain more weight in plastics than fish by the year 2050. Marine life is one of the most important when one is affected by plastic pollution. Plastic pollution puts animals' lives in danger and is in constant fear of extinction. Marine wildlife such as seabirds, whales, fish and turtles mistake plastic waste for prey; most then die of starvation as their stomachs become filled with plastic. They also suffer from lacerations, infections, reduced ability to swim, and internal injuries.  This evidence tells us how damaged marine wildlife is being affected by plastic pollution, they bring up how many animals mistake plastic for prey and eat it without knowing. ""Globally, 100,000 marine mammals die every year as a result of plastic pollution. This includes whales, dolphins, porpoises, seals and sea lions"".  This evidence tells us the statistics of how many marine mammals really are negatively affected enough to die from plastic pollution."|2023-09-26-20-47-14
Plastic pollution|Effects on freshwater ecosystems| Research into freshwater plastic pollution has been largely ignored over marine ecosystems, comprising only 13% of published papers on the topic. Plastics make their way into bodies of freshwater, underground aquifers, and moving freshwaters through runoff and erosion of mismanaged plastic waste (MMPW). In some areas, the direct waste disposal into rivers is a remaining factor of historical practices, and has only been somewhat limited by modern legislation.  Rivers are the primary transport of plastics into marine ecosystems, sourcing potentially 80% of the plastic pollution in the oceans.  Research on the top ten river catchments ranked by annual amount of MMPW showed that some rivers contribute as high as 88–95% of ocean-bound plastics, the highest being the Yangtze River into the East China Sea.  Asian rivers contribute nearly 67% of plastic waste found in the ocean annually, largely influenced by the high density coastal populations all throughout the continent as well as relatively intense bouts of seasonal rainfall. A study analyzing ingestion of plastics across a variety of previously published experiments showed that out of the 206 species covered, the majority of papers documented ingestion in fish.  This doesn't quite mean that fish ingest plastic more than other organisms, but instead highlights the underrepresentation of plastic effects in equally important organisms, like aquatic plants, amphibians and invertebrates. Despite this disparity, controlled experiments analyzing microplastic impact on aquatic plants like the algae Chlorella spp and common duckweed Lemna minor have yielded significant results. Between microplastics of polypropylene (PP) and polyvinyl chloride (PVC), PVC demonstrated greater toxicity to Chlorella pyrenoidosa, overall negatively impacting their photosynthetic ability. This effect on photosynthesis is likely due to the 60% reduction of algal chlorophyll a associated with high PVC concentrations found in the same study.  When analyzing the effect of polyethylene microbeads (origin: cosmetic exfoliants) on the aquatic macrophyte L. minor, no effect on photosynthetic pigments & productivity was found, but root growth and root cell viability decreased.  These results are concerning as plants and algae are integral to nutrient and gas cycling within an aquatic system, and have the capacity to create significant changes in water composition due to their sheer density. Crustaceans have also been analyzed for their response to plastic presence. There is proof that freshwater crustaceans, specifically European crabs and crayfish, suffer entanglement in polyamide ghost nets used in lake fishing.  When exposed to plastic nanoparticles of polystyrene, Daphnia galeata (common water flea) experienced reduced survival within 48 hours as well as reproductive issues. Over a span of 5 days, the amount of pregnant Daphnia decreased by nearly 50%, and less than 20% of exposed embryos survived without any immediate repercussions.  Other arthropods, like juvenile stages of insects are susceptible to similar plastic exposure as some spend part of their adolescence fully submerged in a freshwater resource. This similarity in lifestyle to other aquatic invertebrates indicates that insects may experience similar side effects of plastic exposure. Plastic exposure in amphibians has mostly been studied in adolescent life stages, when the test subjects are still dependent on an aquatic environment where it can be easier to manipulate variables experimentally. Studies on a common South American freshwater frog, Physalaemus cuvieri indicated that plastics may have the potential to induce mutagenic and cytotoxic morphological changes.  Much more research needs to be done on amphibian response to plastic pollution, especially since amphibians can serve as initial indicator species of environmental decline.  Freshwater mammals and birds have long been known to have negative interactions with plastic pollution, often resulting in entanglement or suffocation/choking after ingesting. While inflammation within the gastrointestinal tract in both groups has been noted, unfortunately there is little to no data on the toxicological effects of plastic pollutants in these organisms.  Fish have been studied the most regarding plastic pollution in freshwater organisms, with the majority of studies indicating evidence of plastic ingestion in wild-caught samples and lab specimens.  There have been some attempts to look at lethality of plastics in a common freshwater model species, Danio rerio, aka zebrafish. Increased mucus production and inflammation response in the D. rerio GI-tract was noted, but additionally, researchers noted a distinct shift in the microbial communities within the zebrafish intestinal microbiome.  This finding is significant, as research within the last few decades has increasingly revealed how much power intestinal microbiomes have regarding their host's nutrient absorption and endocrine systems.  Because of this, plastics may have a far more drastic effect on individual organism health than is currently known so far, thus warranting the need for further research as soon as possible. Many of these findings also have been found in a laboratory setting, so more effort needs to be channeled into measuring plastic abundance & toxicology in wild populations.|2023-09-26-20-47-14
Plastic pollution|Effects on humans|" Compounds that are used in manufacturing pollute the environment by releasing chemicals into the air and water. Some compounds that are used in plastics, such as phthalates, bisphenol A (BRA), polybrominated diphenyl ether (PBDE), are under close statute and might be very hurtful. Even though these compounds are unsafe, they have been used in the manufacturing of food packaging, medical devices, flooring materials, bottles, perfumes, cosmetics and much more. Inhalation of microplastics (MPs) have been shown to be one of the major contributors to MP uptake in humans. MPs in the form of dust particles are circulated constantly through ventilation and air conditioning systems indoors.  The large dosage of these compounds are hazardous to humans, destroying the endocrine system. BRA imitates the female's hormone called estrogen. PBD destroys and causes damage to thyroid hormones, which are vital hormone glands that play a major role in the metabolism, growth and development of the human body. MPs can also have a detrimental effect on male reproductive success. MPs such as BPA can interfere with steroid biosynthesis in the male endocrine system and with early stages of spermatogenesis.  MPs in men can also create oxidative stress and DNA damage in spermatozoa, causing reduced sperm viability.  Although the level of exposure to these chemicals varies depending on age and geography, most humans experience simultaneous exposure to many of these chemicals. Average levels of daily exposure are below the levels deemed to be unsafe, but more research needs to be done on the effects of low dose exposure on humans. A lot is unknown on how severely humans are physically affected by these chemicals. Some of the chemicals used in plastic production can cause dermatitis upon contact with human skin. In many plastics, these toxic chemicals are only used in trace amounts, but significant testing is often required to ensure that the toxic elements are contained within the plastic by inert material or polymer. Children and women during their reproduction age are at most at risk and more prone to damaging their immune as well as their reproductive system from these hormone-disrupting chemicals. Pregnancy and nursing products such as baby bottles, pacifiers, and plastic feeding utensils place infants and children at a very high risk of exposure. Human health has also been negatively impacted by plastic pollution. ""Almost a third of groundwater sites in the US contain BPA. BPA is harmful at very low concentrations as it interferes with our hormone and reproductive systems.  This quote tells us how much of a percentage of our water is contaminated and should not be drunk on a daily basis. ""At every stage of its lifecycle, plastic poses distinct risks to human health, arising from both exposure to plastic particles themselves and associated chemicals"".  This quote is an intro to numerous points of why plastic is damaging to us, such as the carbon that is released when it is being made and transported which is also related to how plastic pollution harms our environment. A 2022 study published in Environment International found microplastic in the blood of 80% of people tested in the study, and such microplastic has the potential to become embedded in human organs. Due to the pervasiveness of plastic products, most of the human population is constantly exposed to the chemical components of plastics. In the United States, 95% of adults have had detectable levels of BPA in their urine. Exposure to chemicals such as BPA have been correlated with disruptions in fertility, reproduction, sexual maturation, and other health effects.  Specific phthalates have also resulted in similar biological effects. Bisphenol A affects gene expression related to the thyroid hormone axis, which affects biological functions such as metabolism and development. BPA can decrease thyroid hormone receptor (TR) activity by increasing TR transcriptional corepressor activity. This then decreases the level of thyroid hormone binding proteins that bind to triiodothyronine. By affecting the thyroid hormone axis, BPA exposure can lead to hypothyroidism. BPA can disrupt normal, physiological levels of sex hormones. It does this by binding to globulins that normally bind to sex hormones such as androgens and estrogens, leading to the disruption of the balance between the two. BPA can also affect the metabolism or the catabolism of sex hormones. It often acts as an antiandrogen or as an estrogen, which can cause disruptions in gonadal development and sperm production."|2023-09-26-20-47-14
Plastic pollution|Disease|" In 2023, plasticosis, a new disease caused solely by plastics, was discovered in seabirds.
The birds identified as having the disease have scarred digestive tracts from ingesting plastic waste.   ""When birds ingest small pieces of plastic, they found, it inflames the digestive tract. Over time, the persistent inflammation causes tissues to become scarred and disfigured, affecting digestion, growth and survival."""|2023-09-26-20-47-14
Plastic pollution|Reduction efforts|" Efforts to reduce the use of plastics, to promote plastic recycling and to reduce mismanaged plastic waste or plastic pollution have occurred or are ongoing. The first scientific review in the professional academic literature about global plastic pollution in general found that the rational response to the ""global threat"" would be ""reductions in consumption of virgin plastic materials, along with internationally coordinated strategies for waste management"" – such as banning export of plastic waste unless it leads to better recycling – and describes the state of knowledge about ""poorly reversible"" impacts which are one of the rationales for its reduction. Some supermarkets charge their customers for plastic bags, and in some places more efficient reusable or biodegradable materials are being used in place of plastics. Some communities and businesses have put a ban on some commonly used plastic items, such as bottled water and plastic bags.  Some non-governmental organizations have launched voluntary plastic reduction schemes like certificates that can be adapted by restaurants to be recognized as eco-friendly among customers. In January 2019 a ""Global Alliance to End Plastic Waste"" was created by companies in the plastics industry. The alliance aims to clean the environment from existing waste and increase recycling, but it does not mention reduction in plastic production as one of its targets.  Moreover, subsequent reporting has suggested the group is a greenwashing initiative. On 2 March 2022 in Nairobi, representatives of 175 countries pledged to create a legally binding agreement to end plastic pollution. The agreement should address the full lifecycle of plastic and propose alternatives including reusability. An Intergovernmental Negotiating Committee (INC) that should conceive the agreement by the end of the year 2024 was created. The agreement should facilitate the transition to a circular economy, which will reduce GHG emissions by 25%. Inger Andersen, executive director of UNEP called the decision ""a triumph by planet earth over single-use plastics"". In the lead up to the Assembly, global public opinion on a plastic treaty was surveyed, analysed and reported by The Plastic Free Foundation in partnership with Ipsos and WWF-International. The report identified that nearly 90% of survey participants – over 20,000 adults across 28 countries – believed that having a global plastics treaty will help to effectively address the plastic pollution crisis."|2023-09-26-20-47-14
Plastic pollution|Biodegradable and degradable plastics| The use of biodegradable plastics has many advantages and disadvantages. Biodegradables are biopolymers that degrade in industrial composters. Biodegradables do not degrade as efficiently in domestic composters, and during this slower process, methane gas may be emitted. There are also other types of degradable materials that are not considered to be biopolymers, because they are oil-based, similar to other conventional plastics. These plastics are made to be more degradable through the use of different additives, which help them degrade when exposed to UV rays or other physical stressors.  yet, biodegradation-promoting additives for polymers have been shown not to significantly increase biodegradation. Although biodegradable and degradable plastics have helped reduce plastic pollution, there are some drawbacks. One issue concerning both types of plastics is that they do not break down very efficiently in natural environments. There, degradable plastics that are oil-based may break down into smaller fractions, at which point they do not degrade further. A parliamentary committee in the United Kingdom also found that compostable and biodegradable plastics could add to marine pollution because there is a lack of infrastructure to deal with these new types of plastic, as well as a lack of understanding about them on the part of consumers.  For example, these plastics need to be sent to industrial composting facilities to degrade properly, but no adequate system exists to make sure waste reaches these facilities.  The committee thus recommended to reduce the amount of plastic used rather than introducing new types of it to the market. Also worth noting is the evolution of new enzymes allowing microorganisms living in polluted locations to digest normal, hard-to-degrade plastic.   An 2021 study looking for homologs of 95 known plastic-degrading enzymes spanning 17 plastic types found a further 30,000 possible enzymes. Despite their apparent ubiquity, there is no current evidence that these novel enzymes are breaking down any meaningful amount of plastic to reduce pollution.|2023-09-26-20-47-14
Plastic pollution|Incineration| Up to 60% of used plastic medical equipment is incinerated rather than deposited in a landfill as a precautionary measure to lessen the transmission of disease. This has allowed for a large decrease in the amount of plastic waste that stems from medical equipment. At a large scale, plastics, paper, and other materials provides waste-to-energy plants with useful fuel. About 12% of total produced plastic has been incinerated.  Many studies have been done concerning the gaseous emissions that result from the incineration process.  Incinerated plastics release a number of toxins in the burning process, including Dioxins, Furans, Mercury and Polychlorinated Biphenyls.  When burned outside of facilities designed to collect or process the toxins, this can have significant health effects and create significant air pollution.|2023-09-26-20-47-14
Plastic pollution|Policy|" Agencies such as the US Environmental Protection Agency and US Food and Drug Administration often do not assess the safety of new chemicals until after a negative side effect is shown. Once they suspect a chemical may be toxic, it is studied to determine the human reference dose, which is determined to be the lowest observable adverse effect level. During these studies, a high dose is tested to see if it causes any adverse health effects, and if it does not, lower doses are considered to be safe as well. This does not take into account the fact that with some chemicals found in plastics, such as BPA, lower doses can have a discernible effect.  Even with this often complex evaluation process, policies have been put into place in order to help alleviate plastic pollution and its effects. Government regulations have been implemented that ban some chemicals from being used in specific plastic products. In Canada, the United States, and the European Union, BPA has been banned from being incorporated in the production of baby bottles and children's cups, due to health concerns and the higher vulnerability of younger children to the effects of BPA.  Taxes have been established in order to discourage specific ways of managing plastic waste. The landfill tax, for example, creates an incentive to choose to recycle plastics rather than contain them in landfills, by making the latter more expensive.  There has also been a standardization of the types of plastics that can be considered compostable.  The European Norm EN 13432, which was set by the European Committee for Standardization (CEN), lists the standards that plastics must meet, in terms of compostability and biodegradability, in order to officially be labeled as compostable. Given the significant threat that oceans face, the European Investment Bank Group aims to increase its funding and advisory assistance for ocean cleanup. For example, the Clean Oceans Initiative (COI) was established in 2018. The European Investment Bank, the German Development Bank, and the French Development Agency (AFD) agreed to invest a total of €2 billion under the COI from October 2018 to October 2023 in initiatives aimed at reducing pollution discharge into the oceans, with a special focus on plastics. The Clean Ocean Initiative plans to give €4 billion in funding towards decreasing plastic waste at sea by the end of 2025. Improved wastewater treatment in Sri Lanka, Egypt, and South Africa are some examples, as is solid waste management in Togo and Senegal. Major plastic producers continue to lobby governments to refrain from imposing restrictions on plastic production and to advocate for voluntary corporate targets to reduce new plastic output. However, the world's top 10 plastic producers, including The Coca-Cola Company, Nestle SA and PepsiCo have been failing to meet even their own minimum targets for virgin plastic use. The export of plastic waste from rich countries to poorer countries has been well documented.
Differences between countries in environmental policy and costs relating to taxes, disposal, and transport, are important determinants on legal and illegal international traffic in hazardous and nonhazardous waste and scrap products, including plastics. There have been several international covenants which address marine plastic pollution, such as the Convention on the Prevention of Marine Pollution by Dumping of Wastes and Other Matter 1972, the International Convention for the Prevention of Pollution from Ships, 1973 and the Honolulu Strategy, there is nothing around plastics which infiltrate the ocean from the land. In 2019, the Basel Convention was amended to include plastic waste.  187 countries agreed to limit the export of plastic waste following rules from the Basel Convention.  The Convention prohibits Parties from trading with non-Parties (e.g. United States) unless the countries have a pre-determined agreement that meets Basel criteria.  During January 2021, the first month that the agreement was in effect, trade data showed that overall scrap exports from the U.S. actually increased. Some academics and NGOs believe that a legally binding international treaty to deal with plastic pollution is necessary. They think this because plastic pollution is an international problem, moving between maritime borders, and also because they believe there needs to be a cap on plastic production.    Lobbyists were hoping that UNEA-5 would lead to a plastics treaty, but the session ended without a legally binding agreement. In 2022, countries agreed to devise a Global plastic pollution treaty by 2024. Since around 2017, China,  Turkey,  Malaysia,  Cambodia,  and Thailand  have banned certain waste imports. It has been suggested that such bans may increase automation  and recycling, decreasing negative impacts on the environment. According to an analysis of global trade data by the nonprofit Basel Action Network, violations of the Basel Convention, active since January 1, 2021, have been rampant during 2021. The U.S., Canada, and the European Union have sent hundreds of millions of tons of plastic to countries with insufficient waste management infrastructure, where much of it is landfilled, burned, or littered into the environment. Laws related to recyclability, waste management, domestic materials recovery facilities, product composition, biodegradability and prevention of import/export of specific wastes may support prevention of plastic pollution.[citation needed] A study considers producer/manufacturer responsibility ""a practical approach toward addressing the issue of plastic pollution"", suggesting that ""Existing and adopted policies, legislations, regulations, and initiatives at global, regional, and national level play a vital role"". Standardization of products, especially of packaging  [additional citation(s) needed] which are, as of 2022, often composed of different materials (each and across products) that are hard or currently impossible to either separate or recycle together in general or in an automated way   could support recyclability and recycling. For instance, there are systems that can theoretically distinguish between and sort 12 types of plastics such as PET using hyperspectral imaging and algorithms developed via machine learning   while only an estimated 9% of the estimated 6.3 billion tonnes of plastic waste from the 1950s up to 2018 has been recycled (12% has been incinerated and the rest reportedly being ""dumped in landfills or the natural environment"")."|2023-09-26-20-47-14
Plastic pollution|Collection, recycling and reduction|" The two common forms of waste collection include curbside collection and the use of drop-off recycling centers. About 87 percent of the population in the United States (273 million people) have access to curbside and drop-off recycling centers. In curbside collection, which is available to about 63 percent of the United States population (193 million people), people place designated plastics in a special bin to be picked up by a public or private hauling company.  Most curbside programs collect more than one type of plastic resin, usually both PETE and HDPE.  At drop-off recycling centers, which are available to 68 percent of the United States population (213 million people), people take their recyclables to a centrally located facility.  Once collected, the plastics are delivered to a materials recovery facility (MRF) or handler for sorting into single-resin streams to increase product value. The sorted plastics are then baled to reduce shipping costs to reclaimers. There are varying rates of recycling per type of plastic, and in 2017, the overall plastic recycling rate was approximately 8.4% in the United States. Approximately 2.7 million tonnes (3.0 million short tons) of plastics were recycled in the U.S. in 2017, while 24.3 million tonnes (26.8 million short tons) plastic were dumped in landfills the same year. Some plastics are recycled more than others; in 2017 about 31.2 percent of HDPE bottles and 29.1 percent of PET bottles and jars were recycled. Reusable packaging refers to packaging that is manufactured of durable materials and is specifically designed for multiple trips and extended life. There are zero-waste stores and refill shops   for selected products as well as conventional supermarkets that enable refilling of selected plastics-packaged products or voluntarily sell products with no or more sustainable packaging. On 21 May 2019, a new service model called ""Loop"" to collect packaging from consumers and reuse it, began to function in the New York region, US, supported by multiple larger companies. Consumers drop packages in special shipping totes and then a pick up collect, clean, refill and return them.  It has begun with several thousand households and aims to not only stop single use plastic, but to stop single use generally by recycling consumer product containers of various materials. Another effective strategy, that could be supported by policies, is eliminating the need for plastic bottles such as by using refillable e.g. steel bottles,  and water carbonators, [additional citation(s) needed] which may also prevent potential negative impacts on human health due to microplastics release. Reducing plastic waste could support recycling and is often taken together with recycling: the ""3R"" refer to Reduce, Reuse and Recycle. The organization ""The Ocean Cleanup"" is trying to collect plastic waste from the oceans by nets. There are concerns from harm to some forms of sea organisms, especially neuston. In the Netherlands, plastic litter from some rivers is collected by a bubble barrier, to prevent plastics from floating into the sea. This so-called 'Great Bubble Barrier' catches plastics bigger than 1 mm.   The bubble barrier is implemented in the River IJssel (2017) and in Amsterdam (2019)   and will be implemented in Katwijk at the end of the river Rhine."|2023-09-26-20-47-14
Plastic pollution|Mapping and tracking| Our World In Data provides graphics about some analyses, including maps, to show sources of plastic pollution   – including that of oceans in specific. Identifying largest sources of ocean plastics in high fidelity may help to discern causes, to measure progress and to develop effective countermeasures. A large fraction of ocean plastics may come from – also non-imported (see above) – plastic waste of coastal cities  as well as from rivers (with top 1000 rivers estimated by one 2021 study to account for 80% of global annual emissions).  These two sources may be interlinked.  The Yangtze river into the East China Sea is identified by some studies that use sampling evidence as the highest plastic-emitting (sampled) river,   in contrast to the beforementioned 2021 study that ranks it at place 64.  Management interventions at the local level at coastal areas were found to be crucial to the global success of reducing plastic pollution. There is one global, interactive machine learning- and satellite monitoring-based, map of plastic waste sites which could help identify who and where mismanages plastic waste, dumping it into oceans.|2023-09-26-20-47-14
Plastic pollution|By country/region|" In July 2018, Albania became the first country in Europe to ban lightweight plastic bags.    Albania's environment minister Blendi Klosi said that businesses importing, producing or trading plastic bags less than 35 microns in thickness risk facing fines between 1 million to 1.5 million lek (€7,900 to €11,800). It has been estimated that each year, Australia produces around 2.5m tonnes of plastic waste annually, of which about 84% ends up as landfill, and around 130,000 tonnes of plastic waste leaks into the environment.  Six of the eight states and territories had by December 2021 committed to banning a range of plastics. The federal government's National Packaging Targets created the goal of phasing out the worst of single-use plastics by 2025,  and under the National Plastics Plan 2021,  it has committed ""to phase out loose fill and moulded polystyrene packaging by July 2022, and various other products by December 2022. An Australian single-use plastic reduction initiative, Plastic Free July, that began in 2011 in Perth, Western Australia has gained a significant global outreach.  As of 2022, it had a record 140 million participants making conscious changes and reducing their waste by 2.6 million tonnes in 2022.   In 2022, in recognition of its contributions to promoting single-use plastic pollution solutions, Plastic Free July was one of two finalists in the annual UN Sustainable Development Action Awards. In the year 2022 Canada announced a ban on producing and importing single use plastic from December 2022. The sale of those items will be banned from December 2023 and the export from 2025. The prime minister of Canada Justin Trudeau pledged to ban single use plastic in 2019. China is the biggest consumer of single-use plastics.  In 2020 China published a plan to cut 30% of plastic waste in 5 years. As part of this plan, single use plastic bags and straws will be banned In 2015 the European Union adopted a directive requiring a reduction in the consumption of single use plastic bags per person to 90 by 2019 and to 40 by 2025.  In April 2019, the EU adopted a further directive banning almost all types of single use plastic, except bottles, from the beginning of the year 2021. On 3 July 2021, the EU Single-Use Plastics Directive (SUPD, EU 2019/904) went into effect in EU member states. The directive aims to reduce plastic pollution from single-use disposable plastics. It focuses on the 10 most commonly found disposable plastics at beaches, which make up 43% of marine litter (fishing gear another 27%). According to the SUP directive, there is a ban on: plastic cotton buds and balloon sticks; plastic plates, cutlery, stirrers and straws; Styrofoam drinks and food packaging (f.e. disposable cups, one-person meals); products made of oxo-degradable plastics, which degrade into microplastics. cigarette filters, drinking cups, wet wipes, sanitary towels and tampons receive a label indicating the product contains plastic, that it belongs in the trash, and that litter has negative effects on the environment. In December 2022 the EU took the first steps for banning the export of plastic waste to other countries. In 2021 France banned ""free plastic bottles, plastic confetti, and single-use plastic bags"", in 2022 restrictions were made on plastic packaging and toys and in the first of January 2023 many types of single use plastic were banned from restaurants that have more than 20 places. Some were concerned the measures will not be implemented well due to the current energy crisis. The government of India decided to ban single use plastics and take a number of measures to recycle and reuse plastic, from 2 October 2019 The Ministry of Drinking Water and Sanitation, Government of India, has requested various governmental departments to avoid the use of plastic bottles to provide drinking water during governmental meetings, etc., and to instead make arrangements for providing drinking water that do not generate plastic waste.   The state of Sikkim has restricted the usage of plastic water bottles (in government functions and meetings) and styrofoam products.  The state of Bihar has banned the usage of plastic water bottles in governmental meetings. The 2015 National Games of India, organised in Thiruvananthapuram, was associated with green protocols.  This was initiated by Suchitwa Mission that aimed for ""zero-waste"" venues. To make the event ""disposable-free"", there was ban on the usage of disposable water bottles.  The event witnessed the usage of reusable tableware and stainless steel tumblers.  Athletes were provided with refillable steel flasks.  It is estimated that these green practices stopped the generation of 120 tonnes of disposable waste. The city of Bangalore in 2016 banned the plastic for all purpose other than for few special cases like milk delivery etc. The state of Maharashtra, India effected the Maharashtra Plastic and Thermocol Products ban 23 June 2018, subjecting plastic users to fines and potential imprisonment for repeat offenders. In the year 2022 India has begun to implement a country wide ban on different sorts of plastic. This is necessary also for achieving the climate targets of the country as in plastic production are used more than 8,000 additives,  part of them are thousands times more powerful greenhouse gases than CO2. In Bali, one of the many islands of Indonesia, two sisters, Melati and Isabel Wijsen, made efforts to ban plastic bags in 2019.   As of January 2022 their organization Bye Bye Plastic Bags had spread to over 50 locations around the world. In Israel, two cities: Eilat and Herzliya, decided to ban the usage of single use plastic bags and cutlery on the beaches.  In 2020 Tel Aviv joined them, banning also the sale of single use plastic on the beaches. In August 2017, Kenya has one of the world's harshest plastic bag bans. Fines of $38,000 or up to four years in jail to anyone that was caught producing, selling, or using a plastic bag. New Zealand announced a ban on many types of hard-to-recycle single use plastic by 2025. In 2019, The House of Representatives of Nigeria banned the production, import and usage of plastic bags in the country. Spain banned several types of single use plastic at the beginning of the year 2023. In February 2018, Taiwan restricted the use of single-use plastic cups, straws, utensils and bags; the ban will also include an extra charge for plastic bags and updates their recycling regulations and aiming by 2030 it would be completely enforced. In January 2019, the Iceland supermarket chain, which specializes in frozen foods, pledged to ""eliminate or drastically reduce all plastic packaging for its store-brand products by 2023."" As of 2020, 104 communities achieved the title of ""Plastic free community"" in United Kingdom, 500 want to achieve it. After two schoolgirls Ella and Caitlin launched a petition about it, Burger King and McDonald's in the United Kingdom and Ireland pledged to stop sending plastic toys with their meals. McDonald's pledged to do it from the year 2021. McDonald's also pledged to use a paper wrap for it meals and books that will be sent with the meals. The transmission will begin already in March 2020. From October 2023 many types of single use plastic will be banned in England including cutlery and plates. Scotland and Wales have already implemented such bans. In 2009, Washington University in St. Louis became the first university in the United States to ban the sale of plastic, single-use water bottles. In 2009, the District of Columbia required all businesses that sell food or alcohol to charge an additional 5 cents for each carryout plastic or paper bag. In 2011 and 2013, Kauai, Maui and Hawaii prohibit non-biodegradable plastic bags at checkout as well as paper bags containing less than 40 percent recycled material. In 2015, Honolulu was the last major county approving the ban. In 2015, California prohibited large stores from providing plastic bags, and if so a charge of $0.10 per bag and has to meet certain criteria. In 2016, Illinois adopted the legislation and established ""Recycle Thin Film Friday"" in effort toe reclaim used thin-film plastic bags and encourage reusable bags. In 2019, the state New York banned single use plastic bags and introduced a 5-cent fee for using single use paper bags. The ban will enter into force in 2020. This will not only reduce plastic bag usage in New York state (23 billion every year until now), but also eliminate 12 million barrels of oil used to make plastic bags used by the state each year. The state of Maine ban Styrofoam (polystyrene) containers in May 2019. In 2019 the Giant Eagle retailer became the first big US retailer that committed to completely phase out plastic by 2025. The first step – stop using single use plastic bags – will begun to be implemented already on January 15, 2020. In 2019, Delaware, Maine, Oregon and Vermont enacted on legislation. Vermont also restricted single-use straws and polystyrene containers. In 2019, Connecticut imposed a $0.10 charge on single-use plastic bags at point of sale, and is going to ban them on July 1, 2021. On July 30, 2017, Vanuatu's Independence Day, made an announcement of stepping towards the beginning of not using plastic bags and bottles. Making it one of the first Pacific nations to do so and will start banning the importation of single-use plastic bottles and bags."|2023-09-26-20-47-14
Plastic pollution|Obstruction by major plastic producers| The ten corporations that produce the most plastic on the planet, The Coca-Cola Company, Colgate-Palmolive, Danone, Mars, Incorporated, Mondelēz International, Nestlé, PepsiCo, Perfetti Van Melle, Procter & Gamble, and Unilever, formed a well-financed network that has sabotaged for decades government and community efforts to address the plastic pollution crisis, according to a detailed investigative report by the Changing Markets Foundation. The investigation documents how these companies delay and derail legislation so that they can continue to inundate consumers with disposable plastic packaging. These large plastic producers have exploited public fears of the COVID-19 pandemic to work toward delaying and reversing existing regulation of plastic disposal. Big ten plastic producers have advanced voluntary commitments for plastic waste disposal as a stratagem to deter governments from imposing additional regulations.|2023-09-26-20-47-14
Plastic pollution|Deception of the public about recycling|" As early as the early 1970s, petrochemical industry leaders understood that the vast majority of plastic they produced would never be recycled. For example, an April 1973 report written by industry scientists for industry executive states that sorting the hundreds of different kinds plastic is ""infeasible"" and cost-prohibitive. By the late 1980s, industry leaders also knew that the public must be kept feeling good about purchasing plastic products if their industry was to continue to prosper, and needed to quell proposed legislation to regulate the plastic being sold. So the industry launched a $50 million/year corporate propaganda campaign targeting the American public with the message that plastic can be, and is being, recycled, and lobbied American municipalities to launch expensive plastic waste collection programs, and lobbied U.S. states to require the labeling of plastic products and containers with recycling symbols. They were confident, however, that the recycling initiatives would not end up recovering and reusing plastic in amounts anywhere near sufficient to hurt their profits in selling new ""virgin"" plastic products because they understood that the recycling efforts that they were promoting were likely to fail. Industry leaders more recently have planned 100% recycling of the plastic they produce by 2040, calling for more efficient collection, sorting and processing."|2023-09-26-20-47-14
Plastic pollution|Earth Day| In 2019, the Earth Day Network partnered with Keep America Beautiful and National Cleanup Day for the inaugural nationwide Earth Day CleanUp. Cleanups were held in all 50 states, five US territories, 5,300 sites and had more than 500,000 volunteers. Earth Day 2020 is the 50th Anniversary of Earth Day. Celebrations will include activities such as the Great Global CleanUp, Citizen Science, Advocacy, Education, and art. This Earth Day aims to educate and mobilize more than one billion people to grow and support the next generation of environmental activists, with a major focus on plastic waste|2023-09-26-20-47-14
Plastic pollution|World Environment Day|" Every year, 5 June is observed as World Environment Day to raise awareness and increase government action on the pressing issue. In 2018, India was host to the 43rd World Environment Day and the theme was ""Beat Plastic Pollution"", with a focus on single-use or disposable plastic. The Ministry of Environment, Forest, and Climate Change of India invited people to take care of their social responsibility and urged them to take up green good deeds in everyday life. Several states presented plans to ban plastic or drastically reduce their use."|2023-09-26-20-47-14
Plastic pollution|Other actions| On 11 April 2013 in order to create awareness, artist Maria Cristina Finucci founded The Garbage Patch State at UNESCO  headquarters in Paris, France, in front of Director General Irina Bokova. This was the first of a series of events under the patronage of UNESCO and of the Italian Ministry of the Environment. Mexico City implemented a ban on single-use plastics, starting with plastic bags in 2020 and expanding to items like utensils, straws, and to-go trays in 2021. In 2020, China disclosed a three-part proposal to reduce plastic pollution. The plan includes a nationwide prohibition on single-use plastics, introduced as the country's plastic waste had risen to an anticipated 45 million tons in 2025, partly as a result of a surge in e-commerce packaging.|2023-09-26-20-47-14
Plastic pollution|Sources| This article incorporates text from a free content work.  Licensed under Cc BY-SA 3.0 IGO (license statement/permission). Text taken from Drowning in Plastics – Marine Litter and Plastic Waste Vital Graphics​,   United Nations Environment Programme.|2023-09-26-20-47-14
Plastic pollution in the Mediterranean sea|General|" The Mediterranean Sea has been defined as one of the seas most affected by marine plastic pollution.       [excessive citations] It has concentrations of microplastics which are estimated to be higher than those on average found at the global level.  Studies conducted within the WWF Mediterranean Marine Initiative of 2019  have estimated that 0.57 million metric tons of plastic enter the Mediterranean Sea every year; this quantity corresponds to the dumping of 33.800 bottles made of plastic into waters every minute, representing important risks for marine ecosystems, human health, but also for the blue economy of the area, whose coastal zones are very densely populated and among the first tourist destinations worldwide. Marine plastic pollution was found in Mediterranean waters in amounts similar to those present in the ocean gyres (Indian Ocean Gyre, North Atlantic Gyre, North Pacific Gyre, South Atlantic Gyre, South Pacific Gyre).  Therefore, the Mediterranean Sea is oftentimes being defined as the ""world's sixth greatest accumulation zone"" for marine plastic litter     or as an invisible ""sixth garbage patch"", primarily composed of microplastics.  This is an invisible garbage patch as there is no trace of permanent litter accumulation areas in the Mediterranean Sea, primarily because of the semi-enclosed shape of its basin, the cyclonic circulation and the currents present in the region. The Mediterranean Sea receives waste from coastal areas and from waters, such as rivers (like in the case of the Nile river, which, as of 2017,  brought around 200 tonnes of plastic waste into the Mediterranean basin yearly).
A World Wide Fund for Nature report of 2019  estimates that, considering the Mediterranean countries, around 70% of plastic pollution coming from water-based sources comes from three areas: Egypt (41.3%), Turkey (19.1%) and Italy (7.6%). Plastic litter originating from land-based sources is instead estimated to be coming from, in decreasing order: Turkey, Morocco, Israel, Spain, France, Syria, Egypt, Albania, Tunisia and Italy. Initiatives are being implemented at various levels to reduce and end the problem of marine plastics pollution in the Mediterranean Sea; however, the governance of this problem is very complex because of the nature of such plastics (especially microplastics), the transboundary character of this matter, the difficulties connected with the multiplicity of the actors involved, the increasing levels of production of plastics and the issues connected with responsibility at different levels."|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|History of research on the problem|" Studies on marine plastic pollution began in the 1970s, when plastic debris was found in gyres in the Sargasso Sea and the first scientific findings were published in 1972. Around a decade later, in the 1980s, the first researches focusing on the problem of marine plastic pollution in the Mediterranean Sea were published. The first  study on the Mediterranean basin focused on an area 40 miles SW of Malta and reported the presence of floating debris, the majority  of which was constituted by plastic. This body of research focusing on the Mediterranean continued growing in the 1990s, with numerous studies on the impacts, especially biological, of plastic pollution in the marine environment  and with the start of interstate discussions, primarily in the form of conferences such as, among others, that of the ""International Symposium on plastics in Mediterranean countries"".  However, the real increase in the number and variety of such studies only came later, around 2010, when more pieces of information on the quantity and distribution of plastics in the Mediterranean Sea was made accessible, when the knowledge on microplastics was being spread and when the impacts started to be addressed through diverse investigations as a concern. Recent researches, conducted at various regional, national and international levels, address the impacts of plastic pollution in the Mediterranean Sea from ecological, biological, economic and social points of view and the possibilities in terms of reduction efforts and governance of the problem, also while awaiting the prospective Global plastic pollution treaty."|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Types of plastics found in the Mediterranean Sea|" Plastics accounts for 80% of waste dispersed in the marine and coastal environment of the Mediterranean Sea. 
Recent studies focus on the types of plastics found and primarily on the issue of microplastics, both at a global but also at a regional level, as in the case of the Mediterranean Sea, which was identified as a ""target hotspot of the world"" due to its amounts of microplastics, around four times higher than those present in the North Pacific Ocean.  In the Mediterranean Sea, microplastics are found on surface waters, as well as on beaches and deep seafloor. 94% of microplastic items present in the Mediterranean Sea are thought to be in the seabed, 5% on beaches and 1% on surface waters."|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Plastics on Mediterranean beaches and surface waters|" Plastics has been identified as the primary source of litter found on beaches of the Mediterranean Sea. 
Researches conducted between 2019 and 2023 within the framework of the COMMON project (""COastal Management and MOnitoring Network for tackling marine litter in the Mediterranean Sea"", financed by the European Union through the ENI CBC MED programme ) show that, out of more than 90.000 collected objects on beaches of Italy, Tunisia and Lebanon, 17.000 are cigarette butts and 6.000 are cotton buds sticks, which contain plastics. Earlier researches, conducted in 2016, present similar findings and show that the ten ""top marine litter items"" found on beaches and surface waters of the Mediterranean Sea were, from the most abundant: cigarette butts and filters, plastic and polystyrene pieces, plastic caps and lids and plastic rings from bottle caps and lids, cotton bud sticks, nets and pieces of nets, bottles, foam sponge, cigarette packets, plastic bags and what remains from these when they are ripped off. These items, primarily made of single-use plastics, account for more than 60% of marine waste in the beaches and surface waters of the Mediterranean Sea. The surface waters of the Mediterranean Sea present concentrations of microplastics that, according to a 2015 study (UNEP/MAP),  are above 100.000 objects per km2, with more than 64.000.000 floating particles per km2.  As of 2019, the most common types of microplastics found are polyethylene, polystyrene, polyester and polypropylene."|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Plastics in the Mediterranean seafloor|" Plastic litter that accumulates in the Mediterranean Sea is fragmented into small particles that then tend to gather in the seabed. 
Studies aimed at analysing plastic pollution in the sea floor of the Mediterranean have shown that plastic debris can be found in every depth from 900 m to 3000 m; plastic litter could be found in 92.8% of the surveys collected from the Mediterranean deep-sea. Particular attention is now being devoted to the presence of microplastics and nanoplastics in the Mediterranean seabed and sediments, the two regions with the highest levels of microplastics in the Mediterranean Sea.  
In 2020, scientists from the University of Manchester analysed sediment samples of the Mediterranean Sea and identified the highest concentration ever recorded of microplastics on sediments of the sea floor; the scientists also discovered that such microplastics are moved by wind, storms, hurricane and currents present in the sea bed which then make them accumulate in specific areas. Once these items are deposited, their degradation is minimal because of a lack of oxygen or light and therefore the microplastics, once they come to the seafloor, remain preserved. Another study from 2020 has led to the discovery of two types of plastic debris in the Mediterranean seabed for the first time, on Giglio Island, Italy: plasticrust and pyroplastics. Plasticrust had been found for the first time in the Atlantic Ocean. This discovery revealed that the contamination by plastic debris may be more widespread than previously expected."|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Plastics in Marine Protected Areas|" Researches on the sediment of the Cabrera Archipelago National Maritime-Terrestrial Park marine protected area has found that the majority of plastic debris in these waters is microplastics, probably transported by wave currents and winds; the study also showed that higher amounts of microplastics were found in the marine protected area rather than on other samples taken from different sites, thus opening the debate regarding the protection and conservation of these areas.  
Other studies on the presence of plastic debris in marine protected areas have been conducted, among others, in the waters of the Natural Park of Telaščica bay in Croatia  and on the EU Site of Community Interest in the North-West Adriatic;  both areas present marine litter, primarily in the form of microplastics and mesoplastics."|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Sources of plastic litter in the Mediterranean Sea| The Mediterranean Sea is considered as a hotspot for plastic pollution and one of the most impacted regions of the world by this problem.   As of 2019, of the estimated 10.000 tons of plastic input per year in the Mediterranean Sea, half of this litter had land-based origins (from coastal zones) whereas the other half originated from rivers and maritime routing.  This is the case for example of the Nile River which, as of 2017,  brought around 200 tonnes of plastic waste into the Mediterranean basin yearly. Once plastic waste reaches Mediterranean waters, it hardly gets out: the peculiar shape of the Mediterranean sea and its currents mean that the outflow circulation of waters is limited and, therefore, that the waste that enters from the coastlines remains inside the Mediterranean, accumulating within it. Researchers identify some main causes of the large presence of plastic pollution in this Sea: namely its semi-enclosed shape, its densely populated coasts, tourism activities, fishing, shipping, waste disposal problems, increase of the use of plastic and unsustainable consumption patterns.|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Land-based sources of plastic litter in the Mediterranean Sea| The primary land-based sources of plastic pollution in the Mediterranean Sea are tourism activities, vast population on the coasts, inefficient management of waste, unsustainable consumption patterns  and increase on the use of plastics.  According to a UNEP  report, inhabitants and tourists of the Mediterranean region produce 24 million metric tons of plastic waste (of which less than 6% is recycled  ) each year; this figure normally grows during the summer because of tourism presence and activities.  Mediterranean countries are the world's number one tourism destination (considering both international and domestic visitors)    and waste management facilities frequently experience overload. Most of the waste produced is dumped into unprotected landfills  and gets into the Mediterranean Sea through stormwater runoffs, wind currents, rivers, wastewater streams.   To this extent, considering the fact that wastewater is a pathway for waste disposal in open waters, a key challenge is also constituted by the fact that only small percentages of wastewater in the Mediterranean area undergo basic and tertiary treatment.|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Ocean-based sources of plastic litter in the Mediterranean Sea| Ocean-based sources of plastic litter are connected with some important economic sectors in the Mediterranean: fisheries, shipping  and acquaculture,  which generate diverse types of debris that ends up in waters.  The shipping sector in the Mediterranean Sea is very important as about 15% of global shipping passes through this region.[citation needed] Cruise-liners, military fleets, oil and gas stations, drilling activities are other ocean-based sources of plastic litter: the debris produced is then fragmented into microplastics present at all levels of the Mediterranean basin.|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Impacts and costs of plastic pollution in the Mediterranean Sea| As the Mediterranean Sea is characterised by a rich biodiversity , ecological value and intense presence of economic activities, the current and future impacts of marine plastic pollution in this area are particularly high.   The Mediterranean Sea in fact hosts between 4% and 18% of all marine  exemplars,  and tourism on the coastal zones, aquaculture, the fishing industry and maritime transport represent substantial sources of income for the Mediterranean countries . Research shows that marine plastic pollution has impacts on marine ecosystems and economic activities at various levels,   but further studies are currently being conducted to thoroughly   investigate the size of such impacts in the Mediterranean area,  both on marine biota and on human health.|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Biological and ecological impacts| A study conducted by Legambiente  on 700 individuals of 6 different fish species discovered that one out of three fish had ingested plastic items and that more than half of the sea turtles that were analysed presented plastic litter either inside or around the body.  Other studies found traces of ingested plastic debris also in the stomachs of seabirds and nanoplastic particles in mussels. These are just some of the signals of the negative impact that plastic litter has on the marine biodiversity of the Mediterranean basin. Marine plastic litter causes problems not only in terms of the accidental ingestion of plastic items  (which can lead to gastrointestinal blockages, diseases and mortality ), but also in terms of the toxic effects that additives used in plastics can have. Moreover, studies have shown that plastic items soaks up contaminants in a more rapid and efficient way than organic items floating on sea waters.  Long-term exposure to such plastic items and the contaminants, in particular microplastic debris, has been shown to have topological effects on species living in regions where marine plastic pollution is intense, like in the case of the Mediterranean region, whose ecosystem is in constant contact with plastics.|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Impacts on human health| Potential impacts on human health are connected, among others, with the possibility of eating fish that had ingested plastics, of drinking waters that do not undergo treatment, or through releases of chemical substances.  However, effects of plastics (and especially microplastics) on human health are a particularly debated topic within scholars and researchers and more studies are being conducted to assess these effects.|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Impacts and costs on marine industries|" The dangerous effect of plastic pollution has also noticeable impacts on the blue economy of the Mediterranean basin, in particular on the tourism, fishing and shipping sectors.   Calculating the complete economic impacts of marine plastic litter in the Mediterranean Sea is very complicated due to the various sources, the impacts at the environmental, social and economic levels, and the number of involved sectors in different geographic locations.  A 2021 report by UNEP tried estimating annual losses of US$700 million on the entire blue economy of the Mediterranean Sea.  Because of current waves, high amounts of plastic objects accumulate on beaches, thus demanding continuous clean-ups and potentially disincentivizing tourism. Aquaculture and fishery are impacted as the marine ecosystem is affected and as litter can damage nets, contaminate the catches and also reduce them.  For the fishing sector, researches of 2021 estimate a general annual loss of euros 61.7 millions due to marine debris in European countries; this figure is also comprising a loss due to the decreasing of demand of fish products due to concerns on the quality of these items.  
The specific loss at the Mediterranean level is more complicated to calculate but scholars argue the costs are considered to be higher to the substantial concentration of marine litter. The shipping sector suffers economic pressures too, as plastic marine litter, among others, can damage motors, which then have to be repaired. Assessments on the impact of plastic marine litter on the shipping sector in the Mediterranean region are being conducted."|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Social and clean up costs| Marine pollution in the Mediterranean has led to an increase in the costs sustained by diverse local, regional and national authorities to clean-up the beaches and coastal areas. For example, as of 2016, Nice's administration spent around euros 2 million to clean up beaches. Analysis conducted within the CleanSea project estimated a cost of Euros 3.800 to clean up one tonne of marine litter in European countries' beaches: Euros 2.200 have been estimated to be spent annually to clean up each tonne of floating plastic litter. Other impacts have been registered at the local level with losses of income and jobs in the tourism sector, as well as losses in values of residential property.|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Reduction efforts|" Marine plastic pollution has been defined as a global concern by the European Union, the G7 and G20, the United Nations Environment Programme (UNEP) and various organisations and institutions at local, regional and international levels. Over the last few years, marine plastic debris has started to be recognised as a relevant issue also in terms of its governance and regulatory complexities, which are also due to the fact that it is a transboundary, ""multifaceted"" problem, with multiple causes, sources and actors involved, and that requires integrated approaches and solutions at various levels. National, regional and international actors, along with civil society and private industries are trying to address the problem of pollution in the Mediterranean Sea with initiatives, policies, campaigns. The majority of these initiatives addresses marine pollution in general, while also focusing, among others, on the problem of marine plastic pollution in the Mediterranean Sea and region."|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Policies and conventions on the Mediterranean Sea|" The Barcelona Convention, which was adopted in 1995, was the first regional treaty aiming at reducing pollution, and marine plastic pollution, in the Mediterranean region; the European Union and all countries with a Mediterranean shoreline are parties to the Convention and to the Protocol for the Protection of the Mediterranean Sea against Pollution from Land Based Sources and to Activities that concern plastic pollution in the Mediterranean basin. The Barcelona Convention and its Protocols were established within the regional cooperation platform ""Mediterranean Action Plan of the United Nations Environment Programme"" (UNEP/MAP), the first regional action plan of the UNEP Regional Seas Programme, which was instrumental in the adoption of the Convention itself.  The UNEP/MAP - Barcelona Convention System has been playing a role for responding to environmental challenges threatening marine and coastal ecosystems in the Mediterranean region.  It collects data for marine debris, litter in waters and on coastlines, amounts of plastic litter ingested by marine species. The first ever legal binding instrument with the purpose of preventing and limiting marine plastic pollution and of cleaning up marine litter already affecting the area of the Mediterranean Sea is the ""Regional Plan on Marine Litter Management"" (RPML) in the Mediterranean, which was adopted, among others, in the framework of the Barcelona Convention in 2013.  The Plan is further supported by the EU funded ""Marine Litter MED II project"" (2020-2023), which is focused on countries of the Southern Mediterranean (Algeria, Egypt, Libya, Morocco, Tunisia, Israel and Lebanon) and is built on the results of the Marine Litter MED project, carried out between 2016 and 2019. Scholars have argued that an international agreement among countries with shorelines on the Mediterranean Sea could be pursued, with actions focused on eliminating plastic waste in nature, on creating plans for the prevention, control and removal of plastic litter from marine ecosystems, on banning specific types of plastic products and prevent their dumping into waters, and on establishing international committees. The prospective Global Plastic Pollution Treaty is awaited."|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Marine Protected Areas|" Marine Protected Areas represent a policy instrument which can be helpful in reducing plastic pollution in seas and its impacts on marine ecosystems, as they ban or limit fisheries, some tourism activities, dumping of materials, mining and building of harbours and offshore wind farms.  
Nevertheless, high levels of plastic pollution, especially microplastics, have been recorded in Marine Protected Areas in the Mediterranean Sea.   Initiatives focusing specifically on Marine Protected Areas and plastic pollution in the Mediterranean region are awaited."|2023-09-18-18-37-14
Plastic pollution in the Mediterranean sea|Other actors, initiatives and programmes|" Programmes and strategies at the EU level address the problem of plastic pollution in Europe's seas, therefore also the Mediterranean Sea. Key policies are the EU Green Deal  and the Zero Pollution Action Plan, of which an important goal is that of reducing waste, marine plastic pollution and the dispersal of microplastics. 
Among the relevant strategies, some are the Water Framework Directive,  the Industrial Emissions Directive,  the Environmental Liability Directive,  the Environmental Crimes Directive,  the Waste Framework Directive,  the Waste Shipment Regulation,  the Packaging and Packaging Waste Directive,  the Single-Use Plastics Directive. 
The Marine Strategy Framework Directive constitutes the EU legal framework for the safeguard and preservation of the European Seas, also from marine plastic litter;  the Directive addresses the importance of identifying the sources of marine litter and its impacts to deploy efficient and comprehensive measures.  Among various actions, there is the European Union's ban on diverse kinds of single-use plastics. The EU has invited Mediterranean countries to implement legal, administrative and financial actions to create sustainable waste management systems to limit the problem of plastic pollution in the Mediterranean Sea. Some of the other actors carrying out activities to raise awareness and build knowledge on the topic of plastic pollution in the Mediterranean Sea involve: the Union for the Mediterranean;  the International Union for the Conservation of Nature and IUCN-Med,  which conducts researches on macro, micro and nano plastics in the Mediterranean Sea and builds partnerships and alliances for the implementation of projects in the region;WWF with different analyses and projects, such as the WWF Mediterranean Marine Initiative;  the Mediterranean Information Office for Environment, Culture and Sustainable Development (MIO-ECSDE)  and the MARLISCO project;  Mediterranean Experts on Climate and Environmental Change. States and civil societies actors are also operating and creating partnerships (like in the case of the COastal Management and MOnitoring Network for tackling marine litter in the Mediterranean Sea ) in awareness-raising initiatives and in clean-up activities on the coastlines of the Mediterranean Sea, like in the case of OGYRE  and ENALEIA,  who are directly cooperating with fishermen in cleaning various seas, including the Mediterranean Sea. Other clean-up activities comprise the ""Mediterranean CleanUP"" (MCU),  ""Clean up the Med"" by Legambiente   and spontaneous initiatives at various levels. The Day of the Mediterranean is celebrated each year on 28 November to commemorate the foundation of the Barcelona Convention and to raise awareness on various issues of the Mediterranean basin, among which that of plastic pollution.[citation needed]"|2023-09-18-18-37-14
Plasticosis|General|" Plasticosis is a form of fibrotic scarring that is caused by small pieces of plastic which inflame the digestive tract. A 2023 study by Hayley Charlton-Howard, Alex Bond, Jack Rivers-Auty, and Jennifer Lavers, found that plastic pollution caused disease in seabirds. The researchers coined the term plasticosis to indicate the first recorded instance of plastic-induced fibrosis in wild animals.  “Further, the ingestion of plastic has far-reaching and severe consequences, many of which we are only just beginning to fully document and understand.” The disease is caused only by plastic according to the study. Plasticosis is a pathological wound healing in which connective tissue replaces normal parenchymal tissue to the extent that it goes unchecked, leading to considerable tissue remodelling and the formation of permanent scar tissue.
Repeated injuries, chronic inflammation and repair are susceptible to fibrosis where an accidental excessive accumulation of extracellular matrix components, such as the collagen is produced by fibroblasts, leading to the formation of a permanent fibrotic scar."|2023-07-08-16-47-33
Plasticosis|Pathogenesis|" Plasticosis was first identified in Flesh-footed Shearwaters (Ardenna carneipes) on Lord Howe Island, Australia. Previous research on this population found that ~90% of necropsied chicks contained ingested plastics, which are thought to negatively affect chick growth and survival. Microscopic pieces of plastic have also been found embedded in tissues of this species, causing inflammation and tissue damage, as well as loss of tubular glands and rugae.
Plastic-induced fibrosis has been previously demonstrated in recent laboratory studies, in organs such as the heart, liver, ovaries, and uterus. Plasticosis is found to cause increased collagen prevalence in the tubular glands and submucosa, and widespread scar tissue formation across the whole organ, leading to extensive reorganisation and tissue damage, and potentially a loss of tissue function. Since tubular glands produce mucus to protect the epithelium, as well as fluids that are crucial for digestion and nutrient absorption, plasticosis may affect the ability of birds to prevent injury or infection in the stomach and reduce stomach function. Excessive scar tissue formation in the stomach wall and loss of rugae induced by plastics may also reduce the ability of the stomach to expand, potentially reducing stomach capacity and function. 
Plasticosis has been compared to asbestosis and silicosis, where plastic acts a similar persistent irritant leading to fibrosis."|2023-07-08-16-47-33
Plasticosis|Effects|" Plasticosis may be a sign that a new age of disease is upon us because of human overuse of plastics and other long-lasting contaminants, and their leakage into the environment. ""In 2022, United Nations member nations voted to negotiate a global treaty to end plastic pollution, with a target completion date of 2024. This would be the first binding agreement to address plastic pollution in a concerted and coordinated manner. The identification of plasticosis in shearwaters shows that there is no time to waste."""|2023-07-08-16-47-33
Plasticrust|General| Plasticrusts are a new type of plastic pollution in the form of plastic debris, covering rocks in intertidal shorelines which vary in thickness and in color and are composed of polyethylene based on fourier-transform infrared spectroscopy (FTIR) analysis.   They were first discovered on the South coast of the volcanic island of Madeira in the Atlantic Ocean in 2016 and have additionally been found on Giglio Island, Italy.    They are considered a sub-type of plastiglomerate and could possibly have negative effects on surrounding fauna by entering the food web through consumption by benthic invertebrates.|2023-05-04-06-38-14
Plasticrust|Formation| Plasticrusts are formed by waves smashing plastic debris against rugose rocks.    A report concluded that “plasticrust abundance may depend on the local levels of nearshore plastic debris, wave exposure and tidal amplitude.”   This was based on the abundance of plasticrusts compared to the other listed factors at each location where plasticrust was detected or searched for.|2023-05-04-06-38-14
Plasticrust|Locations| Plasticrust has so far been identified at two locations, the coasts of Madeira in the NE Atlantic Ocean and Giglio Island in the Tyrrhenian Sea, Italy, shown below in three maps. In Madeira, the plasticrust was found for the first time in 2016, on the south coast of the island, specifically at 32o44’31” N, 16o42’27” W.  The Madeira location is an isolated and off-shore island in the NE Atlantic Ocean, and is thus exposed to open waves and a tidal amplitude of 2.6 meters (8.5 feet).  Additionally, it has been confirmed that the plasticrust has persisted since then and that the percent coverage of the plasticrust has actually increased, based on a sampling performed in January 2019. The Giglio Island plasticrusts were found on ocean facing mid-intertidal rocks in a wave exposed rocky intertidal habitat in the Franco Promontory in western Giglio Island, Tuscan Archipelago, Tyrrhenian Sea, Italy on October 19, 2019.  This location, in contrast to Madeira, is a nearshore island found among other, larger, islands and the Italian mainland. Because of this, it is relatively wave sheltered. Additionally, wind speed and wave height are lower in the Tyrrhenian Sea than in the NE Atlantic and tidal amplitude is less wide at 0.45 meters (1.5 feet). Three bays were also studied in Giglio which were wave-sheltered instead of wave exposed, including Campese Bay East: 42.3657, 10.8747, Campese Bay West: 42.3700, 10.8813, and Allume Bay: 42.3528, 10.8802. However, no plasticrust was found at these locations, and this information combined with the abundance of plasticrust in Giglio relative to the abundance of plasticrust in Madeira was the basis for determining that plasticrust abundance is related to wave exposure.  Similarly, the difference in tidal amplitude between the two locations was the basis for the conclusion that tidal amplitude influences plasticrust abundance. Although plasticrust has so far only been identified in two locations, according to a study, their findings of plasticrust and pyroplastic, a similar type of plastic pollution, are evidence that plasticrust, among other new plastic debris types, is not a local phenomenon.|2023-05-04-06-38-14
Plasticrust|Properties| The plasticrust samples from both locations were studied using fourier-transform infrared spectroscopy analysis.   The results of the analyses at each location determined that the plasticrust at both locations is composed of polyethylene, one of the most frequently used plastics worldwide.[citation needed] The plasticrust identified in Madeira was found in two colors, blue and white. They had a thickness of 0.77±0.10 mm and a percent coverage as of sampling in January 2019 of 9.46±1.77.   FTIR analysis did not determine the density of the polyethylene that composed the plasticrust however, the plasticrusts off the coast of Madeira likely originate from packaging materials, such as plastic bags, which are made of low density polyethylene. The evidence for this being that pieces of marine litter found in Madeira’s coastal waters often have domestic origin. The Giglio Island plasticrusts had similar properties to those found in Madeira. They were white and blue, and they had a thickness of 0.5 to 0.7 mm.   Additionally, they covered an area of 0.46±0.08 mm2 and had a percent coverage of 0.02±0.01%.|2023-05-04-06-38-14
Plasticrust|Potential Environmental Effects| The research done on plasticrusts at both locations suggest that plasticrust may harm surrounding fauna through microplastic particles entering the food web, specifically via consumption by benthic invertebrates such as barnacles, snails, and crabs. It’s been suggested that plasticrusts in Madeira may be consumed by intertidal snails (Tectarius striatus), which are found around and on top of the plasticrust. They usually eat diatoms and algae, however another species, common periwinkle, has been shown to not be able to distinguish between algae with adherent microplastics and clean algae, so according to the initial study “we cannot discard its potential grazing on plasticrusts”. In Giglio, gastropods were not found at the same location as the plasticrusts as they were in Madeira. However, marbled rock crabs, which eat algae and invertebrates, have been observed at the same elevation along the shoreline that plasticrusts were found.|2023-05-04-06-38-14
Plasticrust|Similar Forms of Plastic Pollution| There are three other types of plastic debris pollution which are similar to plasticrust, including plastiglomerate, pyroplastic, and anthropoquina.|2023-05-04-06-38-14
Plasticrust|Plastiglomerate| Plastiglomerate is melted plastic mixed with substrate to create new fragments of material with greater density.   First discovered in 2014, it is also described as “an indurated, multi-composite material made hard by agglutination of rock and molten plastic”.   It subdivides into two types, in-situ and clastic. The in-situ type is where the plastic is adhered to rock outcrops, similar to plasticrusts. The clastic type is where basalt, coral, shells, woody debris, and sand are cemented with a matrix of plastic.  They are often referred to as a new type of rock, but they are not defined as such due to the fact that they are anthropogenic in origin and rocks by definition from naturally.|2023-05-04-06-38-14
Plasticrust|Pyroplastic| Pyroplastics are considered a sub-type of plastiglomerates which are formed from the melting or burning of plastic.   Similar to plasticrusts, they can be found adhered to outcrops of rock, and along the shoreline as distinct clasts which show various degrees of weathering.  Pyroplastics were also found on Giglio Island with the plasticrusts.|2023-05-04-06-38-14
Plasticrust|Anthropoquina| Anthropoquinas refer to sedimentary rocks that contain objects with an anthropogenic source, which are referred to as technofossils, such as plastic fragments or metal nails, along with natural materials such as mollusk shells and siliciclastic grains.   Like plasticrusts, which are created by sea waves, the formation of anthropoquinas is not contributed to by human activity.    They are formed, like all sedimentary rocks, through cementation.|2023-05-04-06-38-14
Plastiglomerate|General| Plastiglomerate is a rock made of a mixture of sedimentary grains, and other natural debris (e.g. shells, wood) that is held together by plastic.  It has been considered a potential marker of the Anthropocene, an informal epoch of the Quaternary proposed by some social scientists, environmentalists, and geologists.|2023-08-23-23-30-05
Plastiglomerate|Origin| Plastiglomerates form along shorelines where natural sedimentary grains and organic debris are agglutinated by melted plastic. They can be created during campfire burning, as has been reported from Kamilo Beach on the island of Hawaii,  or during hot weather, as is the case on Trindade Island.|2023-08-23-23-30-05
Plastiglomerate|Depositional environment|" Plastiglomerate could potentially form a marker horizon of human pollution on the geologic record.     and may survive as future fossils.   Plastiglomerate may also conceivably form in plastic-polluted regions affected by lava flows or forest fires.    They have been found on the surface as well as beneath the sand.   This suggests that plastiglomerates are being actively deposited into the sedimentary record.  Some geophysicists and geologists speculate that plastiglomerates will not persist in the fossil record, however, or that they might ""revert back to a source of oil from whence they came, given the right conditions of burial"". ""In situ"" plastiglomerate forms where plastic melts and fills in rock cavities.    ""Clastic"" plastiglomerate are smaller solitary pieces that form where larger fused items become fragmented by waves. Plastiglomerate is denser than particles that are solely composed of plastic, which gives them greater potential to become buried and preserved in the rock record. Plastiglomerates have been featured at the Yale Peabody Museum of Natural History;  Oakville Galleries in Oakville, Ontario, Canada;  Louis B. James Gallery in New York City; Carleton University Gallery in Ottawa, Ontario, Canada; Prosjektrom Normanns, Stavanger, Norway; and Museon in The Hague, Netherlands. Museon claims to have probably the largest piece of plastiglomerate, which is more than 1m long and 50cm wide. [better source needed]"|2023-08-23-23-30-05
Plastiglomerate|History|" Charles Moore, a sea captain and oceanographer for the Algalita Marine Research Institute in Long Beach, California, discovered this substance in 2006 while surveying Kamilo Beach on the Big Island of Hawai’i.  Geology professor, Dr. Patricia Corcoran and Visual artist professor, Kelly Jazvac of the University of Western Ontario investigated the samples on Kamilo Beach in 2012 where they also coined the term ""plastiglomerate"".  Approximately one-fifth of the plastiglomerates found at Kamilo Beach of fishing debris, one quarter of broken lid containers, and one half consisted of plastic ""confetti"".   The plastiglomerate at Kamilo Beach was more likely created from human campfires than from molten lava flows.    In 2023, it was reported plastiglomerates had been discovered on remote Trindade Island, a known refuge for turtles."|2023-08-23-23-30-05
Pneumatic barrier|General|" A pneumatic barrier is a method to contain oil spills. It is also called a bubble curtain. Air bubbling through a perforated pipe causes an upward water flow that slows the spread of oil. It can also be used to stop fish from entering polluted water. A further application of the pneumatic barrier is to decrease the salt-water exchange in navigation locks and prevent salt intrusion in rivers.
.  
Pneumatic barriers are also known as air curtains.
The pneumatic barrier is a (non-patented) invention of the Dutch engineer Johan van Veen from around 1940 
. A pneumatic barrier is an active (as opposed to passive) method of waterway oil spill control.  (An example of a passive method would be a containment boom.)"|2023-07-25-23-09-54
Pneumatic barrier|Method of operation|" The pneumatic barrier consists of a perforated pipe and a compressed air source.  Air escaping from the pipe provides a ""hump"" of rising water and air which contains the oil spill.  Anchors to keep the pipe in a particular spot are helpful. In case of a density current due to salinity differences the barrier mixes the salt water, but also slows down the speed of the density current."|2023-07-25-23-09-54
Pneumatic barrier|Unique considerations| At water-current speeds exceeding one foot per second, the pneumatic barrier no longer functions effectively, limiting deployable sites.|2023-07-25-23-09-54
Pneumatic barrier|Environmental issues| The release of compressed air in the water adds oxygen to the local environment.  This may be particularly useful in areas that have become a dead zone due to eutrophication. Air curtains may have another application. Dolphin and whale beaching has increased with the rise in ocean temperatures. On Thursday February 12th, 2017, a group of nearly 400 whales beached near Golden Bay on the tip of New Zealand’s South Island, following a similar incident earlier that week. The simplicity of an air curtain system, requiring only air compressors and perforated hoses, could allow for rapid deployment and create aerated zones of oxygenated seawater during a marine emergency. Air curtains are also used to control the release of smoke particles into the environment. After a natural disaster, or during brush clearing activities, debris is disposed of by incineration in either a ceramic or earth pit containment. Similar to an air curtain to separate indoor air from outdoor air, for instance in restaurants and walk-in refrigerators, a powerful air curtain can defeat the chimney effect of the incineration process to eliminate any smoke from a brush incinerator. The air curtain acts as a lid on the process, and forces the smoke back into the fuel bed for a cleaner burn.|2023-07-25-23-09-54
Pneumatic barrier|Disadvantages| Like all active systems of any type, a mechanical failure can result in total failure of protection.|2023-07-25-23-09-54
Pneumatic barrier|External links| Development of an air bubble curtain to reduce underwater noise of percussive piling  Marine Environmental Research 49(2000)79-93, Elsevier  Retrieved 2/16/2017 Bubble Curtains: Can They Dampen Offshore Energy Sound for Whales?  National Geographic  Retrieved 2/16/2017 You Tube: How an Air Curtain Works  by Berner International  Retrieved 2/16/2017|2023-07-25-23-09-54
Pneumatic fracturing|General|" Pneumatic fracturing is a method that has become very popular in the last ten years used to remediate contaminated sites. The method consists of injecting gas into a contaminated subsurface at a pressure higher than that of the gases that are present. By doing this fractures ""spider-web"" throughout the subsurface so that pumps may be placed in the ground to suck out the contaminated water through these cracks. Substrates may also be injected into the soil through the cracks to further the remediation of the soil and ground water. The clean-up technique was developed and patented through the research of various professors at the New Jersey Institute of Technology in 1996 with hopes of cleaning up various United States Environmental Protection Agency (EPA) Superfund sites which are some of the most heavily contaminated sites in the country. The patent is held by John R. Schuring, PhD and PE, professor of civil and environmental engineering at the New Jersey Institute of Technology, developed in conjunction with Thomas M. Boland, Trevor C. King, Sean T. McGonigal, David S. Kosson, Conan D. Fitzgerald, and Sankar Venkatraman. This method has been adopted by environmental contractors all over the country since it has been patented."|2021-05-25-12-45-32
Point source pollution|General| A point source of pollution is a single identifiable source of air, water, thermal, noise or light pollution. A point source has negligible extent, distinguishing it from other pollution source geometries (such as nonpoint source or area source). The sources are called point sources because in mathematical modeling, they can be approximated as a mathematical point to simplify analysis.  Pollution point sources are identical to other physics, engineering, optics, and chemistry point sources and include:|2023-05-06-22-57-20
Pollutant|General| Lists Categories A pollutant or novel entity  is a substance or energy introduced into the environment that has undesired effects, or adversely affects the usefulness of a resource. These can be both naturally forming (i.e. minerals or extracted compounds like oil) or anthropogenic in origin  (i.e. manufactured materials or byproducts). Pollutants result in environmental pollution or become public health concerns when they reach a concentration high enough to have significant negative impacts. A pollutant may cause long- or short-term damage by changing the growth rate of plant or animal species, or by interfering with resources used by humans, human health or wellbeing, or property values. Some pollutants are biodegradable and therefore will not persist in the environment in the long term. However, the degradation products of some pollutants are themselves polluting such as the products DDE and DDD produced from the degradation of DDT. Pollution has widespread negative impacts on the environment.  When analyzed from a planetary boundaries perspective, human society has released novel entities that  well exceed safe levels.|2023-07-14-16-47-37
Pollutant|Different types of pollutants in the environment| Pollutants can be categorized in a variety of different ways.  For example, it is sometimes useful to distinguish between stock pollutants and fund pollutants.  Another way is to group them together according to more specific properties, such as organic, particulate, pharmaceutical, et cetera.  The environment has some capacity to absorb many discharges without measurable harm, and this is called “assimilative capacity (or absorptive capacity); a pollutant actually causes pollution when the assimilative capacity is exceeded.|2023-07-14-16-47-37
Pollutant|Stock pollutants| Pollutants, towards which the environment has low absorptive capacity are called stock pollutants.  Examples include persistent organic pollutants like PCBs, non-biodegradable plastics and heavy metals. Stock pollutants accumulate in the environment over time. The damage they cause increases as more pollutant is emitted, and persists as the pollutant accumulates. Stock pollutants can create a burden for the future generations, bypassing on the damage that persists well after the benefits received from incurring that damage, have been forgotten.  Scientists have officially deemed that the planetary boundaries safe chemical pollutant levels (novel entities) have been surpassed.|2023-07-14-16-47-37
Pollutant|Fund pollutants| In contrast to stock pollutants, for which the environment has low absorptive capacity, fund pollutants are those for which the environment has a moderate absorptive capacity. Fund pollutants do not cause damage to the environment unless the emission rate exceeds the receiving environment's absorptive capacity (e.g. carbon dioxide, which is absorbed by plants and oceans).  Fund pollutants are not destroyed, but rather converted into less harmful substances, or diluted/dispersed to non-harmful concentrations.|2023-07-14-16-47-37
Pollutant|Specific groups of pollutants| Many pollutants are within the following notable groups:|2023-07-14-16-47-37
Pollutant|Light pollutant| Light pollution is the impact that anthropogenic light has on the visibility of the night sky. It also encompasses ecological light pollution which describes the effect of artificial light on individual organisms and on the structure of ecosystems as a whole.|2023-07-14-16-47-37
Pollutant|Zones of influence| Pollutants can also be defined by their zones of influence, both horizontally and vertically.|2023-07-14-16-47-37
Pollutant|Horizontal zone| The horizontal zone refers to the area that is damaged by a pollutant. Local pollutants cause damage near the emission source. Regional pollutants cause damage further from the emission source.|2023-07-14-16-47-37
Pollutant|Vertical zone| The vertical zone refers to whether the damage is ground-level or atmospheric. Surface pollutants cause damage by accumulating near the Earth's surface. Global pollutants cause damage by concentrating on the atmosphere.|2023-07-14-16-47-37
Pollutant|Measuring concentration| Measures of pollutant concentration are used to determine risk assessment in public health. Industry is continually synthesizing new chemicals, the regulation of which requires evaluation of the potential danger for human health and the environment. Risk assessment is nowadays considered essential for making these decisions on a scientifically sound basis. Measures or defined limits include:|2023-07-14-16-47-37
Pollutant|International| Pollutants can cross international borders and therefore international regulations are needed for their control. The Stockholm Convention on Persistent Organic Pollutants, which entered into force in 2004, is an international legally binding agreement for the control of persistent organic pollutants. Pollutant Release and Transfer Registers (PRTR) are systems to collect and disseminate information on environmental releases and transfers of toxic chemicals from industrial and other facilities.|2023-07-14-16-47-37
Pollutant|European Union| The European Pollutant Emission Register is a type of PRTR providing access to information on the annual emissions of industrial facilities in the Member States of the European Union, as well as Norway.|2023-07-14-16-47-37
Pollutant|United States|" Clean Air Act standards. Under the Clean Air Act, the National Ambient Air Quality Standards (NAAQS) are developed by the Environmental Protection Agency (EPA) for six common air pollutants, also called ""criteria pollutants"": particulates; smog and  ground-level ozone; carbon monoxide; sulfur oxides; nitrogen oxides; and lead.  The National Emissions Standards for Hazardous Air Pollutants are additional emission standards that are set by EPA for toxic air pollutants. Clean Water Act standards. Under the Clean Water Act, EPA promulgated national standards for municipal sewage treatment plants, also called publicly owned treatment works, in the Secondary Treatment Regulation.  National standards for industrial dischargers are called Effluent guidelines (for existing sources) and New Source Performance Standards, and currently cover over 50 industrial categories.  In addition, the Act requires states to publish water quality standards for individual water bodies to provide additional protection where the national standards are insufficient. RCRA standards. The Resource Conservation and Recovery Act (RCRA) regulates the management, transport and disposal of municipal solid waste, hazardous waste and underground storage tanks."|2023-07-14-16-47-37
Polluter pays principle|General| In environmental law, the polluter pays principle is enacted to make the party responsible for producing pollution responsible for paying for the damage done to the natural environment. This principle has also been used to put the costs of pollution prevention on the polluter.   It is regarded as a regional custom because of the strong support it has received in most Organisation for Economic Co-operation and Development (OECD) and European Union countries,  and has a strong scientific basis in economics. It is a fundamental principle in US environmental law.|2023-09-24-21-54-49
Polluter pays principle|Historical basis|" According to the French historian of the environment Jean-Baptiste Fressoz, financial compensation (not named ""polluter pays principle"" at that time) is already the regulation principle of pollution favoured by industrials in the nineteenth century.  He wrote that: ""This principle, which is now offered as a new solution, actually accompanied the process of industrialisation, and was intended by the manufacturers themselves."" In modern times, the continued adherence to the polluter pays principle is supported scientifically by economics. One condition that must be satisfied in order to maximise Pareto efficiency is the assignment of all costs of a decision, such as the harm resulting from a decision to pollute, to the agent making the decision, effectively removing all externalities."|2023-09-24-21-54-49
Polluter pays principle|Applications in environmental law| The polluter pays principle underpins environmental policy such as an ecotax, which, if enacted by government, deters and essentially reduces greenhouse gas emissions. This principle is based on the fact that as much as pollution is unavoidable, the person or industry that is responsible for the pollution must pay some money for the rehabilitation of the polluted environment.|2023-09-24-21-54-49
Polluter pays principle|Australia| The state of New South Wales in Australia has included the polluter pay principle with the other principles of ecologically sustainable development in the objectives of the Environment Protection Authority.|2023-09-24-21-54-49
Polluter pays principle|Canada| The Canadian Energy Regulator mandates that oil companies must pay for any environmental impacts from a spill. This mandate requires oil companies to pay for damages, regardless of whether or not the spill is their fault.|2023-09-24-21-54-49
Polluter pays principle|European Union| The polluter pays principle is set out in the Treaty on the Functioning of the European Union  and Directive 2004/35/EC of the European Parliament and of the Council of 21 April 2004 on environmental liability with regard to the prevention and remedying of environmental damage is based on this principle. The directive entered into force on 30 April 2004; member states were allowed three years to transpose the directive into their domestic law and by July 2010 all member states had completed this.|2023-09-24-21-54-49
Polluter pays principle|France| In France, the Charter for the Environment contains a formulation of the polluter pays principle (article 4): Everyone shall be required, in the conditions provided for by law, to contribute to the making good of any damage he or she may have caused to the environment.|2023-09-24-21-54-49
Polluter pays principle|Ghana| In Ghana, the polluter pays principle was adopted in 2011.|2023-09-24-21-54-49
Polluter pays principle|Sweden| The polluter pays principle is also known as extended producer responsibility (EPR). This is a concept that was probably first described by Thomas Lindhqvist for the Swedish government in 1990.  EPR seeks to shift the responsibility of dealing with waste from governments (and thus, taxpayers and society at large) to the entities producing it. In effect, it internalised the cost of waste disposal into the cost of the product, theoretically meaning that the producers will improve the waste profile of their products, thus decreasing waste and increasing possibilities for reuse and recycling. The Organisation for Economic Co-operation and Development defines extended producer responsibility as: a concept where manufacturers and importers of products should bear a significant degree of responsibility for the environmental impacts of their products throughout the product life-cycle, including upstream impacts inherent in the selection of materials for the products, impacts from manufacturers’ production process itself, and downstream impacts from the use and disposal of the products. Producers accept their responsibility when designing their products to minimise life-cycle environmental impacts, and when accepting legal, physical or socio-economic responsibility for environmental impacts that cannot be eliminated by design.|2023-09-24-21-54-49
Polluter pays principle|United Kingdom| Part IIA of the Environmental Protection Act 1990 established the operation of the polluter pays principle. This was further built upon by The Environmental Damage (Prevention and Remediation) Regulations 2009 (for England) and the Environmental Damage (Prevention and Remediation) (Wales) Regulations 2009 (for Wales).|2023-09-24-21-54-49
Polluter pays principle|United States| The principle is employed in all of the major US pollution control laws: Clean Air Act,   Clean Water Act,  Resource Conservation and Recovery Act (solid waste and hazardous waste management),  and Superfund (cleanup of abandoned waste sites). Some eco-taxes underpinned by the polluter pays principle include: The US Environmental Protection Agency (EPA) has observed that the polluter pays principle has typically not been fully implemented in US laws and programs. For example, drinking water and sewage treatment services are subsidized and there are limited mechanisms in place to fully assess polluters for treatment costs.|2023-09-24-21-54-49
Polluter pays principle|Zimbabwe|" The Zimbabwe Environmental Management Act of 2002  [full citation needed] prohibits the discharge of pollutants into the environment. In line with the ""Polluter Pays"" principle, the Act requires a polluter to meet the cost of decontaminating the polluted environment."|2023-09-24-21-54-49
Polluter pays principle|In international environmental law| In international environmental law it is mentioned in the principle 16 of the Rio Declaration on Environment and Development of 1992|2023-09-24-21-54-49
Polluter pays principle|Exception for excusable ignorance| The polluter pays principle (PPP) has been doubted in cases where no one recognized that a type of pollution posed any danger until after the pollution began.  An example occurs in the history of climate change science which shows that considerable carbon dioxide was emitted into the atmosphere by industrialized countries before there was scientific awareness or consensus that it could be dangerous.|2023-09-24-21-54-49
Pollution in the Arctic Ocean|General|" Pollution in the Arctic Ocean is primarily the result of economic activities carried out on land, which is sources from locally, regionally, and globally origins.  There is also the inclusion of industrial development in the Arctic region, northern rivers, and the effects of military activities, particularly nuclear activity – as well as the influx of pollutants from other regions of the world.[citation needed]However, the Arctic Ocean remains relatively clean compared to other marine regions of the world. Common contaminants found in the Arctic region can include heavy metals and persistent organic pollutants (POPs) which subsequently accumulate in the food chain.   These contaminants come from wastewater as well as long-range pollution both based in the atmosphere and from oceanic movement.  Commercial fisheries as well as chemical and waste emissions from resource exploitation including mining, minerals, oil and gas extraction are among the many pollutants.  Moreover, there are approximately 8.3 Billion metric tons of plastic in the Arctic ocean ( dated 2017 ) and an expected 34 billion metric tons in 2050. Economic activity in the Arctic seas is not the only source of pollution. The growing presence of military weapons systems in the region raises concerns of increased pollution.[citation needed] Management of specific risks of marine pollution in the Arctic is governed primarily by national legislation in coastal states, although these take existing international standards into account. Bilateral agreements exist between Arctic states on cooperation in the prevention of marine pollution in the Arctic seas and immediate responses in case of oil spills. Nevertheless, there is no legal framework relating to weapons and other military presence. The first steps in this direction have already been made. After signing the 2010 Treaty on Maritime Delimitation of the continental shelf in the Barents Sea and the Arctic Ocean, Russia and Norway began bilateral consultations on the harmonization of national environmental standards used for the exploration and development of the mineral resources of the shelf. The parties came to an agreement to make a comparative analysis of national legislation and to identify differences concerning measures for preventing the pollution of the environment. A recent report   published by the International Council on Clean Transportation (ICCT) suggested that the reduction of the polar ice caps and the projected increase in shipping activity in the region
could have a severe impact on the levels of pollution experienced across the entire Arctic region but notes that a shift to cleaner sulphur-based fuel could resolve the issue."|2023-09-07-20-59-11
Pollution in the Arctic Ocean|Economic activities| Russia operates a fleet of nuclear-powered icebreakers to open up shipping lanes in the Arctic Ocean year round.  The operation of these vessels is contributing to radioactive contamination of the Arctic environment. The Yamal Megaproject, a natural gas project developed by the Russian oil and gas company, Gazprom, is a development designed to exploit the largest gas reserves on Earth.  Gas spills and climate change effect the indigenous Nentsy population, as the permafrost and water supplies are contaminated.|2023-09-07-20-59-11
Pollution in the Arctic Ocean|Military activities and their effects|" Military activities in the Arctic Ocean have increased substantially. The United States Navy performs an exercise called ICEX where the US Navy practices arctic operations. However, the US military is not the only force that has increased activity in the Arctic Ocean. For example, Russia has recently launched a fleet of ships that are used to transports various cargo, such as cargo and minerals. In order to conduct this transport, these ships, known as ice breakers, are specially designed to break through the beds of ice that fill the Arctic Ocean. Part of the increase in military activity in the arctic stems from the Cold War, in which the US and USSR have made a practice of engaging in trade wars that seep into the modern day. While from a national security and economic perspective, these countries may feel justified in their campaigns, their increased activity has highly disturbed the original environment that once existed. The activity of ice breakers continues to amplify the effects of climate change. As ice serves as a buffer for heat, its breaking results in temperatures reaching extreme heat and cold. In the past, these temperatures were within a much smaller range. Furthermore, icebreakers have caused unprecedented rises in sea level, manifesting in the ""erosion of beaches"" and ""indentation of deltas."" Finally, as ice breakers produce large amounts of sound pollution, this interferes with the sonic communication patterns of sea mammals such as whales and others. While every country's military has a need for efficient cargo transport, the environmental effects of their methods are undeniable."|2023-09-07-20-59-11
Pollution in the Arctic Ocean|Initiatives| In 1996, the Arctic Council was founded to combat conflict and promote cooperation amongst the Arctic States, which encompasses the indigenous peoples and other inhabitants. Their initiative encourages sustainable development and protection against pollution. Furthermore, in 2018, an initiative was launched by the Washington D.C. organization Ocean Conservancy launched a petition for major U.S. firms to stop using arctic trade shipping routes to transport their goods. Nike even joined them in their initiative, pledging not to use arctic trade routes themselves, and helping Ocean Conservancy with the PR aspects of their campaign. Other major firms that followed Nike in their pledge to not use arctic trade routes include Gap, H&M, Columbia, and Kering Group. Ocean Conservancy has also collaborated with the UN in order to help launch this initiative on a more global scale.|2023-09-07-20-59-11
Pollution insurance|General|" Pollution insurance is a type of insurance that covers costs related to pollution. This can include the costs of brownfield restoration and cleanup, liability for injuries and deaths caused by pollution. Most businesses will purchase broad commercial general insurance or property insurance policies but these usually contain an ""absolute pollution exclusion"" and thus rarely cover pollution, although there may be limited pollution coverage. Pollution insurance usually takes the form of first-party coverage for contamination of insured property either by external or on-site sources. Coverage is also afforded for liability to third parties arising from contamination of air, water, or land due to the sudden and accidental release of hazardous materials from the insured site. The policy usually covers the costs of cleanup and may include coverage for releases from underground storage tanks. Intentional acts are specifically excluded. The largest players in this industry are AIG, XL, Ace and Zurich.  One of the purposes for such insurance policies is so that when companies that cause environmental disasters go bankrupt, the victims can still be compensated.  The insurance may also protect against cost overruns or regulatory changes that increase the cost of cleanup.  The director of China's EPA (SEPA) has called for imposing mandatory pollution insurance on polluting industries. According to the Cato Institute, legal theories of joint and several liability (e.g. under Superfund) and requirements by courts that insurers pay to help polluters clean up their own property (regardless of the insurance contract) have hurt the pollution insurance industry; but nonetheless, the basic idea of pollution insurance remains sound. Cato claims, ""With the help of insurers and risk analysts, corporate risk managers select cost-effective products and processes, which minimize the sum of insurance premiums, expected payments to victims (in excess of insurance coverage), and risk-reduction expenditures."""|2019-02-03-15-56-10
Pollution insurance|United States| In the United States, the federal U.S. Comprehensive Environmental Response, Compensation & Liability Act of 1980 (CERCLA), also known as Superfund, is the predominant law. However, there are also important state laws with similar or additional requirements. In 2013, the State of Oregon passed Oregon Environmental Cleanup Assistance Act which significantly increased the responsibility of insurers to cover pollution.|2019-02-03-15-56-10
Pollution insurance|External links| Lexis Nexis 26/2/2013 published the article “Environmental Insurance” on both the Environmental Law Community and the Insurance Law Community|2019-02-03-15-56-10
Pollution prevention in the US|General| Lists Categories Pollution prevention (P2) is a strategy for reducing the amount of waste created and released into the environment, particularly by industrial facilities, agriculture, or consumers. Many large corporations view P2 as a method of improving the efficiency and profitability of production processes through waste reduction and technology advancements.  Legislative bodies have enacted P2 measures, such as the Pollution Prevention Act of 1990 and the Clean Air Act Amendments of 1990 in the United States Congress.|2023-07-04-06-40-08
Pollution prevention in the US|Significance| Pollution prevention is any action that reduces the amount of contaminants released into the environment. Implementation of such processes reduces the severity and/or number of hazards posed to both public health and the environment. Prevention of pollution preserves natural resources and can also have significant financial benefits in large scale processes.  If companies produce less waste, they do not have to worry about proper disposal. Thus, P2 is also a proactive measure to reduce the costs of waste disposal and elimination.|2023-07-04-06-40-08
Pollution prevention in the US|Sources|" Shipping ports are a significant source of pollution due to the heavy cargo traffic that these areas receive. The impact of these ships is quite widespread, affecting coastal communities and ecosystems across the globe. Most major shipping ports are located near environmentally sensitive estuaries. These areas are particularly impacted by high levels of diesel exhaust, particulate matter, nitrogen oxides, ozone, and sulfur oxides. The solution for reducing port-related pollution is multi-fold, encompassing attainable alternatives and long-term reduction goals. Examples of simple steps include a restriction on engine idling for ships in the port and the use of cleaner grade diesel fuels. Some more expensive measures can be taken to mitigate the pollution of ships. Replacing older model ships with ships containing new engines allows the ships to meet modern emission standards. Exhaust systems can also be retrofitted in order to reduce exhaust emissions. Looking ahead into the future, there are a few technologies being developed. For example, plugging ships into ""shore-side"" power sources may eliminate the need for idling engines. Additionally, various sources of alternative fuel are being developed, the most significant of which is a fuel cell unit. Due to increased trade, the emissions from ships are expected to become the second largest source of diesel particulate matter by 2020. One approach to reduction as set forth by the International Forum on Globalization (IFG) is to increase the amount of local trading, thereby reducing the number of miles that ships have to travel. Another approach regards the strategic placement of ports close to land transportation infrastructure such as roads and railroads. Again, this reduces the distance that vehicles have to travel between the initial and final destinations. Railroads that reach all the way to ports are a significant way to produce less toxic pollutants, as this eliminates the need for less-efficient trucks to transport the goods from the coastal port to the inland railroad infrastructure. In 2017, the biggest pollutants included carbon dioxide, nitrogen oxide, hydrocarbons, lead, and particulate matter according to Theilmann in the U.S. Clean Air Act.  These pollutants harm the environment as well as the citizens living in these areas. The pollutants contribute to climate change and can result in acid rain. Citizens living in car-dominant highly populated areas are at the risk of health issues caused by these pollutants, ranging from chronic cough to death. According to Singh, the groups of people most affected by air pollution include children, people suffering from an underlying chronic disease, the asthmatic, and elderly. These groups are faced with an increase in trips to the hospital, worsened cough, episodes of rhinitis, and asthma attacks.  Theilman states that the Clean Air Act has done a successful job at assessing and limiting the pollutants that harm humans from stationary and mobile sources. With policies like the Clean Air Act, and replacement of trees removed by deforestation, humans can reduct their carbon footprint and improve the quality of air."|2023-07-04-06-40-08
Pollution prevention in the US|Health hazards| P2 strategies can mitigate many health hazards associated with pollution. Long-term exposure to certain pollutants can cause cancer, heart disease, asthma, birth defects, and premature death.  Additionally, pollution of bodies of water can be detrimental to biodiversity.|2023-07-04-06-40-08
Pollution prevention in the US|Pollution Prevention Act of 1990| To promote pollution prevention, the United States Congress passed the Pollution Prevention Act of 1990.  Congress declared that pollution should be prevented and reduced wherever possible; in addition, any waste that must be released into the environment must be done in a responsible, environmentally-conscious manner. The law requires the United States Environmental Protection Agency (EPA) to: In order to enforce the points outlined in the act, EPA is directed to present a report to Congress biennially. The act requires that companies fill out a toxic chemical release form allowing EPA to collect information on the levels of pollution released into the environment.|2023-07-04-06-40-08
Pollution prevention in the US|Clean Air Act| The Clean Air Act Amendments of 1990 provided many P2 strategies, including governmental intervention, research and development programs, guidelines for efficient technologies, reduction of vehicle emissions, and a suggested Congressional status report.|2023-07-04-06-40-08
Pollution prevention in the US|2010–2014 Pollution Prevention Program Strategic Plan| The EPA 2010–2014 Pollution Prevention Program Strategic Plan introduced a number of ways to reduce harmful industrial outputs (i.e. greenhouse gases, hazardous materials) while conserving natural resources.|2023-07-04-06-40-08
Pollution prevention in the US|Production techniques| As an environmental management strategy, P2 shares many attributes with cleaner production, a term used more commonly outside the United States. Pollution prevention encompasses more specialized sub-disciplines including green chemistry and green design (also known as environmentally conscious design).|2023-07-04-06-40-08
Pollution prevention in the US|In industrial processes|" The possibilities of P2 strategies in industrial processes are still being implemented at the corporate level, but benefits are already being realized by many companies. The view of P2 in industrial businesses has shifted from one of necessity to one of strategic advantage. If companies invest in P2 methods early in their development, they realize greater gains not too far down the road.  Additionally, if companies do not produce waste, they do not have to worry about properly disposing of it. Thus, P2 is a proactive measure taken to reduce costs in the long run that would have been dedicated to disposal and elimination of waste. There are two main ways to reduce waste through P2: increased efficiency and technology improvements. Waste reduction at the source implies the same amount of input raw materials with less waste and more output of the product. Technology improvements imply changes to the production process that reduce the amount of output waste, such as an improved recycling process. Companies are moving past simply complying with the minimum environmental requirements, and they are taking a more strategic, forward-thinking stance on tackling the issue. One strategy is ""in-process recycling"". Though it is not the most efficient form of ""reduction at the source"", recycling is very profitable due to its ease of process. By engaging in recycling practices, industries not only cut down on the amount of material discarded as environmentally-hazardous waste, but they also increase profitability by reducing the amount of raw material purchased. The most widespread strategy is ""reduction at the source"", which is the idea that byproducts of production can be reduced through efficient and careful use of natural resources. This method reduces the amount of dangerous pollutants present in waste before the waste is released. In turn, this creates a safer environment free of hazardous waste. This idea ties strongly into the benefits to corporations of investing in newer, more efficient technology."|2023-07-04-06-40-08
Pollution prevention in the US|P2 task force| In order to reduce costs of P2 techniques, many officials are turning to pollution elimination strategies, thereby eliminating any need for end-of-pipe solutions. A task force was created by the EPA in order to directly target reduction strategies. The P2 program task force has 5 main goals:|2023-07-04-06-40-08
Pollution prevention in the US|Voluntary approaches|" Voluntary approaches to P2 are on the rise. Governmental organization often collaborate with businesses and regulatory agencies to create a structure of guidelines. There are four types of voluntary approach programs: public voluntary programs, negotiated agreements, unilateral commitments, and private agreements. Public voluntary agreements are the least restrictive. Environmental authorities collaborate and create specific guidelines. Companies are then invited to follow these procedures on a strictly voluntary basis. Negotiated agreements are created through collaboration between public authorities and industry authorities. The agreement establishes bargains that are beneficial to the industry. Unilateral commitments are established by industry authorities alone, and the guidelines they set are self-regulated. Private agreements are established between ""polluters"" and other affected parties. The regulations set forth create a compromise regarding a variety of pollution regulation strategies. The United States mainly follows the end-of-pipe prevention strategy. However, US President Richard Nixon created the Environmental Protection Agency (EPA) in 1970, and one of its principal missions was to regulate pollution. EPA's implementation of policies is almost entirely voluntary. There are a few keys to a successful voluntary approach. First, the program needs a dependable source of funding (from the government, usually). The program also needs a dynamic relationship with the targeted industries. This creates a base of trust between all involved in the agreement. In terms of regulation, the program should be monitored by a reliable source. In order to assure that the program will establish itself long term, there should be visible benefits to the participants and obvious results to the greater community. The long-term establishment of the program also comes from setting attainable goals to measure progress."|2023-07-04-06-40-08
Pollution prevention in the US|Governmental approaches| EPA has published waste minimization guidelines that comprise 5 major steps: This framework mainly benefits smaller facilities.|2023-07-04-06-40-08
Pollution prevention in the US|Waste reduction algorithm|" The EPA makes available software that employs the Waste Reduction Algorithm. They use the acronym WAR for this method and state ""the goal of WAR is to reduce environmental and related human health impacts at the design stage"".  The WAR tracks pollutants through the entire production process in order to obtain accurate measurements."|2023-07-04-06-40-08
Pollution prevention in the US|Industrial efforts| By maximizing P2 opportunities, some companies choose to redesign their entire industrial process. Managers focus more on what enters and moves through the entire process, instead of only focusing on the output. Overall, the P2 strategies that financially benefit companies are the most likely to be implemented. However, since P2 has only recently[when?] been realized as a benefit, many corporations have not adopted significant measures to realize the potential gain. Pollution prevention can also be viewed as a form of environmental entrepreneurship, as companies see opportunities to reduce costs of waste treatment, storage, and disposal. For example, 3M has accrued a savings of over $750 million since 1973 due to their implementation of P2 incentives.[citation needed] If implemented correctly, P2 strategies can result in an increase in process yield. By reducing the amount of pollution released, companies can avoid some of the liability costs accrued when large amounts of pollution are released and contaminate the land on which the facility is located.|2023-07-04-06-40-08
Pollution prevention in the US|Individual efforts| According to EPA, there are some everyday steps that can be taken to prevent pollution: Additional examples of P2 include using energy efficient machinery, developing clean-burning fuel, reducing the amount of chemicals released into water sources, creating a production process that results in a reduced amount of waste, and utilizing water conservation techniques.|2023-07-04-06-40-08
Pollution Pricing|General| Pollution pricing reform (PPR) is the process of adjusting market prices to include direct environmental impact on measurable parameters, as e.g. dust and gas exhaust from combustion engines especially in road traffic.   This approach differs from general ecologic taxation schemes, where the measurability of the impact is not available on short term or direct comparison of the tax affected choice for investment or operation. An externality (a type of market failure) exists where a market price omits pollution raise and affection. In such a situation, rational (self-interested) fiscal or individual decisions can lead to environmental harm, as well as to economic distortions and inefficiencies.|2021-10-05-12-33-57
Pollution Pricing|Pollution adjusted taxation schemes|" Environmental pricing reform can be economy-wide, or more focused (e.g. specific to a sector (such as electric power generation or traffic) or a particular environmental issue (such as climate change). ""Market based instruments"" or ""economic instrument for environmental protections"" may be tailored according to measurable impact of pollution as an escape to the permanent neglection of climate impact by polluting technologies. 
Examples include green tax-shifting (ecotaxation), tradeable pollution permits, or the creation of markets for ecological technologies. There are generally three types of options available:"|2021-10-05-12-33-57
Pollution-induced community tolerance|General| Pollution-induced community tolerance (PICT)   is an approach to measuring the response of pollution-induced selective pressures on a community. It is an eco-toxicological tool that approaches community tolerance to pollution from a holistic standpoint. Community Tolerance can increase in one of three ways: physical adaptations or phenotypic plasticity, selection of favorable genotypes, and the replacement of sensitive species by tolerant species in a community. PICT differs from the population tolerance approach to community tolerance in that it can be easily applied to any ecosystem and it is not critical to use a representative test organism, as with the population tolerance approach.|2023-09-17-10-02-27
Pollution-induced community tolerance|Community tolerance| Community tolerance can be used as indicator for determining if a toxicant has a disturbance on an exposed community for multiple types of organisms.  Tolerance of a toxicant can increase by three ways: physiological adaptation, also known as the phenotypic plasticity of an individual; tolerant genotypes selected within a population over time; and the replacement of species with more tolerant ones within a community.  Physiological adaptation, or phenotypic plasticity, is the ability of an individual organism to change its phenotype in response to changes in the environment.  This can occur with huge variance between the type of organism and the type of the disturbance they experience. Natural selection that occurs over several generations causes an entire population to exhibit specific selection of genotypes.  Over time, tolerant genotypes can be selected over non-tolerant ones and can cause a shift in a population’s genome.  Natural selection can also cause a replacement of less tolerant species with more tolerant species.  All of these aspects can alter a community's structure drastically, and if a toxicant can be identified as the culprit, action can take place to prevent  the accumulation and environmental impacts of that toxicant.  PICT can be used for linkage between cause  (exposure) and effect of the toxicants due to the structure of a community that has survived the event, also known as toxicant-induced succession (TIS).  Toxicant-induced succession would be the development of more tolerant generations once a chemical was introduced into the environment. This is why the PICT method is most often applied to communities with short generation times such as microbial and algal communities,    whereas there are rare works that use the PICT tool on organisms other than microorganisms. There are two types of tolerances that can occur: multiple and co-tolerance.   Multiple tolerances can elevate an individual' ability to tolerate several toxicants present at once.   This means that the type of chemicals present in the environment, the concentration, and the organisms that are affected could alter the environment in multiple different ways. Co-tolerance is the ability of an organism to develop a tolerance to a certain toxicant in short-term tests, and obtain that tolerance for other toxicants similar to the first.   Furthermore, co-tolerance depends on the interaction of different factors, such as the toxicant to which communities have been chronically exposed, the mode(s) of action of the different toxicants, but also the detoxification mechanisms implemented by the organisms, and the targeted biological community (e.g., heterotrophs, heterotrophs)  It can be difficult to determine which type of tolerance is occurring if there are multiple types of toxicants in a community because they could be acting simultaneously. Basically it is difficult to understand what exactly may be going on in a community without testing it with multiple ecotoxicological tools with long- and short-term toxicity tests.  However, in the context of biomonitoring with the PICT method, co-tolerance could be an advantage, as it would allow working step by step. A first step would then be to use a model toxicant from a class of pollutants (selected on the basis of their co-tolerance properties) in order to limit the number of suspect pollutants in the ecotoxicological assessment of an environment   For example, a PICT study on soil in microcosms amended with organic manure observed co-tolerance only between antibiotics of the same group (oxytetracycline and tetracycline), as expected from their identical mode of action.  This type of approach is still preliminary, there is a need for further in situ studies combining PICT with chemical monitoring of environments, experimental work under controlled laboratory conditions, and integration of PICT into modeling approaches to precise theses parterns of co-tolerance.     A complementary approach derived from Pesce et al. (2011)  and Foulquier et al. (2015)  was recently applied by Tlili et al (2020),  combining passive sampling systems with PICT bioassays on river biofilms collected upstream and downstream of wastewater treatment plants at two-year intervals (before and after a change in effluent treatment). Their results demonstrate the value of combining the PICT approach with the use of passive sampler extracts to establish causality between in situ exposure to complex mixtures of micropollutants and ecotoxic effects on autotrophic and heterotrophic microbial communities.|2023-09-17-10-02-27
Pollution-induced community tolerance|Field studies| Assessing pollution-induced community tolerance can be done utilizing in situ techniques, many of which involve the use of known or created chemical exposure gradients. One example is the use of a known concentration gradient of Tri-n-butylin to assess PICT in periphyton.  Tolerance patterns showed that tolerance was highest closest to the marina that was the source of contamination. The use of reference sites in addition to contaminated sites is also commonly used for translocation assessments of PICT. A study in Germany cultured periphyton on glass discs in two river systems north of Leipzig, Germany. One system was the contaminated area of study and the other was 10 km upstream and uncontaminated, intended to be used as a reference.  After the colonization period, 6 of the 10 racks of glass discs were trans-located to the other river system. During the experiment the community structure present on the glass discs from the reference site, when translocated to the contaminated site changed to mirror that of the control discs that were left in the contaminated sites.  Note the interest of using long-term observation sites to address the consequences of contamination and restoration of environments with PICT. One example is the Ardières-Morcille river hydrology observatory in the Beaujolais (vineyard region, France) where numerous in situ PICT studies have been conducted,      and which has also been used for many PICT experiments in the laboratory (see for example  ). Of particular note at this site is the study conducted by Pesce et al. (2016)  during three consecutive years (2009-2011): The authors monitored the decrease in tolerance of periphyton to diuron (herbicide applied in vineyards and banned since 2008) downstream of the river crossing this wine-growing watershed. Their results showed a direct link between the evolution of an agricultural practice (here the banning of diuron) and ecological changes in the river (through the loss of tolerance measured by the PICT). Lake Geneva is one of the subalpine lakes studied for a long time by the International Commission for the Protection of Lake Geneva (CIPEL). A PICT study carried out 12 years apart showed the restoration of the lake's phytoplankton with a concomitant decrease in the tolerance of the communities towards atrazine and copper and in the concentrations of these two contaminants (following legislation limiting the use of atrazine in agriculture and framing the management of industrial waste).   In another study in Denmark, enclosure experiments were done allowing for an assessment of PICT utilizing the lake water from Lake Bure as a baseline. By using this water from the lake potentially confounding variables would be nullified by comparing results to the control. Concentrations of atrazine and copper were added to these enclosures in varying concentrations. As in other experiments previously discussed periphyton communities were used in this experiment and were cultured using glass discs. Photosynthetic activity was measured and used as a measurement of PICT throughout the experiment. The experiment showed that elevated levels of Cu lead to community tolerance of the phytoplankton community as well as co-tolerance of zinc. Total Biomass decreased at the outset of the trials involving high concentrations of Cu indicating that Community Tolerance was increased due to direct mortality of the sensitive species.  The in situ study of Bérard & Benninghoff, (2001)  in enclosures repeated over several years in the lake of Geneva, showed that the tolerance of phytoplankton to the herbicide atrazine (photosynthesis inhibitor) varied according to the seasons during which the experiment was carried out. These changes in tolerance for the same toxicant and at the same concentration (10 µg L-1) were probably linked to the initial compositions of the algal and cyanobacterial communities and to environmental factors associated with seasonal parameters (temperature, light, nutrients, etc.). PICT studies on large spatial scales are rare and difficult to conduct, note this European study by Blanck et al (2003)  on zinc tolerance in river periphyton. The use of PICT in an in situ fashion is not limited to aquatic systems.  Studies on soil microbial communities have been conducted in industrial and agricultural contexts  (see the review of Boivin et al., 2002).  Thus, for examples in the context of industrial contamination a study involving 2,4,6-Trinitrotoluene utilized respirometric techniques to measure Pollution-Induced Community Tolerance in soil microbial communities in response to the presence of TNT. The results of this study further corroborate the PICT Theory, in that treatments with long-term exposure to TNT had a larger proportion of TNT-resistant bacteria than soils with low levels of TNT.  This PICT caused by TNT was also present in another study.  Microrespirometry measurements carried out on soil samples taken along three transects have highlighted the tolerance to Pb in a site bordering a lead smelter with long-term polymetallic contamination dominated by Pb. The PICT established causal relationships between Pb and its effect on microbial communities by considering the history of environmental contamination at the community level. Furthermore, a positive correlation between community metabolic quotient and PICT suggested that the acquisition of Pb stress-induced tolerance would have resulted in a higher energetic cost for microbial communities to cope with metal toxicity.  Other studies have highlighted in various contexts, this cost of contaminant-induced tolerance.    In the context of agricultural contamination, Bérard et al. (2004)  validated the PICT tool (by measuring photosynthetic activity) for atrazine tolerance on edaphic microalgae, comparing soils of conventionally and organically farmed corn fields. Changes in taxonomic structure of diatom communities sampled from soils under both types of farming practices, as well as nanocosm experiments, confirmed the selective effect of atrazine. In terrestrial environment, the PICT method is widely applied to metal contamination and since 2010, to antibiotic tolerance in interaction with metal contamination. Ideally, pollution-induced community tolerance can be assessed in the field by using a representative sample of the natural community in response to environmental contamination. However, this is not always the case, which is why laboratory studies are necessary supplements to properly assess PICT.|2023-09-17-10-02-27
Pollution-induced community tolerance|Experimental studies| Experimental investigation of PICT is perform to eliminate factors other than pollution that may affect community structure and ecophysiology,  or on the contrary to study them (by controlling them).     Much work has been done in controlled systems (see reviews by Blanck, 2002  and Tlili et al. 2016 ). They can be conducted in conjunction with field work, as in the study by Blanck and Dahl (1996). In this study, the results from laboratory acute toxicity tests of TBT on periphyton corroborated the results from the field study, supporting the conclusion that toxicity to periphyton resulted from TBT pollution at the site under investigation.  The results from acute toxicity tests can thus help determine whether the effect identified is due to a specific contaminant. Bérard et al. (2003)  used experimental systems of increasing complexity associated with monitoring of sites in Lake Geneva more or less contaminated by Irgarol (antifouling inhibitor of photosynthesis), and crossed the results of ecotoxicity of strains isolated from these sites and from non-contaminated control sites with experiments in nanocosms and PICT measurements. This work highlighted the high toxicity of Irgarol (compared to atrazine having the same site of action, and presenting a co-tolerance) on periphyton and phytoplankton and its potential for selection pressure at existing concentrations in the lake.|2023-09-17-10-02-27
Pollution-induced community tolerance|The PICT methodology|" There are a variety of methods for laboratory testing PICT, but a general format includes sampling, a bioassay, and an analysis of community structure. Samples can be collected on artificial or natural substrata, either in situ or in the laboratory.  There must be a series of samples exposed to different concentrations of contaminant and a control sample. In situ sampling involves setting up a sampling device in an aquatic ecosystem and allowing it to colonize for some time (e.g. a couple of weeks). One example is the diatometer, a device that is deployed in the water that becomes colonized by diatoms, and then is removed for analysis.  In situ sampling devices are set up at increasing distances from the pollution source in the case of point source pollution. The samples thus represent a gradient in contaminant concentration, assuming that the contaminant becomes more dilute with increasing distance from the point source. An example of laboratory sampling was used in a study by Schmitt-Jansen and Altenburger (2005). For 14 days communities were allowed to establish on discs set up in laboratory aquariums which were continuously mixed and inoculated with algae from a pond. The aquariums were dosed with different concentrations of herbicide to get a gradient of long-term (14-day) contaminant exposures. Once a week the aquarium water was completely replaced and re-dosed with herbicide.  Terrestrial studies pose other difficulties because it is difficult to use colonization systems by the communities investigated.  Generally one samples the soil with its intrinsic heterogeneity and components other than microorganisms (minerals, organics ..), which increases measurement difficulties and biases related to contaminant bioavailability. A bioassay is conducted on the samples to test for correlation between tolerance and long-term contaminant exposure. First, samples are exposed to different concentrations of contaminant. Then an endpoint is measured to determine the toxic effect on the sample organisms. The results from these measurements are used to produce  a dose-response curve and an EC50.  Both Blanck (1996) and Schmitt-Jansen and Altenburger (2005) photosynthesis as their endpoint.   Since the work of Blanck et al. (1988), other endpoints have been tested such as: induced fluorescence,  PAM fluorimetry,    leucine incorporation and eco-plates,    microbial respiration,     enzyme activities,    potential ammonium oxidation assay ... In order to use the PICT method in biomonitoring and environmental risk assessment, it is necessary to advance in the standardization of these bioassays, both for the sampling of the tested communities, but also for the bioassays themselves. Community structure of the samples is analyzed to check for a correlation between species prevalence and long-term contaminant exposure. Samples are taxonomically classified to determine the composition and species diversity of the communities that established over the long term exposures. The results are compared to the concentration of contaminant in the long-term exposure to conclude if a relationship was found in the study.  Recent developments in microbial ecology using molecular biology    and ""omics"" methods,  chemotaxonomy methods,  functional diversity measurements,   biological trait and biological interaction network approaches,  are varied, complementary and promising ecological tools to address PICT selection pressure."|2023-09-17-10-02-27
Radio spectrum pollution|General|" Lists Categories Radio spectrum pollution is the straying of waves in the radio and electromagnetic spectrums outside their allocations that cause problems for some activities.   It is of particular concern to radio astronomers. Radio spectrum pollution is mitigated by effective spectrum management.  Within the United States, the Communications Act of 1934 grants authority for spectrum management to the President for all federal use (47 U.S.C. 305). The National Telecommunications and Information Administration (NTIA) manages the spectrum for the Federal Government. Its rules are found in the ""NTIA Manual of Regulations and Procedures for Federal Radio Frequency Management"".  The Federal Communications Commission (FCC) manages and regulates all domestic non-federal spectrum use (47 U.S.C. 301).   Each country typically has its own spectrum regulatory organization.  Internationally, the International Telecommunication Union (ITU) coordinates spectrum policy."|2022-11-22-14-37-09
Radio spectrum pollution|External links| This astronomy-related article is a stub. You can help Wikipedia by expanding it.|2022-11-22-14-37-09
Radioactive contamination|General|" Lists Categories Radioactive contamination, also called radiological pollution, is the deposition of, or presence of radioactive substances on surfaces or within solids, liquids, or gases (including the human body), where their presence is unintended or undesirable (from the International Atomic Energy Agency (IAEA) definition). Such contamination presents a hazard because the radioactive decay of the contaminants produces ionizing radiation (namely alpha, beta, gamma rays and free neutrons). The degree of hazard is determined by the concentration of the contaminants, the energy of the radiation being emitted, the type of radiation, and the proximity of the contamination to organs of the body. It is important to be clear that the contamination gives rise to the radiation hazard, and the terms ""radiation"" and ""contamination"" are not interchangeable. The sources of radioactive pollution can be classified into two groups: natural and man-made. Following an atmospheric nuclear weapon discharge or a nuclear reactor containment breach, the air, soil, people, plants, and animals in the vicinity will become contaminated by nuclear fuel and fission products. A spilled vial of radioactive material like uranyl nitrate may contaminate the floor and any rags used to wipe up the spill. Cases of widespread radioactive contamination include the Bikini Atoll, the Rocky Flats Plant in Colorado, the area near the Fukushima Daiichi nuclear disaster, the area near the Chernobyl disaster, and the area near the Mayak disaster."|2023-07-02-14-09-55
Radioactive contamination|Sources of contamination|" The sources of radioactive pollution can be natural or man-made. Radioactive contamination can be due to a variety of causes. It may occur due to the release of radioactive gases, liquids or particles. For example, if a radionuclide used in nuclear medicine is spilled (accidentally or, as in the case of the Goiânia accident, through ignorance), the material could be spread by people as they walk around. Radioactive contamination may also be an inevitable result of certain processes, such as the release of radioactive xenon in nuclear fuel reprocessing. In cases that radioactive material cannot be contained, it may be diluted to safe concentrations. For a discussion of environmental contamination by alpha emitters please see actinides in the environment. Nuclear fallout is the distribution of radioactive contamination by the 520 atmospheric nuclear explosions that took place from the 1950s to the 1980s. In nuclear accidents, a measure of the type and amount of radioactivity released, such as from a reactor containment failure, is known as the source term. The United States Nuclear Regulatory Commission defines this as ""Types and amounts of radioactive or hazardous material released to the environment following an accident."" Contamination does not include residual radioactive material remaining at a site after the completion of decommissioning. Therefore, radioactive material in sealed and designated containers is not properly referred to as contamination, although the units of measurement might be the same."|2023-07-02-14-09-55
Radioactive contamination|Containment|" Containment is the primary way of preventing contamination from being released into the environment or coming into contact with or being ingested by humans. Being within the intended Containment differentiates radioactive material from radioactive contamination. When radioactive materials are concentrated to a detectable level outside a containment, the area affected is generally referred to as ""contaminated"". There are a large number of techniques for containing radioactive materials so that it does not spread beyond the containment and become contaminated. In the case of liquids, this is by the use of high integrity tanks or containers, usually with a sump system so that leakage can be detected by radiometric or conventional instrumentation. Where the material is likely to become airborne, then extensive use is made of the glovebox, which is a common technique in hazardous laboratory and process operations in many industries. The gloveboxes are kept under slight negative pressure and the vent gas is filtered in high-efficiency filters, which are monitored by radiological instrumentation to ensure they are functioning correctly."|2023-07-02-14-09-55
Radioactive contamination|Naturally occurring radioactivity| A variety of radionuclides occur naturally in the environment. Elements like uranium and thorium, and their decay products, are present in rock and soil. Potassium-40, a primordial nuclide, makes up a small percentage of all potassium and is present in the human body. Other nuclides, like carbon-14, which is present in all living organisms, are continuously created by cosmic rays. These levels of radioactivity pose little danger but can confuse measurement. A particular problem is encountered with naturally generated radon gas which can affect instruments that are set to detect contamination close to normal background levels and can cause false alarms. Because of this skill is required by the operator of radiological survey equipment to differentiate between background radiation and the radiation which emanates from contamination. Naturally occurring radioactive materials (NORM) can be brought to the surface or concentrated by human activities such as mining, oil and gas extraction, and coal consumption.|2023-07-02-14-09-55
Radioactive contamination|Control and monitoring of contamination| Radioactive contamination may exist on surfaces or in volumes of material or air, and specialized techniques are used to measure the levels of contamination by detection of the emitted radiation.|2023-07-02-14-09-55
Radioactive contamination|Contamination monitoring|" Contamination monitoring depends entirely upon the correct and appropriate deployment and utilisation of radiation monitoring instruments. Surface contamination may either be fixed or ""free"". In the case of fixed contamination, the radioactive material cannot by definition be spread, but its radiation is still measurable. In the case of free contamination, there is the hazard of contamination spread to other surfaces such as skin or clothing, or entrainment in the air.  A concrete surface contaminated by radioactivity can be shaved to a specific depth, removing the contaminated material for disposal. For occupational workers, controlled areas are established where there may be a contamination hazard. Access to such areas is controlled by a variety of barrier techniques, sometimes involving changes of clothing and footwear as required. The contamination within a controlled area is normally regularly monitored. Radiological protection instrumentation (RPI) plays a key role in monitoring and detecting any potential contamination spread, and combinations of hand held survey instruments and permanently installed area monitors such as Airborne particulate monitors and area gamma monitors are often installed. Detection and measurement of surface contamination of personnel and plant are normally by Geiger counter, scintillation counter or proportional counter. Proportional counters and dual phosphor scintillation counters can discriminate between alpha and beta contamination, but the Geiger counter cannot. Scintillation detectors are generally preferred for hand-held monitoring instruments and are designed with a large detection window to make monitoring of large areas faster. Geiger detectors tend to have small windows, which are more suited to small areas of contamination. The spread of contamination by personnel exiting controlled areas in which nuclear material is used or processed is monitored by specialised installed exit control instruments such as frisk probes, hand contamination monitors and whole body exit monitors. These are used to check that persons exiting controlled areas do not carry contamination on their bodies or clothes. In the United Kingdom, HSE has issued a user guidance note on selecting the correct portable radiation measurement instrument for the application concerned.  This covers all radiation instrument technologies and is a useful comparative guide for selecting the correct technology for the contamination type. The UK NPL publishes a guide on the alarm levels to be used with instruments for checking personnel exiting controlled areas in which contamination may be encountered. 
Surface contamination is usually expressed in units of radioactivity per unit of area for alpha or beta emitters. For SI, this is becquerels per square meter (or Bq/m2). Other units such as picoCuries per 100 cm2 or disintegrations per minute per square centimeter (1 dpm/cm2 = 167 Bq/m2) may be used."|2023-07-02-14-09-55
Radioactive contamination|Airborne contamination|" The air can be contaminated with radioactive isotopes in particulate form, which poses a particular inhalation hazard. Respirators with suitable air filters or completely self-contained suits with their own air supply can mitigate these dangers. Airborne contamination is measured by specialist radiological instruments that continuously pump the sampled air through a filter. Airborne particles accumulate on the filter and can be measured in a number of ways: Commonly a semiconductor radiation detection sensor is used that can also provide spectrographic information on the contamination being collected. A particular problem with airborne contamination monitors designed to detect alpha particles is that naturally occurring radon can be quite prevalent and may appear as contamination when low contamination levels are being sought. Modern instruments consequently have ""radon compensation"" to overcome this effect."|2023-07-02-14-09-55
Radioactive contamination|Internal human contamination| Radioactive contamination can enter the body through ingestion, inhalation, absorption, or injection. This will result in a committed dose. For this reason, it is important to use personal protective equipment when working with radioactive materials. Radioactive contamination may also be ingested as the result of eating contaminated plants and animals or drinking contaminated water or milk from exposed animals. Following a major contamination incident, all potential pathways of internal exposure should be considered. Successfully used on Harold McCluskey, chelation therapy and other treatments exist for internal radionuclide contamination.|2023-07-02-14-09-55
Radioactive contamination|Decontamination|" Cleaning up contamination results in radioactive waste unless the radioactive material can be returned to commercial use by reprocessing. In some cases of large areas of contamination, the contamination may be mitigated by burying and covering the contaminated substances with concrete, soil, or rock to prevent further spread of the contamination to the environment. If a person's body is contaminated by ingestion or by injury and standard cleaning cannot reduce the contamination further, then the person may be permanently contaminated.[citation needed] Contamination control products have been used by the U.S. Department of Energy (DOE) and the commercial nuclear industry for decades to minimize contamination on radioactive equipment and surfaces and fix contamination in place.  ""Contamination control products"" is a broad term that includes fixatives, strippable coatings, and decontamination gels.  A fixative product functions as a permanent coating to stabilize residual loose/transferable radioactive contamination by fixing it in place; this aids in preventing the spread of contamination and reduces the possibility of the contamination becoming airborne, reducing workforce exposure and facilitating future deactivation and decommissioning (D&D) activities. Strippable coating products are loosely adhered to paint-like films and are used for their decontamination abilities.  They are applied to surfaces with loose/transferable radioactive contamination and then, once dried, are peeled off, which removes the loose/transferable contamination along with the product.  The residual radioactive contamination on the surface is significantly reduced once the strippable coating is removed.  Modern strippable coatings show high decontamination efficiency and can rival traditional mechanical and chemical decontamination methods. Decontamination gels work in much the same way as other strippable coatings. The results obtained through the use of contamination control products are variable and depend on the type of substrate, the selected contamination control product, the contaminants, and the environmental conditions (e.g., temperature, humidity, etc.).[2] Some of the largest areas committed to be decontaminated are in the Fukushima Prefecture, Japan. The national government is under pressure to clean up radioactivity due to the Fukushima nuclear accident of March 2011 from as much land as possible so that some of the 110,000 displaced people can return.  Stripping out the key radioisotope threatening health (caesium-137) from low-level waste could also dramatically decrease the volume of waste requiring special disposal.  A goal is to find techniques that might be able to strip out 80 to 95% of the caesium from contaminated soil and other materials, efficiently and without destroying the organic content in the soil. One being investigated is termed hydrothermal blasting.  The caesium is broken away from soil particles and then precipitated with ferric ferricyanide (Prussian blue). It would be the only component of the waste requiring special burial sites.  The aim is to get annual exposure from the contaminated environment down to one millisievert (mSv) above background. The most contaminated area where radiation doses are greater than 50 mSv/year must remain off-limits, but some areas that are currently less than 5 mSv/year may be decontaminated allowing 22,000 residents to return. To help protect people living in geographical areas which have been radioactively contaminated, the International Commission on Radiological Protection has published a guide: ""Publication 111 – Application of the Commission's Recommendations to the Protection of People Living in Long-term Contaminated Areas after a Nuclear Accident or a Radiation Emergency""."|2023-07-02-14-09-55
Radioactive contamination|Low-level contamination|" The hazards to people and the environment from radioactive contamination depend on the nature of the radioactive contaminant, the level of contamination, and the extent of the spread of contamination. Low levels of radioactive contamination pose little risk, but can still be detected by radiation instrumentation.[citation needed] If a survey or map is made of a contaminated area, random sampling locations may be labeled with their activity in becquerels or curies on contact. Low levels may be reported in counts per minute using a scintillation counter. In the case of low-level contamination by isotopes with a short half-life, the best course of action may be to simply allow the material to naturally decay. Longer-lived isotopes should be cleaned up and properly disposed of because even a very low level of radiation can be life-threatening when in long exposure to it. Facilities and physical locations that are deemed to be contaminated may be cordoned off by a health physicist and labeled ""Contaminated area."" Persons coming near such an area would typically require anti-contamination clothing (""anti-Cs"")."|2023-07-02-14-09-55
Radioactive contamination|High-level contamination|" High levels of contamination may pose major risks to people and the environment. People can be exposed to potentially lethal radiation levels, both externally and internally, from the spread of contamination following an accident (or a deliberate initiation) involving large quantities of radioactive material. The biological effects of external exposure to radioactive contamination are generally the same as those from an external radiation source not involving radioactive materials, such as x-ray machines, and are dependent on the absorbed dose. When radioactive contamination is being measured or mapped in situ, any location that appears to be a point source of radiation is likely to be heavily contaminated. A highly contaminated location is colloquially referred to as a ""hot spot."" On a map of a contaminated place, hot spots may be labeled with their ""on contact"" dose rate in mSv/h. In a contaminated facility, hot spots may be marked with a sign, shielded with bags of lead shot, or cordoned off with warning tape containing the radioactive trefoil symbol. The hazard from contamination is the emission of ionizing radiation. The principal radiations which will be encountered are alpha, beta and gamma, but these have quite different characteristics. They have widely differing penetrating powers and radiation effects, and the accompanying diagram shows the penetration of these radiations in simple terms. For an understanding of the different ionising effects of these radiations and the weighting factors applied, see the article on absorbed dose. Radiation monitoring involves the measurement of radiation dose or radionuclide contamination for reasons related to the assessment or control of exposure to radiation or radioactive substances, and the interpretation of the results. The methodological and technical details of the design and operation of environmental radiation monitoring programmes and systems for different radionuclides, environmental media and types of facility are given in IAEA Safety Standards Series No. RS–G-1.8  and in IAEA Safety Reports Series No. 64."|2023-07-02-14-09-55
Radioactive contamination|Biological effects|" Radioactive contamination by definition emits ionizing radiation, which can irradiate the human body from an external or internal origin. This is due to radiation from contamination located outside the human body. The source can be in the vicinity of the body or can be on the skin surface. The level of health risk is dependent on duration and the type and strength of irradiation. Penetrating radiation such as gamma rays, X-rays, neutrons or beta particles pose the greatest risk from an external source. Low penetrating radiation such as alpha particles have a low external risk due to the shielding effect of the top layers of skin. See the article on sievert for more information on how this is calculated. Radioactive contamination can be ingested into the human body if it is airborne or is taken in as contamination of food or drink, and will irradiate the body internally. The art and science of assessing internally generated radiation dose is Internal dosimetry. The biological effects of ingested radionuclides depend greatly on the activity, the biodistribution, and the removal rates of the radionuclide, which in turn depends on its chemical form, the particle size, and route of entry. Effects may also depend on the chemical toxicity of the deposited material, independent of its radioactivity. Some radionuclides may be generally distributed throughout the body and rapidly removed, as is the case with tritiated water. Some organs concentrate certain elements and hence radionuclide variants of those elements. This action may lead to much lower removal rates. For instance, the thyroid gland takes up a large percentage of any iodine that enters the body. Large quantities of inhaled or ingested radioactive iodine may impair or destroy the thyroid, while other tissues are affected to a lesser extent. Radioactive iodine-131 is a common fission product; it was a major component of the radioactivity released from the Chernobyl disaster, leading to nine fatal cases of pediatric thyroid cancer and hypothyroidism. On the other hand, radioactive iodine is used in the diagnosis and treatment of many diseases of the thyroid precisely because of the thyroid's selective uptake of iodine. The radiation risk proposed by the International Commission on Radiological Protection (ICRP) predicts that an effective dose of one sievert (100 rem) carries a 5.5% chance of developing cancer. Such a risk is the sum of both internal and external radiation doses. The ICRP states ""Radionuclides incorporated in the human body irradiate the tissues over time periods determined by their physical half-life and their biological retention within the body. Thus they may give rise to doses to body tissues for many months or years after the intake. The need to regulate exposures to radionuclides and the accumulation of radiation dose over extended periods of time has led to the definition of committed dose quantities"". 
The ICRP further states ""For internal exposure, committed effective doses are generally determined from an assessment of the intakes of radionuclides from bioassay measurements or other quantities (e.g., activity retained in the body or in daily excreta). The radiation dose is determined from the intake using recommended dose coefficients"". The ICRP defines two dose quantities for individual committed dose: Committed equivalent dose, H T(t) is the time integral of the equivalent dose rate in a particular tissue or organ that will be received by an individual following intake of radioactive material into the body by a Reference Person, where t is the integration time in years. 
This refers specifically to the dose in a specific tissue or organ, in a similar way to external equivalent dose. Committed effective dose, E(t) is the sum of the products of the committed organ or tissue equivalent doses and the appropriate tissue weighting factors WT, where t is the integration time in years following the intake. The commitment period is taken to be 50 years for adults, and to age 70 years for children.  This refers specifically to the dose to the whole body, in a similar way to external effective dose."|2023-07-02-14-09-55
Radioactive contamination|Social and psychological effects|" A 2015 report in Lancet explained that serious impacts of nuclear accidents were often not directly attributable to radiation exposure, but rather social and psychological effects.  The consequences of low-level radiation are often more psychological than radiological. Because damage from very low-level radiation cannot be detected, people exposed to it are left in anguished uncertainty about what will happen to them. Many believe they have been fundamentally contaminated for life and may refuse to have children for fear of birth defects. They may be shunned by others in their community who fear a sort of mysterious contagion. Forced evacuation from a radiological or nuclear accident may lead to social isolation, anxiety, depression, psychosomatic medical problems, reckless behavior, even suicide. Such was the outcome of the 1986 Chernobyl nuclear disaster in Ukraine. A comprehensive 2005 study concluded that ""the mental health impact of Chernobyl is the largest public health problem unleashed by the accident to date"".  Frank N. von Hippel, a U.S. scientist, commented on 2011 Fukushima nuclear disaster, saying that ""fear of ionizing radiation could have long-term psychological effects on a large portion of the population in the contaminated areas"".  Evacuation and long-term displacement of affected populations create problems for many people, especially the elderly and hospital patients. Such great psychological danger does not accompany other materials that put people at risk of cancer and other deadly illness. Visceral fear is not widely aroused by, for example, the daily emissions from coal burning, although, as a National Academy of Sciences study found, this causes 10,000 premature deaths a year in the US population of 317,413,000. Medical errors leading to death in U.S. hospitals are estimated to be between 44,000 and 98,000. It is ""only nuclear radiation that bears a huge psychological burden – for it carries a unique historical legacy""."|2023-07-02-14-09-55
Radioactive contamination|See also| Nuclear technology portal|2023-07-02-14-09-55
Radium and radon in the environment|General| Lists Categories Radium and radon are important contributors to environmental radioactivity. Radon occurs naturally as a result of decay of radioactive elements in soil and it can accumulate in houses built on areas where such decay occurs. Radon is a major cause of cancer; it is estimated to contribute to ~2% of all cancer related deaths in Europe. Radium, like radon, is radioactive and is found in small quantities in nature and is hazardous to life if radiation exceeds 20-50 mSv/year. Radium is a decay product of uranium and thorium.  Radium may also be released into the environment by human activity, for example, in improperly discarded products painted with radioluminescent paint.|2023-09-02-18-15-50
Radium and radon in the environment|In the oil and gas industries| Residues from the oil and gas industry often contain radium and its daughters. The sulfate scale from an oil well can be very radium rich. The water inside an oil field is often very rich in strontium, barium and radium while seawater is very rich in sulfate so if water from an oil well is discharged into the sea or mixed with seawater the radium is likely to be brought out of solution by the barium/strontium sulfate which acts as a carrier precipitate.|2023-09-02-18-15-50
Radium and radon in the environment|Radioluminescent (glow in the dark) products| Local contamination from radium-based radioluminescent paints having been improperly disposed of is not unknown.|2023-09-02-18-15-50
Radium and radon in the environment|In radioactive quackery| Eben Byers was a wealthy American socialite whose death in 1932 from using a radioactive quackery product called Radithor is a prominent example of a death caused by radium. Radithor contained ~1 μCi (40 kBq) of 226Ra and 1 μCi of 228Ra per bottle. Radithor was taken by mouth and radium, being a calcium mimic, has a very long biological halflife in bone.|2023-09-02-18-15-50
Radium and radon in the environment|Radon| Most of the dose is due to the decay of the polonium (218Po) and lead (214Pb) daughters of 222Rn.  By controlling exposure to the daughters the radioactive dose to the skin and lungs can be reduced by at least 90%. This can be done by wearing a dust mask, and wearing a suit to cover the entire body. Note that exposure to smoke at the same time as radon and radon daughters will increase the harmful effect of the radon. In uranium miners radon has been found to be more carcinogenic in smokers than in non-smokers.|2023-09-02-18-15-50
Radium and radon in the environment|Occurrence| Radon concentration in open air varies between 1 and 100 Bq m−3.  Radon can be found in some spring waters and hot springs.  The towns of Misasa, Japan, and Bad Kreuznach, Germany boast radium-rich springs which emit radon, as does Radium Springs, New Mexico. Radon exhausts naturally from the ground, particularly in certain regions, especially but not only regions with granitic soils. Not all granitic regions are prone to high emissions of radon, for instance while the rock which Aberdeen is on is very radium rich the rock lacks the cracks required for the radon to migrate. In other nearby areas of Scotland (to the north of Aberdeen) and in Cornwall/Devon the radon is very much able to leave the rock. Radon is a decay product of radium which in turn is a decay product of uranium. One can get maps of average radon levels in houses to help in planning radon mitigation measures for homes. While high uranium in the soil/rock under a house does not always lead to a high radon level in air, a positive correlation between the uranium content of the soil and the radon level in air can be seen.|2023-09-02-18-15-50
Radium and radon in the environment|In air|" Radon harms indoor air quality in many homes. (See ""Radon in Houses"" below.) Radon (222Rn) released into the air decays to 210Pb and other radioisotopes and the levels of 210Pb can be measured. It is important to note that the rate of deposition of this radioisotope is very dependent on the season. Here is a graph of the deposition rate observed in Japan."|2023-09-02-18-15-50
Radium and radon in the environment|In groundwater| Well water can be very rich in radon; the use of this water inside a house is another route allowing radon to enter the house. The radon can enter the air and then be a source of exposure to the humans, or the water can be consumed by humans which is a different exposure route.|2023-09-02-18-15-50
Radium and radon in the environment|Radon in rainwater| Rainwater can be highly radioactive due to high levels of radon and its decay progenies 214Bi and 214Pb; the concentrations of these radioisotopes can be high enough to seriously disrupt radiation monitoring at nuclear power plants.  The highest levels of radon in rainwater occurs during thunderstorms, and it is hypothesized that radon is concentrated in thunderstorms on account of the atom's positive electrical charge.  Estimates of the age of rain drops have been obtained from measuring the isotopic abundance of radon's short-lived decay progeny in rainwater.|2023-09-02-18-15-50
Radium and radon in the environment|In the oil and gas industries| Water, oil and gas from a well often contain radon. The radon decays to form solid radioisotopes which form coatings on the inside of pipework. In an oil processing plant the area of the plant where propane is processed is often one of the more contaminated areas of the plant as radon has a similar boiling point to propane.|2023-09-02-18-15-50
Radium and radon in the environment|In mines| Because uranium minerals emit radon gas, and their harmful and highly radioactive decay products, uranium mining is considerably more dangerous than other (already dangerous) hard rock mining, requiring adequate ventilation systems if the mines are not open pit. In the 1950s, a significant number of American uranium miners were Navajo, as many uranium deposits were discovered on Navajo reservations. A statistically significant subset of these miners later developed small-cell lung cancer, a type of cancer usually not associated with smoking, after exposure to uranium ore and radon-222, a natural decay product of uranium.  The radon, which is produced by the uranium, and not the uranium itself has been shown to be the cancer causing agent.  Some survivors and their descendants received compensation under the Radiation Exposure Compensation Act in 1990. Currently the level of radon in the air of mines is normally controlled by law. In a working mine, the radon level can be controlled by ventilation, sealing off old workings and controlling the water in the mine. The level in a mine can go up when a mine is abandoned, it can reach a level which is able to cause the skin to become red (a mild radiation burn). The radon levels in some of the mines can reach 400 to 700 kBq m−3. A common unit of exposure of lung tissue to alpha emitters is the working level month (WLM), this is where the human lungs have been exposed for 170 hours (a typical month worth of work for a miner) to air which has 3.7 kBq of 222Rn (in equilibrium with its decay products). This is air which has the alpha dose rate of 1 working level (WL). It is estimated that the average person (general public) is subject to 0.2 WLM per year, which works out at about 15 to 20 WLM in a lifetime. According to the NRC 1 WLM is a 5 to 10 mSv lung dose (0.5 to 1.0 rem), while the Organisation for Economic Co-operation and Development (OECD) consider that 1 WLM is equal to a lung dose of 5.5 mSv, the International Commission on Radiological Protection (ICRP) consider 1 WLM to be a 5 mSv lung dose for professional workers (and 4 mSv lung dose for the general public). Lastly the United Nations Scientific Committee on the Effects of Atomic Radiation (UNSCEAR) consider that the exposure of the lungs to 1 Bq of 222Rn (in equilibrium with its decay products) for one year will cause a dose of 61 μSv. In humans a relationship between lung cancer and radon has been shown to exist (beyond all reasonable doubt) for exposures of 100 WLM and above. By using the data from several studies it has been possible to show that an increased risk can be caused by a dose as low as 15 to 20 WLM. Sadly these studies have been difficult as the random errors in the data are very large. It is likely that the miners are also subject to other effects which can harm their lungs while at work (for example dust and diesel fumes).[citation needed]|2023-09-02-18-15-50
Radium and radon in the environment|In houses| The fact that radon is present in indoor air has been known since at least the 1950s and research into its effects on human health started in the early 1970s.  The danger of radon exposure in dwellings received more widespread public awareness after 1984, as a result of a case of Stanley Watras, an employee at the Limerick nuclear power plant in Pennsylvania.  Mr. Watras set off the radiation alarms (see Geiger counter) on his way into work for two weeks straight while authorities searched for the source of the contamination. They were shocked to find that the source was astonishingly high levels of radon in his basement and it was not related to the nuclear plant. The risks associated with living in his house were estimated to be equivalent to smoking 135 packs of cigarettes every day. Depending how houses are built and ventilated, radon may accumulate in basements and dwellings. The European Union recommends that mitigation should be taken starting from concentrations of 400 Bq/m3 for old houses, and 200 Bq/m3 for new ones. The National Council on Radiation Protection and Measurements (NCRP) recommends action for any house with a concentration higher than 8 pCi/L (300 Bq/m3). The United States Environmental Protection Agency recommends action for any house with a concentration higher than 148 Bq/m3 (given as 4 pCi/L). Nearly one in 15 homes in the U.S. has a high level of indoor radon according to their statistics. The U.S. Surgeon General and EPA recommend all homes be tested for radon. Since 1985, millions of homes have been tested for radon in the U.S. By adding a crawl space under the ground floor, which is subject to forced ventilation the radon level in the house can be lowered.|2023-09-02-18-15-50
Regulation and monitoring of pollution|General| To protect the environment from the adverse effects of pollution, many nations worldwide have enacted legislation to regulate various types of pollution as well as to mitigate the adverse effects of pollution. At the local level, regulation usually is supervised by environmental agencies or the broader public health system. Different jurisdictions often have different levels regulation and policy choices about pollution. Historically, polluters will lobby governments in less economically developed areas or countries to maintain lax regulation in order to protect industrialisation at the cost of human and environmental health.[citation needed] The modern environmental regulatory environment has its origins in the United States with the beginning of industrial regulations around Air and Water pollution connected to industry and mining during the 1960s and 1970s. Because many of pollutants have trans-boundary impacts, the UN and other treaty  bodies have been used to regulate pollutants that circulate as air pollution, water pollution or trade in wastes. Early international agreements were successful at addressing Global Environmental issues, such as Montreal Protocol, which banned Ozone depleting chemicals in 1987, with more recent agreements focusing on broader, more widely dispersed chemicals such as persistent organic pollutants in the Stockholm Convention on Persistent Organic Pollutants created in 2001, such as PCBs, and the Kyoto Protocol in 1997 which initiated collaboration on addressing greenhouse gases to mitigate climate change.|2023-09-26-23-48-21
Regulation and monitoring of pollution|International|" Since pollution crosses political boundaries international treaties have been made through the United Nations and its agencies to address international pollution issues. The Kyoto Protocol  is an amendment to the United Nations Framework Convention on Climate Change (UNFCCC), an international treaty on global warming. It also reaffirms sections of the UNFCCC. Countries which ratify this protocol commit to reduce their emissions of carbon dioxide and five other greenhouse gases, or engage in emissions trading if they maintain or increase emissions of these gases.  A total of 141 countries have ratified the agreement. Notable exceptions include the United States and Australia, who have signed but not ratified the agreement. The stated reason for the United States not ratifying is the exemption of large emitters of greenhouse gases who are also developing countries, like China and India. An UN environmental conference held in Bali 3–14 December 2007 with the participation from 180 countries aims to replace the Kyoto Protocol, which will end in 2012. During the first day of the conference United States, Saudi Arabia and Canada were presented with the ""Fossil-of-the-day-award"", a symbolic bag of coal for their negative impact on the global climate. The bags included the flags of the respective countries."|2023-09-26-23-48-21
Regulation and monitoring of pollution|Canada| In Canada the regulation of pollution and its effects are monitored by a number of organizations depending on the nature of the pollution and its location. The three levels of government (Federal – Canada Wide; Provincial; and Municipal) equally share in the responsibilities, and in the monitoring and correction of pollution.|2023-09-26-23-48-21
Regulation and monitoring of pollution|China|" China's rapid industrialization has substantially increased pollution. China has some relevant regulations: the 1979 Environmental Protection Law, which was largely modeled on U.S. legislation, but the environment continues to deteriorate.  Twelve years after the law, only one Chinese city was making an effort to clean up its water discharges.  This indicates that China is about 30 years behind the U.S. schedule of environmental regulation and 10 to 20 years behind Europe. In July 2007, it was reported that the World Bank reluctantly censored a report revealing that 750,000 people in China die every year as a result of pollution-related diseases. China's State Environment Protection Agency and the Health Ministry asked the World Bank to cut the calculations of premature deaths from the report fearing the revelation would provoke ""social unrest""."|2023-09-26-23-48-21
Regulation and monitoring of pollution|Europe| The basic European rules are included in the Directive 96/61/EC of 24 September 1996 concerning integrated pollution prevention and control (IPPC)  and the National Emission Ceilings Directive. In the 1840s, the United Kingdom brought onto the statute books legislation to control water pollution and was strengthened in 1876 in the Rivers Pollution Prevention Act  and was subsequently extended to all freshwaters in the Rivers (Prevention of Pollution) Act 1951 and applied to coastal waters by the Rivers (Prevention of Pollution) Act 1961 The Environmental Protection Act of 1990 established the system of Integrated Pollution Control(IPC). Currently[when?] the clean up of historic contamination is controlled under a specific statutory scheme found in Part IIA of the Environmental Protection Act 1996 (Part IIA), as inserted by the Environment Act 1995, and other ‘rules’ found in regulations and statutory guidance.  The Act came into force in England in April 2000. Within the current[when?] regulatory framework, Pollution Prevention and Control (PPC) is a regime for controlling pollution from certain designated industrial activities. The regime introduces the concept of Best Available Techniques (BAT) to environmental regulations. Operators must use the BAT to control pollution from their industrial activities to prevent, and where that is not practicable, to reduce to acceptable levels, pollution to air, land and water from industrial activities. The Best Available Techniques also aim to balance the cost to the operator against benefits to the environment. The system of Pollution Prevention and Control is replacing that of IPC  and has been taking effect between 2000 and 2007. The Pollution Prevention and Control regime implements the European Directive (EC/96/61) on integrated pollution prevention and control.|2023-09-26-23-48-21
Regulation and monitoring of pollution|United States|" The United States Congress passed the Clean Air Act in 1963 to legislate the reduction of smog and atmospheric pollution in general. That legislation has subsequently been amended and extended in 1966, 1970, 1977 and 1990. In 1968 AP 42 Compilation of Air Pollutant Emission Factors. Numerous state and local governments have enacted similar legislation either implementing or filling in locally important gaps in the national program. The national Clean Air Act and similar state legislative acts have led to the widespread use of atmospheric dispersion modeling  in order to analyze the air quality impacts of proposed major actions. With the 1990 Clean Air Act, the United States Environmental Protection Agency (EPA) began a controversial carbon trading system in which tradable rights to emit a specified level of carbon are granted to polluters.[citation needed] Enactment of the 1972 Clean Water Act (CWA) required thousands of facilities to obtain permits for discharges to navigable waters, through the National Pollutant Discharge Elimination System (NPDES). It also required EPA to establish national technology-based discharge standards for municipal sewage treatment plants, and for many industrial categories (the latter are called ""effluent guidelines."") Municipal and industrial permittees are required to regularly collect and analyze wastewater samples, and submit Discharge Monitoring Reports to a state agency or EPA.  Amendments in 1977 required stricter regulation of toxic pollutants.  In 1987 Congress expanded NPDES permit coverage to include municipal and industrial stormwater discharges. The Act also requires use of best management practices for a wide range of other water discharges including nonpoint source pollution. Thermal pollution discharges are regulated under section 316(a) of the CWA.  NPDES permits include effluent limitations on water temperature to protect the biotic life supported by a water body. A permittee may request a variance to the typical thermal limitations. Alternate limitations may be issued in limited circumstances, if the permittee has provided sufficient proof through data submission that aquatic life in the water body will be protected. In addition to wastewater discharge monitoring, EPA works with federal, state and local environmental agencies to conduct ambient water monitoring programs in water bodies nationwide.  The CWA requires EPA and the states to prepare reports to Congress on the condition of the nation's waters.  Ambient water quality data collected by EPA, the US Geological Survey and other organizations are available to the public in several online databases. Congress passed the Resource Conservation and Recovery Act (RCRA) in 1976, which created a regulatory framework for both municipal solid waste and hazardous waste disposed on land.  RCRA requires that all hazardous wastes be managed and tracked from generation of the waste, through transport and processing, to final disposal, by means of a nationwide permit system. The Hazardous and Solid Waste Amendments of 1984 mandated regulation of underground storage tanks containing petroleum and hazardous chemicals, and the phasing out of land disposal of hazardous waste.  The Federal Facilities Compliance Act, passed in 1992, clarified RCRA coverage of federally owned properties such as military bases. Illegal disposal of waste is punishable by fines of up to $25,000 per occurrence. Alongside municipal and hazardous waste the EPA is in charge of soil conservation. The EPA, often with the help of state partners, manages soil contamination through contaminant sites and facilities. An annual report on the Environment and a Toxics Release Inventory is produced as a result of these efforts. To specifically mitigate soil pollution from fertilizers, the USDA, National Resources Conservation Service (NRCS), National Institutue of Food and Agriculture (NIFA), and Agricultural Research Service (ARS) monitor soil resources and provide guidelines to prevent nutrient loss. Passage of the Noise Control Act in 1972 established mechanisms of setting emission standards for virtually every source of noise including motor vehicles, aircraft, certain types of HVAC equipment and major appliances. It also put local government on notice as to their responsibilities in land use planning to address noise mitigation. This noise regulation framework comprised a broad data base detailing the extent of noise health effects. Congress ended funding of the federal noise control program in 1981, which curtailed development of further national regulations. Light Pollution in the United States is not federally regulated. The Environmental Protection Agency (EPA), in charge of most environmental regulations, does not manage light pollution. 18 states and one territory have implemented laws that regulate light pollution to some extent. State legislation includes restrictions on hardware, protective equipment, and net light pollution ratings. Such legislation has been coined ""Dark Skies"" Legislation. States have implemented light pollution regulation for many factors including; public safety, energy conservation, improved astronomy research, and reduced environmental effects. The state of California Office of Environmental Health Hazard Assessment (OEHHA) has maintained an independent list of substances with product labeling requirements as part of Proposition 65 since 1986. The Toxicology and Environmental Health Information Program (TEHIP)  at the United States National Library of Medicine (NLM) maintains a comprehensive toxicology and environmental health web site that includes access to resources produced by TEHIP and by other government agencies and organizations. This web site includes links to databases, bibliographies, tutorials, and other scientific and consumer-oriented resources. TEHIP also is responsible for the Toxicology Data Network (TOXNET)  an integrated system of toxicology and environmental health databases that are available free of charge on the web. TOXMAP is a Geographic Information System (GIS) that is part of TOXNET. TOXMAP uses maps of the United States to help users visually explore data from the United States Environmental Protection Agency's (EPA) Toxics Release Inventory and Superfund Basic Research Programs."|2023-09-26-23-48-21
Rubber pollution|General| Rubber pollution, similar to plastic pollution, occurs in various environments, and originates from a variety of sources, ranging from the food industry processing chain to tire wear.   Synthetic and natural rubber dust and fragments now occur in food, airborne as particulates in air pollution, hidden in the earth as soil pollution, and in waterways, lakes and the sea.|2023-08-30-00-01-50
Rubber pollution|Causes|" Tire wear is a major source of rubber pollution.   
A concern is that, unlike exhaust emissions, vehicle tire wear pollution is not regulated. 
Some devices are nonetheless being developed in an effort to reduce the amount of particulates coming from the tire and otherwise ending up in the atmosphere.   
Although not immediately visible to the naked eye, tire dust makes up a significant portion of road debris. Other sources can be artificial turf  and rubber O-rings and seals."|2023-08-30-00-01-50
Rubber pollution|Classification| Very fine rubber dust particles can depending on the classification be counted among microplastic (because rubber is just another polymer) or separately (because its constituent monomers, the required additives, and the type of chemical bond mesh is slightly different). In a similar vein, rubber pollution is often implicitly mentioned when plastic pollution is addressed. 6PPD-quinone, an antiozonant used in rubber tires, has been found to kill salmon when it accumulates into waterways from tire wear pollution.|2023-08-30-00-01-50
Smog|General|" Lists Categories Smog, or smoke fog, is a type of intense air pollution. The word ""smog"" was coined in the early 20th century, and is a portmanteau of the words smoke and fog  to refer to smoky fog due to its opacity, and odor.   The word was then intended to refer to what was sometimes known as pea soup fog, a familiar and serious problem in London from the 19th century to the mid-20th century, where it was commonly known as a London particular or London fog. This kind of visible air pollution is composed of nitrogen oxides, sulfur oxide, ozone, smoke and other particulates. Man-made smog is derived from coal combustion emissions, vehicular emissions, industrial emissions, forest and agricultural fires and photochemical reactions of these emissions. Smog is often categorized as being either summer smog or winter smog. Summer smog is primarily associated with the photochemical formation of ozone. During the summer season when the temperatures are warmer and there is more sunlight present, photochemical smog is the dominant type of smog formation. During the winter months when the temperatures are colder, and atmospheric inversions are common, there is an increase in coal and other fossil fuel usage to heat homes and buildings. These combustion emissions, together with the lack of pollutant dispersion under inversions, characterize winter smog formation. Smog formation in general relies on both primary and secondary pollutants. Primary pollutants are emitted directly from a source, such as emissions of sulfur dioxide from coal combustion. Secondary pollutants, such as ozone, are formed when primary pollutants undergo chemical reactions in the atmosphere. Photochemical smog, as found for example in Los Angeles, is a type of air pollution derived from vehicular emission from internal combustion engines and industrial fumes. These pollutants react in the atmosphere with sunlight to form secondary pollutants that also combine with the primary emissions to form photochemical smog. In certain other cities, such as Delhi, smog severity is often aggravated by stubble burning in neighboring agricultural areas since the 1980s. The atmospheric pollution levels of Los Angeles, Beijing, Delhi, Lahore, Mexico City, Tehran and other cities are often increased by an inversion that traps pollution close to the ground. The developing smog is usually toxic to humans and can cause severe sickness, a shortened life span, or premature death."|2023-09-15-12-19-02
Smog|Etymology|" Coinage of the term ""smog"" has been attributed to Henry Antoine Des Voeux in his 1905 paper, ""Fog and Smoke"" for a meeting of the Public Health Congress. The 26 July 1905 edition of the London newspaper Daily Graphic quoted Des Voeux, ""He said it required no science to see that there was something produced in great cities which was not found in the country, and that was smoky fog, or what was known as 'smog'.""  [dead link] The following day the newspaper stated that ""Dr. Des Voeux did a public service in coining a new word for the London fog."" However, the term appeared twenty-five years earlier than Voeux's paper, in the Santa Cruz & Monterey Illustrated Handbook published in 1880   and also appears in print in a column quoting from the book in the 3 July 1880, Santa Cruz Weekly Sentinel.  On 17 December 1881, in the publication Sporting Times, the author claims to have invented the word: ""The 'Smog' – a word I have invented, combined of smoke and fog, to designate the London atmosphere..."""|2023-09-15-12-19-02
Smog|Coal|" Coal fire can emit significant clouds of smoke that contribute to the formation of winter smog. Coal fires can be used to heat individual buildings or to provide energy in a power-producing plant. Air pollution from this source has been reported in England since the Middle Ages.   London, in particular, was notorious up through the mid-20th century for its coal-caused smogs, which were nicknamed ""pea-soupers"". Air pollution of this type is still a problem in areas that generate significant smoke from burning coal. The emissions from coal combustion are one of the main causes of air pollution in China.  Especially during autumn and winter when coal-fired heating ramps up, the amount of produced smoke at times forces some Chinese cities to close down roads, schools or airports. One prominent example for this was China's Northeastern city of Harbin in 2013."|2023-09-15-12-19-02
Smog|Transportation emissions| Traffic emissions – such as from trucks, buses, and automobiles – also contribute to the formation of smog.  Airborne by-products from vehicle exhaust systems and air conditioning cause air pollution and are a major ingredient in the creation of smog in some large cities. The major culprits from transportation sources are carbon monoxide (CO),   nitrogen oxides (NO and NO2),    volatile organic compounds,   and hydrocarbons (hydrocarbons are the main component of petroleum fuels such as gasoline and diesel fuel).  Transportation emissions also include sulfur dioxides and particulate matter but in much smaller quantities than the pollutants mentioned previously. The nitrogen oxides and volatile organic compounds can undergo a series of chemical reactions with sunlight, heat, ammonia, moisture, and other compounds to form the noxious vapors, ground level ozone, and particles that comprise smog.|2023-09-15-12-19-02
Smog|Photochemical smog|" Photochemical smog, often referred to as ""summer smog"", is the chemical reaction of sunlight, nitrogen oxides and volatile organic compounds in the atmosphere, which leaves airborne particles and ground-level ozone.  Photochemical smog depends on primary pollutants as well as the formation of secondary pollutants. These primary pollutants include nitrogen oxides, particularly nitric oxide (NO) and nitrogen dioxide (NO2), and volatile organic compounds. The relevant secondary pollutants include peroxylacyl nitrates (PAN), tropospheric ozone, and aldehydes. An important secondary pollutant for photochemical smog is ozone, which is formed when hydrocarbons (HC) and nitrogen oxides (NOx) combine in the presence of sunlight; nitrogen dioxide (NO2), which is formed as nitric oxide (NO) combines with oxygen (O2) in the air.  In addition, when SO2 and NOx are emitted they eventually are oxidized in the troposphere to nitric acid and sulfuric acid, which, when mixed with water, form the main components of acid rain.  All of these harsh chemicals are usually highly reactive and oxidizing. Photochemical smog is therefore considered to be a problem of modern industrialization. It is present in all modern cities, but it is more common in cities with sunny, warm, dry climates and a large number of motor vehicles.  Because it travels with the wind, it can affect sparsely populated areas as well. The composition and chemical reactions involved in photochemical smog were not understood until the 1950s. In 1948, flavor chemist Arie Haagen-Smit adapted some of his equipment to collect chemicals from polluted air, and identified ozone as a component of Los Angeles smog. Haagen-Smit went on to discover that nitrogen oxides from automotive exhausts and gaseous hydrocarbons from cars and oil refineries, exposed to sunlight, were key ingredients in the formation of ozone and photochemical smog.     Haagen-Smit worked with Arnold Beckman, who developed various equipment for detecting smog, ranging from an ""Apparatus for recording gas concentrations in the atmosphere"" patented on 7 October 1952, to ""air quality monitoring vans"" for use by government and industry."|2023-09-15-12-19-02
Smog|Formation and reactions| During the morning rush hour, a high concentration of nitric oxide and hydrocarbons are emitted to the atmosphere, mostly via on-road traffic but also from industrial sources. Some hydrocarbons are rapidly oxidized by OH· and form peroxy radicals, which convert nitric oxide (NO) to nitrogen dioxide (NO2). (1) (2) (3) Nitrogen dioxide (NO2) and nitric oxide (NO) further react with ozone (O3) in a series of chemical reactions: (4) , (5) (6) This series of equations is referred to as the photostationary state (PSS). However, because of the presence of Reaction 2 and 3, NOx and ozone are not in a perfectly steady state. By replacing Reaction 6 with Reaction 2 and Reaction 3, the O3 molecule is no longer destroyed. Therefore, the concentration of ozone keeps increasing throughout the day. This mechanism can escalate the formation of ozone in smog. Other reactions such as the photooxidation of formaldehyde (HCHO), a common secondary pollutant, can also contribute to the increased concentration of ozone and NO2. Photochemical smog is more prevalent during summer days since incident solar radiation fluxes are high, which favors the formation of ozone (reactions 4 and 5). The presence of a temperature inversion layer is another important factor. That is because it prevents the vertical convective mixing of the air and thus allows the pollutants, including ozone, to accumulate near the ground level, which again favors the formation of photochemical smog. There are certain reactions that can limit the formation of O3 in smog. The main limiting reaction in polluted areas is: (7) This reaction removes NO2 which limits the amount of O3 that can be produced from its photolysis (reaction 4). HNO3 is a sticky compound that can easily be removed onto surfaces (dry deposition) or dissolved in water and be rained out (wet deposition). Both ways are common in the atmosphere and can efficiently remove radicals and nitrogen dioxide.|2023-09-15-12-19-02
Smog|Natural causes| An erupting volcano can emit high levels of sulfur dioxide along with a large quantity of particulates matter; two key components to the creation of smog. However, the smog created as a result of a volcanic eruption is often known as vog to distinguish it as a natural occurrence. The chemical reactions that form smog following a volcanic eruption are different than the reactions that form photochemical smog. The term smog encompasses the effect when a large number of gas-phase molecules and particulate matter are emitted to the atmosphere, creating a visible haze. The event causing a large number of emissions can vary but still result in the formation of smog. Plants are another natural source of hydrocarbons that could undergo reactions in the atmosphere and produce smog. Globally both plants and soil contribute a substantial amount to the production of hydrocarbons, mainly by producing isoprene and terpenes.  Hydrocarbons released by plants can often be more reactive than man-made hydrocarbons. For example when plants release isoprene, the isoprene reacts very quickly in the atmosphere with hydroxyl radicals. These reactions produce hydroperoxides which increase ozone formation.|2023-09-15-12-19-02
Smog|Health effects|" Smog is a serious problem in many cities and continues to harm human health.   Ground-level ozone, sulfur dioxide, nitrogen dioxide and carbon monoxide are especially harmful for senior citizens, children, and people with heart and lung conditions such as emphysema, bronchitis, and asthma.  It can inflame breathing passages, decrease the lungs' working capacity, cause shortness of breath, pain when inhaling deeply, wheezing, and coughing. It can cause eye and nose irritation and it dries out the protective membranes of the nose and throat and interferes with the body's ability to fight infection, increasing susceptibility to illness.  Hospital admissions and respiratory deaths often increase during periods when ozone levels are high. There is a lack of knowledge on the long-term effects of air pollution exposure and the origin of asthma. An experiment was carried out using intense air pollution similar to that of the 1952 Great Smog of London. The results from this experiment concluded that there is a link between early-life pollution exposure that leads to the development of asthma, proposing the ongoing effect of the Great Smog. 
Modern studies continue to find links between mortality and the presence of smog. One study, published in Nature magazine, found that smog episodes in the city of Jinan, a large city in eastern China, during 2011–15, were associated with a 5.87% (95% CI 0.16–11.58%) increase in the rate of overall mortality. This study highlights the effect of exposure to air pollution on the rate of mortality in China.  A similar study in X'ian found an association between ambient air pollution and increased mortality associated with respiratory diseases."|2023-09-15-12-19-02
Smog|Levels of unhealthy exposure|" The U.S. EPA has developed an air quality index to help explain air pollution levels to the general public. 8 hour average ozone concentrations of 85 to 104 ppbv are described as ""Unhealthy for Sensitive Groups"", 105 ppbv to 124 ppbv as ""unhealthy"" and 125 ppb to 404 ppb as ""very unhealthy"".  The ""very unhealthy"" range for some other pollutants are: 355 μg m−3 – 424 μg m−3 for PM10; 15.5 ppm – 30.4ppm for CO and 0.65 ppm – 1.24 ppm for NO2."|2023-09-15-12-19-02
Smog|Premature deaths due to cancer and respiratory disease| In 2016, the Ontario Medical Association announced that smog is responsible for an estimated 9,500 premature deaths in the province each year. A 20-year American Cancer Society study found that cumulative exposure also increases the likelihood of premature death from respiratory disease, implying the 8-hour standard may be insufficient.|2023-09-15-12-19-02
Smog|Alzheimer risk|" Tiny magnetic particles from air pollution have for the first time been discovered to be lodged in human brains– and researchers think they could be a possible cause of Alzheimer's disease.
Researchers at Lancaster University found abundant magnetite nanoparticles in the brain tissue from 37 individuals aged three to 92-years-old who lived in Mexico City and Manchester. This strongly magnetic mineral is toxic and has been implicated in the production of reactive oxygen species (free radicals) in the human brain, which is associated with neurodegenerative diseases including Alzheimer's disease."|2023-09-15-12-19-02
Smog|Risk of certain birth defects| A study examining 806 women who had babies with birth defects between 1997 and 2006, and 849 women who had healthy babies, found that smog in the San Joaquin Valley area of California was linked to two types of neural tube defects: spina bifida (a condition involving, among other manifestations, certain malformations of the spinal column), and anencephaly (the underdevelopment or absence of part or all of the brain, which if not fatal usually results in profound impairment).  An emerging cohort study in China linked early-life smog exposure to an increased risk for adverse pregnancy outcomes, in particular oxidative stress.|2023-09-15-12-19-02
Smog|Low birth weight| According to a study published in The Lancet, even a very small (5 μg) change in PM2.5 exposure was associated with an increase (18%) in risk of a low birth weight at delivery, and this relationship held even below the current accepted safe levels.|2023-09-15-12-19-02
Smog|Other negative effects| Although severe health effects caused by smog are the chief issue, intense air pollution caused by haze from air pollution, dust storm particles, and bush fire smoke, cause a reduction in irradiance that hurts both solar photovoltaic  production as well as agricultural yield.|2023-09-15-12-19-02
Smog|Areas affected| Smog can form in almost any climate where industries or cities release large amounts of air pollution, such as smoke or gases. However, it is worse during periods of warmer, sunnier weather when the upper air is warm enough to inhibit vertical circulation. It is especially prevalent in geologic basins encircled by hills or mountains. It often stays for an extended period of time over densely populated cities or urban areas and can build up to dangerous levels.|2023-09-15-12-19-02
Smog|Canada| According to the Canadian Science Smog Assessment published in 2012, smog is responsible for detrimental effects on human and ecosystem health, as well as socioeconomic well-being across the country. It was estimated that the province of Ontario sustains $201 million in damages annually for selected crops, and an estimated tourism revenue degradation of $7.5 million in Vancouver and $1.32 million in The Fraser Valley due to decreased visibility. Air pollution in British Columbia is of particular concern, especially in the Fraser Valley, because of a meteorological effect called inversion which decreases air dispersion and leads to smog concentration.|2023-09-15-12-19-02
Smog|Delhi, India|" For the past few years, cities in northern India have been covered in a thick layer of winter smog. The situation has turned quite drastic in the National Capital, Delhi. This smog is caused by the collection of Particulate Matter (a very fine type of dust and toxic gases) in the air due to stagnant movement of air during winters. Delhi is the most polluted  city in the world and according to one estimate, air pollution causes the death of about 10,500 people in Delhi every year.    During 2013–14, peak levels of fine particulate matter (PM) in Delhi increased by about 44%, primarily due to high vehicular and industrial emissions, construction work and crop burning in adjoining states.     Delhi has the highest level of the airborne particulate matter, PM2.5 considered most harmful to health, with 153 micrograms.  Rising air pollution level has significantly increased lung-related ailments (especially asthma and lung cancer) among Delhi's children and women.   The dense smog in Delhi during winter season results in major air and rail traffic disruptions every year.  According to Indian meteorologists, the average maximum temperature in Delhi during winters has declined notably since 1998 due to rising air pollution. Environmentalists have criticized the Delhi government for not doing enough to curb air pollution and to inform people about air quality issues.  Most of Delhi's residents are unaware of alarming levels of air pollution in the city and the health risks associated with it.  Since the mid-1990s, Delhi has undertaken some measures to curb air pollution – Delhi has the third highest quantity of trees among Indian cities  and the Delhi Transport Corporation operates the world's largest fleet of environmentally friendly compressed natural gas (CNG) buses.  In 1996, the Centre for Science and Environment (CSE) started a public interest litigation in the Supreme Court of India that ordered the conversion of Delhi's fleet of buses and taxis to run on CNG and banned the use of leaded petrol in 1998. In 2003, Delhi won the United States Department of Energy's first 'Clean Cities International Partner of the Year' award for its ""bold efforts to curb air pollution and support alternative fuel initiatives"".  The Delhi Metro has also been credited for significantly reducing air pollutants in the city. However, according to several authors, most of these gains have been lost, especially due to stubble burning, rise in market share of diesel cars and a considerable decline in bus ridership.   According to CUE and System of Air Quality Weather Forecasting and Research (SAFER), burning of agricultural waste in nearby Punjab, Haryana and Uttar Pradesh regions results in severe intensification of smog over Delhi.   The state government of adjoining Uttar Pradesh is considering imposing a ban on crop burning to reduce pollution in Delhi NCR and an environmental panel has appealed to India's Supreme Court to impose a 30% cess on diesel cars."|2023-09-15-12-19-02
Smog|Beijing, China|" Joint research between American and Chinese researchers in 2006 concluded that much of the city's pollution comes from surrounding cities and provinces. On average 35–60% of the ozone can be traced to sources outside the city. Shandong Province and Tianjin Municipality have a ""significant influence on Beijing's air quality"",  partly due to the prevailing south/southeasterly flow during the summer and the mountains to the north and northwest."|2023-09-15-12-19-02
Smog|United Kingdom|" In 1306, concerns over air pollution were sufficient for Edward I to (briefly) ban coal fires in London.  In 1661, John Evelyn's Fumifugium suggested burning fragrant wood instead of mineral coal, which he believed would reduce coughing. The ""Ballad of Gresham College"" the same year describes how the smoke ""does our lungs and spirits choke, Our hanging spoil, and rust our iron."" Severe episodes of smog continued in the 19th and 20th centuries, mainly in the winter, and were nicknamed ""pea-soupers,"" from the phrase ""as thick as pea soup"". The Great Smog of 1952 darkened the streets of London and killed approximately 4,000 people in the short time of four days (a further 8,000  died from its effects in the following weeks and months). Initially, a flu epidemic was blamed for the loss of life. In 1956 the Clean Air Act started legally enforcing smokeless zones in the capital. There were areas where no soft coal was allowed to be burned in homes or in businesses, only coke, which produces no smoke. Because of the smokeless zones, reduced levels of sooty particulates eliminated the intense and persistent London smog. It was after this that the great clean-up of London began. One by one, historical buildings which, during the previous two centuries had gradually completely blackened externally, had their stone facades cleaned and restored to their original appearance. Victorian buildings whose appearance changed dramatically after cleaning included the British Museum of Natural History. A more recent example was the Palace of Westminster, which was cleaned in the 1980s. A notable exception to the restoration trend was 10 Downing Street, whose bricks upon cleaning in the late 1950s proved to be naturally yellow; the smog-derived black color of the façade was considered so iconic that the bricks were painted black to preserve the image.   Smog caused by traffic pollution, however, does still occur in modern London. Other areas of the United Kingdom were affected by smog, especially heavily industrialised areas. The cities of Glasgow and Edinburgh, in Scotland, suffered smoke-laden fogs in 1909. Des Voeux, commonly credited with creating the ""smog"" moniker, presented a paper in 1911 to the Manchester Conference of the Smoke Abatement League of Great Britain about the fogs and resulting deaths. One Birmingham resident described near black-out conditions in the 1900s before the Clean Air Act, with visibility so poor that cyclists had to dismount and walk in order to stay on the road. On 29 April 2015, the UK Supreme Court ruled that the government must take immediate action to cut air pollution,  following a case brought by environmental lawyers at ClientEarth."|2023-09-15-12-19-02
Smog|Mexico City, Mexico|" Due to its location in a highland ""bowl"", cold air sinks down onto the urban area of Mexico City, trapping industrial and vehicle pollution underneath, and turning it into the most infamously smog-plagued city of Latin America. Within one generation, the city has changed from being known for some of the cleanest air of the world into one with some of the worst pollution, with pollutants like nitrogen dioxide being double or even triple international standards."|2023-09-15-12-19-02
Smog|Santiago, Chile| Similar to Mexico City, the air pollution of Santiago valley, located between the Andes and the Chilean Coast Range, turn it into the most infamously smog-plagued city of South America. Other aggravates of the situation reside in its high latitude (31 degrees South) and dry weather during most of the year.|2023-09-15-12-19-02
Smog|Tehran, Iran| In December 2005, schools and public offices had to close in Tehran and 1,600 people were taken to hospital, in a severe smog blamed largely on unfiltered car exhaust.|2023-09-15-12-19-02
Smog|United States|" Smog was brought to the attention of the general U.S. public in 1933 with the publication of the book ""Stop That Smoke"", by Henry Obermeyer, a New York public utility official, in which he pointed out the effect on human life and even the destruction of 3,000 acres (12 km2) of a farmer's spinach crop.  Since then, the United States Environmental Protection Agency has designated over 300 U.S. counties to be non-attainment areas for one or more pollutants tracked as part of the National Ambient Air Quality Standards.  These areas are largely clustered around large metropolitan areas, with the largest contiguous non-attainment zones in California and the Northeast. Various U.S. and Canadian government agencies collaborate to produce real-time air quality maps and forecasts.  To combat smog conditions, localities may declare ""smog alert"" days, such as in the Spare the Air program in the San Francisco Bay Area. By 1970, Congress enacted the Clean Air Act to regulate air pollutant emissions. In the United States, smog pollution kills 24,000 Americans every year. The U.S. is among the dirtier countries in terms of smog, ranked 123 out of 195 countries measured, where 1 is cleanest and 195 is most smog polluted. Because of their locations in low basins surrounded by mountains, Los Angeles and the San Joaquin Valley are notorious for their smog. Heavy automobile traffic, combined with the additional effects of the San Francisco Bay and Los Angeles/Long Beach port complexes, frequently contribute to further air pollution. Los Angeles, in particular, is strongly predisposed to the accumulation of smog, because of the peculiarities of its geography and weather patterns. Los Angeles is situated in a flat basin with the ocean on one side and mountain ranges on three sides. A nearby cold ocean current depresses surface air temperatures in the area, resulting in an inversion layer: a phenomenon where air temperature increases, instead of decreasing, with altitude, suppressing thermals and restricting vertical convection. All taken together, this results in a relatively thin, enclosed layer of air above the city that cannot easily escape out of the basin and tends to accumulate pollution. Los Angeles was one of the best-known cities suffering from transportation smog for much of the 20th century, so much so that it was sometimes said that Los Angeles was a synonym for smog.  In 1970, when the Clean Air Act was passed, Los Angeles was the most polluted basin in the country, and California was unable to create a State Implementation Plan that would enable it to meet the new air quality standards.  However, ensuing strict regulations by state and federal government agencies overseeing this problem (such as the California Air Resources Board and the United States Environmental Protection Agency), including tight restrictions on allowed emissions levels for all new cars sold in California and mandatory regular emission tests of older vehicles, resulted in significant improvements in air quality.  For example, air concentrations of volatile organic compounds declined by a factor of 50 between 1962 and 2012.  Concentrations of air pollutants such as nitrous oxides and ozone declined by 70% to 80% over the same period of time."|2023-09-15-12-19-02
Smog|Ulaanbaatar, Mongolia|" In the late 1990s, massive immigration to Ulaanbaatar from the countryside began. An estimated 150,000 households, mainly living in traditional Mongolian gers on the outskirts of Ulaanbaatar, burn wood and coal (some poor families burn even car tires and trash) to heat themselves during the harsh winter, which lasts from October to April, since these outskirts are not connected to the city's central heating system. A temporary solution to decrease smog was proposed in the form of stoves with improved efficiency, although with no visible results. 
Coal-fired ger stoves release high levels of ash and other particulate matter (PM). When inhaled, these particles can settle in the lungs and respiratory tract and cause health problems. At two to 10 times above Mongolian and international air quality standards, Ulaanbaatar's PM rates are among the worst in the world, according to a December 2009 World Bank report. The Asian Development Bank (ADB) estimates that health costs related to this air pollution account for as much as 4 percent of Mongolia's GDP."|2023-09-15-12-19-02
Smog|Southeast Asia|" Smog is a regular problem in Southeast Asia caused by land and forest fires in Indonesia, especially Sumatra and Kalimantan, although the term haze is preferred in describing the problem. Farmers and plantation owners are usually responsible for the fires, which they use to clear tracts of land for further plantings. Those fires mainly affect Brunei, Indonesia, Philippines, Malaysia, Singapore and Thailand, and occasionally Guam and Saipan.   The economic losses of the fires in 1997 have been estimated at more than US$9 billion.  This includes damages in agriculture production, destruction of forest lands, health, transportation, tourism, and other economic endeavours. Not included are social, environmental, and psychological problems and long-term health effects. The second-latest bout of haze to occur in Malaysia, Singapore and the Malacca Straits is in October 2006, and was caused by smoke from fires in Indonesia being blown across the Straits of Malacca by south-westerly winds. A similar haze has occurred in June 2013, with the PSI setting a new record in Singapore on 21 June at 12pm with a reading of 401, which is in the ""Hazardous"" range. The Association of Southeast Asian Nations (ASEAN) reacted. In 2002, the Agreement on Transboundary Haze Pollution was signed between all ASEAN nations.  ASEAN formed a Regional Haze Action Plan (RHAP) and established a co-ordination and support unit (CSU).  RHAP, with the help of Canada, established a monitoring and warning system for forest/vegetation fires and implemented a Fire Danger Rating System (FDRS). The Malaysian Meteorological Department (MMD) has issued a daily rating of fire danger since September 2003.  Indonesia has been ineffective at enforcing legal policies on errant farmers.[citation needed]"|2023-09-15-12-19-02
Smog|Pakistan| Since the start of the winter season, heavy smog loaded with pollutants covered major parts of Punjab, especially the city of Lahore,  causing breathing problems and disrupting normal traffic.  A recent study from 2022 shows that the primary cause of pollution in Lahore is from traffic-related PM (both exhausts and non exhaust sources) Doctors advised residents to stay indoors and wear facemasks outside.|2023-09-15-12-19-02
Smog|Pollution index| The severity of smog is often measured using automated optical instruments such as nephelometers, as haze is associated with visibility and traffic control in ports. Haze, however, can also be an indication of poor air quality, though this is often better reflected using accurate purpose-built air indexes such as the American Air Quality Index, the Malaysian API (Air Pollution Index), and the Singaporean Pollutant Standards Index. In hazy conditions, it is likely that the index will report the suspended particulate level. The disclosure of the responsible pollutant is mandated in some jurisdictions. The Malaysian API does not have a capped value. Hence, its most hazardous readings can go above 500. When the reading goes above 500, a state of emergency is declared in the affected area. Usually, this means that non-essential government services are suspended, and all ports in the affected area are closed. There may also be prohibitions on private sector commercial and industrial activities in the affected area excluding the food sector. So far, the state of emergency rulings due to hazardous API levels was applied to the Malaysian towns of Port Klang, Kuala Selangor, and the state of Sarawak during 1997 Southeast Asian haze and the 2005 Malaysian haze.[needs update]|2023-09-15-12-19-02
Soil contamination|General| Lists Categories Soil contamination, soil pollution, or land pollution as a part of land degradation is caused by the presence of xenobiotic (human-made) chemicals or other alteration in the natural soil environment.  It is typically caused by industrial activity, agricultural chemicals or improper disposal of waste. The most common chemicals involved are petroleum hydrocarbons, polynuclear aromatic hydrocarbons (such as naphthalene and benzo(a)pyrene), solvents, pesticides, lead, and other heavy metals. Contamination is correlated with the degree of industrialization and intensity of chemical substance. The concern over soil contamination stems primarily from health risks, from direct contact with the contaminated soil, vapour from the contaminants, or from secondary contamination of water supplies within and underlying the soil.  Mapping of contaminated soil sites and the resulting cleanups are time-consuming and expensive tasks, and require expertise in geology, hydrology, chemistry, computer modeling, and GIS in Environmental Contamination, as well as an appreciation of the history of industrial chemistry. In North America and Western Europe the extent of contaminated land is best known, with many of countries in these areas having a legal framework to identify and deal with this environmental problem.  Developing countries tend to be less tightly regulated despite some of them having undergone significant industrialization.|2023-08-25-06-27-26
Soil contamination|Causes|" Soil pollution can be caused by the following (non-exhaustive list) : The most common chemicals involved are petroleum hydrocarbons, solvents, pesticides, lead, and other heavy metals. Any activity that leads to other forms of soil degradation (erosion, compaction, etc.) may indirectly worsen the contamination effects in that soil remediation becomes more tedious. Historical deposition of coal ash used for residential, commercial, and industrial heating, as well as for industrial processes such as ore smelting, were a common source of contamination in areas that were industrialized before about 1960. Coal naturally concentrates lead and zinc during its formation, as well as other heavy metals to a lesser degree. When the coal is burned, most of these metals become concentrated in the ash (the principal exception being mercury). Coal ash and slag may contain sufficient lead to qualify as a ""characteristic hazardous waste"", defined in the US as containing more than 5 mg/L of extractable lead using the TCLP procedure. In addition to lead, coal ash typically contains variable but significant concentrations of polynuclear aromatic hydrocarbons (PAHs; e.g., benzo(a)anthracene, benzo(b)fluoranthene, benzo(k)fluoranthene, benzo(a)pyrene, indeno(cd)pyrene, phenanthrene, anthracene, and others). These PAHs are known human carcinogens and the acceptable concentrations of them in soil are typically around 1 mg/kg. Coal ash and slag can be recognised by the presence of off-white grains in soil, gray heterogeneous soil, or (coal slag) bubbly, vesicular pebble-sized grains. Treated sewage sludge, known in the industry as biosolids, has become controversial as a ""fertilizer"". As it is the byproduct of sewage treatment, it generally contains more contaminants such as organisms, pesticides, and heavy metals than other soil. In the European Union, the Urban Waste Water Treatment Directive allows sewage sludge to be sprayed onto land. The volume is expected to double to 185,000 tons of dry solids in 2005. This has good agricultural properties due to the high nitrogen and phosphate content. In 1990/1991, 13% wet weight was sprayed onto 0.13% of the land; however, this is expected to rise 15 fold by 2005.[needs update]  Advocates[who?] say there is a need to control this so that pathogenic microorganisms do not get into water courses and to ensure that there is no accumulation of heavy metals in the top soil."|2023-08-25-06-27-26
Soil contamination|Pesticides and herbicides| A pesticide is a substance used to kill a pest. A pesticide may be a chemical substance, biological agent (such as a virus or bacteria), antimicrobial, disinfectant or device used against any pest. Pests include insects, plant pathogens, weeds, mollusks, birds, mammals, fish, nematodes (roundworms) and microbes that compete with humans for food, destroy property, spread or are a vector for disease or cause a nuisance. Although there are benefits to the use of pesticides, there are also drawbacks, such as potential toxicity to humans and other organisms. Herbicides are used to kill weeds, especially on pavements and railways. They are similar to auxins and most are biodegradable by soil bacteria. However, one group derived from trinitrotoluene (2:4 D and 2:4:5 T) have the impurity dioxin, which is very toxic and causes fatality even in low concentrations. Another herbicide is Paraquat. It is highly toxic but it rapidly degrades in soil due to the action of bacteria and does not kill soil fauna. Insecticides are used to rid farms of pests which damage crops. The insects damage not only standing crops but also stored ones and in the tropics it is reckoned that one third of the total production is lost during food storage. As with fungicides, the first insecticides used in the nineteenth century were inorganic e.g. Paris Green and other compounds of arsenic. Nicotine has also been used since 1690. There are now two main groups of synthetic insecticides – 1. Organochlorines include DDT, Aldrin, Dieldrin and BHC. They are cheap to produce, potent and persistent. DDT was used on a massive scale from the 1930s, with a peak of 72,000 tonnes used 1970. Then usage fell as the harmful environmental effects were realized. It was found worldwide in fish and birds and was even discovered in the snow in the Antarctic. It is only slightly soluble in water but is very soluble in the bloodstream. It affects the nervous and endocrine systems and causes the eggshells of birds to lack calcium causing them to be  easily breakable. It is thought to be responsible for the decline of the numbers of birds of prey like ospreys and peregrine falcons in the 1950s – they are now recovering.  As well as increased concentration via the food chain, it is known to enter via permeable membranes, so fish get it through their gills. As it has low water solubility, it tends to stay at the water surface, so organisms that live there are most affected. DDT found in fish that formed part of the human food chain caused concern, but the levels found in the liver, kidney and brain tissues was less than 1 ppm and in fat was 10 ppm, which was below the level likely to cause harm. However, DDT was banned in the UK and the United States to stop the further buildup of it in the food chain. U.S. manufacturers continued to sell DDT to developing countries, who could not afford the expensive replacement chemicals and who did not have such stringent regulations governing the use of pesticides. 2. Organophosphates, e.g. parathion, methyl parathion and about 40 other insecticides are available nationally. Parathion is highly toxic, methyl-parathion is less so and Malathion is generally considered safe as it has low toxicity and is rapidly broken down in the mammalian liver. This group works by preventing normal nerve transmission as cholinesterase is prevented from breaking down the transmitter substance acetylcholine, resulting in uncontrolled muscle movements.|2023-08-25-06-27-26
Soil contamination|Agents of war| The disposal of munitions, and a lack of care in manufacture of munitions caused by the urgency of production, can contaminate soil for extended periods. There is little published evidence on this type of contamination largely because of restrictions placed by governments of many countries on the publication of material related to war effort. However, mustard gas stored during World War II has contaminated some sites for up to 50 years  and the testing of Anthrax as a potential biological weapon contaminated the whole island of Gruinard.|2023-08-25-06-27-26
Soil contamination|Exposure pathways| Contaminated or polluted soil directly affects human health through direct contact with soil or via inhalation of soil contaminants that have vaporized; potentially greater threats are posed by the infiltration of soil contamination into groundwater aquifers used for human consumption, sometimes in areas apparently far removed from any apparent source of above-ground contamination. Toxic metals can also make their way up the food chain through plants that reside in soils containing high concentrations of heavy metals.  This tends to result in the development of pollution-related diseases. Most exposure is accidental, and exposure can happen through: However, some studies estimate that 90% of exposure is through eating contaminated food.|2023-08-25-06-27-26
Soil contamination|Consequences| Health consequences from exposure to soil contamination vary greatly depending on pollutant type, the pathway of attack, and the vulnerability of the exposed population. Researchers suggest that pesticides and heavy metals in soil may harm cardiovascular health, including inflammation and change in the body's internal clock. Chronic exposure to chromium, lead , and other metals, petroleum, solvents, and many pesticide and herbicide formulations can be carcinogenic, can cause congenital disorders, or can cause other chronic health conditions.  Industrial or man-made concentrations of naturally occurring substances, such as nitrate and ammonia associated with livestock manure from agricultural operations, have also been identified as health hazards in soil and groundwater. Chronic exposure to benzene at sufficient concentrations is known to be associated with a higher incidence of leukemia. Mercury and cyclodienes are known to induce higher incidences of kidney damage and some irreversible diseases. PCBs and cyclodienes are linked to liver toxicity. Organophosphates and carbonates can cause a chain of responses leading to neuromuscular blockage. Many chlorinated solvents induce liver changes, kidney changes, and depression of the central nervous system. There is an entire spectrum of further health effects such as headache, nausea, fatigue, eye irritation and skin rash for the above cited and other chemicals. At sufficient dosages a large number of soil contaminants can cause death by exposure via direct contact, inhalation or ingestion of contaminants in groundwater contaminated through soil. The Scottish Government has commissioned the Institute of Occupational Medicine to undertake a review of methods to assess risk to human health from contaminated land. The overall aim of the project is to work up guidance that should be useful to Scottish Local Authorities in assessing whether sites represent a significant possibility of significant harm (SPOSH) to human health. It is envisaged that the output of the project will be a short document providing high level guidance on health risk assessment with reference to existing published guidance and methodologies that have been identified as being particularly relevant and helpful. The project will examine how policy guidelines have been developed for determining the acceptability of risks to human health and propose an approach for assessing what constitutes unacceptable risk in line with the criteria for SPOSH as defined in the legislation and the Scottish Statutory Guidance.|2023-08-25-06-27-26
Soil contamination|Ecosystem effects| Not unexpectedly, soil contaminants can have significant deleterious consequences for ecosystems.  There are radical soil chemistry changes which can arise from the presence of many hazardous chemicals even at low concentration of the contaminant species. These changes can manifest in the alteration of metabolism of endemic microorganisms and arthropods resident in a given soil environment. The result can be virtual eradication of some of the primary food chain, which in turn could have major consequences for predator or consumer species. Even if the chemical effect on lower life forms is small, the lower pyramid levels of the food chain may ingest alien chemicals, which normally become more concentrated for each consuming rung of the food chain. Many of these effects are now well known, such as the concentration of persistent DDT materials for avian consumers, leading to weakening of egg shells, increased chick mortality and potential extinction of species. Effects occur to agricultural lands which have certain types of soil contamination.  Contaminants typically alter plant metabolism, often causing a reduction in crop yields.  This has a secondary effect upon soil conservation, since the languishing crops cannot shield the Earth's soil from erosion. Some of these chemical contaminants have long half-lives and in other cases derivative chemicals are formed from decay of primary soil contaminants.|2023-08-25-06-27-26
Soil contamination|Potential effects of contaminants to soil functions| Heavy metals and other soil contaminants can adversely affect the activity, species composition and abundance of soil microorganisms, thereby threatening soil functions such as biochemical cycling of carbon and nitrogen.  However, soil contaminants can also become less bioavailable by time, and microorganisms and ecosystems can adapt to altered conditions. Soil properties such as pH, organic matter content and texture are very important and modify mobility, bioavailability and toxicity of pollutants in contaminated soils.  The same amount of contaminant can be toxic in one soil but totally harmless in another soil. This stresses the need for soil-specific risks assessment and measures.|2023-08-25-06-27-26
Soil contamination|Cleanup options| Cleanup or environmental remediation is analyzed by environmental scientists who utilize field measurement of soil chemicals and also apply computer models (GIS in Environmental Contamination) for analyzing transport  and fate of soil chemicals. Various technologies have been developed for remediation of oil-contaminated soil and sediments   There are several principal strategies for remediation:|2023-08-25-06-27-26
Soil contamination|By country| Various national standards for concentrations of particular contaminants include the United States EPA Region 9 Preliminary Remediation Goals (U.S. PRGs), the U.S. EPA Region 3 Risk Based Concentrations (U.S. EPA RBCs) and National Environment Protection Council of Australia Guideline on Investigation Levels in Soil and Groundwater.|2023-08-25-06-27-26
Soil contamination|People's Republic of China| The immense and sustained growth of the People's Republic of China since the 1970s has exacted a price from the land in increased soil pollution. The Ministry of Ecology and Environment believes it to be a threat to the environment, to food safety and to sustainable agriculture. According to a scientific sampling, 150 million mu (100,000 square kilometres) of China's cultivated land have been polluted, with contaminated water being used to irrigate a further 32.5 million mu (21,670 square kilometres) and another 2 million mu (1,300 square kilometres) covered or destroyed by solid waste. In total, the area accounts for one-tenth of China's cultivatable land, and is mostly in economically developed areas. An estimated 12 million tonnes of grain are contaminated by heavy metals every year, causing direct losses of 20 billion yuan ($2.57 billion USD).  Recent survey shows that 19% of the agricultural soils are contaminated which contains heavy metals and metalloids. And the rate of these heavy metals in the soil has been increased dramatically.|2023-08-25-06-27-26
Soil contamination|European Union| According to the received data from Member states, in the European Union the number of estimated potential contaminated sites is more than 2.5 million  and the identified contaminated sites around 342 thousand. Municipal and industrial wastes contribute most to soil contamination (38%), followed by the industrial/commercial sector (34%). Mineral oil and heavy metals are the main contaminants contributing around 60% to soil contamination. In terms of budget, the management of contaminated sites is estimated to cost around 6 billion Euros (€) annually.|2023-08-25-06-27-26
Soil contamination|United Kingdom|" Generic guidance commonly used in the United Kingdom are the Soil Guideline Values published by the Department for Environment, Food and Rural Affairs (DEFRA) and the Environment Agency. These are screening values that demonstrate the minimal acceptable level of a substance. Above this there can be no assurances in terms of significant risk of harm to human health. These have been derived using the Contaminated Land Exposure Assessment Model (CLEA UK). Certain input parameters such as Health Criteria Values, age and land use are fed into CLEA UK to obtain a probabilistic output[citation needed]. Guidance by the Inter Departmental Committee for the Redevelopment of Contaminated Land (ICRCL)  has been formally withdrawn by DEFRA, for use as a prescriptive document to determine the potential need for remediation or further assessment. The CLEA model published by DEFRA and the Environment Agency (EA) in March 2002 sets a framework for the appropriate assessment of risks to human health from contaminated land, as required by Part IIA of the Environmental Protection Act 1990. As part of this framework, generic Soil Guideline Values (SGVs) have currently been derived for ten contaminants to be used as ""intervention values"".  These values should not be considered as remedial targets but values above which further detailed assessment should be considered; see Dutch standards. Three sets of CLEA SGVs have been produced for three different land uses, namely It is intended that the SGVs replace the former ICRCL values. The CLEA SGVs relate to assessing chronic (long term) risks to human health and do not apply to the protection of ground workers during construction, or other potential receptors such as groundwater, buildings, plants or other ecosystems.  The CLEA SGVs are not directly applicable to a site completely covered in hardstanding, as there is no direct exposure route to contaminated soils. To date, the first ten of fifty-five contaminant SGVs have been published, for the following: arsenic, cadmium, chromium, lead, inorganic mercury, nickel, selenium ethyl benzene, phenol and toluene.  Draft SGVs for benzene, naphthalene and xylene have been produced but their publication is on hold.  Toxicological data (Tox) has been published for each of these contaminants as well as for benzo[a]pyrene, benzene, dioxins, furans and dioxin-like PCBs, naphthalene, vinyl chloride, 1,1,2,2 tetrachloroethane and 1,1,1,2 tetrachloroethane, 1,1,1 trichloroethane, tetrachloroethene, carbon tetrachloride, 1,2-dichloroethane, trichloroethene and xylene.  The SGVs for ethyl benzene, phenol and toluene are dependent on the soil organic matter (SOM) content (which can be calculated from the total organic carbon (TOC) content).  As an initial screen the SGVs for 1% SOM are considered to be appropriate.[citation needed]"|2023-08-25-06-27-26
Soil contamination|Canada| As of February 2021, there are a total of 2,500 plus contaminated sites in Canada.  One infamous contaminated sited is located near a nickel-copper smelting site in Sudbury, Ontario.  A study investigating the heavy metal pollution in the vicinity of the smelter reveals that elevated levels of nickel and copper were found in the soil; values going as high as 5,104ppm Ni, and 2,892 ppm Cu within a 1.1 km range of the smelter location. Other metals were also found in the soil; such metals include iron, cobalt, and silver. Furthermore, upon examining the different vegetation surrounding the smelter it was evident that they too had been affected; the results show that the plants contained nickel, copper and aluminium as a result of soil contamination.|2023-08-25-06-27-26
Soil contamination|India| In March 2009, the issue of uranium poisoning in Punjab attracted press coverage. It was alleged to be caused by fly ash ponds of thermal power stations, which reportedly lead to severe birth defects in children in the Faridkot and Bhatinda districts of Punjab. The news reports claimed the uranium levels were more than 60 times the maximum safe limit.   In 2012, the Government of India confirmed  that the ground water in Malwa belt of Punjab has uranium metal that is 50% above the trace limits set by the United Nations' World Health Organization (WHO). Scientific studies, based on over 1000 samples from various sampling points, could not trace the source to fly ash and any sources from thermal power plants or industry as originally alleged. The study also revealed that the uranium concentration in ground water of Malwa district is not 60 times the WHO limits, but only 50% above the WHO limit in 3 locations. This highest concentration found in samples was less than those found naturally in ground waters currently used for human purposes elsewhere, such as Finland.  Research is underway to identify natural or other sources for the uranium.|2023-08-25-06-27-26
Soot|General| Lists Categories Soot (/sʊt/ suut) is a mass of impure carbon particles resulting from the incomplete combustion of hydrocarbons.  It is more properly restricted to the product of the gas-phase combustion process[citation needed] but is commonly extended to include the residual pyrolysed fuel particles such as coal, cenospheres, charred wood, and petroleum coke that may become airborne during pyrolysis and that are more properly identified as cokes or char. Soot causes various types of cancer and lung disease.|2023-08-28-11-07-14
Soot|Sources|" Soot as an airborne contaminant in the environment has many different sources, all of which are results of some form of pyrolysis. They include soot from coal burning, internal-combustion engines,  power-plant boilers, hog-fuel boilers, ship boilers, central steam-heat boilers, waste incineration, local field burning, house fires, forest fires, fireplaces, and furnaces. These exterior sources also contribute to the indoor environment sources such as smoking of plant matter, cooking, oil lamps, candles, quartz/halogen bulbs with settled dust, fireplaces, exhaust emissions from vehicles,  and defective furnaces. Soot in very low concentrations is capable of darkening surfaces or making particle agglomerates, such as those from ventilation systems, appear black. Soot is the primary cause of ""ghosting"", the discoloration of walls and ceilings or walls and flooring where they meet. It is generally responsible for the discoloration of the walls above baseboard electric heating units. The formation of soot depends strongly on the fuel composition.  The rank ordering of sooting tendency of fuel components is: naphthalenes  benzenes  aliphatics. However, the order of sooting tendencies of the aliphatics (alkanes, alkenes, and alkynes) varies dramatically depending on the flame type. The difference between the sooting tendencies of aliphatics and aromatics is thought to result mainly from the different routes of formation. Aliphatics appear to first form acetylene and polyacetylenes, which is a slow process; aromatics can form soot both by this route and also by a more direct pathway involving ring condensation or polymerization reactions building on the existing aromatic structure."|2023-08-28-11-07-14
Soot|Description|" The Intergovernmental Panel on Climate Change (IPCC) adopted the description of soot particles given in the glossary of Charlson and Heintzenberg (1995), ""Particles formed during the quenching of gases at the outer edge of flames of organic vapours, consisting predominantly of carbon, with lesser amounts of oxygen and hydrogen present as carboxyl and phenolic groups and exhibiting an imperfect graphitic structure"". Formation of soot is a complex process, an evolution of matter in which a number of molecules undergo many chemical and physical reactions within a few milliseconds.  Soot is a powder-like form of amorphous carbon. Gas-phase soot contains polycyclic aromatic hydrocarbons (PAHs).   The PAHs in soot are known mutagens  and are classified as a ""known human carcinogen"" by the International Agency for Research on Cancer (IARC).  Soot forms during incomplete combustion from precursor molecules such as acetylene. It consists of agglomerated nanoparticles with diameters between 6 and 30 nm. The soot particles can be mixed with metal oxides and with minerals and can be coated with sulfuric acid."|2023-08-28-11-07-14
Soot|Soot formation mechanism| Many details of soot formation chemistry remain unanswered and controversial, but there have been a few agreements:|2023-08-28-11-07-14
Soot|Hazards| Soot, particularly diesel exhaust pollution, accounts for over one-quarter of the total hazardous pollution in the air. Among these diesel emission components, particulate matter has been a serious concern for human health due to its direct and broad impact on the respiratory organs. In earlier times, health professionals associated PM10 (diameter < 10 μm) with chronic lung disease, lung cancer, influenza, asthma, and increased mortality rate. However, recent scientific studies suggest that these correlations be more closely linked with fine particles (PM2.5) and ultra-fine particles (PM0.1). Long-term exposure to urban air pollution containing soot increases the risk of coronary artery disease. Diesel exhaust (DE) gas is a major contributor to combustion-derived particulate-matter air pollution.  In human experimental studies using an exposure chamber setup, DE has been linked to acute vascular dysfunction and increased thrombus formation.   This serves as a plausible mechanistic link between the previously described association between particulate matter air pollution and increased cardiovascular morbidity and mortality. Soot also tends to form in chimneys in domestic houses possessing one or more fireplaces. If a large deposit collects in one, it can ignite and create a chimney fire. Regular cleaning by a chimney sweep should eliminate the problem.|2023-08-28-11-07-14
Soot|Soot modeling|" Soot mechanism is difficult to model mathematically because of the large number of primary components of diesel fuel, complex combustion mechanisms, and the heterogeneous interactions during soot formation.  Soot models are broadly categorized into three subgroups: empirical (equations that are adjusted to match experimental soot profiles), semi-empirical (combined mathematical equations and some empirical models which used for particle number density and soot volume and mass fraction), and detailed theoretical mechanisms (covers detailed chemical kinetics and physical models in all phases). First, empirical models use correlations of experimental data to predict trends in soot production. Empirical models are easy to implement and provide excellent correlations for a given set of operating conditions. However, empirical models cannot be used to investigate the underlying mechanisms of soot production. Therefore, these models are not flexible enough to handle changes in operating conditions. They are only useful for testing previously established designed experiments under specific conditions. Second, semi-empirical models solve rate equations that are calibrated using experimental data. Semi-empirical models reduce computational costs primarily by simplifying the chemistry in soot formation and oxidation. Semi-empirical models reduce the size of chemical mechanisms and use simpler molecules, such as acetylene as precursors. 
Detailed theoretical models use extensive chemical mechanisms containing hundreds of chemical reactions in order to predict concentrations of soot. Detailed theoretical soot models contain all the components present in the soot formation with a high level of detailed chemical and physical processes. Finally, comprehensive models (detailed models) are usually expensive and slow to compute, as they are much more complex than empirical or semi-empirical models. Thanks to recent technological progress in computation, it has become more feasible to use detailed theoretical models and obtain more realistic results; however, further advancement of comprehensive theoretical models is limited by the accuracy of modeling of formation mechanisms. Additionally, phenomenological models have found wide use recently. Phenomenological soot models, which may be categorized as semi-empirical models, correlate empirically observed phenomena in a way that is consistent with the fundamental theory, but is not directly derived from the theory. These models use sub-models developed to describe the different processes (or phenomena) observed during the combustion process. Examples of sub-models of phenomenological empirical models include spray model, lift-off model, heat release model, ignition delay model, etc. These sub-models can be empirically developed from observation or by using basic physical and chemical relations. Phenomenological models are accurate for their relative simplicity. They are useful, especially when the accuracy of the model parameters is low. Unlike empirical models, phenomenological models are flexible enough to produce reasonable results when multiple operating conditions change."|2023-08-28-11-07-14
Space debris|General|" Lists Categories Space debris (also known as space junk, space pollution,  space waste, space trash, space garbage, or cosmic debris ) are defunct human-made objects in space – principally in Earth orbit – which no longer serve a useful function. These include derelict spacecraft – nonfunctional spacecraft and abandoned launch vehicle stages – mission-related debris, and particularly numerous in Earth orbit, fragmentation debris from the breakup of derelict rocket bodies and spacecraft. In addition to derelict human-made objects left in orbit, other examples of space debris include fragments from their disintegration, erosion and collisions or even paint flecks, solidified liquids expelled from spacecraft, and unburned particles from solid rocket motors. Space debris represents a risk to spacecraft. Space debris is typically a negative externality – it creates an external cost on others from the initial action to launch or use a spacecraft in near-Earth orbit – a cost that is typically not taken into account nor fully accounted for in the cost   by the launcher or payload owner. Several spacecraft, both crewed and uncrewed, have been damaged or destroyed by space debris.
The measurement, mitigation, and potential removal of debris are conducted by some participants in the space industry. As of November 2022, the US Space Surveillance Network reported 25,857 artificial objects in orbit above the Earth,  including 5,465 operational satellites.  However, these are just the objects large enough to be tracked and in an orbit that makes tracking possible. Satellite debris that is in a Molniya orbit, such as the Kosmos Oko series, might be too high above the northern hemisphere to be tracked.  As of January 2019, more than 128 million pieces of debris smaller than 1 cm (0.4 in), about 900,000 pieces of debris 1–10 cm, and around 34,000 of pieces larger than 10 cm (3.9 in) were estimated to be in orbit around the Earth.  When the smallest objects of artificial space debris (paint flecks, solid rocket exhaust particles, etc.) are grouped with micrometeoroids, they are together sometimes referred to by space agencies as MMOD (Micrometeoroid and Orbital Debris). Collisions with debris have become a hazard to spacecraft; the smallest objects cause damage akin to sandblasting, especially to solar panels and optics like telescopes or star trackers that cannot easily be protected by a ballistic shield. Below 2,000 km (1,200 mi) Earth-altitude, pieces of debris are denser than meteoroids; most are dust from solid rocket motors, surface erosion debris like paint flakes, and frozen coolant from Soviet nuclear-powered satellites.    For comparison, the International Space Station orbits in the 300–400 kilometres (190–250 mi) range, while the two most recent large debris events – the 2007 Chinese antisat weapon test and the 2009 satellite collision – occurred at 800 to 900 kilometres (500 to 560 mi) altitude.  The ISS has Whipple shielding to resist damage from small MMOD; however, known debris with a collision chance over 1/10,000 are avoided by maneuvering the station."|2023-09-16-04-34-57
Space debris|History|" Space debris began to accumulate in Earth orbit immediately with the first launch of an artificial satellite Sputnik 1 into orbit in October 1957. But even before that humans might have produced ejecta that became space debris, as in the August 1957 Pascal B test.   Going back even earlier, there was natural ejecta from Earth in orbit. After the launch of Sputnik, the North American Aerospace Defense Command (NORAD) began compiling a database (the Space Object Catalog) of all known rocket launches and objects reaching orbit: satellites, protective shields and upper-stages of launch vehicles. NASA later published modified versions of the database in two-line element set,  and beginning in the early 1980s the CelesTrak bulletin board system re-published them. The trackers (NORAD) who fed the database were aware of other objects in orbit, many of which were the result of in-orbit explosions.  Some were deliberately caused during the 1960s anti-satellite weapon (ASAT) testing, and others were the result of rocket stages blowing up in orbit as leftover propellant expanded and ruptured their tanks. To improve tracking, NORAD employee John Gabbard kept a more detailed database of as many objects as he could identify. Studying the explosions, in March 1971, Gabbard developed a technique for predicting the orbital paths of their products, and Gabbard diagrams (or plots) are now widely used. These studies were used to improve the modeling of orbital evolution and decay. When the NORAD database became publicly available during the 1970s,[clarification needed] techniques developed for the asteroid-belt were applied to the study[by whom?] to the database of known artificial satellite Earth objects.[citation needed] In addition to approaches to debris reduction where time and natural gravitational/atmospheric effects help to clear space debris, or a variety of technological approaches that have been proposed (with most not implemented) to reduce space debris, a number of scholars have observed that institutional factors – political, legal, economic, and cultural ""rules of the game"" – are the greatest impediment to the cleanup of near-Earth space. By 2014, there was little commercial incentive to reduce space debris, since the cost of dealing with it is not assigned to the entity producing it, but rather falls on all users of the space environment, and rely on human society as a whole that benefits from space technologies and knowledge. A number of suggestions for improving institutions so as to increase the incentives to reduce space debris have been made. These include government mandates to create incentives, as well as companies coming to see economic benefit to reducing debris more aggressively than existing government standard practices. 
In 1979 NASA founded the Orbital Debris Program to research mitigation measures for space debris in Earth orbit."|2023-09-16-04-34-57
Space debris|Debris growth|" During the 1980s, NASA and other U.S. groups attempted to limit the growth of debris. One trial solution was implemented by McDonnell Douglas in 1981 for the Delta launch vehicle by having the booster move away from its payload and vent any propellant remaining in its tanks.  This eliminated one source for pressure buildup in the tanks which had previously caused them to explode and create additional orbital debris.  Other countries were slower to adopt this measure and, due especially to a number of launches by the Soviet Union, the problem grew throughout the decade. A new battery of studies followed as NASA, NORAD, and others attempted to better understand the orbital environment, with each adjusting the number of pieces of debris in the critical-mass zone upward. Although in 1981 (when Schefter's article was published) the number of objects was estimated at 5,000,  new detectors in the Ground-based Electro-Optical Deep Space Surveillance system found new objects. By the late 1990s, it was thought that most of the 28,000 launched objects had already decayed and about 8,500 remained in orbit.  By 2005 this was adjusted upward to 13,000 objects,  and a 2006 study increased the number to 19,000 as a result of an ASAT and a satellite collision.  In 2011, NASA said that 22,000 objects were being tracked. A 2006 NASA model suggested that if no new launches took place the environment would retain the then-known population until about 2055, when it would increase on its own.   Richard Crowther of Britain's Defence Evaluation and Research Agency said in 2002 that he believed the cascade would begin about 2015.  The National Academy of Sciences, summarizing the professional view, noted widespread agreement that two bands of LEO space – 900 to 1,000 km (620 mi) and 1,500 km (930 mi) – were already past critical density. In the 2009 European Air and Space Conference, University of Southampton researcher Hugh Lewis predicted that the threat from space debris would rise 50 percent in the next decade and quadruple in the next 50 years. As of 2009, more than 13,000 close calls were tracked weekly. A 2011 report by the U.S. National Research Council warned NASA that the amount of orbiting space debris was at a critical level. According to some computer models, the amount of space debris ""has reached a tipping point, with enough currently in orbit to continually collide and create even more debris, raising the risk of spacecraft failures."" The report called for international regulations limiting debris and research of disposal methods."|2023-09-16-04-34-57
Space debris|Size and numbers| As of January 2019 there were estimated to be over 128 million pieces of debris smaller than 1 cm (0.39 in), and approximately 900,000 pieces between 1 and 10 cm. The count of large debris (defined as 10 cm across or larger ) was 34,000 in 2019,  and at least 37,000 by June 2023.  The technical measurement cut-off[clarification needed] is c. 3 mm (0.12 in). As of 2020, there were 8,000 metric tons of debris in orbit, a figure that is expected to increase.|2023-09-16-04-34-57
Space debris|Low Earth orbit|" In the orbits nearest to Earth – less than 2,000 km (1,200 mi) orbital altitude, referred to as low-Earth orbit (LEO) – there have traditionally been few ""universal orbits"" that keep a number of spacecraft in particular rings (in contrast to GEO, a single orbit that is widely used by over 500 satellites). This is beginning to change in 2019, and several companies have begun to deploy the early phases of satellite internet constellations, which will have many universal orbits in LEO with 30 to 50 satellites per orbital plane and altitude. Traditionally, the most populated LEO orbits have been a number of sun-synchronous satellites that keep a constant angle between the Sun and the orbital plane, making Earth observation easier with consistent sun angle and lighting. Sun-synchronous orbits are polar, meaning they cross over the polar regions. LEO satellites orbit in many planes, typically up to 15 times a day, causing frequent approaches between objects. The density of satellites – both active and derelict – is much higher in LEO. Orbits are affected by gravitational perturbations (which in LEO include unevenness of the Earth's gravitational field due to variations in the density of the planet), and collisions can occur from any direction. The average impact speed of collisions in Low Earth Orbit is 10 km/s with maximums reaching above 14 km/s due to orbital eccentricity.  The 2009 satellite collision occurred at a closing speed of 11.7 km/s (26,000 mph),  creating over 2,000 large debris fragments.  These debris cross many other orbits and increase debris collision risk. It is theorized that a sufficiently large collision of spacecraft could potentially lead to a cascade effect, or even make some particular low Earth orbits effectively unusable for long term use by orbiting satellites, a phenomenon known as the Kessler syndrome.  The theoretical effect is projected to be a theoretical runaway chain reaction of collisions that could occur, exponentially increasing the number and density of space debris in low-Earth orbit, and has been hypothesized to ensue beyond some critical density. Crewed space missions are mostly at 400 km (250 mi) altitude and below, where air drag helps clear zones of fragments. The upper atmosphere is not a fixed density at any particular orbital altitude; it varies as a result of atmospheric tides and expands or contracts over longer time periods as a result of space weather.  These longer-term effects can increase drag at lower altitudes; the 1990s expansion was a factor in reduced debris density.  Another factor was fewer launches by Russia; the Soviet Union made most of their launches in the 1970s and 1980s."|2023-09-16-04-34-57
Space debris|Higher altitudes| At higher altitudes, where air drag is less significant, orbital decay takes longer. Slight atmospheric drag, lunar perturbations, Earth's gravity perturbations, solar wind and solar radiation pressure can gradually bring debris down to lower altitudes (where it decays), but at very high altitudes this may take centuries.  Although high-altitude orbits are less commonly used than LEO and the onset of the problem is slower, the numbers progress toward the critical threshold more quickly.[contradictory][page needed] Many communications satellites are in geostationary orbits (GEO), clustering over specific targets and sharing the same orbital path. Although velocities are low between GEO objects, when a satellite becomes derelict (such as Telstar 401) it assumes a geosynchronous orbit; its orbital inclination increases about .8° and its speed increases about 160 km/h (99 mph) per year. Impact velocity peaks at about 1.5 km/s (0.93 mi/s). Orbital perturbations cause longitude drift of the inoperable spacecraft and precession of the orbital plane. Close approaches (within 50 meters) are estimated at one per year.  The collision debris pose less short-term risk than from an LEO collision, but the satellite would likely become inoperable. Large objects, such as solar-power satellites, are especially vulnerable to collisions. Although the ITU now requires proof a satellite can be moved out of its orbital slot at the end of its lifespan, studies suggest this is insufficient.  Since GEO orbit is too distant to accurately measure objects under 1 m (3 ft 3 in), the nature of the problem is not well known.  Satellites could be moved to empty spots in GEO, requiring less maneuvering and making it easier to predict future motion.  Satellites or boosters in other orbits, especially stranded in geostationary transfer orbit, are an additional concern due to their typically high crossing velocity. Despite efforts to reduce risk, spacecraft collisions have occurred. The European Space Agency telecom satellite Olympus-1 was struck by a meteoroid on 11 August 1993 and eventually moved to a graveyard orbit.  On 29 March 2006, the Russian Express-AM11 communications satellite was struck by an unknown object and rendered inoperable;  its engineers had enough contact time with the satellite to send it into a graveyard orbit.|2023-09-16-04-34-57
Space debris|Dead spacecraft|" In 1958, the United States of America had launched Vanguard I into a medium Earth orbit (MEO). As of October 2009, it, the upper stage of Vanguard 1's launch rocket and associated piece of debris, are the oldest surviving artificial space objects still in orbit and are expected to be until after the year 2250.   As of May 2022, the Union of Concerned Scientists listed 5,465 operational satellites from a known population of 27,000 pieces of orbital debris tracked by NORAD. Occasionally satellites are left in orbit when they're no longer useful, many countries require that satellites go through passivation at the end of their life. The satellites are then either boosted into a higher, ""graveyard"" orbit or a lower, short-term orbit. Nonetheless, satellites that have been properly moved to a higher orbit have an eight-percent probability of puncture and coolant release over a 50-year period. The coolant freezes into droplets of solid sodium-potassium alloy, creating more debris. Despite the use of passivization, or prior to its standardization, many satellites and rocket bodies have exploded or broken apart on orbit. In February 2015, for example, the USAF Defense Meteorological Satellite Program Flight 13 (DMSP-F13) exploded on orbit, creating at least 149 debris objects, which were expected to remain in orbit for decades.  Later that same year, NOAA-16 which had been decommissioned after an anomaly in June 2014, broke apart on orbit into at least 275 pieces.  For older programs, such as the Soviet-era Meteor 2 and Kosmos satellites, design flaws resulted in numerous break-ups – at least 68 by 1994 – following decommissioning, resulting in more debris. In addition to the accidental creation of debris, some has been made intentionally through the deliberate destruction of satellites. This has been done as a test of anti-satellite or anti-ballistic missile technology, or to prevent a sensitive satellite from being examined by a foreign power.  The United States has conducted over 30 anti-satellite weapons tests (ASATs), the Soviet Union/Russia has performed at least 27, China has performed 10 and India has performed at least one.   The most recent ASATs were the Chinese interception of FY-1C, Russian trials of its PL-19 Nudol, the American interception of USA-193 and India's interception of an unstated live satellite."|2023-09-16-04-34-57
Space debris|Lost equipment| Space debris includes a glove lost by astronaut Ed White on the first American space-walk (EVA), a camera lost by Michael Collins near Gemini 10, a thermal blanket lost during STS-88, garbage bags jettisoned by Soviet cosmonauts during Mir's 15-year life,  a wrench, and a toothbrush.  Sunita Williams of STS-116 lost a camera during an EVA. During an STS-120 EVA to reinforce a torn solar panel, a pair of pliers was lost, and in an STS-126 EVA, Heidemarie Stefanyshyn-Piper lost a briefcase-sized tool bag.|2023-09-16-04-34-57
Space debris|Boosters| Rocket upper stages which end up in orbit are a significant source of space debris. In characterizing the problem of space debris, it was learned that much debris was due to rocket upper stages (e.g. the Inertial Upper Stage) which end up in orbit, and break up due to decomposition of unvented unburned fuel.  The first such instance involved the launch of the Transit-4a satellite in 1961. Two hours after insertion the Ablestar upper stage exploded. But even boosters that don't break apart can be a problem as a major known impact event involved an (intact) Ariane booster. Although NASA and the United States Air Force now require upper-stage passivation, other launchers – such as the Chinese and Russian space agencies – do not. Lower stages, like the Space Shuttle's solid rocket boosters or Apollo program's Saturn IB launch vehicles, do not reach orbit. Examples:|2023-09-16-04-34-57
Space debris|Weapons|" A past debris source was the testing of anti-satellite weapons (ASATs) by the U.S. and Soviet Union during the 1960s and 1970s. North American Aerospace Defense Command (NORAD) files only contained data for Soviet tests, and debris from U.S. tests were only identified later.  By the time the debris problem was understood, widespread ASAT testing had ended; the U.S. Program 437 was shut down in 1975. The U.S. restarted their ASAT programs in the 1980s with the Vought ASM-135 ASAT. A 1985 test destroyed a 1-tonne (2,200 lb) satellite orbiting at 525 km (326 mi), creating thousands of debris larger than 1 cm (0.39 in). Due to the altitude, atmospheric drag decayed the orbit of most debris within a decade. A de facto moratorium followed the test. China's government was condemned for the military implications and the amount of debris from the 2007 anti-satellite missile test,  the largest single space debris incident in history (creating over 2,300 pieces golf-ball size or larger, over 35,000 1 cm (0.4 in) or larger, and one million pieces 1 mm (0.04 in) or larger). The target satellite orbited between 850 km (530 mi) and 882 km (548 mi), the portion of near-Earth space most densely populated with satellites.  Since atmospheric drag is low at that altitude, the debris is slow to return to Earth, and in June 2007 NASA's Terra environmental spacecraft maneuvered to avoid impact from the debris.  Dr. Brian Weeden, U.S. Air Force officer and Secure World Foundation staff member, noted that the 2007 Chinese satellite explosion created an orbital debris of more than 3,000 separate objects that then required tracking. 
On 20 February 2008, the U.S. launched an SM-3 missile from the USS Lake Erie to destroy a defective U.S. spy satellite thought to be carrying 450 kg (1,000 lb) of toxic hydrazine propellant. The event occurred at about 250 km (155 mi), and the resulting debris has a perigee of 250 km (155 mi) or lower.  The missile was aimed to minimize the amount of debris, which (according to Pentagon Strategic Command chief Kevin Chilton) had decayed by early 2009. On 27 March 2019, Indian Prime Minister Narendra Modi announced that India shot down one of its own LEO satellites with a ground-based missile. He stated that the operation, part of Mission Shakti, would defend the country's interests in space. Afterwards, US Air Force Space Command announced they were tracking 270 new pieces of debris but expected the number to grow as data collection continues. On 15 November 2021, the Russian Defense Ministry destroyed Kosmos 1408  orbiting at around 450 km, creating ""more than 1,500 pieces of trackable debris and hundreds of thousands of pieces of un-trackable debris"" according to the US State Department. The vulnerability of satellites to debris and the possibility of attacking LEO satellites to create debris clouds has triggered speculation that it is possible for countries unable to make a precision attack.[clarification needed] An attack on a satellite of 10 t (22,000 lb) or more would heavily damage the LEO environment."|2023-09-16-04-34-57
Space debris|To spacecraft|" Space junk can be a hazard to active satellites and spacecraft. It has been theorized that Earth orbit could even become impassable if the risk of collision grows too high. [failed verification] However, since the risk to spacecraft increases with exposure to high debris densities, it is more accurate to say that LEO would be rendered unusable by orbiting craft. The threat to craft passing through LEO to reach a higher orbit would be much lower owing to the very short time span of the crossing. Although spacecraft are typically protected by Whipple shields, solar panels, which are exposed to the Sun, wear from low-mass impacts. Even small impacts can produce a cloud of plasma which is an electrical risk to the panels. Satellites are believed to have been destroyed by micrometeorites and (small) orbital debris (MMOD). The earliest suspected loss was of Kosmos 1275, which disappeared on 24 July 1981 (a month after launch). Kosmos contained no volatile fuel, therefore, there appeared to be nothing internal to the satellite which could have caused the destructive explosion which took place. However, the case has not been proven and another hypothesis forwarded is that the battery exploded. Tracking showed it broke up, into 300 new objects. Many impacts have been confirmed since. For example, on 24 July 1996, the French microsatellite Cerise was hit by fragments of an Ariane-1 H-10 upper-stage booster which exploded in November 1986.   On 29 March 2006, the Russian Ekspress AM11 communications satellite was struck by an unknown object and rendered inoperable.  On 13 October 2009, Terra suffered a single battery cell failure anomaly and a battery heater control anomaly which were subsequently considered likely the result of an MMOD strike.  On 12 March 2010, Aura lost power from one-half of one of its 11 solar panels and this was also attributed to an MMOD strike.  On 22 May 2013, GOES 13 was hit by an MMOD which caused it to lose track of the stars that it used to maintain an operational attitude. It took nearly a month for the spacecraft to return to operation. The first major satellite collision occurred on 10 February 2009. The 950 kg (2,090 lb) derelict satellite Kosmos 2251 and the operational 560 kg (1,230 lb) Iridium 33 collided, 500 mi (800 km)  over northern Siberia. The relative speed of impact was about 11.7 km/s (7.3 mi/s), or about 42,120 km/h (26,170 mph).  Both satellites were destroyed, creating thousands of pieces of new smaller debris, with legal and political liability issues unresolved even years later.    On 22 January 2013, BLITS (a Russian laser-ranging satellite) was struck by debris suspected to be from the 2007 Chinese anti-satellite missile test, changing both its orbit and rotation rate. Satellites sometimes[clarification needed] perform Collision Avoidance Maneuvers and satellite operators may monitor space debris as part of maneuver planning. For example, in January 2017, the European Space Agency made the decision to alter orbit of one of its three  Swarm mission spacecraft, based on data from the US Joint Space Operations Center, to lower the risk of collision from Cosmos-375, a derelict Russian satellite. Crewed flights are naturally particularly sensitive to the hazards that could be presented by space debris conjunctions in the orbital path of the spacecraft. Examples of occasional avoidance maneuvers, or longer-term space debris wear, have occurred in Space Shuttle missions, the MIR space station, and the International Space Station. From the early Space Shuttle missions, NASA used NORAD space monitoring capabilities to assess the Shuttle's orbital path for debris. In the 1980s, this used a large proportion of NORAD capacity.  The first collision-avoidance maneuver occurred during STS-48 in September 1991,  a seven-second thruster burn to avoid debris from the derelict satellite Kosmos 955.  Similar maneuvers were initiated on missions 53, 72 and 82. One of the earliest events to publicize the debris problem occurred on Space Shuttle Challenger's second flight, STS-7. A fleck of paint struck its front window, creating a pit over 1 mm (0.04 in) wide. On STS-59 in 1994, Endeavour's front window was pitted about half its depth. Minor debris impacts increased from 1998. Window chipping and minor damage to thermal protection system tiles (TPS) were already common by the 1990s. The Shuttle was later flown tail-first to take a greater proportion of the debris load on the engines and rear cargo bay, which are not used in orbit or during descent, and thus are less critical for post-launch operation. When flying attached to the ISS, the two connected spacecraft were flipped around so the better-armoured station shielded the orbiter. A NASA 2005 study concluded that debris accounted for approximately half of the overall risk to the Shuttle.   Executive-level decision to proceed was required if the catastrophic impact was likelier than 1 in 200. On a normal (low-orbit) mission to the ISS, the risk was approximately 1 in 300, but the Hubble telescope repair mission was flown at the higher orbital altitude of 560 km (350 mi) where the risk was initially calculated at a 1-in-185 (due in part to the 2009 satellite collision). A re-analysis with better debris numbers reduced the estimated risk to 1 in 221, and the mission went ahead. Debris incidents continued on later Shuttle missions. During STS-115 in 2006 a fragment of circuit board bored a small hole through the radiator panels in Atlantis's cargo bay.  On STS-118 in 2007 debris blew a bullet-like hole through Endeavour's radiator panel. Impact wear was notable on Mir, the Soviet space station since it remained in space for long periods with its original solar module panels. The ISS also uses Whipple shielding to protect its interior from minor debris.  However, exterior portions (notably its solar panels) cannot be protected easily. In 1989, the ISS panels were predicted to degrade approximately 0.23% in four years due to the ""sandblasting"" effect of impacts with small orbital debris.  An avoidance maneuver is typically performed for the ISS if ""there is a greater than one-in-10,000 chance of a debris strike"".  As of January 2014, there have been sixteen maneuvers in the fifteen years the ISS had been in orbit.  By 2019, over 1,400 meteoroid and orbital debris (MMOD) impacts had been recorded on the ISS. As another method to reduce the risk to humans on board, ISS operational management asked the crew to shelter in the Soyuz on three occasions due to late debris-proximity warnings. In addition to the sixteen thruster firings and three Soyuz-capsule shelter orders, one attempted maneuver was not completed due to not having the several days' warning necessary to upload the maneuver timeline to the station's computer.    A March 2009 event involved debris believed to be a 10 cm (3.9 in) piece of the Kosmos 1275 satellite.  In 2013, the ISS operations management did not make a maneuver to avoid any debris, after making a record four debris maneuvers the previous year. The Kessler syndrome,   proposed by NASA scientist Donald J. Kessler in 1978, is a theoretical scenario in which the density of objects in low Earth orbit (LEO) is high enough that collisions between objects could cause a cascade effect where each collision generates space debris that increases the likelihood of further collisions.  He further theorized that one implication, if this were to occur, is that the distribution of debris in orbit could render space activities and the use of satellites in specific orbital ranges economically impractical for many generations. The growth in the number of objects as a result of the late-1990s studies sparked debate in the space community on the nature of the problem and the earlier dire warnings. According to Kessler's 1991 derivation and 2001 updates,  the LEO environment in the 1,000 km (620 mi) altitude range should be cascading. However, only one major satellite collision incident occurred: the 2009 satellite collision between Iridium 33 and Cosmos 2251. The lack of obvious short-term cascading has led to speculation that the original estimates overstated the problem.  According to Kessler in 2010 however, a cascade may not be obvious until it is well advanced, which might take years."|2023-09-16-04-34-57
Space debris|On Earth| Although most debris burns up in the atmosphere, larger debris objects can reach the ground intact. According to NASA, an average of one cataloged piece of debris has fallen back to Earth each day for the past 50 years. Despite their size, there has been no significant property damage from the debris.  Burning up in the atmosphere may also contribute to atmospheric pollution.  Numerous small cylindrical tanks from space objects have been found, designed to hold fuel or gasses.|2023-09-16-04-34-57
Space debris|Tracking from the ground| Radar and optical detectors such as lidar are the main tools for tracking space debris. Although objects under 10 cm (4 in) have reduced orbital stability, debris as small as 1 cm can be tracked,   however determining orbits to allow re-acquisition is difficult. Most debris remain unobserved. The NASA Orbital Debris Observatory tracked space debris with a 3 m (10 ft) liquid mirror transit telescope.  FM Radio waves can detect debris, after reflecting off them onto a receiver.  Optical tracking may be a useful early-warning system on spacecraft. The U.S. Strategic Command keeps a catalog of known orbital objects, using ground-based radar and telescopes, and a space-based telescope (originally to distinguish from hostile missiles). The 2009 edition listed about 19,000 objects.  Other data come from the ESA Space Debris Telescope, TIRA,  the Goldstone, Haystack,  and EISCAT radars and the Cobra Dane phased array radar,  to be used in debris-environment models like the ESA Meteoroid and Space Debris Terrestrial Environment Reference (MASTER).|2023-09-16-04-34-57
Space debris|Measurement in space| Returned space hardware is a valuable source of information on the directional distribution and composition of the (sub-millimetre) debris flux. The LDEF satellite deployed by mission STS-41-C Challenger and retrieved by STS-32 Columbia spent 68 months in orbit to gather debris data. The EURECA satellite, deployed by STS-46 Atlantis in 1992 and retrieved by STS-57 Endeavour in 1993, was also used for debris study. The solar arrays of Hubble were returned by missions STS-61 Endeavour and STS-109 Columbia, and the impact craters studied by the ESA to validate its models. Materials returned from Mir were also studied, notably the Mir Environmental Effects Payload (which also tested materials intended for the ISS ).|2023-09-16-04-34-57
Space debris|Gabbard diagrams| A debris cloud resulting from a single event is studied with scatter plots known as Gabbard diagrams, where the perigee and apogee of fragments are plotted with respect to their orbital period. Gabbard diagrams of the early debris cloud prior to the effects of perturbations, if the data were available, are reconstructed. They often include data on newly observed, as yet uncatalogued fragments. Gabbard diagrams can provide important insights into the features of the fragmentation, the direction and point of impact.|2023-09-16-04-34-57
Space debris|Dealing with debris|" An average of about one tracked object per day has been dropping out of orbit for the past 50 years,  averaging almost three objects per day at solar maximum (due to the heating and expansion of the Earth's atmosphere), but one about every three days at solar minimum, usually five and a half years later.  In addition to natural atmospheric effects, corporations, academics and government agencies have proposed plans and technology to deal with space debris, but as of November 2014, most of these are theoretical, and there is no extant business plan for debris reduction. A number of scholars have also observed that institutional factors – political, legal, economic, and cultural ""rules of the game"" – are the greatest impediment to the cleanup of near-Earth space. There is little commercial incentive to act, since costs are not assigned to polluters, though a number of technological solutions have been suggested.  However, effects to date are limited. In the US, governmental bodies have been accused of backsliding on previous commitments to limit debris growth, ""let alone tackling the more complex issues of removing orbital debris.""  The different methods for removal of space debris have been evaluated by the Space Generation Advisory Council, including French astrophysicist Fatoumata Kébé."|2023-09-16-04-34-57
Space debris|National and international regulation|" There is no international treaty minimizing space debris. However, the United Nations Committee on the Peaceful Uses of Outer Space (COPUOS) published voluntary guidelines in 2007, 
using a variety of earlier national regulatory attempts at developing standards for debris mitigation.
As of 2008, the committee was discussing international ""rules of the road"" to prevent collisions between satellites. 
By 2013, a number of national legal regimes existed,    typically instantiated in the launch licenses that are required for a launch in all spacefaring nations. The U.S. issued a set of standard practices for civilian (NASA) and military (DoD and USAF) orbital-debris mitigation in 2001.    The standard envisioned disposal for final mission orbits in one of three ways: 1) atmospheric reentry where even with ""conservative projections for solar activity, atmospheric drag will limit the lifetime to no longer than 25 years after completion of mission;"" 2) maneuver to a ""storage orbit:"" move the spacecraft to one of four very broad parking orbit ranges (2,000–19,700 km (1,200–12,200 mi), 20,700–35,300 km (12,900–21,900 mi), above 36,100 km (22,400 mi), or out of Earth orbit completely and into any heliocentric orbit; 3) ""Direct retrieval: Retrieve the structure and remove it from orbit as soon as practicable after completion of mission.""  The standard articulated in option 1, which is the standard applicable to most satellites and derelict upper stages launched, has come to be known as the ""25-year rule"".  The US updated the Orbital Debris Mitigation Standard Practices (ODMSP) in December 2019, but made no change to the 25-year rule even though ""[m]any in the space community believe that the timeframe should be less than 25 years.""  There is no consensus however on what any new timeframe might be. In 2002, the European Space Agency (ESA) worked with an international group to promulgate a similar set of standards, also with a ""25-year rule"" applying to most Earth-orbit satellites and upper stages. Space agencies in Europe began to develop technical guidelines in the mid-1990s, and ASI, UKSA, CNES, DLR and ESA signed a ""European Code of Conduct"" in 2006,  which was a predecessor standard to the ISO international standard work that would begin the following year. In 2008, ESA further developed ""its own ""Requirements on Space Debris Mitigation for Agency Projects"" which ""came into force on 1 April 2008."" Germany and France have posted bonds to safeguard the property from debris damage.[clarification needed]  The ""direct retrieval"" option (option no. 3 in the US ""standard practices"" above) has rarely been done by any spacefaring nation (exception, USAF X-37) or commercial actor since the earliest days of spaceflight due to the cost and complexity of achieving direct retrieval, but the ESA has scheduled a 2025 demonstration mission (Clearspace-1) to do this with a single small 100 kg (220 lb) derelict upper stage at a projected cost of €120 million not including the launch costs. By 2006, the Indian Space Research Organization (ISRO) had developed a number of technical means of debris mitigation (upper stage passivation, propellant reserves for movement to graveyard orbits, etc.) for ISRO launch vehicles and satellites, and was actively contributing to inter-agency debris coordination and the efforts of the UN COPUOS committee. In 2007, the ISO began preparing an international standard for space-debris mitigation. 
By 2010, ISO had published ""a comprehensive set of space system engineering standards aimed at mitigating space debris. [with primary requirements] defined in the top-level standard, ISO 24113."" By 2017, the standards were nearly complete. However, these standards are not binding on any party by ISO or any international jurisdiction. They are simply available for use in any of a variety of voluntary ways. They ""can be adopted voluntarily by a spacecraft manufacturer or operator, or brought into effect through a commercial contract between a customer and supplier, or used as the basis for establishing a set of national regulations on space debris mitigation."" The voluntary ISO standard also adopted the ""25-year rule"" for the ""LEO protected region"" below 2,000 km (1,200 mi) altitude that has been previously (and still is, as of 2019) used by the US, ESA, and UN mitigation standards, and identifies it as ""an upper limit for the amount of time that a space system shall remain in orbit after its mission is completed. Ideally, the time to deorbit should be as short as possible (i.e., much shorter than 25 years)"". Holger Krag of the European Space Agency states that as of 2017 there is no binding international regulatory framework with no progress occurring at the respective UN body in Vienna."|2023-09-16-04-34-57
Space debris|Environmental concerns|" The continued practice of disposing of space debris on Earth in areas such as the spacecraft cemetery has raised environmental concerns.  Klinger states that ""the environmental geopolitics of Earth and outer space are inextricably linked by the spatial politics of privilege and sacrifice – among people, places, and institutions"".   Since 1971, 273 spacecraft and satellites have been directed to Point Nemo; this number includes the Mir Space Station (142 tonnes) and will include the International Space Station in 2024 (240 tonnes). In 2018, it was found that the water had 26 microplastic particles per cubic metre, meaning it is highly polluted.  The prevalence of orbital debris has been likened to the terrestrial environmental phenomenon of ""sacrifice zones"", which are designated geographic regions with high levels of environmental degradation. Since the 1960s, over three hundred rocket launch sites have been built globally.  Among these launch sites, 17 hosted 90 launches in 2017 alone.  Rocket launches affect local and global environments through the construction of necessary infrastructure, exposure of local environments to toxic residue and the dispersal of pollutants.  Rockets are the only source of direct anthropogenic emissions into the stratosphere and emit ozone depleting substances such as nitrous oxide, hydrogen chloride and aluminium oxide; these substances can destroy 105 ozone molecules before depleting.  Each launch showers an area concentrated within a kilometre with toxins, heavy metals, and acids.  This results in localised regional acid rain, plant death, fish kills, and failed seed germination.  Furthermore, studies on trace elements concentration in alligators, near NASA launch activities in Florida (USA), showed that over 50% of alligators had ""greater than toxic levels"" of trace elements in their liver.  Similarly, research in Kazakhstan, Russia and China has found that unsymmetrical dimethylhydrazine (UDMH) has carcinogenic, mutagenic, convulsant, teratogenic, embryotoxic and DNA damaging effects on rodents living near the Baikonur Cosmodrome, Kazakhstan.  It is unknown, however, at what trace concentrations these toxic effects manifest in humans or how it may bioaccumulate up the food chain.  A lack of adequate resourcing to maintain safe, non-toxic environments makes these areas sacrifice zones and spaces of waste. The relative remoteness of these spaces makes them attractive launch sites, yet this ""periphery"" remain central to both their human and non-human inhabitants, who become ""sacrificial"". An increase in anthropogenic activity in outer space has resulted in large amounts of physical pollution known as orbital debris.  This has resulted in space becoming more and more congested due to this rapid increase.  This increase in congestion threatens the orbital operations of nations and corporations alike.  However, orbital debris also has an impact on the wider population through various means, such as potentially lethal debris falling from the sky,  or orbital debris ruining night skies through light pollution.  Environmental justice as described by the United States Environmental Protection Agency is the fair and meaningful involvement of all demographics in the development and implementation of environmental regulations and policies.  Outer space can be seen as an environment like any other environment on Earth,  making orbital debris a potential issue of environmental justice. The idea of environmental injustice is also that the rights of those who have suffered environmental harm or have been disrupted and encroached upon by more powerful actors are protected.  In the context of the space industry large space organisations and nations are the powerful actors and those who have suffered are public. Recent research shows that space exploration opens many questions about environmental justice.  Environmental geopolitics of the earth and that of outer space are linked, particularly by the geopolitics of privilege from one party and sacrifice from another.  They are also linked in the premise of environmental justice, this injustice can unfold on multiple scales such as emission from space launches, placement of outer space-related infrastructure and of course orbital debris in this case. There is an argument that as well as viewing space as an environment space should also be viewed as a commons.  If space is viewed as a commons then its resources need to be managed for the common good of all people. If not, then this poses a risk of injustice. By managing space as a resource that all nations have access to, and none can claim sovereignity over, outer space can be also understood as an example of a global commons. However, legally defining space as a common is challenging, as the idea of global commons is a social construct.  Many space treaties include various phrases describing and defining the usage of outer space, including phrases such as ""for the benefit of all people"" and ""shall be the providence of all mankind"".  However, none of these treaties provides an adequate framework for handling resources or resolving issues. Governance frameworks for outer space have a very narrow and utilitarian view of outer space that looks at what they can gain from exploration."|2023-09-16-04-34-57
Space debris|Growth mitigation|" As of the 2010s, several technical approaches to the mitigation of the growth of space debris are typically undertaken, yet no comprehensive legal regime or cost assignment structure is in place to reduce space debris in the way that terrestrial pollution has reduced since the mid-20th century. To avoid excessive creation of artificial space debris, many – but not all – satellites launched to above-low-Earth-orbit are launched initially into elliptical orbits with perigees inside Earth's atmosphere so the orbit will quickly decay and the satellites then will destroy themselves upon reentry into the atmosphere. Other methods are used for spacecraft in higher orbits. These include passivation of the spacecraft at the end of its useful life; as well as the use of upper stages that can reignite to decelerate the stage to intentionally deorbit it, often on the first or second orbit following payload release; satellites that can, if they remain healthy for years, deorbit themselves from the lower orbits around Earth. Other satellites (such as many CubeSats) in low orbits below approximately 400 km (250 mi) orbital altitude depend on the energy-absorbing effects of the upper atmosphere to reliably deorbit a spacecraft within weeks or months. Increasingly, spent upper stages in higher orbits – orbits for which low-delta-v deorbit is not possible, or not planned for – and architectures that support satellite passivation, at end of life are passivated at end of life. This removes any internal energy contained in the vehicle at the end of its mission or useful life. While this does not remove the debris of the now derelict rocket stage or satellite itself, it does substantially reduce the likelihood of the spacecraft destructing and creating many smaller pieces of space debris, a phenomenon that was common in many of the early generations of US and Soviet  spacecraft. Upper stage passivation (e.g. of Delta boosters ) by releasing residual propellants reduces debris from orbital explosions; however even as late as 2011, not all upper stages implement this practice.  SpaceX used the term ""propulsive passivation"" for the final maneuver of their six-hour demonstration mission (STP-2) of the Falcon 9 second stage for the US Air Force in 2019, but did not define what all that term encompassed. With a ""one-up, one-down"" launch-license policy for Earth orbits, launchers would rendezvous with, capture and de-orbit a derelict satellite from approximately the same orbital plane.  Another possibility is the robotic refueling of satellites. Experiments have been flown by NASA,  and SpaceX is developing large-scale on-orbit propellant transfer technology. Another approach to debris mitigation is to explicitly design the mission architecture to always leave the rocket second-stage in an elliptical geocentric orbit with a low-perigee, thus ensuring rapid orbital decay and avoiding long-term orbital debris from spent rocket bodies. Such missions will often complete the payload placement in a final orbit by the use of low-thrust electric propulsion or with the use of a small kick stage to circularize the orbit. The kick stage itself may be designed with the excess-propellant capability to be able to self-deorbit."|2023-09-16-04-34-57
Space debris|Self-removal|" Although the ITU requires geostationary satellites to move to a graveyard orbit at the end of their lives, the selected orbital areas do not sufficiently protect GEO lanes from debris.  Rocket stages (or satellites) with enough propellant may make a direct, controlled de-orbit, or if this would require too much propellant, a satellite may be brought to an orbit where atmospheric drag would cause it to eventually de-orbit. This was done with the French Spot-1 satellite, reducing its atmospheric re-entry time from a projected 200 years to about 15 by lowering its altitude from 830 km (516 mi) to about 550 km (342 mi). The Iridium constellation – 95 communication satellites launched during the five-year period between 1997 and 2002 – provides a set of data points on the limits of self-removal. The satellite operator – Iridium Communications – remained operational over the two-decade life of the satellites (albeit with a company name change through a corporate bankruptcy during the period) and, by December 2019, had ""completed disposal of the last of its 65 working legacy satellites.""  However, this process left 30 satellites with a combined mass of (20,400 kg (45,000 lb), or nearly a third of the mass of this constellation) in LEO orbits at approximately 700 km (430 mi) altitude, where self-decay is quite slow. Of these satellites, 29 simply failed during their time in orbit and were thus unable to self-deorbit, while one – Iridium 33 – was involved in the 2009 satellite collision with the derelict Russian military satellite Kosmos-2251.  No contingency plan was laid for the removal of satellites that were unable to remove themselves. In 2019, the Iridium CEO, Matt Desch, said that Iridium would be willing to pay an active-debris-removal company to deorbit its remaining first-generation satellites if it were possible for an unrealistically low cost, say ""US$10,000 per deorbit, but [he] acknowledged that price would likely be far below what a debris-removal company could realistically offer. 'You know at what point [it's] a no-brainer, but [I] expect the cost is really in the millions or tens of millions, at which price I know it doesn't make sense.'"" Passive methods of increasing the orbital decay rate of spacecraft debris have been proposed. Instead of rockets, an electrodynamic tether could be attached to a spacecraft at launch; at the end of its lifetime, the tether would be rolled out to slow the spacecraft.  Other proposals include a booster stage with a sail-like attachment  and a large, thin, inflatable balloon envelope. In late December 2022, ESA successfully carried out a demonstration of a breaking sail-based satellite deorbiter, ADEO, which could be used by mitigation measures and is part of ESA's Zero Debris Initiative. Around one year earlier, China also tested a drag sail."|2023-09-16-04-34-57
Space debris|External removal|" A variety of approaches have been proposed, studied, or had ground subsystems built to use other spacecraft to remove existing space debris. A consensus of speakers at a meeting in Brussels in October 2012, organized by the Secure World Foundation (a U.S. think tank) and the French International Relations Institute,  reported that removal of the largest debris would be required to prevent the risk to spacecraft becoming unacceptable in the foreseeable future (without any addition to the inventory of dead spacecraft in LEO). To date in 2019, removal costs and legal questions about ownership and the authority to remove defunct satellites have stymied national or international action. Current space law retains ownership of all satellites with their original operators, even debris or spacecraft which are defunct or threaten active missions. Multiple companies made plans in the late 2010s to conduct external removal on their satellites in mid-LEO orbits. For example, OneWeb planned to utilize onboard self-removal as ""plan A"" for satellite deorbiting at the end of life, but if a satellite were unable to remove itself within one year of end of life, OneWeb would implement ""plan B"" and dispatch a reusable (multi-transport mission) space tug to attach to the satellite at an already built-in capture target via a grappling fixture, to be towed to a lower orbit and released for re-entry. A well-studied solution uses a remotely controlled vehicle to rendezvous with, capture, and return debris to a central station. 
One such system is Space Infrastructure Servicing, a commercially developed refueling depot and service spacecraft for communications satellites in geosynchronous orbit originally scheduled for a 2015 launch.  The SIS would be able to ""push dead satellites into graveyard orbits.""  The Advanced Common Evolved Stage family of upper stages is being designed with a high leftover-propellant margin (for derelict capture and de-orbit) and in-space refueling capability for the high delta-v required to de-orbit heavy objects from geosynchronous orbit.  A tug-like satellite to drag debris to a safe altitude for it to burn up in the atmosphere has been researched.  When debris is identified the satellite creates a difference in potential between the debris and itself, then using its thrusters to move itself and the debris to a safer orbit. A variation of this approach is for the remotely controlled vehicle to rendezvous with debris, capture it temporarily to attach a smaller de-orbit satellite and drag the debris with a tether to the desired location. The ""mothership"" would then tow the debris-smallsat combination for atmospheric entry or move it to a graveyard orbit. One such system is the proposed Busek ORbital DEbris Remover (ORDER), which would carry over 40 SUL (satellite on umbilical line) de-orbit satellites and propellant sufficient for their removal. On 7 January 2010 Star, Inc. reported that it received a contract from the Space and Naval Warfare Systems Command for a feasibility study of the ElectroDynamic Debris Eliminator (EDDE) propellantless spacecraft for space-debris removal.  In February 2012 the Swiss Space Center at École Polytechnique Fédérale de Lausanne announced the Clean Space One project, a nanosatellite demonstration project for matching orbit with a defunct Swiss nanosatellite, capturing it and de-orbiting together.  The mission has seen several evolutions to reach a pac-man inspired capture model.  In 2013, Space Sweeper with Sling-Sat (4S), a grappling satellite which captures and ejects debris was studied. [needs update] In 2022, a Chinese satellite, SJ-21, grabbed an unused satellite and ""threw"" it into an orbit with a lower risk for it to collide. In December 2019, the European Space Agency awarded the first contract to clean up space debris. The €120 million mission dubbed ClearSpace-1 (a spinoff from the EPFL project) is slated to launch in 2025. It aims to remove a 100 kg VEga Secondary Payload Adapter (Vespa)  left by Vega flight VV02 in an 800 km (500 mi) orbit in 2013. A ""chaser"" will grab the junk with four robotic arms and drag it down to Earth's atmosphere where both will burn up. The laser broom uses a ground-based laser to ablate the front of the debris, producing a rocket-like thrust that slows the object. With continued application, the debris would fall enough to be influenced by atmospheric drag.   During the late 1990s, the U.S. Air Force's Project Orion was a laser-broom design.  Although a test-bed device was scheduled to launch on a Space Shuttle in 2003, international agreements banning powerful laser testing in orbit limited its use to measurements.  The 2003 Space Shuttle Columbia disaster postponed the project and according to Nicholas Johnson, chief scientist and program manager for NASA's Orbital Debris Program Office, ""There are lots of little gotchas in the Orion final report. There's a reason why it's been sitting on the shelf for more than a decade."" The momentum of the laser-beam photons could directly impart a thrust on the debris sufficient to move small debris into new orbits out of the way of working satellites. NASA research in 2011 indicates that firing a laser beam at a piece of space junk could impart an impulse of 1 mm (0.039 in) per second, and keeping the laser on the debris for a few hours per day could alter its course by 200 m (660 ft) per day.  One drawback is the potential for material degradation; the energy may break up the debris, adding to the problem.  A similar proposal places the laser on a satellite in Sun-synchronous orbit, using a pulsed beam to push satellites into lower orbits to accelerate their reentry.  A proposal to replace the laser with an Ion Beam Shepherd has been made,  and other proposals use a foamy ball of aerogel or a spray of water,  inflatable balloons,  electrodynamic tethers,  electroadhesion,  and dedicated anti-satellite weapons. On 28 February 2014, Japan's Japan Aerospace Exploration Agency (JAXA) launched a test ""space net"" satellite. The launch was an operational test only.  In December 2016 the country sent a space junk collector via Kounotori 6 to the ISS by which JAXA scientists experiment to pull junk out of orbit using a tether.   The system failed to extend a 700-meter tether from a space station resupply vehicle that was returning to Earth.   On 6 February the mission was declared a failure and leading researcher Koichi Inoue told reporters that they ""believe the tether did not get released"". Since 2012, the European Space Agency has been working on the design of a mission to remove large space debris from orbit. The mission, e.Deorbit, is scheduled for launch during 2023 with an objective to remove debris heavier than 4,000 kilograms (8,800 lb) from LEO.  Several capture techniques are being studied, including a net, a harpoon and a combination robot arm and clamping mechanism. The RemoveDEBRIS mission plan is to test the efficacy of several ADR technologies on mock targets in low Earth orbit. In order to complete its planned experiments the platform is equipped with a net, a harpoon, a laser ranging instrument, a dragsail, and two CubeSats (miniature research satellites).  The mission was launched on 2 April 2018.[citation needed] Metal processing technologies to melt space debris and transform it into other useful form factors are developed by CisLunar Industries. Their system uses electromagnetic heating to melt metal and shape it into metal wire, sheet metal, and metal fuel. A propulsion system dubbed the Neumann Drive has been developed in Adelaide, Australia, with first sent into space in June 2023. Metal space junk is converted into fuel rods, which can be plugged into the Neumann Drive, ""basically converting the solid metal propellant into plasma"". The Drive will be used by American space companies which already carry nets or robotic arms to capture orbital waste. The thruster enables these satellites to return to Earth with the waste they have collected, allowing it to be melted down to make more fuel."|2023-09-16-04-34-57
Space debris|Barriers to dealing with debris|" With the rapid development of the computer and digitalization industries, more countries and companies have engaged in space activities since the turn of the 20th century. The tragedy of the commons is an economic theory referring to a situation where maximizing self-interest through using a shared resource can finally lead to the resource degradation shared by all.  Based on the theory, individuals' rational action in space will finally lead to an irrational collective result: orbits are crowded with debris. As a common-pool resource, the Earth's orbits, especially LEO and GEO that accommodate most satellites, are nonexcludable and rivalry.  To address the tragedy and ensure space sustainability, many technical approaches have been developed. And in terms of governance mechanisms, the top-down centralized one is less suitable to tackle the complex debris problem due to the increasing number of space actors.  Instead, much evidence has proved that polycentric form of governance developed by Elinor Ostrom can work in space. In the process of promoting the polycentric network, there are some existing barriers needed to be dealt with. As orbital debris is a global problem affecting both spacefaring and non-spacefaring nations, it is necessary to be handled in a worldwide context.  Because of the complexity and dynamics of object movements like spacecraft, debris, meteorites, etc., many countries and regions including the United States, Europe, Russia and China have developed their space situational awareness (SSA) to avoid potential threats in space or plan actions in advance.  To a certain extent, SSA plays a role in tracking space debris. In order to build a powerful SSA system, there are two prerequisites: international cooperation and exchange of information and data.  However, limitations still exist in spite of the substantially improving data quality over the past decades. Some space powers are not willing to share the information that they have collected, and those, such as the U.S., that have shared the data keep parts of it secret.  Instead of joining in a coordinated way, a great deal of SSA programs and national databases run parallel to each other with some overlaps, hindering the formation of a collaborative monitoring system. Some private actors are also trying to establish SSA systems. For example, the Space Data Association (SDA) formed in 2009 is a non-governmental entity. It currently consists of 21 global satellite operators and 4 executive members: Eutelsat, Inmarsat, Intelsat and SES. SDA is a non-profit platform, aiming to avoid radio interference and space collisions through pooling data from operators independently.  Researchers suggest that it is essential to establish an international center for exchanging information on space debris because SSA networks do not completely equal debris tracking systems – the former ones focus more on active and threatening objects in space.  And in terms of debris populations and defunct satellites, not very much operators have provided data. In a polycentric governance network, a resource that cannot be holistically monitored is less possible to be well managed.  Both insufficient transnational cooperation and information sharing bring resistance to addressing the debris problem. There is still a long way to go before building a global network that covers complete data and has strong interconnection and interoperability. With the commercialization of satellites and space, the private sector is getting more interested in space activities. For example, SpaceX is planning to create a network of around 12,000 small satellites that can transmit high-speed internet to any place in the world.  The proportion of commercial spacecraft has increased from 4.6% in the 1980s to 55.6% in the 2010s.  Despite the high participation rate of commercial entities, UN COPUOS once deliberately excluded them from having a voice in discussions unless being formally invited by a member state.  Ostrom said that the involvement of all relevant stakeholders in the rule-design and implementation process is one of the critical elements of successful governance.  The exclusion of private actors largely reduces the effectiveness of the committee's role in making collective-choice arrangements that reflect the interests of all space users. The limited engagement of private actors slows down the process of addressing space debris to some degree.  Ties existing between dissimilar stakeholders in the governance network offer access to diverse resources.  Different competence among stakeholders can help allocate the tasks more reasonably. In that case, the expertise and experience of private operators are critical to help the world achieve space sustainability.  The complementary strengths of different stakeholders enable the governance network to be more adaptable to changes and reach common goals more effectively.  In recent years, many private actors have seen commercial opportunities of eliminating space debris. It is estimated that by 2022 the global market for debris monitoring and removal will generate a revenue of around $2.9 billion.  For example, Astroscale has contracted with European and Japanese space agencies to develop the capacity of removing orbital debris.  Despite that, they are still in small quantity compared to the number of those who have placed satellites in space. Privateer Space, a Hawaiian-based startup company started by American engineer Alex Fielding, space environmentalist Dr. Moriba Jah, and Apple co-founder Steve Wozniak, announced plans in September 2021 to launch hundreds of satellites into orbit in order to study space debris.  However, the company stated it is in ""stealth mode"" and no such satellites have been launched. Fortunately, the current space exploration is not completely driven by competition, and there still exists a chance for dialogues and cooperation among all stakeholders in both developed and developing countries, to reach an agreement on tackling space debris and assure an equitable and orderly exploration.  Besides private actors, network governance does not necessarily exclude the states from playing a role. Instead, the different functions of states might promote the governance process.  To improve the polycentric governance network of space debris, researchers suggest: encourage data-sharing among different national and organizational databases at the political level; develop shared standards for data collection systems to improve interoperability; enhance the participation of private actors through involving them in national and international discussions."|2023-09-16-04-34-57
Space debris|At other celestial bodies| The issue of space debris has been raised as a mitigation challenge for missions around the Moon with the danger of increasing space debris around it. In 2022, several elements of space debris were found on Mars, Perseverance's backshell was found on the surface of Jezero Crater,  and a piece of a thermal blanket that may have come from the descent stage of the rover. It is thought that on 4 March 2022, for the first time, human space debris – most likely a spent rocket body, Long March 3C third stage from the 2014 Chang'e 5 T1 mission – unintentionally hit the lunar surface, creating an unexpected double crater.|2023-09-16-04-34-57
Space debris|In popular culture|" Until the End of the World (1991) is a French sci-fi drama set under the backdrop of an out-of-control Indian nuclear satellite, predicted to re-enter the atmosphere, threatening vast populated areas of the Earth. Gravity, a 2013 survival film directed by Alfonso Cuaron, is about a disaster on a space mission caused by Kessler syndrome. In season 1 of Love, Death & Robots (2019), episode 11, ""Helping Hand"", revolves around an astronaut being struck by a screw from space debris which knocks her off a satellite in orbit. Manga and anime Planetes tells a story about a crew of Space Debris station that collects and disposes space debris."|2023-09-16-04-34-57
Space sustainability|General| Space sustainability aims to maintain the safety and health of the space environment. Similar to sustainability initiatives on Earth, space sustainability seeks to use the environment of space to meet the current needs of society without compromising the needs of future generations.    It usually focuses on space closest to Earth, Low Earth Orbit (LEO), since this environment is the one most used and therefore most relevant to humans.  It also considers Geostationary Equatorial Orbit (GEO) as this orbit is another popular choice for Earth-orbiting mission designs. The issue of space sustainability is a new phenomenon that is gaining more attention in recent years as the launching of satellites and other space objects has increased.  These launches have resulted in more space debris orbiting Earth, hindering the ability of nations to operate in the space environment while increasing the risk of a future launch-related accident that could disrupt its proper use.   Space weather also acts as an outstanding factor for spacecraft failure.  The current protocol for spacecraft disposal at end-of-life has, at large, not been followed in mission designs and demands extraneous amounts of time for disposal. Precedent created through prior policy initiatives has facilitated initial mitigation of space pollution and created a foundation for space sustainability efforts.  To further mitigation, international and transdisciplinary consortia have stepped forward to analyze existing operations, develop standards, and incentivize future procedures to prioritize a sustainable approach.  A shift towards sustainable interactions with the space environment is growing in urgency due to the implications of climate change and increasing risk to spacecraft as time presses on.|2023-09-26-22-28-23
Space sustainability|Fundamentals| Space sustainability requires all space participants to have three consensuses. The space field should be used peacefully, jointly protect the space field from harm, and maximize space utilization through environmental, economic, and security exploration of space.  These consensuses also clarify the relationship between space sustainability and international security, that states and individuals explore space for various purposes. Their reliance on space needs to be guided by rules, order, and policies and obtain more benefits without negatively affecting the space environment and space activities. However, striking an agreement remains challenging even with such demands in place. In the discussions between countries on long-term sustainability, technical improvements are given more importance than introducing and applying new legal regimes.  Specifically, technical approaches to space debris have been proposed, such as debris removal.  Specific data on space debris is also being explored to help study its impact on sustainability and promote further cooperation between countries.|2023-09-26-22-28-23
Space sustainability|Current state| Space sustainability comes into play to address the pressing current state of near-Earth orbits and its high amounts of orbital debris.  Spacecraft collisions with orbital debris, space weather, overcrowding in low Earth orbit (LEO) makes spacecraft susceptible to higher rates of failure.   The current end-of-life protocol for spacecraft exacerbates the space sustainability crisis; many spacecraft are not properly disposed, which increasing the likelihood of further collisions.|2023-09-26-22-28-23
Space sustainability|Orbital debris| Orbital debris is defined as unmanned, inoperate objects that exist in space.  This orbital debris breaks down further as time progresses as a result of naturally occurring events, such as high-velocity collisions with micrometeoroids, and forced events, such as a controlled release of a launch vehicle.  In LEO, these collisions can take place at speeds anywhere between an average velocity of 9 kilometers per second (km/s) and 14 km/s relative to the debris and spacecraft.  In GEO, however, these high-speed collisions are a much lower risk as the average relative velocity between the debris and spacecraft is typically between 0 km/s and 2.5 km/s.  As of 2012, the United States Joint Space Operations Center tracked 21,000 pieces of orbital debris larger than 10 cm in Earth's nearby orbits (LEO, GEO, and Sun-synchronous), where 16,000 of these pieces are catalogued. Space debris can be categorized into three categories: small, medium, and large.  Small debris is for pieces that are less than 10 centimeters (cm).  Medium-sized debris is for pieces larger than 10 cm, but not an entire spacecraft.  Large-sized debris has no official classification, but typically refers to entire spacecraft, such as an out of use satellite or launch vehicle.  It is difficult to track small-sized debris in LEO, and challenging to track small and medium-sized debris in GEO.  Yet this statement is not to discount the abilities of LEO and GEO tracking capabilities, the smallest piece of tracked debris can weigh as low as ten grams.  If the size of the debris prohibits it from being tracked, it also cannot be avoided by the spacecraft and does not allow the spacecraft to lower its risk of collisions.  The likelihood of the Kessler syndrome, which essentially states that each collision produces more debris, grows larger as the amount of orbital debris multiplies, increasing the amount of further collisions until space cannot be used entirely.|2023-09-26-22-28-23
Space sustainability|Space weather| Space weather poses a risk to satellite health, consequently, resulting in greater amounts of orbital debris.  Space weather impacts satellite health in a variety of ways. Firstly, surface charging from the sun's surface facilitates electrical discharges, damaging on-orbit electronics, posing a threat to mission failure.  Single Event Upsets (SEUs) can also damage electronics.  Dielectric charging and bulk charging can also occur, causing energy problems within the spacecraft.  Additionally, at altitudes less than one thousand kilometers, atmospheric drag can increase during solar storms by increasing the altitude of the spacecraft, only adding more drag onto the spacecraft.  These factors degrade performance over the spacecraft's lifetime, leaving the spacecraft more susceptible to further system and mission failures.|2023-09-26-22-28-23
Space sustainability|Overcrowding| There has been a dramatic increase in the use of LEO and GEO orbits over the last sixty years since the first satellite launch in 1957. To date, there have been approximately ten thousand satellite launches, whereas only approximately 2000 are still active.  These satellites can be used for a variety of purposes, which are telecommunications, navigation, weather monitoring, and exploration. Within the coming decade, companies like SpaceX are predicted to launch an additional fifteen thousand satellites into LEO and GEO orbits.  Microsatellites built by universities or research organizations have also increased in popularity, contributing to the overcrowding of near earth orbits.  This overcrowding of LEO and GEO orbits increases the likelihood of potential collisions among satellites and orbital debris, contributing further to the large amount of orbital debris present in space.|2023-09-26-22-28-23
Space sustainability|End of life protocol| The current end of life protocol is that at the end of mission, spacecraft are either added to the graveyard orbit or at a low enough altitude that drag will allow the spacecraft to burn up upon reentry and fall back to Earth.  Approximately twenty satellites are put into the graveyard orbit each year.  There is no current process to return satellites to Earth after entering the graveyard orbit.  The process of a spacecraft returning to Earth via drag can take between ten and one hundred years.  This protocol is critical to reduce overcrowding in near-Earth orbits.|2023-09-26-22-28-23
Space sustainability|Mega constellation and space debris| The impact of constellations on the space environment has also been studied, such as the probability of collisions of mega constellations in the presence of large amounts of space debris. Although studies have shown that the predictors of mega constellations are highly variable, specific information related to mega constellations is not transparent. But any catastrophic collision, as in the case of Kessler syndrome, has consequences for people and the environment. Putting this thinking into mega constellations, mega constellations existence may have potential benefits, but it will not bring adequate help to the governance of space debris.  At the same time, the space debris situation cannot be underestimated or ignored because of the existence of mega constellations.|2023-09-26-22-28-23
Space sustainability|Concern| The existence of orbital debris has caused great trouble to the conduct of space activities. The development of space sustainability has not given sufficient political attention, although some warnings and discussions have made this abundantly clear.  Debris management is still voluntary on the part of the state, and there are no laws mandating debris management practices, including the amount of debris to be managed.  Although the UN Space Debris Mitigation Guidelines were promulgated in 2007 as an initial measure of space debris governance, there is still no broad consensus or action on further limits on space debris after that. The difficulties for individuals wishing to participate in debris management initiatives cannot be ignored. Any individual or sector desiring to participate in space debris operations needs to obtain permission from the launching state, which is difficult for the launching state to do.  This is because the process of space debris management inevitably has a negative impact on other space objects, and there is a lot of subsequent liability in terms of financial consumption.  Therefore, the launching state would argue that space debris management requires the joint efforts of all states.  However, it is difficult to determine what actions can be taken to gain acceptance between countries.|2023-09-26-22-28-23
Space sustainability|Regulations| Current space sustainability efforts rely heavily on the precedent set by regulatory agreements and conventions of the twentieth century.  Much of this precedent is included in or is related to the Outer Space Treaty of 1963, which represented one of the initial major efforts by the United Nations to create legal frameworks for the operation of nations in space.|2023-09-26-22-28-23
Space sustainability|Pre-Outer Space Treaty| The international community has had concerns about space contamination since the 1950s prior to the launch of Sputnik I.  These concerns stemmed from the idea that increasing rates of exploration into further areas of outer space could lead to contamination capable of damaging other planetary bodies, resulting in limitations to human exploration on these bodies and potential harm to the Earth.  Efforts to combat these concerns began in 1956 with the International Astronautical Federation (IAF) and the United Nations Committee on the Peaceful Uses of Outer Space (COPUOUS). These efforts continued to 1957 through the National Academy of Sciences and International Council for Science (ICSU).  Each of these organizations aimed to study space contamination and develop strategies for how to best address its potential consequences.  The ICSU went on to create the Committee on Contamination by Extraterrestrial Exploration (CETEX) that put forward recommendations leading to the establishment of the Committee on Space Research (COSPAR).  COSPAR continues to address outer space research on an international scale today [cite cospar].|2023-09-26-22-28-23
Space sustainability|Outer Space Treaty|" Relevant regulations of international space law to sustainability in space can be found in the Outer Space Treaty, which was adopted by the UN General Assembly in 1963.  The Outer Space Treaty contains seventeen articles designed to create a basic framework for how international law can be applied in outer space.  Basic principles of the Outer Space Treaty include the provision in Article IX that parties should ""avoid harmful contamination of space and celestial bodies;""   definitions of ""harmful contamination"" are not provided.   Other articles of relevance to space sustainability include articles I, II, and III that concern the fair and inclusive international use of space in a manner free from sovereignty, ownership, or occupation by any nation.  In addition, articles VII and VIII protect ownership by their respective countries of any objects launched to space while attributing responsibility for any damages to the property or personnel of other countries by those objects to said countries.  Descriptions or definitions for what these damages may entail are not provided."|2023-09-26-22-28-23
Space sustainability|COSPAR Planetary Protection Policy|" Principles of Article IX provided the basis for the Committee of Space Research  (COSPAR) Planetary Protection Policy guidelines, which are generally well-regarded among scientific experts.  Such guidelines, however, are non-binding and often described as ""soft-law,"" as they lack legal mandate.  The Planetary Protection Policy is primarily concerned with providing information regarding best practices to avoid contamination of the space environment during space exploration missions.  COSPAR believes that the prevention of such contamination is in the best interest of humanity as it may impede scientific progress, exploration, and the mission of a search for life.  In addition, the argument is made that cross-contamination of the Earth can be potentially harmful to its environment due to the largely unknown nature of potential space contaminants."|2023-09-26-22-28-23
Space sustainability|Other relevant regulations| Regulatory clarifications concerning the Outer Space Treaty of 1963 of relevance to space sustainability were made in subsequent years. The 1967 Return Agreement relates mainly to the return of lost astronauts to their appropriate nations, but also requires Outer Space Treaty signing nations to assist other nations with the return of objects that return to Earth from orbit to their proper owners  The 1972 Liability Convention attributes liability for damages from space objects to the nation that launched the object, regardless of whether that damage occurred in space or on Earth.  Other clarifications include the 1975 registration convention that attempted to create mechanisms for nations to identify space objects, and the 1979 Moon Agreement that established protections for the environments of the Moon and other nearby planetary bodies.   These agreements and conventions represented attempts to improve the initial Outer Space Treaty as space exploration continued to grow in importance throughout the 20th century.|2023-09-26-22-28-23
Space sustainability|Countries and major international institutions| Both the state and space agencies are working to improve the laws and regulations that facilitate the long-term sustainability of space. For example, the European Code of Conduct for Space Debris Mitigation signed by France, the UK and other countries in 2016.  China, Brazil, Mexico and others have legal background and methodological measures under long-term space sustainability.    However, the main problem is that until the concept of space sustainability is agreed between countries, inter-regional efforts are not working well. Currently, the Committee on the Peaceful Uses of Outer Space (COPUOS) encourages states to incorporate the space debris mitigation guidelines developed by bodies such as the Inter-Agency Space Debris Coordination (IADC) into their national legislation, thereby regulating state behavior.  Some countries have responded positively to this, such as Switzerland, the Netherlands and Spain. However, there are still some countries that do not consider debris management approaches in their national legislation, such as Japan and Australia.  Many delegates at the COPUOS meeting expressed their reasons for doing so, arguing that space debris management is closely linked to technology and funding. Technology is dynamic and constantly evolving. Therefore, the incorporation of debris governance guidelines into national law is not an immediate priority at this time.|2023-09-26-22-28-23
Space sustainability|Scientific attitudes|" A study outlined rationale for governance that regulates the current free externalization of true costs and risks, treating orbital space around the Earth as an ""additional ecosystem"" or a common ""part of the human environment"" which should be subject to the same concerns and regulations like oceans on Earth. While scientists may not have the means to make and enforce global laws themselves, the study concluded in 2022 that it needs ""new policies, rules and regulations at national and international level""."|2023-09-26-22-28-23
Space sustainability|Mitigation| Sustainability mitigation efforts include but are not limited to design specifications, policy change, removal of space debris, and restoration of orbiting semi-functional technologies.     Efforts begin by regulating the debris released during normal operations and post-mission breakups [6]. Due to the increased awareness of high-velocity collisions and orbital debris in the previous decades, missions have adapted design specifications to account for these risks.  For example, the RADARSAT program implemented 17 kilograms of shielding to their spacecraft, which increased the program's predicted success rate to 87% from 50%.  Another effort in mitigation is restoring semi-functional satellites, which allows a spacecraft classified as “debris” to “functional.”   Space debris mitigation focuses on limiting debris release during normal operations, collisions and intentional destruction.  Mitigation also includes reducing the possibility for post-mission breakups due to stored energy and/or operations phases, as well as addressing procedure for end of mission disposal for spacecraft.|2023-09-26-22-28-23
Space sustainability|Space Sustainability Rating| One example leading the regulatory sustainability measures is the Space Sustainability Rating (SSR), which is an instigator for industry competitors to incorporate sustainability into spacecraft design.  The Space Sustainability Rating was first conceptualized at the World Economic Forum Global Future Council on Space Technologies designed by international and transdisciplinary consortia.  The four leading organizations are the European Space Agency, Massachusetts Institute of Technology, University of Texas at Austin, and BryceTech with the goal to define the technical and programmatic aspects of the SSR.  The SSR represents an innovative approach to combating orbital debris through incentivizing the industry to prioritize sustainable and responsible operations.  This response entails the consideration of potential harm to the space environment and other spacecraft, all while maintaining mission objectives and high-quality service.  The rating takes inspiration from other standards, like leadership in energy and environmental design (LEED) for the building sector. Several of the factors emphasized in the rating were extracted from LEED design considerations like the incorporation of feedback and public comments, or the rating's advocacy to influence policy, such as orbit fragmentation risks, collision avoidance capabilities, trackability, and adoption of international standards.|2023-09-26-22-28-23
Space sustainability|Tracking|" Tracking is one of the main Space Sustainability Rating modules’ efforts. The module ""Detectability, Identification and Tracking"" (DIT) consists of standardizing the comparison of satellite missions to encourage satellite operators to improve their satellite design and operational approaches for the observer to detect, identify, and track the satellites.  Tracking presents challenges when the observer seeks to monitor and predict the spacecraft behavior over time.  While the observer may know the name, owner, and instantaneous location of the satellite, the operator controls the full knowledge of the orbital parameters.  The Space Situational Awareness (SSA) is one the tools geared towards solving the challenges presented when tracking orbiting satellites and debris.  The SSA continuously tracks objects using ground-based radar and optical stations so the orbital paths of debris can be predicted and operations avoid collisions.  It feeds data to 30 different systems like satellites, optical telescopes, radar systems, and supercomputers to predict risk of collision days in advance.  Other efforts in tracking orbital debris are made by the US Space Surveillance Network (SSN)."|2023-09-26-22-28-23
Space sustainability|Removal|" Under the ""External Services"" module of the SSR, the rating offers commitment to use or demonstration of use of end-of-life removal services.  Space debris mitigation measures are found to be inadequate to stabilize debris environments with an actual current compliance of approximately sixty percent.  Moreover, a low compliance rate of approximately thirty percent of the 103 spacecraft that reached end of life between 1997 and 2003 were disposed of in a graveyard orbit.  Since policy has not caught up to ensure the longevity of LEO for future generations, actions like Active Debris Removal (ADR) are being considered to stabilize the future of LEO environment.  Most famous removal concepts are based on directed energy, momentum exchange or electrodynamics, aerodynamic drag augmentation, solar sails, auxiliary propulsion units, retarding surfaces and on-orbit capture.  As ADR consists of an external disposal method to remove obsolete satellites or spacecraft fragments.  Since large-sized debris objects in orbit provide a potential source for tens of thousands of fragments in the future, ADR efforts focus on objects with high mass and large cross-sectional areas, in densely populated regions, and at high altitudes; in this instance, retired satellites and rocket bodies are a priority.  Other practical advancements toward space debris removal include missions like RemoveDEBRIS and End-of-Life Service (ELS-d)."|2023-09-26-22-28-23
Space sustainability|Growing urgency| The previous reduced state of regulation and mitigation on space debris  and rocket fuel emissions  is aggravating the Earth's stratosphere through collisions and ozone depletion, increasing the risk for spacecraft health through its lifetime.|2023-09-26-22-28-23
Space sustainability|Inaccessibility to LEO| Due to the increase of satellites being launched and the growing amount of orbital debris in LEO,  the risk of LEO becoming inaccessible over time (in accordance with the Kessler syndrome) is increasing in likelihood. The mitigation policies for creating space debris fall under an area of voluntary codes by the states, although it has been disputed whether the Article I Outer Space Treaty or the Article IX Outer Space Treaty protects the space environment from deliberate harm, which has yet to be upheld.   In 2007, an inactive Chinese satellite was purposefully destroyed by the Chinese government as a part of their anti-satellite weapon test (ASAT), spreading nearly 2800 objects of space debris five centimeters or larger into LEO.  An analysis concluded that about eighty percent of the debris will remain in LEO nine years after this destruction.  In addition, the destruction increased the collision likelihood for three Italian satellites that launched the same year as the Fengyun-1C destruction.  The increase in collision ranged between ten and sixty percent.  However, there were no legal consequences against the Chinese government.|2023-09-26-22-28-23
Space sustainability|Rocket fuel emissions| When rockets are launched into space, parts of their fuel enter the stratosphere of the Earth. Rocket fuel emissions are made up of carbon dioxide, water, hydrochloric acid, alumina and soot particles. The most concerning emissions from rocket fuel are chlorine and alumina particles from solid rocket motors (SRMs) and soot from kerosene fueled engines. When the hydrochloric acid from the engine exhaust dissociates, the free chlorine roams freely in the stratosphere.  The chemical reaction between these chlorine and alumina causes ozone depletion. In addition, the soot particles form over a black umbrella over the stratosphere which can cause the temperature of the Earth's surface to lower and further depleting the ozone layer, an unintentional form of geoengineering.  The nature of geoengineering has been disputed as a form of mitigating global warming and has the possibility of being banned and holding rockets accountable for the soot particles they distribute to the stratosphere. New types of engines and fuels are emerging, mainly the liquid oxygen (LOX) and monomethylhydrazine engine, but there is minimal research on their impact on the environment besides their emission of hydroxide and nitrogen oxide compounds, two molecules that have significant impact on the ozone layer.  Currently, rocket fuel emissions have been deemed insignificant when it comes to their consequences to Earth's environment and LEO.  However, emissions will increase in the coming years, making rocket fuel's contribution to global warming much more significant.|2023-09-26-22-28-23
Space sustainability|Beyond LEO| Space sustainability concepts and mindsets tend to stay in Low Earth Orbit (LEO).  One reason that cannot be ignored is that it is easier to discuss the problem at hand than to speculate on the unknown.  There are also examples to prove that since Apollo 17 completed its mission and stayed in Low Earth orbit in 1972, human-crewed space missions in Low Earth orbit have ceased to exist.  In this way, it is a reasonable assumption that the closer Moon could be the next object to be explored when the gaze is not limited to LEO.  Both lunar orbit and LEO are part of the space environment. In the context of the presence of space debris in LEO, it is normal to speculate that lunar orbit also possesses the nuisance of debris. Space debris measures similar to those in LEO related to space sustainability would be taken. Not only has the Moon been the subject of study, but other bodies have also been taken into account. Elon Musk, the chief executive of SpaceX at the International Astronautical Congress in 2016, explained the ambitious goal of exploring Mars in the 22nd century.  But complicated issues remain, such as the technical aspects of achieving long-distance space flight and the rules and legal aspects associated with the technology, all of which need to be considered.|2023-09-26-22-28-23
Subnature|General| Subnature is the undesirable by-products of urbanization, industrialization, war, abandonment, and societal collapse. Subnature includes things such as smog, dust, exhaust gas, industrial smoke, sewage, debris, rubble, vermin, and weeds. The concept has been used in historical and theoretical writing on architecture,  literature,  music,  and food studies.|2021-02-02-19-42-00
Subnature|References| This industry-related article is a stub. You can help Wikipedia by expanding it.|2021-02-02-19-42-00
Toxic waste|General| Lists Categories Toxic waste is any unwanted material in all forms that can cause harm (e.g. by being inhaled, swallowed, or absorbed through the skin). Mostly generated by industry, consumer products like televisions, computers, and phones contain toxic chemicals that can pollute the air and contaminate soil and water. Disposing of such waste is a major public health issue.|2023-09-09-10-32-25
Toxic waste|Classifying toxic materials| Toxic materials are poisonous byproducts as a result of industries such as manufacturing, farming, construction, automotive, laboratories, and hospitals which may contain heavy metals, radiation, dangerous pathogens, or other toxins. Toxic waste has become more abundant since the industrial revolution, causing serious global issues. Disposing of such waste has become even more critical with the addition of numerous technological advances containing toxic chemical components. Products such as cellular telephones, computers, televisions, and solar panels contain toxic chemicals that can harm the environment if not disposed of properly to prevent air pollution and the contamination of soils and water. A material is considered toxic when it causes death or harm by being inhaled, swallowed, or absorbed through the skin. The waste can contain chemicals, heavy metals, radiation, dangerous pathogens, or other toxins. Even households generate hazardous waste from items such as batteries, used computer equipment, and leftover paints or pesticides.  Toxic material can be either human-made and others are naturally occurring in the environment. Not all hazardous substances are considered toxic. The United Nations Environment Programme (UNEP) has identified 11 key substances that pose a risk to human health: The most overlooked toxic and hazardous wastes are the household products in everyday homes that are improperly disposed of such as old batteries, pesticides, paint, and car oil. Toxic waste can be reactive, ignitable, and corrosive. In the United States, these wastes are regulated under the Resource Conservation and Recovery Act (RCRA). With the increase of worldwide technology, there are more substances that are considered toxic and harmful to human health. Technology growth at this rate is extremely daunting for civilization and can eventually lead to more harm/negative outcomes. Some of this technology includes cell phones and computers. Such items have been given the name e-waste or EEE, which stands for Electrical and Electronic Equipment. This term is also used for goods such as refrigerators, toys, and washing machines. These items can contain toxic components that can break down into water systems when discarded. The reduction in the cost of these goods has allowed for these items to be distributed globally without thought or consideration to managing the goods once they become ineffective or broken. In the US, the Environmental Protection Agency (EPA) and state environmental agencies develop and enforce regulations on the storage, treatment, and disposal of hazardous waste. The EPA requires that toxic waste be handled with special precautions, and be disposed of in designated facilities around the country. Also, many US cities have collection days where household toxic waste is gathered. Some materials that may not be accepted at regular landfills are ammunition, commercially generated waste, explosives/shock sensitive items, hypodermic needles/syringes, medical waste, radioactive materials, and smoke detectors.|2023-09-09-10-32-25
Toxic waste|Health effects| Toxic wastes often contain carcinogens, and exposure to these by some route, such as leakage or evaporation from the storage, causes cancer to appear at increased frequency in exposed individuals. For example, a cluster of the rare blood cancer polycythemia vera was found around a toxic waste dump site in northeast Pennsylvania in 2008. The Human & Ecological Risk Assessment Journal conducted a study that focused on the health of individuals living near municipal landfills to see if it would be as harmful as living near hazardous landfills. They conducted a 7-year study that specifically tested for 18 types of cancers to see if the participants had higher rates than those that don't live around landfills. They conducted this study in western Massachusetts within a 1-mile radius of the North Hampton Regional Landfill. People encounter these toxins buried in the ground, in stream runoff, in groundwater that supplies drinking water, or in floodwaters, as happened after Hurricane Katrina. Some toxins, such as mercury, persist in the environment and accumulate. As a result of the bioaccumulation of mercury in both freshwater and marine ecosystems, predatory fish are a significant source of mercury in human and animal diets.|2023-09-09-10-32-25
Toxic waste|Prevalence of Toxic Waste Sites in the United States|" In the United States today millions of Americans live near a toxic waste site that is in a radius of three miles of where they currently reside. For example, In February 2022 it was found that ""Black Americans are 75 percent more likely to live near waste-producing facilities."" Residents that live in or near communities that are 1.8 miles around a Superfund or hazardous site have a ""high risk for life-long and long-term mental health and physical health challenges, including cancer, birth defects, and developmental disabilities."" There are an estimated 21 million people that reside within a mile of these sites allowing the harsh chemicals and toxins, such as lead and mercury, that have escaped to enter nearby water supplies, to affect the air quality, and ground conditions resulting in destructive environmental surroundings. In the past, recent years' climate change increasing severe rainstorms, flooding, and winds from hurricanes have a greater possibility to disrupt the content of these toxic waste sites allowing unstable organic compounds to go back into the environment."|2023-09-09-10-32-25
Toxic waste|Toxic Lead Waste and Disease in Three Latin American Countries|" It can be seen that in Argentina, Mexico, and Uruguay there has been an increase in industrial development, urbanization, and socioeconomic forces. As these industries grow there is an underlying consequence of pollution that comes from environmental exposure to hazardous waste. When people are exposed to this pollution, they suffer negative health effects. With diseases on the rise, the disability-adjusted life year (DALY) starts to decline, so the time the average person lives decreases, especially in low to middle-income countries. These low to middle-income countries (LMIC) have minimal resources to deal with toxic waste, such as ""inadequate regulation, the informality of many industries, poor surveillance, and improper disposal of contaminants."" For example, ""lead is still used for glazing artisanal ceramics despite the availability of less hazardous alternatives."" Lead enters the soil and water sources if not kept under control. Children are more susceptible than adults to absorbing more lead if exposed early on, causing them to have ""behavioral problems in adolescence, IQ decrements, cognitive impairment, and decreased visuospatial skills."" If adults are exposed occupationally, they can have higher rates of hypertension than the average person. Men can result in low sperm count and females can result in miscarriages. In order to further quantify the burden of diseases caused by toxic wastes TSIP, Toxic Sites Identification Program, "" identifies active and abandoned hazardous waste sites resulting from both formal and informal industrial activities in LMICs"". As an investigation begins a key pollutant is sought out and identified. For example, ""Heavy metals are the most commonly occurring key pollutant, with ingestion of contaminated soils being the most commonly occurring route of exposure listed in the TSIP database."" Argentina, Mexico, and Uruguay were chosen since they had more available data after meeting certain criteria. As of now, there are five criteria that have to be met in order for a hazardous waste site to be included in the analysis; ""a biological or environmental sample had to be present; a population at risk had to be specified; the location of the site was represented by GPS coordinates, and a description of the activities leading to contamination were outlined."" To measure the amount of lead in the soil a handheld X-ray fluorescence (XRF) spectrometer was used. When this method seemed unavailable and an area was suspected of having to lead contamination, the blood of individuals was tested to show the amount the people who were exposed to lead. The exposure data were collected from a total of 129 hazardous waste sites distributed across Argentina (n = 23), Mexico (n = 62), and Uruguay (n = 44). In Figs. 1 and 2 the sites of geographical distributions are shown. An estimated population of 316,703 individuals were at risk of exposure (mean = 2455; median = 250 per site), which is approximately 0.19% of the total population of all three countries. There was an estimation of 80,021 individuals who were women of childbearing age (15–49 years of age), and 122,084 individuals who were younger than 18 years of age."|2023-09-09-10-32-25
Toxic waste|Dioxins and Solid Waste Disposal in Campania, Italy|" In recent years in the region of Campania, Italy there has been a rise in illegal dumping and burning of toxic and solid waste. In response to this, there has been a rise of dangerous chemical molecules like dioxins that are carcinogenic, which implies that they have the potential to cause cancer, that is appearing in humans and animals. For example, there has been a recent increase in sheep that have been born in contaminated areas that have, ""higher rates of chromosome fragility, higher mortality, and a higher incidence of abnormal fetal development when compared with sheep raised in non-contaminated areas."" To assess the causal relations between cancer mortality and congenital malformations in humans coming from illegal dumping a map was drawn using the geographical locations of the sites. As one can see most of these sites are located in Campania where Naples and Caserta are based. The spread of dioxin through food consumption is primarily due to the animal products from the animals that were raised in the geographical locations where dioxins were the highest. Researchers tested mammalian milk from these areas and saw that the levels of dioxin were over the suggested amount. This was greatly seen as an issue because humans have the highest capability to concentrate the dioxin in their fat tissues. To test this, 94 women in Campania who were breastfeeding had samples of their breast milk tested and it was found that every woman had dioxin in their breast milk. A correlation was also discovered that the older you were the more dioxin was in your breast milk."|2023-09-09-10-32-25
Toxic waste|Handling and disposal| One of the biggest problems with today's toxic material is how to dispose of it properly. Before the passage of modern environmental laws (in the US, this was in the 1970s), it was legal to dump such wastes into streams, rivers, and oceans, or bury them underground in landfills. The US Clean Water Act, enacted in 1972, and RCRA, enacted in 1976, created nationwide programs to regulate the handling and disposal of hazardous wastes. The agriculture industry uses over 800,000 tons of pesticides worldwide annually that contaminate soils, and eventually infiltrate into groundwater, which can contaminate drinking water supplies. The oceans can be polluted from the stormwater runoff of these chemicals as well. Toxic waste in the form of petroleum oil can either spill into the oceans from pipe leaks or large ships, but it can also enter the oceans from everyday citizens dumping car oil into the rainstorm sewer systems. Disposal is the placement of waste into or on the land. Disposal facilities are usually designed to permanently contain waste and prevent the release of harmful pollutants to the environment.[citation needed] The most common hazardous waste disposal practice is placement in a land disposal unit such as a landfill, surface impoundment, waste pile, land treatment unit, or injection well. Land disposal is subject to requirements under EPA's Land Disposal Restrictions Program.  Injection wells are regulated under the federal Underground Injection Control program. Organic wastes can be destroyed by incineration at high temperatures.  However, if the waste contains heavy metals or radioactive isotopes, these must be separated and stored, as they cannot be destroyed.  The method of storage will seek to immobilize the toxic components of the waste, possibly through storage in sealed containers, inclusion in a stable medium such as glass or a cement mixture, or burial under an impermeable clay cap. Waste transporters and waste facilities may charge fees; consequently, improper methods of disposal may be used to avoid paying these fees. Where the handling of toxic waste is regulated, the improper disposal of toxic waste may be punishable by fines  or prison terms. Burial sites for toxic waste and other contaminated brownfield land may eventually be used as greenspace or redeveloped for commercial or industrial use.|2023-09-09-10-32-25
Toxic waste|History of US toxic waste regulation|" The RCRA governs the generation, transportation, treatment, storage, and disposal of hazardous waste.  The Toxic Substances Control Act (TSCA), also enacted in 1976, authorizes the EPA to collect information on all new and existing chemical substances, as well as to control any substances that were determined to cause unreasonable risk to public health or the environment.   The Superfund law, passed in 1980, created a cleanup program for abandoned or uncontrolled hazardous waste sites. There has been a long ongoing battle between communities and environmentalists versus governments and corporations about how strictly and how fairly the regulations and laws are written and enforced. That battle began in North Carolina in the late summer of 1979, as EPA's TSCA regulations were being implemented. In North Carolina, PCB-contaminated oil was deliberately dripped along rural Piedmont highways, creating the largest PCB spills in American history and a public health crisis that would have repercussions for generations to come.  The PCB-contaminated material was eventually collected and buried in a landfill in Warren County, but citizens' opposition, including large public demonstrations, exposed the dangers of toxic waste, the fallibility of landfills than in use, and EPA regulations allowing landfills to be built on marginal, but politically acceptable sites. Warren County citizens argued that the toxic waste landfill regulations were based on the fundamental assumption that the EPA's conceptual dry-tomb landfill would contain the toxic waste. This assumption informed the siting of toxic waste landfills and waivers to regulations that were included in EPA's Federal Register. For example, in 1978, the base of a major toxic waste landfill could be no closer than five feet from groundwater, but this regulation and others could be waived. The waiver to the regulation concerning the distance between the base of a toxic waste landfill and groundwater allowed the base to be only a foot above ground water if the owner/operator of the facility could demonstrate to the EPA regional administrator that a leachate collection system could be installed and that there would be no hydraulic connection between the base of the landfill and groundwater. Citizens argued that the waivers to the siting regulations were discriminatory mechanisms facilitating the shift from scientific to political considerations concerning the siting decision and that in the South this would mean a discriminatory proliferation of dangerous waste management facilities in poor black and other minority communities. They also argued that the scientific consensus was that permanent containment could not be assured. As resistance to the siting of the PCB landfill in Warren County continued and studies revealed that EPA dry-tomb landfills were failing, EPA stated in its Federal Register that all landfills would eventually leak and should only be used as a stopgap measure. Years of research and empirical knowledge of the failures of the Warren County PCB landfill led citizens of Warren County to conclude that the EPA's dry-tomb landfill design and regulations governing the disposal of toxic and hazardous waste were not based on sound science and adequate technology. Warren County's citizens concluded also that North Carolina's 1981 Waste Management Act was scientifically and constitutionally unacceptable because it authorized the siting of toxic, hazardous, and nuclear waste facilities prior to public hearings, preempted local authority over the siting of the facilities, and authorized the use of force if needed. In the aftermath of the Warren County protests, the 1984 Federal Hazardous and Solid Waste Amendments to the Resource Conservation and Recovery Act focused on waste minimization and phasing out land disposal of hazardous waste as well as corrective action for releases of hazardous materials. Other measures included in the 1984 amendments included increased enforcement authority for EPA, more stringent hazardous waste management standards, and a comprehensive underground storage tank program. The disposal of toxic waste continues to be a source of conflict in the U.S. Due to the hazards associated with toxic waste handling and disposal, communities often resist the siting of toxic waste landfills and other waste management facilities; however, determining where and how to dispose of waste is a necessary part of economic and environmental policy-making. The issue of handling toxic waste has become a global problem as international trade has arisen out of the increasing toxic byproducts produced with the transfer of them to less developed countries.  In 1995, the United Nations Commission on Human Rights began to notice the illicit dumping of toxic waste and assigned a Special Rapporteur to examine the human rights aspect to this issue (Commission resolution 1995/81). In September 2011, the Human Rights Council decided to strengthen the mandate to include the entire life-cycle of hazardous products from manufacturing to the final destination (aka cradle to grave), as opposed to only movement and dumping of hazardous waste. The title of the Special Rapporteur has been changed to ""Special Rapporteur on the implications for human rights of the environmentally sound management and disposal of hazardous substances and wastes"" (Human Rights Council 18/11). The Human Rights Council has further extended the scope of its mandates as of September 2012 due to the result of the dangerous implications occurring to persons advocating environmentally sound practices regarding the generation, management, handling, distribution, and final disposal of hazardous and toxic materials to include the issue of the protection of the environmental human rights defenders."|2023-09-09-10-32-25
Toxic waste|Mapping of toxic waste in the United States| TOXMAP was a geographic information system (GIS) from the Division of Specialized Information Services  of the United States National Library of Medicine (NLM) that used maps of the United States to help users visually explore data from the United States Environmental Protection Agency's (EPA) Superfund and Toxics Release Inventory programs. The chemical and environmental health information was taken from NLM's Toxicology Data Network (TOXNET)  and PubMed, and from other authoritative sources. The database was removed from the internet by the Trump Administration in December 2019.|2023-09-09-10-32-25
Toxic waste|In popular culture|" ""Toxic waste"" is often utilized in science fiction as a plot device that causes organisms or characters to undergo mutation. Examples of works that feature toxic waste in such a manner include the films Mutant,  C.H.U.D.,  Impulse (all 1984),  and Teenage Mutant Ninja Turtles (1990).   Several films produced by Troma Entertainment involve mutation via toxic or radioactive waste, including The Toxic Avenger (1984) and Class of Nuke 'Em High (1986)."|2023-09-09-10-32-25
Toxicodynamics|General| Toxicodynamics, termed pharmacodynamics in pharmacology, describes the dynamic interactions of a toxicant with a biological target and its biological effects.  A biological target, also known as the site of action, can be binding proteins, ion channels, DNA, or a variety of other receptors. When a toxicant enters an organism, it can interact with these receptors and produce structural or functional alterations. The mechanism of action of the toxicant, as determined by a toxicant’s chemical properties, will determine what receptors are targeted and the overall toxic effect at the cellular level and organismal level. Toxicants have been grouped together according to their chemical properties by way of quantitative structure-activity relationships (QSARs), which allows prediction of toxic action based on these properties.  endocrine disrupting chemicals (EDCs) and carcinogens are examples of classes of toxicants that can act as QSARs. EDCs mimic or block transcriptional activation normally caused by natural steroid hormones.  These types of chemicals can act on androgen receptors, estrogen receptors and thyroid hormone receptors.  This mechanism can include such toxicants as dichlorodiphenyltrichloroethane (DDE) and polychlorinated biphenyls (PCBs). Another class of chemicals, carcinogens, are substances that cause cancer and can be classified as genotoxic or nongenotoxic carcinogens.  These categories include toxicants such as polycyclic aromatic hydrocarbon (PAHs) and carbon tetrachloride (CCl4). The process of toxicodynamics can be useful for application in environmental risk assessment by implementing toxicokinetic-toxicodynamic (TKTD) models.  TKTD models include phenomenas such as time-varying exposure, carry-over toxicity, organism recovery time, effects of mixtures, and extrapolation to untested chemicals and species.  Due to their advantages, these types of models may be more applicable for risk assessment than traditional modeling approaches.|2023-01-19-06-37-52
Toxicodynamics|Overview| While toxicokinetics describes the changes in the concentrations of a toxicant over time due to the uptake, biotransformation, distribution and elimination of toxicants, toxicodynamics involves the interactions of a toxicant with a biological target and the functional or structural alterations in a cell that can eventually lead to a toxic effect. Depending on the toxicant’s chemical reactivity and vicinity, the toxicant may be able to interact with the biological target. Interactions between a toxicant and the biological target may also be more specific, where high-affinity binding sites increase the selectivity of interactions. For this reason, toxicity may be expressed primarily in certain tissues or organs. The targets are often receptors on the cell surface or in the cytoplasm and nucleus. Toxicants can either induce an unnecessary response or inhibit a natural response, which can cause damage. If the biological target is critical and the damage is severe enough, irreversible injury can occur first at the molecular level, which will translate into effects at higher levels of organization.|2023-01-19-06-37-52
Toxicodynamics|Endocrine disruptors| EDCs are generally considered to be toxicants that either mimic or block the transcriptional activation normally caused by natural steroid hormones.  These chemicals include those acting on androgen receptors, estrogen receptors and thyroid hormone receptors.|2023-01-19-06-37-52
Toxicodynamics|Effects of endocrine disruptors| Endocrine disrupting chemicals can interfere with the endocrine system in a number of ways including hormone synthesis, storage/release, transport and clearance, receptor recognition and binding, and postreceptor activation. In wildlife, exposure to EDCs can result in altered fertility, reduced viability of offspring, impaired hormone secretion or activity and modified reproductive anatomy.  The reproductive anatomy of offspring can particularly be affected if maternal exposure occurs.  In females, this includes mammary glands, fallopian tubes, uterus, cervix, and vagina. In males, this includes the prostate, seminal vesicles, epididymitis and testes.  Exposure of fish to EDCs has also been associated with abnormal thyroid function, decreased fertility, decreased hatching success, de-feminization and masculinization of female fish and alteration of immune function. Endocrine disruption as a mode of action for xenobiotics was brought into awareness by Our Stolen Future by Theo Colborn.  Endocrine disrupting chemicals are known to accumulate in body tissue and are highly persistent in the environment.  Many toxicants are known EDCs including pesticides, phthalates, phytoestrogens, some industrial/commercial products, and pharmaceuticals.  These chemicals are known to cause endocrine disruption via a few different mechanisms. While the mechanism associated with the thyroid hormone receptor is not well understood, two more established mechanisms involve the inhibition of the androgen receptor and activation of the estrogen receptor.|2023-01-19-06-37-52
Toxicodynamics|Androgen-receptor mediated| Certain toxicants act as endocrine disruptors by interacting with the androgen receptor. DDE is one example of a chemical that acts via this mechanism. DDE is a metabolite of DDT that is widespread in the environment.  Although production of DDT has been banned in the Western world, this chemical is extremely persistent and is still commonly found in the environment along with its metabolite DDE.  DDE is an antiandrogen, which means it alters the expression of specific androgen-regulated genes, and is an androgen receptor (AR)-mediated mechanism.  DDE is a lipophilic compound which diffuses into the cell and binds to the AR.  Through binding, the receptor is inactivated and cannot bind to the androgen response element on DNA.  This inhibits the transcription of androgen-responsive genes  which can have serious consequences for exposed wildlife. In 1980, there was a spill in Lake Apopka, Florida which released the pesticide dicofol and DDT along with its metabolites.  The neonatal and juvenile alligators present in this lake have been extensively studied and observed to have altered plasma hormone concentrations, decreased clutch viability, increased juvenile mortality, and morphological abnormalities in the testis and ovary.|2023-01-19-06-37-52
Toxicodynamics|Estrogen-receptor mediated| Toxicants may also cause endocrine disruption through interacting with the estrogen receptor. This mechanism has been well-studied with PCBs. These chemicals have been used as coolants and lubricants in transformers and other electrical equipment due to their insulating properties.  A purely anthropogenic substance, PCBs are no longer in production in the United States due to the adverse health effects associated with exposure, but they are highly persistent and are still widespread in the environment.  PCBs are a xenoestrogen, which elicit an enhancing (rather than inhibiting) response, and are mediated by the estrogen receptor.  These are often referred to as estrogen mimics because they mimic the effects of estrogen. PCBs often build up in sediments and bioaccumulate in organisms.  These chemicals diffuse into the nucleus and bind to the estrogen receptor.  The estrogen receptor is kept in an inactive conformation through interactions with proteins such as heat shock proteins 59, 70, and 90.  After the toxicant binding occurs, the estrogen receptor is activated and forms a homodimer complex which seeks out estrogen response elements in the DNA.  The binding of the complex to these elements causes a rearrangement of the chromatin and transcription of the gene, resulting in production of a specific protein.  In doing this, PCBs elicit an estrogenic response which can affect numerous functions within the organism.  These effects are observed in various aquatic species. The levels of PCBs in marine mammals are often very high as a result of bioaccumulation.  Studies have demonstrated that PCBs are responsible for reproductive impairment in the harbor seal (Phoca vitulina).  Similar effects have been found in the grey seal (Halichoerus grypus), the ringed seal (Pusa hispida) and the California sea lion (Zalophys californianus).  In the grey seals and ringed seals, uterine occlusions and stenosis were found which led to sterility.  If exposed to a xenoestrogen such as PCBs, male fish have also been seen to produce vitellogenin.  Vitellogenin is an egg protein female fish normally produce but is not usually present in males except at very low concentrations.  This is often used as a biomarker for EDCs.|2023-01-19-06-37-52
Toxicodynamics|Carcinogens| Carcinogens are defined as any substance that causes cancer. The toxicodynamics of carcinogens can be complex due to the varying mechanisms of action for different carcinogenic toxicants. Because of their complex nature, carcinogens are classified as either genotoxic or nongenotoxic carcinogens.|2023-01-19-06-37-52
Toxicodynamics|Effects of carcinogens| The effects of carcinogens are most often related to human exposures but mammals are not the only species that can be affected by cancer-causing toxicants.   Many studies have shown that cancer can develop in fish species as well.  Neoplasms occurring in epithelial tissue such as the liver, gastrointestinal tract, and the pancreas have been linked to various environmental toxicants.  Carcinogens preferentially target the liver in fish and develop hepatocellular and biliary lesions.|2023-01-19-06-37-52
Toxicodynamics|Genotoxic carcinogens| Genotoxic carcinogens interact directly with DNA and genetic material or indirectly by their reactive metabolites.  Toxicants such as PAHs can be genotoxic carcinogens to aquatic organisms.    PAHs are widely spread throughout the environment through the incomplete burning of coal, wood, or petroleum products.  Although PAHs do not bioaccumulate in vertebrate tissue, many studies have confirmed that certain PAH compounds such as benzo(a)pyrene, benz(a)anthracene, and Benzofluoranthene, are bioavailable and responsible for liver diseases like cancer in wild fish populations.  One mechanism of action for genotoxic carcinogens includes the formation of DNA adducts. Once the PAH compound enters an organism, it becomes metabolized and available for biotransformation. The biotransformation process can activate the PAH compound and transform it into a diol epoxide,[citation needed] which is a very reactive intermediate. These diol-epoxides covalently bind with DNA base pairs, most often with guanine and adenine to form stable adducts within the DNA structure.[citation needed]  The binding of diol epoxides and DNA base pairs blocks polymerase replication activity. This blockage ultimately contributes to an increase in DNA damage by reducing repair activity.[citation needed] Due to these processes, PAH compounds are thought to play a role in the initiation and early promotion stage of carcinogenesis. Fish exposed to PAHs develop a range of liver lesions, some of which are characteristic of hepatocarcinogenicity.|2023-01-19-06-37-52
Toxicodynamics|Nongenotoxic carcinogens| Nongenotoxic, or epigenetic carcinogens are different and slightly more ambiguous than genotoxic carcinogens since they are not directly carcinogenic. Nongenotoxic carcinogens act by secondary mechanisms that do not directly damage genes. This type of carcinogenesis does not change the sequence of DNA; instead it alters the expression or repression of certain genes by a wide variety of cellular processes.  Since these toxicants do not directly act on DNA, little is known about the mechanistic pathway.  It has been proposed that modification of gene expression from nongenotoxic carcinogens can occur by oxidative stress, peroxisome proliferation, suppression of apoptosis, alteration of intercellular communication, and modulation of metabolizing enzymes. Carbon tetrachloride  is an example of a probable nongenotoxic carcinogen to aquatic vertebrates. Historically, carbon tetrachloride has been used in pharmaceutical production, petroleum refining, and as an industrial solvent.  Due to its widespread industrial use and release into the environment, carbon tetrachloride has been found in drinking water and therefore, has become a concern for aquatic organisms.  Because of its high hepatotoxic properties, carbon tetrachloride could potentially be linked to liver cancer. Experimental cancer studies have shown that carbon tetrachloride may cause benign and malignant liver tumors to rainbow trout.   carbon tetrachloride works as a nongenotoxic carcinogen by formulating free radicals which induce oxidative stress.  It has been proposed that once carbon tetrachloride enters the organism, it is metabolized to trichloromethyl and trichloromethyl peroxy radicals by the CYP2E1 enzyme.   The more reactive radical, trichloromethyl peroxy, can attack polyunsaturated fatty acids in the cellular membrane to form fatty acid free radicals and initiate lipid peroxidation.  The attack on the cellular membrane increases its permeability, causing a leakage of enzymes and disrupts cellular calcium homeostasis.  This loss of calcium homeostasis activates calcium dependent degradative enzymes and cytotoxicity, causing hepatic damage.  The regenerative and proliferative changes that occur in the liver during this time could increase the frequency of genetic damage, resulting in a possible increase of cancer.|2023-01-19-06-37-52
Toxicodynamics|Applications| Toxicodynamics can be used in combination with toxicokinetics in environmental risk assessment to determine the potential effects of releasing a toxicant into the environment.  The most widely used method of incorporating this are TKTD models.|2023-01-19-06-37-52
Toxicodynamics|Setup of TKTD models| Both toxicokinetics and toxicodynamics have now been described, and using these definitions models were formed, where the internal concentration (TK) and damage (TD) are simulated in response to exposure.  TK and TD are separated in the model to allow for the identification of properties of toxicants that determine TK and those that determine TD.  To use this type of model, parameter values for TK processes need to be obtained first.  Second, the TD parameters need to be estimated.  Both of these steps require a large database of toxicity information for parameterization.  After establishing all the parameter values for the TKTD model, and using basic scientific precautions, the model can be used to predict toxic effects, calculate recovery times for organisms, or establish extrapolations from the model to toxicity of untested toxicants and species.|2023-01-19-06-37-52
Toxicodynamics|History of TKTD models| It has been argued that the current challenges facing risk assessments can be addressed with TKTD modeling.   TKTD models were derived in response to a couple of factors.  One is the lack of time being considered as a factor in toxicity and risk assessment.  Some of the earliest developed TKTD models, such as the Critical Body Residue (CBR) model and Critical Target Occupation (CTO) model, have considered time as a factor but a criticism has been that they are for very specific circumstances such as reversibly acting toxicants or irreversibly acting toxicants.  Further extrapolation of the CTO and CBR models are DEBtox, which can model sublethal endpoints, and hazard versions of the CTO, which takes into account stochastic death as opposed to individual tolerance.   Another significant step to developing TKTD models was the incorporation of a state variable for damage.  By using damage as a toxicodynamic state-variable, modeling intermediate recovery rates can be accomplished for toxicants that act reversibly with their targets, without the assumptions of instant recovery (CBR model) or irreversible interactions (CTO model). TKTD models that incorporate damage are the Damage Assessment Model (DAM) and the Threshold Damage Model (TDM).   For what may seem like straightforward endpoints, a variety of different TKTD approaches exist.  A review of the assumptions and hypotheses of each was previously published in the creation of a general unified threshold model of survival (GUTS).|2023-01-19-06-37-52
Toxicodynamics|Advantages for risk assessment| As referenced above, TKTD models have several advantages to traditional models for risk assessments.  The principal advantages to using TKTD models are: Due to its advantages, TKTD models may be more powerful than the traditional dose-response models because of their incorporation of chemical concentrations as well as temporal dimensions.   Toxicodynamic modeling (such as TKTD models) has been shown to be a useful tool for toxicological research, with increasing opportunities to use these results in risk assessment to permit a more scientifically based risk assessment that is less reliable on animal testing.   Overall, these types of models can formalize knowledge about the toxicity of toxicants and organism sensitivity, create new hypotheses, and simulate temporal aspects of toxicity, making them useful tools for risk assessment.|2023-01-19-06-37-52
Toxics use reduction|General| Toxics use reduction is an approach to pollution prevention that targets and measures reductions in the upfront use of toxic materials. Toxics use reduction emphasises the more preventive aspects of source reduction but, due to its emphasis on toxic chemical inputs, has been opposed more vigorously by chemical manufacturers. Toxics use reduction (TUR) can be subdivided into direct and indirect. Direct use focuses on substituting inputs in the production process and redesigning products to use less or no toxic chemicals. In the indirect process, there are process modifications, operation improvements, and recycling of chemicals.|2023-06-05-12-37-19
Toxics use reduction|History| In the United States, toxics use reduction programs were set up by some state legislatures during the early 1990s, including in Massachusetts, New Jersey and Oregon. Program elements may include mandatory reporting of toxic chemical use, reduction planning requirements, research and technical assistance. In the mid-1990s, the U.S. Environmental Protection Agency considered toxics use reporting or materials accounting as an expansion of the public right to know on toxic chemical use.  In 1990, Congress issued the Pollution Prevention Act of 1990 to try to reduce toxins. Though the restrictions are limited, it is still practiced in over thirty states.  The agency issued an advance notice of its proposed rule making in 1996,  though toxics use reporting was not adopted.|2023-06-05-12-37-19
Toxics use reduction|Toxics Use Reduction Act| The Massachusetts Toxics Use Reduction Act (TURA) program of 1989   requires facilities that use large amounts of toxic chemicals are required to report on their chemical use, conduct toxics use reduction planning every two years, and pay a fee. The fees paid by TURA filers support the work of the TURA implementing agencies, and are used to provide a wide variety of services to toxics users, including education, training, grant programs and technical assistance.  The  Massachusetts Department of Environmental Protection,  the Massachusetts Toxics Use Reduction Institute,  and the Massachusetts Office of Technical Assistance and Technology (OTA)  implement the goals of TURA. In Europe, attention to toxics use reduction may be seen in the European Union's new regulatory framework (adopted 2003)   for the Registration, Evaluation, Authorisation and Restriction of Chemicals (REACH).|2023-06-05-12-37-19
Toxics use reduction|Latin America and Asia| Latin America and Asia have industrialized at tremendous rates over the past decades. In doing so, governments focused on output to develop the economy while not paying attention to the environment. This has resulted in toxic accumulation in the land and in water sources. Organisations such as Clean Production Action have published resources for activists to formulate strategies to lobby governments for a less toxic producing industrial processes. In Southern China, the Pearl River delta has a high level of toxic accumulation because the area has heavy production facilities which provide 30% of China’s exports. The rivers serves as source of water supply to 47 million people yet still continues to be a dumping site.  TURA’s guideline are being used in China to account for the toxic waste in the river and to come up with solution to reduce it.   In a study conducted in Thailand, it was concluded that implementing a tax policy can greatly reduce toxicity emissions. The main concern is to ensure that the lower and middle-class families don’t get affected by these policies, thus  progressive toxic emission strategy should be enforced where taxation depends on much toxicity an individual or a family produces.  n Argentina’s Rio Chuela area, there are around 4100 factories. The toxic emissions are so high that the river is turning black. After a lawsuit from the Citizen’s Action, the court directed factories to reduce the toxic emissions by 50% over five years. To achieve this goal, TURA’s guidelines have been effective in helping the Argentinian government build a framework  for its industrial sector.|2023-06-05-12-37-19
Toxics use reduction|Institutional Initiatives| There has been a push to focus on the upstream production process to reduce toxins. The idea is to regulate emissions in the early phase of the production process rather than controlling pollution results once the toxins are released into the environment. Governments, private companies and local communities are being brought together to find ways to reduce pollution output rather than pollution control.  For example, private companies such as PaperWorks Packaging Group from Ontario, Canada have taken initiative to reduce toxic elements in the production process. The company was using toxic materials in the cleaning solutions which would escape into the atmosphere. The company decided to eliminate the Stoddard solvent from its production process to produce high quality products that are environmentally responsible. However, there has been hesitation to wholeheartedly follow these toxics use reduction programs from a business standpoint.  While some businesses found that by switching to a different chemical instead of the toxic ones, some other companies have not changed their production processes or invested in ways to reduce toxic chemical usage because they are unsure if they will make a large enough profit.  Some businesses claim that it is too costly to run a business using alternative chemicals.  Other reasons why businesses have not necessarily switched over to a different alternative chemical in lieu of the toxic one could be a lack of knowledge that an alternative exists or the fact that the alternative may not have been created yet.|2023-06-05-12-37-19
Visual pollution|General| Lists Categories Visual pollution refers to the visible deterioration and negative aesthetic quality of the natural and human-made landscapes around people  and to the study of secondary impacts of manmade interventions.  It also refers to the impacts pollution has in impairing the quality of the landscape, formed from compounding sources of pollution to create the impairment. Visual pollution disturbs the functionality and enjoyment of a given area, limiting the ability for the wider ecological system, from humans to animals, to prosper and thrive within it due to the disruptions to their natural and human-made habitats. Although visual pollution can be caused by natural sources (e.g. wildfires), the predominant cause comes from human sources. As such, visual pollution is not considered a primary source of pollution but a secondary symptom of intersecting pollution sources. Its secondary nature and subjective aspect sometimes makes it difficult to measure and engage with   (e.g. within quantitative figures for policymakers). However, the history of the word pollution, and pollution's effect over time, reveals the fact that every form of pollution can be categorised and studied in its three main characteristics, namely contextual, subjective and complex.  Frameworks for measurement have been established and include public opinion polling and surveys, visual comparison, spatial metrics, and ethnographic work. Visual pollution can manifest across levels of analysis, from micro instances that effect the individual to macro issues that impact society as a whole. Instances of visual pollution can take the form of plastic bags stuck in trees, advertisements with contrasting colors and content, which create an oversaturation of anthropogenic visual information within a landscape,   to community-wide impacts of overcrowding, overhead power lines, or congestion. Poor urban planning and irregular built-up environments contrast with natural spaces, creating alienating landscapes.   Using Pakistan as a case study, a detailed analysis of all visual pollution objects was published in 2022. The effects of visual pollution have primary symptoms, such as distraction, eye fatigue, decreases in opinion diversity, and loss of identity.  It has also been shown to increase biological stress responses and impair balance.  As a secondary source of pollution, these also compound with the impact of its primary source such as light or noise pollution that can create multi-layered public health concerns and crisis.|2023-09-27-09-05-33
Visual pollution|Sources| Local managers of urban areas sometimes lack control over what is built and assembled in public places.  As businesses look for ways to increase the profits, cleanliness, architecture, logic and use of space in urban areas are suffering from visual clutter.   Variations in the built environment are determined by the location of street furniture such as  public transport stations, garbage cans, large panels and stalls.  Insensitivity of local administration is another cause for visual pollution.  For example, poorly planned buildings and transportation systems create visual pollution.  High-rise buildings, if not planned properly or sufficiently, can bring adverse change to the visual and physical characteristics of a city, which may reduce said city's readability. A frequent criticism of advertising is that there is too much of it.  Billboards, for example, have been alleged to distract drivers, corrupt public taste, promote meaningless and wasteful consumerism and clutter the land.  See highway beautification. Vandalism, in the form of graffiti, is defined as street markings, offensive, inappropriate, and tasteless messages made without the owner's consent.  Graffiti adds to visual clutter as it disturbs the view.|2023-09-27-09-05-33
Visual pollution|Visual Pollution Assessment| The process of measuring, quantifying or assessing the level of visual pollution at any place is called a visual pollution assessment (VPA).  In past few years,[as of?] the demand for methods to assess visual pollution in communities has increased. Recently, a tool was introduced for visual pollution measurement which can be used to measure the presence of various visual pollution objects (VPOs) and the resultant level of visual pollution. A detailed analysis of visual pollution, its context, case studies and analysis using the tool is discussed in Visual Pollution: Concepts, Practices and Management Framework by Nawaz et al.|2023-09-27-09-05-33
Visual pollution|United States| In the United States, there are several initiatives gradually taking place to prevent visual pollution. The Federal Highway Beautification Act of 1965 limits placement of billboards on Interstate highways and federally aided roads. It has dramatically reduced the amount of billboards placed on these roads.  Another highway bill, the Intermodal Surface Transportation Efficiency Act (ISTEA) of 1991 has made transportation facilities sync with the needs of communities. This bill created a system of state and national scenic byways and provided funds for biking trails, historic preservation and scenic conservation. Businesses situated near an interstate can create problems of advertising through large billboards; however, now an alternative solution for advertisers is gradually eliminating the problem. For example, logo signs that provide directional information for travelers without disfiguring the landscape are increasing and are a step toward decreasing visual pollution on highways in America.|2023-09-27-09-05-33
Visual pollution|Brazil| In September 2006, São Paulo passed the Cidade Limpa (Clean City Law), outlawing the use of all outdoor advertisements, including on billboards, transit, and in front of stores.|2023-09-27-09-05-33
Volatile organic compound|General| Volatile organic compounds (VOCs) are organic compounds that have a high vapor pressure at room temperature.  High vapor pressure correlates with a low boiling point, which relates to the number of the sample's molecules in the surrounding air, a trait known as volatility. VOCs are responsible for the odor of scents and perfumes as well as pollutants. VOCs play an important role in communication between animals and plants, e.g. attractants for pollinators,  protection from predation,  and even inter-plant interactions.  Some VOCs are dangerous to human health or cause harm to the environment. Anthropogenic VOCs are regulated by law, especially indoors, where concentrations are the highest. Most VOCs are not acutely toxic, but may have long-term chronic health effects. Some VOCs have been used in pharmacy, others are target of administrative controls because of their recreational use.|2023-09-27-17-33-35
Volatile organic compound|Definitions| Diverse definitions of the term VOC are in use. Some examples are presented below.|2023-09-27-17-33-35
Volatile organic compound|Canada| Health Canada classifies VOCs as organic compounds that have boiling points roughly in the range of 50 to 250 °C (122 to 482 °F). The emphasis is placed on commonly encountered VOCs that would have an effect on air quality.|2023-09-27-17-33-35
Volatile organic compound|European Union|" The European Union defines a VOC as ""any organic compound as well as the fraction of creosote, having at 293.15 K a vapour pressure of 0,01 kPa or more, or having a corresponding volatility under the particular conditions of use;"" . The VOC Solvents Emissions Directive was the main policy instrument for the reduction of industrial emissions of volatile organic compounds (VOCs) in the European Union. It covers a wide range of solvent-using activities, e.g. printing, surface cleaning, vehicle coating, dry cleaning and manufacture of footwear and pharmaceutical products. The VOC Solvents Emissions Directive requires installations in which such activities are applied to comply either with the emission limit values set out in the Directive or with the requirements of the so-called reduction scheme. Article 13 of The Paints Directive, approved in 2004, amended the original VOC Solvents Emissions Directive and limits the use of organic solvents in decorative paints and varnishes and in vehicle finishing products. The Paints Directive sets out maximum VOC content limit values for paints and varnishes in certain applications.   The Solvents Emissions Directive was replaced by the Industrial Emissions Directive from 2013."|2023-09-27-17-33-35
Volatile organic compound|China|" The People's Republic of China defines a VOC as those compounds that have ""originated from automobiles, industrial production and civilian use, burning of all types of fuels, storage and transportation of oils, fitment finish, coating for furniture and machines, cooking oil fume and fine particles (PM 2.5)"", and similar sources.  The Three-Year Action Plan for Winning the Blue Sky Defence War released by the State Council in July 2018 creates an action plan to reduce 2015 VOC emissions 10% by 2020."|2023-09-27-17-33-35
Volatile organic compound|India|" The Central Pollution Control Board of India released the Air (Prevention and Control of Pollution) Act in 1981, amended in 1987, to address concerns about air pollution in India.  While the document does not differentiate between VOCs and other air pollutants, the CPCB monitors ""oxides of nitrogen (NOx), sulphur dioxide (SO2), fine particulate matter (PM10) and suspended particulate matter (SPM)""."|2023-09-27-17-33-35
Volatile organic compound|United States|" The definitions of VOCs used for control of precursors of photochemical smog used by the U.S. Environmental Protection Agency (EPA) and state agencies in the US with independent outdoor air pollution regulations include exemptions for VOCs that are determined to be non-reactive, or of low-reactivity in the smog formation process. Prominent is the VOC regulation issued by the South Coast Air Quality Management District in California and by the California Air Resources Board (CARB).  However, this specific use of the term VOCs can be misleading, especially when applied to indoor air quality because many chemicals that are not regulated as outdoor air pollution can still be important for indoor air pollution. Following a public hearing in September 1995, California's ARB uses the term ""reactive organic gases"" (ROG) to measure organic gases. The CARB revised the definition of ""Volatile Organic Compounds"" used in their consumer products regulations, based on the committee's findings. In addition to drinking water, VOCs are regulated in pollutant discharges to surface waters (both directly and via sewage treatment plants)  as hazardous waste,  but not in non-industrial indoor air.  The Occupational Safety and Health Administration (OSHA) regulates VOC exposure in the workplace. Volatile organic compounds that are classified as hazardous materials are regulated by the Pipeline and Hazardous Materials Safety Administration while being transported."|2023-09-27-17-33-35
Volatile organic compound|Biologically generated VOCs| Limonene, a common biogenic VOC, is emitted into the atmosphere primarily by trees which grow in coniferous forests. Most VOCs in earth's atmosphere are biogenic, largely emitted by plants. Biogenic volatile organic compounds (BVOCs) encompass VOCs emitted by plants, animals, or microorganisms, and while extremely diverse, are most commonly terpenoids, alcohols, and carbonyls (methane and carbon monoxide are generally not considered).  Not counting methane, biological sources emit an estimated 760 teragrams of carbon per year in the form of VOCs.  The majority of VOCs are produced by plants, the main compound being isoprene. Small amounts of VOCs are produced by animals and microbes.  Many VOCs are considered secondary metabolites, which often help organisms in defense, such as plant defense against herbivory. The strong odor emitted by many plants consists of green leaf volatiles, a subset of VOCs. Although intended for nearby organisms to detect and respond to, these volatiles can be detected and communicated through wireless electronic transmission, by embedding nanosensors and infrared transmitters into the plant materials themselves. Emissions are affected by a variety of factors, such as temperature, which determines rates of volatilization and growth, and sunlight, which determines rates of biosynthesis. Emission occurs almost exclusively from the leaves, the stomata in particular. VOCs emitted by terrestrial forests are often oxidized by hydroxyl radicals in the atmosphere; in the absence of NOx pollutants, VOC photochemistry recycles hydroxyl radicals to create a sustainable biosphere-atmosphere balance.  Due to recent climate change developments, such as warming and greater UV radiation, BVOC emissions from plants are generally predicted to increase, thus upsetting the biosphere-atmosphere interaction and damaging major ecosystems.  A major class of VOCs is the terpene class of compounds, such as myrcene.  Providing a sense of scale, a forest 62,000 km2 in area, the size of the US state of Pennsylvania, is estimated to emit 3,400,000 kilograms of terpenes on a typical August day during the growing season.   Researchers investigating mechanisms of induction of genes producing volatile organic compounds, and the subsequent increase in volatile terpenes, has been achieved in maize using (Z)-3-hexen-1-ol and other plant hormones.|2023-09-27-17-33-35
Volatile organic compound|Anthropogenic sources| Lists Categories Anthropogenic sources emit about 142 teragrams (1.42 × 1011 kg) of carbon per year in the form of VOCs. The major source of man-made VOCs are:|2023-09-27-17-33-35
Volatile organic compound|Indoor VOCs| Concentrations of VOCs in indoor air may be 2 to 5 times greater than in outdoor air, sometimes far greater.  During certain activities, indoor levels of VOCs may reach 1,000 times that of the outside air. Studies have shown that emissions of individual VOC species are not that high in an indoor environment, but the total concentration of all VOCs (TVOC) indoors can be up to five times higher than that of outdoor levels.  New buildings experience particularly high levels of VOC off-gassing indoors because of the abundant new materials (building materials, fittings, surface coverings and treatments such as glues, paints and sealants) exposed to the indoor air, emitting multiple VOC gases.  This off-gassing has a multi-exponential decay trend that is discernible over at least two years, with the most volatile compounds decaying with a time-constant of a few days, and the least volatile compounds decaying with a time-constant of a few years.  New buildings may require intensive ventilation for the first few months, or a bake-out strategy. Existing buildings may be replenished with new VOC sources, such as new furniture, consumer products, and redecoration of indoor surfaces, all of which lead to a continuous background emission of TVOCs, requiring improved ventilation. Numerous studies  show strong seasonal variations in indoors VOC emissions, with emission rates increasing in summer. This is largely due to the rate of diffusion of VOC species through materials to the surface, increasing with temperature. Most studies have shown that this leads to generally higher concentrations of TVOCs indoors in summer.|2023-09-27-17-33-35
Volatile organic compound|Indoor air quality measurements| Measurement of VOCs from the indoor air is done with sorption tubes e. g. Tenax (for VOCs and SVOCs) or DNPH-cartridges (for carbonyl-compounds) or air detector. The VOCs adsorb on these materials and are afterwards desorbed either thermally (Tenax) or by elution (DNPH) and then analyzed by GC-MS/FID or HPLC. Reference gas mixtures are required for quality control of these VOC-measurements.  Furthermore, VOC emitting products used indoors, e.g. building products and furniture, are investigated in emission test chambers under controlled climatic conditions.  For quality control of these measurements round robin tests are carried out, therefore reproducibly emitting reference materials are ideally required.  Other methods have used silcosteel canisters with constant flow inlets to collect samples over several days.  These methods are not limited by the adsorbing properties of materials like Tenax.|2023-09-27-17-33-35
Volatile organic compound|Regulation of indoor VOC emissions| In most countries, a separate definition of VOCs is used with regard to indoor air quality that comprises each organic chemical compound that can be measured as follows: adsorption from air on Tenax TA, thermal desorption, gas chromatographic separation over a 100% nonpolar column (dimethylpolysiloxane). VOC (volatile organic compounds) are all compounds that appear in the gas chromatogram between and including n-hexane and n-hexadecane. Compounds appearing earlier are called VVOC (very volatile organic compounds); compounds appearing later are called SVOC (semi-volatile organic compounds). France, Germany (AgBB/DIBt), Belgium, Norway (TEK regulation), and Italy (CAM Edilizia) have enacted regulations to limit VOC emissions from commercial products. European industry has developed numerous voluntary ecolabels and rating systems, such as EMICODE,  M1,  Blue Angel,  GuT (textile floor coverings),  Nordic Swan Ecolabel,  EU Ecolabel,  and Indoor Air Comfort.  In the United States, several standards exist; California Standard CDPH Section 01350  is the most common one. These regulations and standards changed the marketplace, leading to an increasing number of low-emitting products.|2023-09-27-17-33-35
Volatile organic compound|Health risks| Respiratory, allergic, or immune effects in infants or children are associated with man-made VOCs and other indoor or outdoor air pollutants. Some VOCs, such as styrene and limonene, can react with nitrogen oxides or with ozone to produce new oxidation products and secondary aerosols, which can cause sensory irritation symptoms.  VOCs contribute to the formation of tropospheric ozone and smog. Health effects include eye, nose, and throat irritation; headaches, loss of coordination, nausea; and damage to the liver, kidney, and central nervous system.  Some organics can cause cancer in animals; some are suspected or known to cause cancer in humans. Key signs or symptoms associated with exposure to VOCs include conjunctival irritation, nose and throat discomfort, headache, allergic skin reaction, dyspnea, declines in serum cholinesterase levels, nausea, vomiting, nose bleeding, fatigue, dizziness. The ability of organic chemicals to cause health effects varies greatly from those that are highly toxic to those with no known health effects. As with other pollutants, the extent and nature of the health effect will depend on many factors including level of exposure and length of time exposed. Eye and respiratory tract irritation, headaches, dizziness, visual disorders, and memory impairment are among the immediate symptoms that some people have experienced soon after exposure to some organics. At present, not much is known about what health effects occur from the levels of organics usually found in homes. While null in comparison to the concentrations found in indoor air, benzene, toluene, and methyl tert-butyl ether (MTBE) were found in samples of human milk and increase the concentrations of VOCs that we are exposed to throughout the day.  A study notes the difference between VOCs in alveolar breath and inspired air suggesting that VOCs are ingested, metabolized, and excreted via the extra-pulmonary pathway.  VOCs are also ingested by drinking water in varying concentrations. Some VOC concentrations were over the EPA’s National Primary Drinking Water Regulations and China’s National Drinking Water Standards set by the Ministry of Ecology and Environment. The presence of VOCs in the air and in groundwater has prompted more studies. Several studies have been performed to measure the effects of dermal absorption of specific VOCs. Dermal exposure to VOCs like formaldehyde and toluene downregulate antimicrobial peptides on the skin like cathelicidin LL-37, human β-defensin 2 and 3.  Xylene and formaldehyde worsen allergic inflammation in animal models.  Toluene also increases the dysregulation of filaggrin: a key protein in dermal regulation.  this was confirmed by immunofluorescence to confirm protein loss and western blotting to confirm mRNA loss. These experiments were done on human skin samples. Toluene exposure also decreased the water in the trans-epidermal layer allowing for vulnerability in the skin’s layers.|2023-09-27-17-33-35
Volatile organic compound|Limit values for VOC emissions| Limit values for VOC emissions into indoor air are published by AgBB,  AFSSET, California Department of Public Health, and others. These regulations have prompted several companies in the paint and adhesive industries to adapt with VOC level reductions their products.[citation needed] VOC labels and certification programs may not properly assess all of the VOCs emitted from the product, including some chemical compounds that may be relevant for indoor air quality.  Each ounce of colorant added to tint paint may contain between 5 and 20 grams of VOCs. A dark color, however, could require 5–15 ounces of colorant, adding up to 300 or more grams of VOCs per gallon of paint.|2023-09-27-17-33-35
Volatile organic compound|VOCs in healthcare settings| VOCs are also found in hospital and health care environments. In these settings, these chemicals are widely used for cleaning, disinfection, and hygiene of the different areas.  Thus, health professionals such as nurses, doctors, sanitation staff, etc., may present with adverse health effects such as asthma; however, further evaluation is required to determine the exact levels and determinants that influence the exposure to these compounds. Studies have shown that the concentration levels of different VOCs such as halogenated and aromatic hydrocarbons differ substantially between areas of the same hospital. However, one of these studies reported that ethanol, isopropanol, ether, and acetone were the main compounds in the interior of the site.   Following the same line, in a study conducted in the United States, it was established that nursing assistants are the most exposed to compounds such as ethanol, while medical equipment preparers are most exposed to 2-propanol. In relation to exposure to VOCs by cleaning and hygiene personnel, a study conducted in 4 hospitals in the United States established that sterilization and disinfection workers are linked to exposures to d-limonene and 2-propanol, while those responsible for cleaning with chlorine-containing products are more likely to have higher levels of exposure to α-pinene and chloroform.  Those who perform floor and other surface cleaning tasks (e.g., floor waxing) and who use quaternary ammonium, alcohol, and chlorine-based products are associated with a higher VOC exposure than the two previous groups, that is, they are particularly linked to exposure to acetone, chloroform, α-pinene, 2-propanol or d-limonene. Other healthcare environments such as nursing and age care homes have been rarely a subject of study, even though the elderly and vulnerable populations may spend considerable time in these indoor settings where they might be exposed to VOCs, derived from the common use of cleaning agents, sprays and fresheners.   In a study conducted in France, a team of researchers developed an online questionnaire for different social and age care facilities, asking about cleaning practices, products used, and the frequency of these activities. As a result, more than 200 chemicals were identified, of which 41 are known to have adverse health effects, 37 of them being VOCs. The health effects include skin sensitization, reproductive and organ-specific toxicity, carcinogenicity, mutagenicity, and endocrine-disrupting properties.  Furthermore, in another study carried out in the same European country, it was found that there is a significant association between breathlessness in the elderly population and elevated exposure to VOCs such as toluene and o-xylene, unlike the remainder of the population.|2023-09-27-17-33-35
Volatile organic compound|Sampling|" Obtaining samples for analysis is challenging.  VOCs, even when at dangerous levels, are dilute, so preconcentration is typically required.  Many components of the atmosphere are mutually incompatible, e.g. ozone and organic compounds, peroxyacyl nitrates and many organic compounds.  Furthermore, collection of VOCs by condensation in cold traps also accumulates a large amount of water, which generally must be removed selectively, depending on the analytical techniques to be employed. 
Solid-phase microextraction (SPME) techniques are used to collect VOCs at low concentrations for analysis.  As applied to breath analysis, the following modalities are employed for sampling: gas sampling bags, syringes, evacuated steel and glass containers."|2023-09-27-17-33-35
Volatile organic compound|Principle and measurement methods| In the U.S., standard methods have been established by the National Institute for Occupational Safety and Health (NIOSH) and another by U.S. OSHA. Each method uses a single component solvent; butanol and hexane cannot be sampled, however, on the same sample matrix using the NIOSH or OSHA method. VOCs are quantified and identified by two broad techniques. The major technique is gas chromatography (GC). GC instruments allow the separation of gaseous components.  When coupled to a flame ionization detector (FID) GCs can detect hydrocarbons at the parts per trillion levels.  Using electron capture detectors, GCs are also effective for organohalide such as chlorocarbons. The second major technique associated with VOC analysis is mass spectrometry, which is usually coupled with GC, giving the hyphenated technique of GC-MS. Direct injection mass spectrometry techniques are frequently utilized for the rapid detection and accurate quantification of VOCs.  PTR-MS is among the methods that have been used most extensively for the on-line analysis of biogenic and anthropogenic VOCs.  PTR-MS instruments based on time-of-flight mass spectrometry have been reported to reach detection limits of 20 pptv after 100 ms and 750 ppqv after 1 min. measurement (signal integration) time. The mass resolution of these devices is between 7000 and 10,500 m/Δm, thus it is possible to separate most common isobaric VOCs and quantify them independently.|2023-09-27-17-33-35
Volatile organic compound|Chemical fingerprinting and breath analysis|" The exhaled human breath contains a few thousand volatile organic compounds and is used in breath biopsy to serve as a VOC biomarker to test for diseases,  such as lung cancer.  One study has shown that ""volatile organic compounds ... are mainly blood borne and therefore enable monitoring of different processes in the body.""  And it appears that VOC compounds in the body ""may be either produced by metabolic processes or inhaled/absorbed from exogenous sources"" such as environmental tobacco smoke.   Chemical fingerprinting and breath analysis of volatile organic compounds has also been demonstrated with chemical sensor arrays, which utilize pattern recognition for detection of component volatile organics in complex mixtures such as breath gas."|2023-09-27-17-33-35
Volatile organic compound|Metrology for VOC measurements| To achieve comparability of VOC measurements, reference standards traceable to SI-units are required. For a number of VOCs gaseous reference standards are available from specialty gas suppliers or national metrology institutes, either in the form of cylinders or dynamic generation methods. However, for many VOCs, such as oxygenated VOCs, monoterpenes, or formaldehyde, no standards are available at the appropriate amount of fraction due to the chemical reactivity or adsorption of these molecules. Currently, several national metrology institutes are working on the lacking standard gas mixtures at trace level concentration, minimising adsorption processes, and improving the zero gas.  The final scopes are for the traceability and the long-term stability of the standard gases to be in accordance with the data quality objectives (DQO, maximum uncertainty of 20% in this case) required by the WMO/GAW program.|2023-09-27-17-33-35
Volcanic ash|General| Volcanic ash consists of fragments of rock, mineral crystals, and volcanic glass, produced during volcanic eruptions and measuring less than 2 mm (0.079 inches) in diameter.  The term volcanic ash is also often loosely used to refer to all explosive eruption products (correctly referred to as tephra), including particles larger than 2 mm. Volcanic ash is formed during explosive volcanic eruptions when dissolved gases in magma expand and escape violently into the atmosphere. The force of the gases shatters the magma and propels it into the atmosphere where it solidifies into fragments of volcanic rock and glass. Ash is also produced when magma comes into contact with water during phreatomagmatic eruptions, causing the water to explosively flash to steam leading to shattering of magma. Once in the air, ash is transported by wind up to thousands of kilometres away. Due to its wide dispersal, ash can have a number of impacts on society, including animal and human health, disruption to aviation, disruption to critical infrastructure (e.g., electric power supply systems, telecommunications, water and waste-water networks, transportation), primary industries (e.g., agriculture), buildings and structures.|2023-08-23-11-55-56
Volcanic ash|Formation| Volcanic ash is formed during explosive volcanic eruptions and phreatomagmatic eruptions,  and may also be formed during transport in pyroclastic density currents. Explosive eruptions occur when magma decompresses as it rises, allowing dissolved volatiles (dominantly water and carbon dioxide) to exsolve into gas bubbles.  As more bubbles nucleate a foam is produced, which decreases the density of the magma, accelerating it up the conduit. Fragmentation occurs when bubbles occupy ~70–80 vol% of the erupting mixture.  When fragmentation occurs, violently expanding bubbles tear the magma apart into fragments which are ejected into the atmosphere where they solidify into ash particles. Fragmentation is a very efficient process of ash formation and is capable of generating very fine ash even without the addition of water. Volcanic ash is also produced during phreatomagmatic eruptions. During these eruptions fragmentation occurs when magma comes into contact with bodies of water (such as the sea, lakes and marshes) groundwater, snow or ice. As the magma, which is significantly hotter than the boiling point of water, comes into contact with water an insulating vapor film forms (Leidenfrost effect).  Eventually this vapor film will collapse leading to direct coupling of the cold water and hot magma. This increases the heat transfer which leads to the rapid expansion of water and fragmentation of the magma into small particles which are subsequently ejected from the volcanic vent. Fragmentation causes an increase in contact area between magma and water creating a feedback mechanism,  leading to further fragmentation and production of fine ash particles. Pyroclastic density currents can also produce ash particles. These are typically produced by lava dome collapse or collapse of the eruption column.  Within pyroclastic density currents particle abrasion occurs as particles violently collide, resulting in a reduction in grain size and production of fine grained ash particles. In addition, ash can be produced during secondary fragmentation of pumice fragments, due to the conservation of heat within the flow.  These processes produce large quantities of very fine grained ash which is removed from pyroclastic density currents in co-ignimbrite ash plumes. Physical and chemical characteristics of volcanic ash are primarily controlled by the style of volcanic eruption.  Volcanoes display a range of eruption styles which are controlled by magma chemistry, crystal content, temperature and dissolved gases of the erupting magma and can be classified using the volcanic explosivity index (VEI). Effusive eruptions (VEI 1) of basaltic composition produce <105 m3 of ejecta, whereas extremely explosive eruptions (VEI 5+) of rhyolitic and dacitic composition can inject large quantities (>109 m3) of ejecta into the atmosphere.|2023-08-23-11-55-56
Volcanic ash|Chemical| The types of minerals present in volcanic ash are dependent on the chemistry of the magma from which it erupted. Considering that the most abundant elements found in silicate magma are silicon and oxygen, the various types of magma (and therefore ash) produced during volcanic eruptions are most commonly explained in terms of their silica content. Low energy eruptions of basalt produce a characteristically dark coloured ash containing ~45–55% silica that is generally rich in iron (Fe) and magnesium (Mg). The most explosive rhyolite eruptions produce a felsic ash that is high in silica (>69%) while other types of ash with an intermediate composition (e.g., andesite or dacite) have a silica content between 55 and 69%. The principal gases released during volcanic activity are water, carbon dioxide, hydrogen, sulfur dioxide, hydrogen sulfide, carbon monoxide and hydrogen chloride.  The sulfur and halogen gases and metals are removed from the atmosphere by processes of chemical reaction, dry and wet deposition, and by adsorption onto the surface of volcanic ash. It has long been recognised that a range of sulfate and halide (primarily chloride and fluoride) compounds are readily mobilised from fresh volcanic ash.   It is considered most likely that these salts are formed as a consequence of rapid acid dissolution of ash particles within eruption plumes, which is thought to supply the cations involved in the deposition of sulfate and halide salts. While some 55 ionic species have been reported in fresh ash leachates,  the most abundant species usually found are the cations Na+, K+, Ca2+ and Mg2+ and the anions Cl−, F− and SO42−.   Molar ratios between ions present in leachates suggest that in many cases these elements are present as simple salts such as NaCl and CaSO4.     In a sequential leaching experiment on ash from the 1980 eruption of Mount St. Helens, chloride salts were found to be the most readily soluble, followed by sulfate salts  Fluoride compounds are in general only sparingly soluble (e.g., CaF2, MgF2), with the exception of fluoride salts of alkali metals and compounds such as calcium hexafluorosilicate (CaSiF6).  The pH of fresh ash leachates is highly variable, depending on the presence of an acidic gas condensate (primarily as a consequence of the gases SO2, HCl and HF in the eruption plume) on the ash surface. The crystalline-solid structure of the salts act more as an insulator than a conductor.     However, once the salts are dissolved into a solution by a source of moisture (e.g., fog, mist, light rain, etc.), the ash may become corrosive and electrically conductive. A recent study has shown that the electrical conductivity of volcanic ash increases with (1) increasing moisture content, (2) increasing soluble salt content, and (3) increasing compaction (bulk density).  The ability of volcanic ash to conduct electric current has significant implications for electric power supply systems.|2023-08-23-11-55-56
Volcanic ash|Physical|" Volcanic ash particles erupted during magmatic eruptions are made up of various fractions of vitric (glassy, non-crystalline), crystalline or lithic (non-magmatic) particles. Ash produced during low viscosity magmatic eruptions (e.g., Hawaiian and Strombolian basaltic eruptions) produce a range of different pyroclasts dependent on the eruptive process. For example, ash collected from Hawaiian lava fountains consists of sideromelane (light brown basaltic glass) pyroclasts which contain microlites (small quench crystals, not to be confused with the rare mineral microlite) and phenocrysts. Slightly more viscous eruptions of basalt (e.g., Strombolian) form a variety of pyroclasts from irregular sideromelane droplets to blocky tachylite (black to dark brown microcrystalline pyroclasts). In contrast, most high-silica ash (e.g. rhyolite) consists of pulverised products of pumice (vitric shards), individual phenocrysts (crystal fraction) and some lithic fragments (xenoliths). Ash generated during phreatic eruptions primarily consists of hydrothermally altered lithic and mineral fragments, commonly in a clay matrix. Particle surfaces are often coated with aggregates of zeolite crystals or clay and only relict textures remain to identify pyroclast types. The morphology (shape) of volcanic ash is controlled by a plethora of different eruption and kinematic processes.   Eruptions of low-viscosity magmas (e.g., basalt) typically form droplet shaped particles. This droplet shape is, in part, controlled by surface tension, acceleration of the droplets after they leave the vent, and air friction. Shapes range from perfect spheres to a variety of twisted, elongate droplets with smooth, fluidal surfaces. The morphology of ash from eruptions of high-viscosity magmas (e.g., rhyolite, dacite, and some andesites) is mostly dependent on the shape of vesicles in the rising magma before disintegration. Vesicles are formed by the expansion of magmatic gas before the magma has solidified. Ash particles can have varying degrees of vesicularity and vesicular particles can have extremely high surface area to volume ratios.  Concavities, troughs, and tubes observed on grain surfaces are the result of broken vesicle walls.  Vitric ash particles from high-viscosity magma eruptions are typically angular, vesicular pumiceous fragments or thin vesicle-wall fragments while lithic fragments in volcanic ash are typically equant, or angular to subrounded. Lithic morphology in ash is generally controlled by the mechanical properties of the wall rock broken up by spalling or explosive expansion of gases in the magma as it reaches the surface. The morphology of ash particles from phreatomagmatic eruptions is controlled by stresses within the chilled magma which result in fragmentation of the glass to form small blocky or pyramidal glass ash particles.  Vesicle shape and density play only a minor role in the determination of grain shape in phreatomagmatic eruptions. In this sort of eruption, the rising magma is quickly cooled on contact with ground or surface water. Stresses within the ""quenched"" magma cause fragmentation into five dominant pyroclast shape-types: (1) blocky and equant; (2) vesicular and irregular with smooth surfaces; (3) moss-like and convoluted; (4) spherical or drop-like; and (5) plate-like. The density of individual particles varies with different eruptions. The density of volcanic ash varies between 700 and 1200 kg/m3 for pumice, 2350–2450 kg/m3 for glass shards, 2700–3300 kg/m3 for crystals, and 2600–3200 kg/m3 for lithic particles.  Since coarser and denser particles are deposited close to source, fine glass and pumice shards are relatively enriched in ash fall deposits at distal locations.  The high density and hardness (~5 on the Mohs Hardness Scale) together with a high degree of angularity, make some types of volcanic ash (particularly those with a high silica content) very abrasive. Volcanic ash consists of particles (pyroclasts) with diameters <2 mm (particles >2 mm are classified as lapilli),  and can be as fine as 1 μm.  The overall grain size distribution of ash can vary greatly with different magma compositions. Few attempts have been made to correlate the grain size characteristics of a deposit with those of the event which produced it, though some predictions can be made. Rhyolitic magmas generally produce finer grained material compared to basaltic magmas, due to the higher viscosity and therefore explosivity. The proportions of fine ash are higher for silicic explosive eruptions, probably because vesicle size in the pre-eruptive magma is smaller than those in mafic magmas.  There is good evidence that pyroclastic flows produce high proportions of fine ash by communition and it is likely that this process also occurs inside volcanic conduits and would be most efficient when the magma fragmentation surface is well below the summit crater."|2023-08-23-11-55-56
Volcanic ash|Dispersal| Ash particles are incorporated into eruption columns as they are ejected from the vent at high velocity. The initial momentum from the eruption propels the column upwards. As air is drawn into the column, the bulk density decreases and it starts to rise buoyantly into the atmosphere.  At a point where the bulk density of the column is the same as the surrounding atmosphere, the column will cease rising and start moving laterally. Lateral dispersion is controlled by prevailing winds and the ash may be deposited hundreds to thousands of kilometres from the volcano, depending on eruption column height, particle size of the ash and climatic conditions (especially wind direction and strength and humidity). Ash fallout occurs immediately after the eruption and is controlled by particle density. Initially, coarse particles fall out close to source. This is followed by fallout of accretionary lapilli, which is the result of particle agglomeration within the column.  Ash fallout is less concentrated during the final stages as the column moves downwind. This results in an ash fall deposit which generally decreases in thickness and grain size exponentially with increasing distance from the volcano.  Fine ash particles may remain in the atmosphere for days to weeks and be dispersed by high-altitude winds. These particles can impact on the aviation industry (refer to impacts section) and, combined with gas particles, can affect global climate. Volcanic ash plumes can form above pyroclastic density currents. These are called co-ignimbrite plumes. As pyroclastic density currents travel away from the volcano, smaller particles are removed from the flow by elutriation and form a less dense zone overlying the main flow. This zone then entrains the surrounding air and a buoyant co-ignimbrite plume is formed. These plumes tend to have higher concentrations of fine ash particles compared to magmatic eruption plumes due to the abrasion within the pyroclastic density current.|2023-08-23-11-55-56
Volcanic ash|Impacts| Lists Categories Population growth has caused the progressive encroachment of urban development into higher risk areas, closer to volcanic centres, increasing the human exposure to volcanic ash fall events. Direct health effects of volcanic ash on humans are usually short-term and mild for persons in normal health, though prolonged exposure potentially poses some risk of silicosis in unprotected workers.  Of greater concern is the impact of volcanic ash on the infrastructure critical to supporting modern societies, particularly in urban areas, where high population densities create high demand for services.   Several recent eruptions have illustrated the vulnerability of urban areas that received only a few millimetres or centimetres of volcanic ash.      This has been sufficient to cause disruption of transportation,  electricity,  water,   sewage and storm water systems.  Costs have been incurred from business disruption, replacement of damaged parts and insured losses. Ash fall impacts on critical infrastructure can also cause multiple knock-on effects, which may disrupt many different sectors and services. Volcanic ash fall is physically, socially, and economically disruptive.  Volcanic ash can affect both proximal areas and areas many hundreds of kilometres from the source,  and causes disruptions and losses in a wide variety of different infrastructure sectors. Impacts are dependent on: ash fall thickness; the grain size and chemistry of the ash; whether the ash is wet or dry; the duration of the ash fall; and any preparedness, management and prevention (mitigation) measures employed to reduce effects from the ash fall. Different sectors of infrastructure and society are affected in different ways and are vulnerable to a range of impacts or consequences. These are discussed in the following sections.|2023-08-23-11-55-56
Volcanic ash|Human and animal health| Ash particles of less than 10 µm diameter suspended in the air are known to be inhalable, and people exposed to ash falls have experienced respiratory discomfort, breathing difficulty, eye and skin irritation, and nose and throat symptoms.  Most of these effects are short-term and are not considered to pose a significant health risk to those without pre-existing respiratory conditions.  The health effects of volcanic ash depend on the grain size, mineralogical composition and chemical coatings on the surface of the ash particles.  Additional factors related to potential respiratory symptoms are the frequency and duration of exposure, the concentration of ash in the air and the respirable ash fraction; the proportion of ash with less than 10 µm diameter, known as PM10. The social context may also be important. Chronic health effects from volcanic ash fall are possible, as exposure to free crystalline silica is known to cause silicosis. Minerals associated with this include quartz, cristobalite and tridymite, which may all be present in volcanic ash. These minerals are described as ‘free’ silica as the SiO2 is not attached to another element to create a new mineral. However, magmas containing less than 58% SiO2 are thought to be unlikely to contain crystalline silica. The exposure levels to free crystalline silica in the ash are commonly used to characterise the risk of silicosis in occupational studies (for people who work in mining, construction and other industries,) because it is classified as a human carcinogen by the International Agency for Research on Cancer. Guideline values have been created for exposure, but with unclear rationale; UK guidelines for particulates in air (PM10) are 50 µg/m3 and USA guidelines for exposure to crystalline silica are 50 µg/m3.  It is thought that the guidelines on exposure levels could be exceeded for short periods of time without significant health effects on the general population. There have been no documented cases of silicosis developed from exposure to volcanic ash. However, long-term studies necessary to evaluate these effects are lacking. For surface water sources such as lakes and reservoirs, the volume available for dilution of ionic species leached from ash is generally large. The most abundant components of ash leachates (Ca, Na, Mg, K, Cl, F and SO4) occur naturally at significant concentrations in most surface waters and therefore are not affected greatly by inputs from volcanic ashfall, and are also of low concern in drinking water, with the exception of fluorine. The elements iron, manganese and aluminium are commonly enriched over background levels by volcanic ashfall. These elements may impart a metallic taste to water, and may produce red, brown or black staining of whiteware, but are not considered a health risk. Volcanic ashfalls are not known to have caused problems in water supplies for toxic trace elements such as mercury (Hg) and lead (Pb) which occur at very low levels in ash leachates. Ingesting ash may be harmful to livestock, causing abrasion of the teeth, and in cases of high fluorine content, fluorine poisoning (toxic at levels of >100 µg/g) for grazing animals.  It is known from the 1783 eruption of Laki in Iceland that fluorine poisoning occurred in humans and livestock as a result of the chemistry of the ash and gas, which contained high levels of hydrogen fluoride. Following the 1995/96 Mount Ruapehu eruptions in New Zealand, two thousand ewes and lambs died after being affected by fluorosis while grazing on land with only 1–3 mm of ash fall.  Symptoms of fluorosis among cattle exposed to ash include brown-yellow to green-black mottles in the teeth, and hypersensibility to pressure in the legs and back.  Ash ingestion may also cause gastrointestinal blockages.  Sheep that ingested ash from the 1991 Mount Hudson volcanic eruption in Chile, suffered from diarrhoea and weakness. Ash accumulating in the back wool of sheep may add significant weight, leading to fatigue and sheep that can not stand up. Rainfall may result in a significant burden as it adds weight to ash.  Pieces of wool may fall away and any remaining wool on sheep may be worthless as poor nutrition associated with volcanic eruptions impacts the quality of the fibre.  As the usual pastures and plants become covered in volcanic ash during eruption some livestock may resort to eat whatever is available including toxic plants.  There are reports of goats and sheep in Chile and Argentina having natural abortions in connection to volcanic eruptions.|2023-08-23-11-55-56
Volcanic ash|Infrastructure| Volcanic ash can disrupt electric power supply systems at all levels of power generation, transformation, transmission, and distribution. There are four main impacts arising from ash-contamination of apparatus used in the power delivery process: Groundwater-fed systems are resilient to impacts from ashfall, although airborne ash can interfere with the operation of well-head pumps. Electricity outages caused by ashfall can also disrupt electrically powered pumps if there is no backup generation. The physical impacts of ashfall can affect the operation of water treatment plants. Ash can block intake structures, cause severe abrasion damage to pump impellers and overload pump motors.  Ash can enter filtration systems such as open sand filters both by direct fallout and via intake waters. In most cases, increased maintenance will be required to manage the effects of an ashfall, but there will not be service interruptions. The final step of drinking water treatment is disinfection to ensure that final drinking water is free from infectious microorganisms. As suspended particles (turbidity) can provide a growth substrate for microorganisms and can protect them from disinfection treatment, it is extremely important that the water treatment process achieves a good level of removal of suspended particles. Chlorination may have to be increased to ensure adequate disinfection. Many households, and some small communities, rely on rainwater for their drinking water supplies. Roof-fed systems are highly vulnerable to contamination by ashfall, as they have a large surface area relative to the storage tank volume. In these cases, leaching of chemical contaminants from the ashfall can become a health risk and drinking of water is not recommended. Prior to an ashfall, downpipes should be disconnected so that water in the tank is protected. A further problem is that the surface coating of fresh volcanic ash can be acidic. Unlike most surface waters, rainwater generally has a very low alkalinity (acid-neutralising capacity) and thus ashfall may acidify tank waters. This may lead to problems with plumbosolvency, whereby the water is more aggressive towards materials that it comes into contact with. This can be a particular problem if there are lead-head nails or lead flashing used on the roof, and for copper pipes and other metallic plumbing fittings. During ashfall events, large demands are commonly placed on water resources for cleanup and shortages can result. Shortages compromise key services such as firefighting and can lead to a lack of water for hygiene, sanitation and drinking. Municipal authorities need to monitor and manage this water demand carefully, and may need to advise the public to utilise cleanup methods that do not use water (e.g., cleaning with brooms rather than hoses). Wastewater networks may sustain damage similar to water supply networks. It is very difficult to exclude ash from the sewerage system. Systems with combined storm water/sewer lines are most at risk. Ash will enter sewer lines where there is inflow/infiltration by stormwater through illegal connections (e.g., from roof downpipes), cross connections, around manhole covers or through holes and cracks in sewer pipes. Ash-laden sewage entering a treatment plant is likely to cause failure of mechanical prescreening equipment such as step screens or rotating screens. Ash that penetrates further into the system will settle and reduce the capacity of biological reactors as well as increasing the volume of sludge and changing its composition. The principal damage sustained by aircraft flying into a volcanic ash cloud is abrasion to forward-facing surfaces, such as the windshield and leading edges of the wings, and accumulation of ash into surface openings, including engines.  Abrasion of windshields and landing lights will reduce visibility forcing pilots to rely on their instruments. However, some instruments may provide incorrect readings as sensors (e.g., pitot tubes) can become blocked with ash. Ingestion of ash into engines causes abrasion damage to compressor fan blades. The ash erodes sharp blades in the compressor, reducing its efficiency. The ash melts in the combustion chamber to form molten glass. The ash then solidifies on turbine blades, blocking air flow and causing the engine to stall. The composition of most ash is such that its melting temperature is within the operating temperature (>1000 °C) of modern large jet engines.  The degree of impact depends upon the concentration of ash in the plume, the length of time the aircraft spends within the plume and the actions taken by the pilots. Critically, melting of ash, particularly volcanic glass, can result in accumulation of resolidified ash on turbine nozzle guide vanes, resulting in compressor stall and complete loss of engine thrust.  The standard procedure of the engine control system when it detects a possible stall is to increase power which would exacerbate the problem. It is recommended that pilots reduce engine power and quickly exit the cloud by performing a descending 180° turn.  Volcanic gases, which are present within ash clouds, can also cause damage to engines and acrylic windshields, and can persist in the stratosphere as an almost invisible aerosol for prolonged periods of time. There are many instances of damage to jet aircraft as a result of an ash encounter. On 24 June 1982, a British Airways Boeing 747-236B (Flight 9) flew through the ash cloud from the eruption of Mount Galunggung, Indonesia resulting in the failure of all four engines. The plane descended 24,000 feet (7,300 m) in 16 minutes before the engines restarted, allowing the aircraft to make an emergency landing. On 15 December 1989, a KLM Boeing 747-400 (Flight 867) also lost power to all four engines after flying into an ash cloud from Mount Redoubt, Alaska. After dropping 14,700 feet (4,500 m) in four minutes, the engines were started just 1–2 minutes before impact. Total damage was US$80 million and it took 3 months' work to repair the plane.  In the 1990s, a further US$100 million of damage was sustained by commercial aircraft (some in the air, others on the ground) as a consequence of the 1991 eruption of Mount Pinatubo in the Philippines. In April 2010, airspace all over Europe was affected, with many flights cancelled-which was unprecedented-due to the presence of volcanic ash in the upper atmosphere from the eruption of the Icelandic volcano Eyjafjallajökull.  On 15 April 2010, the Finnish Air Force halted training flights when damage was found from volcanic dust ingestion by the engines of one of its Boeing F-18 Hornet fighters.  On 22 April 2010, UK RAF Typhoon training flights were also temporarily suspended after deposits of volcanic ash were found in a jet's engines.  In June 2011, there were similar closures of airspace in Chile, Argentina, Brazil, Australia and New Zealand, following the eruption of Puyehue-Cordón Caulle, Chile. Volcanic ash clouds are very difficult to detect from aircraft as no onboard cockpit instruments exist to detect them. However, a new system called Airborne Volcanic Object Infrared Detector (AVOID) has recently been developed by Dr Fred Prata  while working at CSIRO Australia  and the Norwegian Institute for Air Research, which will allow pilots to detect ash plumes up to 60 km (37 mi) ahead and fly safely around them.  The system uses two fast-sampling infrared cameras, mounted on a forward-facing surface, that are tuned to detect volcanic ash. This system can detect ash concentrations of <1 mg/m3 to > 50 mg/m3, giving pilots approximately 7–10 minutes warning.  The camera was tested   by the easyJet airline company,  AIRBUS and Nicarnica Aviation (co-founded by Dr Fred Prata).  The results showed the system could work to distances of ~60 km and up to 10,000 ft   but not any higher without some significant modifications. In addition, ground and satellite based imagery, radar, and lidar can be used to detect ash clouds. This information is passed between meteorological agencies, volcanic observatories and airline companies through Volcanic Ash Advisory Centers (VAAC). There is one VAAC for each of the nine regions of the world. VAACs can issue advisories describing the current and future extent of the ash cloud. Volcanic ash not only affects in-flight operations but can affect ground-based airport operations as well. Small accumulations of ash can reduce visibility, produce slippery runways and taxiways, infiltrate communication and electrical systems, interrupt ground services, damage buildings and parked aircraft.  Ash accumulation of more than a few millimeters requires removal before airports can resume full operations. Ash does not disappear (unlike snowfalls) and must be disposed of in a manner that prevents it from being remobilised by wind and aircraft. Ash may disrupt transportation systems over large areas for hours to days, including roads and vehicles, railways and ports and shipping. Falling ash will reduce the visibility which can make driving difficult and dangerous.  In addition, fast travelling cars will stir up ash, generating billowing clouds which perpetuate ongoing visibility hazards. Ash accumulations will decrease traction, especially when wet, and cover road markings.  Fine-grained ash can infiltrate openings in cars and abrade most surfaces, especially between moving parts. Air and oil filters will become blocked requiring frequent replacement. Rail transport is less vulnerable, with disruptions mainly caused by reduction in visibility. Marine transport can also be impacted by volcanic ash. Ash fall will block air and oil filters and abrade any moving parts if ingested into engines. Navigation will be impacted by a reduction in visibility during ash fall. Vesiculated ash (pumice and scoria) will float on the water surface in ‘pumice rafts’ which can clog water intakes quickly, leading to over heating of machinery. Telecommunication and broadcast networks can be affected by volcanic ash in the following ways: attenuation and reduction of signal strength; damage to equipment; and overloading of network through user demand. Signal attenuation due to volcanic ash is not well documented; however, there have been reports of disrupted communications following the 1969 Surtsey eruption and 1991 Mount Pinatubo eruption. Research by the New Zealand-based Auckland Engineering Lifelines Group determined theoretically that impacts on telecommunications signals from ash would be limited to low frequency services such as satellite communication.  Signal interference may also be caused by lightning, as this is frequently generated within volcanic eruption plumes. Telecommunication equipment may become damaged due to direct ash fall. Most modern equipment requires constant cooling from air conditioning units. These are susceptible to blockage by ash which reduces their cooling efficiency.  Heavy ash falls may cause telecommunication lines, masts, cables, aerials, antennae dishes and towers to collapse due to ash loading.  Moist ash may also cause accelerated corrosion of metal components. Reports from recent eruptions suggest that the largest disruption to communication networks is overloading due to high user demand.  This is common of many natural disasters. Computers may be impacted by volcanic ash, with their functionality and usability decreasing during ashfall, but it is unlikely they will completely fail.  The most vulnerable components are the mechanical components, such as cooling fans, cd drives, keyboard, mice and touch pads. These components can become jammed with fine grained ash causing them to cease working; however, most can be restored to working order by cleaning with compressed air. Moist ash may cause electrical short circuits within desktop computers; however, will not affect laptop computers. Damage to buildings and structures can range from complete or partial roof collapse to less catastrophic damage of exterior and internal materials. Impacts depend on the thickness of ash, whether it is wet or dry, the roof and building design and how much ash gets inside a building. The specific weight of ash can vary significantly and rain can increase this by 50–100%.  Problems associated with ash loading are similar to that of snow; however, ash is more severe as 1) the load from ash is generally much greater, 2) ash does not melt and 3) ash can clog and damage gutters, especially after rain fall. Impacts for ash loading depend on building design and construction, including roof slope, construction materials, roof span and support system, and age and maintenance of the building.  Generally flat roofs are more susceptible to damage and collapse than steeply pitched roofs. Roofs made of smooth materials (sheet metal or glass) are more likely to shed ash than roofs made with rough materials (thatch, asphalt or wood shingles). Roof collapse can lead to widespread injuries and deaths and property damage. For example, the collapse of roofs from ash during the 15 June 1991 Mount Pinatubo eruption killed about 300 people.|2023-08-23-11-55-56
Volcanic ash|Environment and agriculture| Volcanic ash can have a detrimental impact on the environment which can be difficult to predict due to the large variety of environmental conditions that exist within the ash fall zone. Natural waterways can be impacted in the same way as urban water supply networks. Ash will increase water turbidity which can reduce the amount of light reaching lower depths, which can inhibit growth of submerged aquatic plants and consequently affect species which are dependent on them such as fish and shellfish.  High turbidity can also affect the ability of fish gills to absorb dissolved oxygen.  Acidification will also occur, which will reduce the pH of the water and impact the fauna and flora living in the environment. Fluoride contamination will occur if the ash contains high concentrations of fluoride. Ash accumulation will also affect pasture, plants and trees which are part of the horticulture and agriculture industries. Thin ash falls (<20 mm) may put livestock off eating, and can inhibit transpiration and photosynthesis and alter growth. There may be an increase in pasture production due to a mulching effect and slight fertilizing effect, such as occurred following the 1980 Mount St. Helens and 1995/96 Mt Ruapehu eruptions.   Heavier falls will completely bury pastures and soil leading to death of pasture and sterilization of the soil due to oxygen deprivation. Plant survival is dependent on ash thickness, ash chemistry, compaction of ash, amount of rainfall, duration of burial and the length of plant stalks at the time of ash fall. Young forests (trees <2 years old) are most at risk from ash falls and are likely to be destroyed by ash deposits >100 mm.  Ash fall is unlikely to kill mature trees, but ash loading may break large branches during heavy ash falls (>500 mm). Defoliation of trees may also occur, especially if there is a coarse ash component within the ash fall. Land rehabilitation after ash fall may be possible depending on the ash deposit thickness. Rehabilitation treatment may include: direct seeding of deposit; mixing of deposit with buried soil; scraping of ash deposit from land surface; and application of new topsoil over the ash deposit.|2023-08-23-11-55-56
Volcanic ash|Interdependence| Critical infrastructure and infrastructure services are vital to the functionality of modern society, to provide: medical care, policing, emergency services, and lifelines such as water, wastewater, and power and transportation links. Often critical facilities themselves are dependent on such lifelines for operability, which makes them vulnerable to both direct impacts from a hazard event and indirect effects from lifeline disruption. The impacts on lifelines may also be inter-dependent. The vulnerability of each lifeline may depend on: the type of hazard, the spatial density of its critical linkages, the dependency on critical linkages, susceptibility to damage and speed of service restoration, state of repair or age, and institutional characteristics or ownership. The 2010 eruption of Eyjafjallajokull in Iceland highlighted the impacts of volcanic ash fall in modern society and our dependence on the functionality of infrastructure services. During this event, the airline industry suffered business interruption losses of €1.5–2.5 billion from the closure of European airspace for six days in April 2010 and subsequent closures into May 2010.  Ash fall from this event is also known to have caused local crop losses in agricultural industries, losses in the tourism industry, destruction of roads and bridges in Iceland (in combination with glacial melt water), and costs associated with emergency response and clean-up. However, across Europe there were further losses associated with travel disruption, the insurance industry, the postal service, and imports and exports across Europe and worldwide. These consequences demonstrate the interdependency and diversity of impacts from a single event.|2023-08-23-11-55-56
Volcanic ash|Preparedness, mitigation and management| Preparedness for ashfalls should involve sealing buildings, protecting infrastructure and homes, and storing sufficient supplies of food and water to last until the ash fall is over and clean-up can begin. Dust masks can be worn to reduce inhalation of ash and mitigate against any respiratory health affects.  Goggles can be worn to protect against eye irritation. At home, staying informed about volcanic activity, and having contingency plans in place for alternative shelter locations, constitutes good preparedness for an ash fall event. This can prevent some impacts associated with ash fall, reduce the effects, and increase the human capacity to cope with such events. A few items such as a flashlight, plastic sheeting to protect electronic equipment from ash ingress, and battery operated radios, are extremely useful during ash fall events. Communication plans should be made beforehand to inform of mitigation actions being undertaken. Spare parts and back-up systems should be in place prior to ash fall events to reduce service disruption and return functionality as quickly as possible. Good preparedness also includes the identification of ash disposal sites, before ash fall occurs, to avoid further movement of ash and to aid clean-up. Some effective techniques for the management of ash have been developed including cleaning methods and cleaning apparatus, and actions to mitigate or limit damage. The latter include covering of openings such as: air and water intakes, aircraft engines and windows during ash fall events. Roads may be closed to allow clean-up of ash falls, or speed restrictions may be put in place, in order to prevent motorists from developing motor problems and becoming stranded following an ash fall.  To prevent further effects on underground water systems or waste water networks, drains and culverts should be unblocked and ash prevented from entering the system.  Ash can be moistened (but not saturated) by sprinkling with water, to prevent remobilisation of ash and to aid clean-up.  Prioritisation of clean-up operations for critical facilities and coordination of clean-up efforts also constitute good management practice. It is recommended to evacuate livestock in areas where ashfall may reach 5 cm or more.|2023-08-23-11-55-56
Volcanic ash|Volcanic ash soils| Volcanic ash's primary use is that of a soil enricher. Once the minerals in ash are washed into the soil by rain or other natural processes, it mixes with the soil and forms an andisol layer. This layer is highly rich in nutrients and is very good for agricultural use; the presence of lush forests on volcanic islands is often as a result of trees growing and flourishing in the phosphorus and nitrogen-rich andisol.  Volcanic ash can also be used as a replacement for sand.|2023-08-23-11-55-56
Volume source (pollution)|General| A volume source of pollution is a three-dimensional source of pollutant emissions. Essentially, it is an area source with a third dimension. Examples of a volume source of pollution are:|2023-07-29-16-49-34
War sand|General| War sand is sand contaminated by remains of projectiles used in war. This kind of sand has been found in Normandy, since the invasion of Normandy, among other places.  In 1988, the sand on Omaha Beach was discovered to contain man-made metal and glass particles deriving from shrapnel; 4% of the sand in the sample was composed of shrapnel particles ranging in size between 0.06 millimetres (0.0024 in) and 1 mm (0.039 in).  Researchers also discovered trace amounts of iron and glass beads in the sand, originating from the intense heat unleashed by munitions explosions in the air and sand.|2023-08-05-23-47-54
War sand|Composition identification|" In 2013, Dr. Earle McBride, a researcher studying sandstone diagenesis and the textual and compositional maturation of sand during transportation,  mixed samples collected from Omaha Beach in 1988 with a blue epoxy, creating an ""artificial sandstone"", before slicing it into thin sections.  Utilising an optical microscope and an external light source, shiny, opaque grains could be identified. Although wave action had elicited rounding on the edges of some coarser grains, the shard-like angularity and corrosion of both coarse and fine grains suggested these grains were man-made.   It is believed that the roughness of said grains was imparted by microporous surfaces produced during production and corrosion products post-explosion. This inspection, alongside tests revealing that the grains were magnetic, led McBride to conclude these grains were pieces of shrapnel."|2023-08-05-23-47-54
War sand|References| This environment-related article is a stub. You can help Wikipedia by expanding it. This military-related article is a stub. You can help Wikipedia by expanding it.|2023-08-05-23-47-54
Wildfire|General|" A wildfire, forest fire, bushfire, wildland fire or rural fire is an unplanned, uncontrolled and unpredictable fire in an area of combustible vegetation.   Depending on the type of vegetation present, a wildfire may be more specifically identified as a bushfire (in Australia), desert fire, grass fire, hill fire, peat fire, prairie fire, vegetation fire, or veld fire.  Some natural forest ecosystems depend on wildfire.  Wildfires are distinct from beneficial human usage of wildland fire, called controlled or prescribed burning, although controlled burns can turn into wildfires. Modern forest management often engages in prescribed burns to mitigate risk and promote natural forest cycles. Wildfires are often classified by characteristics like cause of ignition, physical properties, combustible material present, and the effect of weather on the fire.  Wildfire behavior and severity result from a combination of factors such as available fuels, physical setting, and weather.     Climatic cycles with wet periods that create substantial fuels, followed by drought and heat, often proceed severe wildfires.  These cycles have been intensified by climate change. Naturally occurring wildfires  may have beneficial effects on native vegetation, animals, and ecosystems that have evolved with fire.   Many plant species depend on the effects of fire for growth and reproduction.  Some natural forests are dependent on wildfire.  High-severity wildfires may create complex early seral forest habitat (also called ""snag forest habitat""), which may have higher species richness and diversity than an unburned old forest. Human societies can be severely impacted by fires. Effects include the direct health impacts of smoke and fire, destruction of property (especially in wildland–urban interfaces) economic and ecosystem services losses, and contamination of water and soil. Wildfires are among the most common forms of natural disaster in some regions, including Siberia, California, British Columbia, and Australia.     Areas with Mediterranean climates or in the taiga biome are particularly susceptible. At a global level, human practices have made the impacts of wildfire worse, with a doubling in land area burned by wildfires compared to natural levels.  Humans have impacted wildfire through climate change, land-use change, and wildfire suppression.  The increase in severity of fires in the US  creates a positive feedback loop by releasing naturally sequestered carbon back into the atmosphere, increasing the atmosphere's greenhouse effect thereby contributing to climate change."|2023-09-25-02-19-54
Wildfire|Ignition| The initial ignition of a fire is usually evaluated for natural or human causes.|2023-09-25-02-19-54
Wildfire|Natural| The following actions can ignite wildfires naturally (without the involvement of humans):   lightning, volcanic eruption, sparks from rock falls, spontaneous combustion.|2023-09-25-02-19-54
Wildfire|Human activity|" Sources of human-caused fire may include arson, accidental ignition, or the uncontrolled use of fire in land-clearing and agriculture such as the slash-and-burn farming in Southeast Asia.  In the tropics, farmers often practice the slash-and-burn method of clearing fields during the dry season. In middle latitudes, the most common human causes of wildfires are equipment generating sparks (chainsaws, grinders, mowers, etc.), overhead power lines, and arson.      However, in the 2019–20 Australian bushfire season ""an independent study found online bots and trolls exaggerating the role of arson in the fires.""  In the 2023 Canadian wildfires false claims of arson gained traction on social media; however, arson is generally not a main cause of wildfires in Canada.   In California, generally 6–10% of wildfires annually are arson.   Arson may account for over 20% of human caused fires. Coal seam fires burn in the thousands around the world, such as those in Burning Mountain, New South Wales; Centralia, Pennsylvania; and several coal-sustained fires in China. They can also flare up unexpectedly and ignite nearby flammable material."|2023-09-25-02-19-54
Wildfire|Spread| The spread of wildfires varies based on the flammable material present, its vertical arrangement and moisture content, and weather conditions.  Fuel arrangement and density is governed in part by topography, as land shape determines factors such as available sunlight and water for plant growth. Overall, fire types can be generally characterized by their fuels as follows:|2023-09-25-02-19-54
Wildfire|Physical properties| Wildfires occur when all the necessary elements of a fire triangle come together in a susceptible area: an ignition source is brought into contact with a combustible material such as vegetation that is subjected to enough heat and has an adequate supply of oxygen from the ambient air. A high moisture content usually prevents ignition and slows propagation, because higher temperatures are needed to evaporate any water in the material and heat the material to its fire point. Dense forests usually provide more shade, resulting in lower ambient temperatures and greater humidity, and are therefore less susceptible to wildfires.  Less dense material such as grasses and leaves are easier to ignite because they contain less water than denser material such as branches and trunks.  Plants continuously lose water by evapotranspiration, but water loss is usually balanced by water absorbed from the soil, humidity, or rain.  When this balance is not maintained, plants dry out and are therefore more flammable, often as a consequence of droughts. A wildfire front is the portion sustaining continuous flaming combustion, where unburned material meets active flames, or the smoldering transition between unburned and burned material.  As the front approaches, the fire heats both the surrounding air and woody material through convection and thermal radiation. First, wood is dried as water is vaporized at a temperature of 100 °C (212 °F). Next, the pyrolysis of wood at 230 °C (450 °F) releases flammable gases. Finally, wood can smolder at 380 °C (720 °F) or, when heated sufficiently, ignite at 590 °C (1,000 °F).   Even before the flames of a wildfire arrive at a particular location, heat transfer from the wildfire front warms the air to 800 °C (1,470 °F), which pre-heats and dries flammable materials, causing materials to ignite faster and allowing the fire to spread faster.   High-temperature and long-duration surface wildfires may encourage flashover or torching: the drying of tree canopies and their subsequent ignition from below. Wildfires have a rapid forward rate of spread (FROS) when burning through dense uninterrupted fuels.  They can move as fast as 10.8 kilometres per hour (6.7 mph) in forests and 22 kilometres per hour (14 mph) in grasslands.  Wildfires can advance tangential to the main front to form a flanking front, or burn in the opposite direction of the main front by backing.  They may also spread by jumping or spotting as winds and vertical convection columns carry firebrands (hot wood embers) and other burning materials through the air over roads, rivers, and other barriers that may otherwise act as firebreaks.   Torching and fires in tree canopies encourage spotting, and dry ground fuels around a wildfire are especially vulnerable to ignition from firebrands.  Spotting can create spot fires as hot embers and firebrands ignite fuels downwind from the fire. In Australian bushfires, spot fires are known to occur as far as 20 kilometres (12 mi) from the fire front. Especially large wildfires may affect air currents in their immediate vicinities by the stack effect: air rises as it is heated, and large wildfires create powerful updrafts that will draw in new, cooler air from surrounding areas in thermal columns.  Great vertical differences in temperature and humidity encourage pyrocumulus clouds, strong winds, and fire whirls with the force of tornadoes at speeds of more than 80 kilometres per hour (50 mph).    Rapid rates of spread, prolific crowning or spotting, the presence of fire whirls, and strong convection columns signify extreme conditions.|2023-09-25-02-19-54
Wildfire|Intensity variations during day and night| Intensity also increases during daytime hours. Burn rates of smoldering logs are up to five times greater during the day due to lower humidity, increased temperatures, and increased wind speeds.  Sunlight warms the ground during the day which creates air currents that travel uphill. At night the land cools, creating air currents that travel downhill. Wildfires are fanned by these winds and often follow the air currents over hills and through valleys.  Fires in Europe occur frequently during the hours of 12:00 p.m. and 2:00 p.m.  Wildfire suppression operations in the United States revolve around a 24-hour fire day that begins at 10:00 a.m. due to the predictable increase in intensity resulting from the daytime warmth.|2023-09-25-02-19-54
Wildfire|Increasing risks due to heat waves and droughts|" Climate variability including heat waves, droughts, and El Niño, and regional weather patterns, such as high-pressure ridges, can increase the risk and alter the behavior of wildfires dramatically.    Years of high precipitation can produce rapid vegetation growth, which when followed by warmer periods can encourage more widespread fires and longer fire seasons.  High temperatures dry out the fuel loads and make them more flammable, increasing tree mortality and posing significant risks to global forest health.    Since the mid-1980s, in the Western US, earlier snowmelt and associated warming has also been associated with an increase in length and severity of the wildfire season, or the most fire-prone time of the year.  A 2019 study indicates that the increase in fire risk in California may be partially attributable to human-induced climate change. In the summer of 1974–1975 (southern hemisphere), Australia suffered its worst recorded wildfire, when 15% of Australia's land mass suffered ""extensive fire damage"".  Fires that summer burned up an estimated 117 million hectares (290 million acres; 1,170,000 square kilometres; 450,000 square miles).   In Australia, the annual number of hot days (above 35 °C) and very hot days (above 40 °C) has increased significantly in many areas of the country since 1950. The country has always had bushfires but in 2019, the extent and ferocity of these fires increased dramatically.  For the first time catastrophic bushfire conditions were declared for Greater Sydney. New South Wales and Queensland declared a state of emergency but fires were also burning in South Australia and Western Australia. In 2019, extreme heat and dryness caused massive wildfires in Siberia, Alaska, Canary Islands, Australia, and in the Amazon rainforest. The fires in the latter were caused mainly by illegal logging. The smoke from the fires expanded on huge territory including major cities, dramatically reducing air quality. As of August 2020, the wildfires in that year were 13% worse than in 2019 due primarily to climate change, deforestation and agricultural burning. The Amazon rainforest's existence is threatened by fires.     Record-breaking wildfires in 2021 occurred in Turkey, Greece, California and Russia, thought to be linked to climate change."|2023-09-25-02-19-54
Wildfire|Carbon dioxide and other emissions from fires| Wildfires release large amounts of carbon dioxide, black and brown carbon particles, and ozone precursors such as volatile organic compounds and nitrogen oxides (NOx) into the atmosphere.   These emissions affect radiation, clouds, and climate on regional and even global scales. Wildfires also emit substantial amounts of semi-volatile organic species that can partition from the gas phase to form secondary organic aerosol (SOA) over hours to days after emission. In addition, the formation of the other pollutants as the air is transported can lead to harmful exposures for populations in regions far away from the wildfires.  While direct emissions of harmful pollutants can affect first responders and residents, wildfire smoke can also be transported over long distances and impact air quality across local, regional, and global scales. Over the past century, wildfires have accounted for 20–25% of global carbon emissions, the remainder from human activities.  Global carbon emissions from wildfires through August 2020 equaled the average annual emissions of the European Union.  In 2020, the carbon released by California's wildfires was significantly larger than the state's other carbon emissions. Forest fires in Indonesia in 1997 were estimated to have released between 0.81 and 2.57 gigatonnes (0.89 and 2.83 billion short tons) of CO2 into the atmosphere, which is between 13%–40% of the annual global carbon dioxide emissions from burning fossil fuels. In June and July 2019, fires in the Arctic emitted more than 140 megatons of carbon dioxide, according to an analysis by CAMS. To put that into perspective this amounts to the same amount of carbon emitted by 36 million cars in a year. The recent wildfires and their massive CO2 emissions mean that it will be important to take them into consideration when implementing measures for reaching greenhouse gas reduction targets accorded with the Paris climate agreement.  Due to the complex oxidative chemistry occurring during the transport of wildfire smoke in the atmosphere,  the toxicity of emissions was indicated to increase over time. Atmospheric models suggest that these concentrations of sooty particles could increase absorption of incoming solar radiation during winter months by as much as 15%.  The Amazon is estimated to hold around 90 billion tons of carbon. As of 2019, the earth's atmosphere has 415 parts per million of carbon, and the destruction of the Amazon would add about 38 parts per million. Research has shown wildfire smoke can have a cooling effect.|2023-09-25-02-19-54
Wildfire|Prevention| Wildfire prevention refers to the preemptive methods aimed at reducing the risk of fires as well as lessening its severity and spread.  Prevention techniques aim to manage air quality, maintain ecological balances, protect resources,  and to affect future fires.  Prevention policies must consider the role that humans play in wildfires, since, for example, 95% of forest fires in Europe are related to human involvement. Wildfire prevention programs around the world may employ techniques such as wildland fire use (WFU) and prescribed or controlled burns.   Wildland fire use refers to any fire of natural causes that is monitored but allowed to burn. Controlled burns are fires ignited by government agencies under less dangerous weather conditions.  Other objectives can include maintenance of healthy forests, rangelands, and wetlands, and support of ecosystem diversity. Strategies for wildfire prevention, detection, control and suppression have varied over the years.  One common and inexpensive technique to reduce the risk of uncontrolled wildfires is controlled burning: intentionally igniting smaller less-intense fires to minimize the amount of flammable material available for a potential wildfire.   Vegetation may be burned periodically to limit the accumulation of plants and other debris that may serve as fuel, while also maintaining high species diversity.   While other people claim that controlled burns and a policy of allowing some wildfires to burn is the cheapest method and an ecologically appropriate policy for many forests, they tend not to take into account the economic value of resources that are consumed by the fire, especially merchantable timber.  Some studies conclude that while fuels may also be removed by logging, such thinning treatments may not be effective at reducing fire severity under extreme weather conditions. Building codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure.   Communities in the Philippines also maintain fire lines 5 to 10 meters (16 to 33 ft) wide between the forest and their village, and patrol these lines during summer months or seasons of dry weather.  Continued residential development in fire-prone areas and rebuilding structures destroyed by fires has been met with criticism.  The ecological benefits of fire are often overridden by the economic and safety benefits of protecting structures and human life.|2023-09-25-02-19-54
Wildfire|Detection| The demand for timely, high-quality fire information has increased in recent years. Fast and effective detection is a key factor in wildfire fighting.  Early detection efforts were focused on early response, accurate results in both daytime and nighttime, and the ability to prioritize fire danger.  Fire lookout towers were used in the United States in the early 20th century and fires were reported using telephones, carrier pigeons, and heliographs.  Aerial and land photography using instant cameras were used in the 1950s until infrared scanning was developed for fire detection in the 1960s. However, information analysis and delivery was often delayed by limitations in communication technology. Early satellite-derived fire analyses were hand-drawn on maps at a remote site and sent via overnight mail to the fire manager. During the Yellowstone fires of 1988, a data station was established in West Yellowstone, permitting the delivery of satellite-based fire information in approximately four hours. Public hotlines, fire lookouts in towers, and ground and aerial patrols can be used as a means of early detection of forest fires. However, accurate human observation may be limited by operator fatigue, time of day, time of year, and geographic location. Electronic systems have gained popularity in recent years as a possible resolution to human operator error. These systems may be semi- or fully automated and employ systems based on the risk area and degree of human presence, as suggested by GIS data analyses. An integrated approach of multiple systems can be used to merge satellite data, aerial imagery, and personnel position via Global Positioning System (GPS) into a collective whole for near-realtime use by wireless Incident Command Centers.|2023-09-25-02-19-54
Wildfire|Local sensor networks| A small, high risk area that features thick vegetation, a strong human presence, or is close to a critical urban area can be monitored using a local sensor network. Detection systems may include wireless sensor networks that act as automated weather systems: detecting temperature, humidity, and smoke.     These may be battery-powered, solar-powered, or tree-rechargeable: able to recharge their battery systems using the small electrical currents in plant material.  Larger, medium-risk areas can be monitored by scanning towers that incorporate fixed cameras and sensors to detect smoke or additional factors such as the infrared signature of carbon dioxide produced by fires. Additional capabilities such as night vision, brightness detection, and color change detection may also be incorporated into sensor arrays. The Department of Natural Resources signed a contract with PanoAI for the installation of 360 degree 'rapid detection' cameras around the Pacific northwest, which are mounted on cell towers and are capable of 24/7 monitoring of a 15 mile radius.  Additionally, Sensaio Tech, based in Brazil and Toronto, has released a sensor device that continuously monitors 14 different variables common in forests, ranging from soil temperature to salinity. This information is connected live back to clients through dashboard visualizations, while mobile notifications are provided regarding dangerous levels.|2023-09-25-02-19-54
Wildfire|Satellite and aerial monitoring| Satellite and aerial monitoring through the use of planes, helicopter, or UAVs can provide a wider view and may be sufficient to monitor very large, low risk areas. These more sophisticated systems employ GPS and aircraft-mounted infrared or high-resolution visible cameras to identify and target wildfires.   Satellite-mounted sensors such as Envisat's Advanced Along Track Scanning Radiometer and European Remote-Sensing Satellite's Along-Track Scanning Radiometer can measure infrared radiation emitted by fires, identifying hot spots greater than 39 °C (102 °F).   The National Oceanic and Atmospheric Administration's Hazard Mapping System combines remote-sensing data from satellite sources such as Geostationary Operational Environmental Satellite (GOES), Moderate-Resolution Imaging Spectroradiometer (MODIS), and Advanced Very High Resolution Radiometer (AVHRR) for detection of fire and smoke plume locations.   However, satellite detection is prone to offset errors, anywhere from 2 to 3 kilometers (1 to 2 mi) for MODIS and AVHRR data and up to 12 kilometers (7.5 mi) for GOES data.  Satellites in geostationary orbits may become disabled, and satellites in polar orbits are often limited by their short window of observation time. Cloud cover and image resolution may also limit the effectiveness of satellite imagery.  Global Forest Watch  provides detailed daily updates on fire alerts. In 2015 a new fire detection tool is in operation at the U.S. Department of Agriculture (USDA) Forest Service (USFS) which uses data from the Suomi National Polar-orbiting Partnership (NPP) satellite to detect smaller fires in more detail than previous space-based products. The high-resolution data is used with a computer model to predict how a fire will change direction based on weather and land conditions. In 2014, an international campaign was organized in South Africa's Kruger National Park to validate fire detection products including the new VIIRS active fire data. In advance of that campaign, the Meraka Institute of the Council for Scientific and Industrial Research in Pretoria, South Africa, an early adopter of the VIIRS 375 m fire product, put it to use during several large wildfires in Kruger.  There have also been numerous companies and start-ups releasing new drone technology, many of which use AI. Data Blanket, a Seattle-based startup backed by Bill Gates, has developed drones capable of performing self-guided flights in order to conduct comprehensive assessments of wildfires and the surrounding site, providing real-time and critical information such as local vegetation and fuels. The drones are equipped with RGB and infrared cameras, AI-based computational software, 5G/Wi-Fi, and advanced navigational features. Data Blanket has also stated that its system will eventually be capable of producing micro-weather data, further supporting firefighter efforts by delivering crucial information. Additionally, scientists from Imperial College London and Swiss Federal Laboratories for Materials Science and Technology, have designed the experimental 'FireDrone', which can handle temperatures of up to 200C for 10 minutes.Another company, the German-based Orora Tech, as of 2023 has two satellites in orbit packaged with infrared sensors that are capable of quickly detecting temperature and soil anomalies, with the ability to predict the likely growth and spread rate of a fire in comparison to others. The company has stated that it will be capable of scanning the earth 48 times per day by 2026.|2023-09-25-02-19-54
Wildfire|Artificial intelligence| Between 2022–2023, wildfires throughout North America prompted an uptake in the delivery and design of various technologies using artificial intelligence for early detection, prevention, and prediction of wildfires.    Cornea (company), centered in New York, has developed a system utilizing topographical and geographical data that can be used to predict potential fire patterns and probability.|2023-09-25-02-19-54
Wildfire|Suppression| Wildfire suppression depends on the technologies available in the area in which the wildfire occurs. In less developed nations the techniques used can be as simple as throwing sand or beating the fire with sticks or palm fronds.  In more advanced nations, the suppression methods vary due to increased technological capacity. Silver iodide can be used to encourage snow fall,  while fire retardants and water can be dropped onto fires by unmanned aerial vehicles, planes, and helicopters.   Complete fire suppression is no longer an expectation, but the majority of wildfires are often extinguished before they grow out of control. While more than 99% of the 10,000 new wildfires each year are contained, escaped wildfires under extreme weather conditions are difficult to suppress without a change in the weather. Wildfires in Canada and the US burn an average of 54,500 square kilometers (13,000,000 acres) per year. Above all, fighting wildfires can become deadly. A wildfire's burning front may also change direction unexpectedly and jump across fire breaks. Intense heat and smoke can lead to disorientation and loss of appreciation of the direction of the fire, which can make fires particularly dangerous. For example, during the 1949 Mann Gulch fire in Montana, United States, thirteen smokejumpers died when they lost their communication links, became disoriented, and were overtaken by the fire.  In the Australian February 2009 Victorian bushfires, at least 173 people died and over 2,029 homes and 3,500 structures were lost when they became engulfed by wildfire.|2023-09-25-02-19-54
Wildfire|Costs of wildfire suppression| The suppression of wild fires takes up a large amount of a country's gross domestic product which directly affects the country's economy.  While costs vary wildly from year to year, depending on the severity of each fire season, in the United States, local, state, federal and tribal agencies collectively spend tens of billions of dollars annually to suppress wildfires. In the United States, it was reported that approximately $6 billion was spent between 2004–2008 to suppress wildfires in the country.  In California, the U.S. Forest Service spends about $200 million per year to suppress 98% of wildfires and up to $1 billion to suppress the other 2% of fires that escape initial attack and become large.|2023-09-25-02-19-54
Wildfire|Wildland firefighting safety| Wildland fire fighters face several life-threatening hazards including heat stress, fatigue, smoke and dust, as well as the risk of other injuries such as burns, cuts and scrapes, animal bites, and even rhabdomyolysis.   Between 2000 and 2016, more than 350 wildland firefighters died on-duty. Especially in hot weather conditions, fires present the risk of heat stress, which can entail feeling heat, fatigue, weakness, vertigo, headache, or nausea. Heat stress can progress into heat strain, which entails physiological changes such as increased heart rate and core body temperature. This can lead to heat-related illnesses, such as heat rash, cramps, exhaustion or heat stroke. Various factors can contribute to the risks posed by heat stress, including strenuous work, personal risk factors such as age and fitness, dehydration, sleep deprivation, and burdensome personal protective equipment. Rest, cool water, and occasional breaks are crucial to mitigating the effects of heat stress. Smoke, ash, and debris can also pose serious respiratory hazards for wildland firefighters. The smoke and dust from wildfires can contain gases such as carbon monoxide, sulfur dioxide and formaldehyde, as well as particulates such as ash and silica. To reduce smoke exposure, wildfire fighting crews should, whenever possible, rotate firefighters through areas of heavy smoke, avoid downwind firefighting, use equipment rather than people in holding areas, and minimize mop-up. Camps and command posts should also be located upwind of wildfires. Protective clothing and equipment can also help minimize exposure to smoke and ash. Firefighters are also at risk of cardiac events including strokes and heart attacks. Firefighters should maintain good physical fitness. Fitness programs, medical screening and examination programs which include stress tests can minimize the risks of firefighting cardiac problems.  Other injury hazards wildland firefighters face include slips, trips, falls, burns, scrapes, and cuts from tools and equipment, being struck by trees, vehicles, or other objects, plant hazards such as thorns and poison ivy, snake and animal bites, vehicle crashes, electrocution from power lines or lightning storms, and unstable building structures.|2023-09-25-02-19-54
Wildfire|Fire retardants| Fire retardants are used to slow wildfires by inhibiting combustion. They are aqueous solutions of ammonium phosphates and ammonium sulfates, as well as thickening agents.  The decision to apply retardant depends on the magnitude, location and intensity of the wildfire. In certain instances, fire retardant may also be applied as a precautionary fire defense measure. Typical fire retardants contain the same agents as fertilizers. Fire retardants may also affect water quality through leaching, eutrophication, or misapplication. Fire retardant's effects on drinking water remain inconclusive.  Dilution factors, including water body size, rainfall, and water flow rates lessen the concentration and potency of fire retardant.  Wildfire debris (ash and sediment) clog rivers and reservoirs increasing the risk for floods and erosion that ultimately slow and/or damage water treatment systems.   There is continued concern of fire retardant effects on land, water, wildlife habitats, and watershed quality, additional research is needed. However, on the positive side, fire retardant (specifically its nitrogen and phosphorus components) has been shown to have a fertilizing effect on nutrient-deprived soils and thus creates a temporary increase in vegetation.|2023-09-25-02-19-54
Wildfire|Modeling| Wildfire modeling is concerned with numerical simulation of wildfires to comprehend and predict fire behavior.   Wildfire modeling aims to aid wildfire suppression, increase the safety of firefighters and the public, and minimize damage. Wildfire modeling can also aid in protecting ecosystems, watersheds, and air quality.|2023-09-25-02-19-54
Wildfire|Impacts on ecosystems| Wildfires are common in climates that are sufficiently moist to allow the growth of vegetation but feature extended dry, hot periods.  Such places include the vegetated areas of Australia and Southeast Asia, the veld in southern Africa, the fynbos in the Western Cape of South Africa, the forested areas of the United States and Canada, and the Mediterranean Basin. High-severity wildfire creates complex early seral forest habitat (also called “snag forest habitat”), which often has higher species richness and diversity than unburned old forest.  Plant and animal species in most types of North American forests evolved with fire, and many of these species depend on wildfires, and particularly high-severity fires, to reproduce and grow. Fire helps to return nutrients from plant matter back to the soil. The heat from fire is necessary to the germination of certain types of seeds, and the snags (dead trees) and early successional forests created by high-severity fire create habitat conditions that are beneficial to wildlife.  Early successional forests created by high-severity fire support some of the highest levels of native biodiversity found in temperate conifer forests.   Post-fire logging has no ecological benefits and many negative impacts; the same is often true for post-fire seeding. Although some ecosystems rely on naturally occurring fires to regulate growth, some ecosystems suffer from too much fire, such as the chaparral in southern California and lower-elevation deserts in the American Southwest. The increased fire frequency in these ordinarily fire-dependent areas has upset natural cycles, damaged native plant communities, and encouraged the growth of non-native weeds.     Invasive species, such as Lygodium microphyllum and Bromus tectorum, can grow rapidly in areas that were damaged by fires. Because they are highly flammable, they can increase the future risk of fire, creating a positive feedback loop that increases fire frequency and further alters native vegetation communities. In the Amazon rainforest, drought, logging, cattle ranching practices, and slash-and-burn agriculture damage fire-resistant forests and promote the growth of flammable brush, creating a cycle that encourages more burning.  Fires in the rainforest threaten its collection of diverse species and produce large amounts of CO2.  Also, fires in the rainforest, along with drought and human involvement, could damage or destroy more than half of the Amazon rainforest by 2030.  Wildfires generate ash, reduce the availability of organic nutrients, and cause an increase in water runoff, eroding other nutrients and creating flash flood conditions.   A 2003 wildfire in the North Yorkshire Moors burned off 2.5 square kilometers (600 acres) of heather and the underlying peat layers. Afterwards, wind erosion stripped the ash and the exposed soil, revealing archaeological remains dating to 10,000 BC.  Wildfires can also have an effect on climate change, increasing the amount of carbon released into the atmosphere and inhibiting vegetation growth, which affects overall carbon uptake by plants.|2023-09-25-02-19-54
Wildfire|Plant and animals adaptations| Fire adaptations are traits of plants and animals that help them survive wildfire or to use resources created by wildfire. These traits can help plants and animals increase their survival rates during a fire and/or reproduce offspring after a fire. Both plants and animals have multiple strategies for surviving and reproducing after fire. Plants in wildfire-prone ecosystems often survive through adaptations to their local fire regime. Such adaptations include physical protection against heat, increased growth after a fire event, and flammable materials that encourage fire and may eliminate competition.|2023-09-25-02-19-54
Wildfire|Impacts on humans| Wildfire risk is the chance that a wildfire will start in or reach a particular area and the potential loss of human values if it does. Risk is dependent on variable factors such as human activities, weather patterns, availability of wildfire fuels, and the availability or lack of resources to suppress a fire.  Wildfires have continually been a threat to human populations. However, human-induced geographic and climatic changes are exposing populations more frequently to wildfires and increasing wildfire risk. It is speculated that the increase in wildfires arises from a century of wildfire suppression coupled with the rapid expansion of human developments into fire-prone wildlands.  Wildfires are naturally occurring events that aid in promoting forest health. Global warming and climate changes are causing an increase in temperatures and more droughts nationwide which contributes to an increase in wildfire risk.|2023-09-25-02-19-54
Wildfire|Airborne hazards| The most noticeable adverse effect of wildfires is the destruction of property. However, hazardous chemicals released also significantly impact human health. Wildfire smoke is composed primarily of carbon dioxide and water vapor. Other common components present in lower concentrations are carbon monoxide, formaldehyde, acrolein, polyaromatic hydrocarbons, and benzene.  Small airborne particulates (in solid form or liquid droplets) are also present in smoke and ash debris. 80–90% of wildfire smoke, by mass, is within the fine particle size class of 2.5 micrometers in diameter or smaller. Carbon dioxide in smoke poses a low health risk due to its low toxicity. Rather, carbon monoxide and fine particulate matter, particularly 2.5 µm in diameter and smaller, have been identified as the major health threats.  High levels of heavy metals, including lead, arsenic, cadmium, and copper were found in the ash debris following the 2007 Californian wildfires. A national clean-up campaign was organised in fear of the health effects from exposure.  In the devastating California Camp Fire (2018) that killed 85 people, lead levels increased by around 50 times in the hours following the fire at a site nearby (Chico). Zinc concentration also increased significantly in Modesto, 150 miles away. Heavy metals such as manganese and calcium were found in numerous California fires as well.  Other chemicals are considered to be significant hazards but are found in concentrations that are too low to cause detectable health effects.[citation needed] The degree of wildfire smoke exposure to an individual is dependent on the length, severity, duration, and proximity of the fire. People are exposed directly to smoke via the respiratory tract through inhalation of air pollutants. Indirectly, communities are exposed to wildfire debris that can contaminate soil and water supplies. The U.S. Environmental Protection Agency (EPA) developed the air quality index (AQI), a public resource that provides national air quality standard concentrations for common air pollutants. The public can use it to determine their exposure to hazardous air pollutants based on visibility range. Whether transported smoke plumes are relevant for surface air quality depends on where they exist in the atmosphere, which in turn depends on the initial injection height of the convective smoke plume into the atmosphere. Smoke that is injected above the planetary boundary layer (PBL) may be detectable from spaceborne satellites and play a role in altering the Earth's energy budget, but would not mix down to the surface where it would impact air quality and human health. Alternatively, smoke confined to a shallow PBL (through nighttime stable stratification of the atmosphere or terrain trapping) may become particularly concentrated and problematic for surface air quality. Wildfire intensity and smoke emissions are not constant throughout the fire lifetime and tend to follow a diurnal cycle that peaks in late afternoon and early evening, and which may be reasonably approximated using a monomodal or bimodal normal distribution. Most of Earth's weather and air pollution resides in the troposphere, the part of the atmosphere that extends from the surface of the planet to a height of about 10 kilometers (6 mi). The vertical lift of a severe thunderstorm or pyrocumulonimbus can be enhanced in the area of a large wildfire, which can propel smoke, soot, and other particulate matter as high as the lower stratosphere.  Previously, prevailing scientific theory held that most particles in the stratosphere came from volcanoes, but smoke and other wildfire emissions have been detected from the lower stratosphere.  Pyrocumulus clouds can reach 6,100 meters (20,000 ft) over wildfires.  Satellite observation of smoke plumes from wildfires revealed that the plumes could be traced intact for distances exceeding 1,600 kilometers (1,000 mi).  Computer-aided models such as CALPUFF may help predict the size and direction of wildfire-generated smoke plumes by using atmospheric dispersion modeling. Wildfires can affect local atmospheric pollution,  and release carbon in the form of carbon dioxide.  Wildfire emissions contain fine particulate matter which can cause cardiovascular and respiratory problems.  Increased fire byproducts in the troposphere can increase ozone concentrations beyond safe levels.|2023-09-25-02-19-54
Wildfire|Water pollution| Debris and chemical runoff into waterways after wildfires can make drinking water sources unsafe.  Benzene is one of many chemicals that have been found in drinking water systems after wildfires. Benzene can permeate certain plastic pipes and thus require long times to be removed from the water distribution infrastructure. Researchers estimated that, in worst case scenarios, more than 286 days of constant flushing of a contaminated HDPE service line were needed to reduce benzene below safe drinking water limits.   Temperature increases caused by fires, including wildfires, can cause plastic water pipes to generate toxic chemicals  such as benzene.|2023-09-25-02-19-54
Wildfire|Post-fire risks| After a wildfire, hazards remain. Residents returning to their homes may be at risk from falling fire-weakened trees. Humans and pets may also be harmed by falling into ash pits. The Intergovernmental Panel on Climate Change (IPCC) also reports that wildfires cause significant damage to electric systems, especially in dry regions. Other post-fire risks, can increase if other extreme weather follows. For example, wildfires make soil less able to absorb precipitation, so heavy rainfall can result in more severe flooding and damages like mud slides.|2023-09-25-02-19-54
Wildfire|At-risk groups|" Firefighters are at greatest risk for acute and chronic health effects resulting from wildfire smoke exposure. Due to firefighters' occupational duties, they are frequently exposed to hazardous chemicals at close proximity for longer periods of time. A case study on the exposure of wildfire smoke among wildland firefighters shows that firefighters are exposed to significant levels of carbon monoxide and respiratory irritants above OSHA-permissible exposure limits (PEL) and ACGIH threshold limit values (TLV). 5–10% are overexposed. Between 2001 and 2012, over 200 fatalities occurred among wildland firefighters. In addition to heat and chemical hazards, firefighters are also at risk for electrocution from power lines; injuries from equipment; slips, trips, and falls; injuries from vehicle rollovers; heat-related illness; insect bites and stings; stress; and rhabdomyolysis. Residents in communities surrounding wildfires are exposed to lower concentrations of chemicals, but they are at a greater risk for indirect exposure through water or soil contamination. Exposure to residents is greatly dependent on individual susceptibility. Vulnerable persons such as children (ages 0–4), the elderly (ages 65 and older), smokers, and pregnant women are at an increased risk due to their already compromised body systems, even when the exposures are present at low chemical concentrations and for relatively short exposure periods.  They are also at risk for future wildfires and may move away to areas they consider less risky. Wildfires affect large numbers of people in Western Canada and the United States. In California alone, more than 350,000 people live in towns and cities in ""very high fire hazard severity zones"". Direct risks to building residents in fire-prone areas can be moderated through design choices such as choosing fire-resistant vegetation, maintaining landscaping to avoid debris accumulation and to create firebreaks, and by selecting fire-retardant roofing materials. Potential compounding issues with poor air quality and heat during warmer months may be addressed with MERV 11 or higher outdoor air filtration in building ventilation systems, mechanical cooling, and a provision of a refuge area with additional air cleaning and cooling, if needed."|2023-09-25-02-19-54
Wildfire|Other impacts| There are also significant indirect or second-order societal impacts from wildfire, such as demands on utilities to prevent power transmission equipment from becoming ignition sources, and the cancelation or nonrenewal of homeowners insurance for residents living in wildfire-prone areas.|2023-09-25-02-19-54
Wildfire|Health effects| Wildfire smoke contains particulates that may have adverse effects upon the human respiratory system. Evidence of the health effects should be relayed to the public so that exposure may be limited. The evidence can also be used to influence policy to promote positive health outcomes. Inhalation of smoke from a wildfire can be a health hazard.  Wildfire smoke is composed of combustion products i.e. carbon dioxide, carbon monoxide, water vapor, particulate matter, organic chemicals, nitrogen oxides and other compounds. The principal health concern is the inhalation of particulate matter and carbon monoxide. Particulate matter (PM) is a type of air pollution made up of particles of dust and liquid droplets. They are characterized into three categories based on particle diameter: coarse PM, fine PM, and ultrafine PM. Coarse particles are between 2.5 micrometers and 10 micrometers, fine particles measure 0.1 to 2.5 micrometers, and ultrafine particle are less than 0.1 micrometer. lmpact on the body upon inhalation varies by size. Coarse PM is filtered by the upper airways and can accumulate and cause pulmonary inflammation. This can result in eye and sinus irritation as well as sore throat and coughing.   Coarse PM is often composed of heavier and more toxic materials that lead to short-term effects with stronger impact. Smaller PM moves further into the respiratory system creating issues deep into the lungs and the bloodstream.   In asthma patients, PM2.5 causes inflammation but also increases oxidative stress in the epithelial cells. These particulates also cause apoptosis and autophagy in lung epithelial cells. Both processes damage the cells and impact cell function. This damage impacts those with respiratory conditions such as asthma where the lung tissues and function are already compromised.   Particulates less than 0.1 micrometer are called ultrafine particle (UFP). It is a major component of wildfire smoke.  UFP can enter the bloodstream like PM2.5-0.1 however studies show that it works into the blood much quicker. The inflammation and epithelial damage done by UFP has also shown to be much more severe.  PM2.5 is of the largest concern in regards to wildfire.  This is particularly hazardous to the very young, elderly and those with chronic conditions such as asthma, chronic obstructive pulmonary disease (COPD), cystic fibrosis and cardiovascular conditions. The illnesses most commonly associated with exposure to fine PM from wildfire smoke are bronchitis, exacerbation of asthma or COPD, and pneumonia. Symptoms of these complications include wheezing and shortness of breath and cardiovascular symptoms include chest pain, rapid heart rate and fatigue.|2023-09-25-02-19-54
Wildfire|Asthma exacerbation| Several epidemiological studies have demonstrated a close association between air pollution and respiratory allergic diseases such as bronchial asthma. An observational study of smoke exposure related to the 2007 San Diego wildfires revealed an increase both in healthcare utilization and respiratory diagnoses, especially asthma among the group sampled.  Projected climate scenarios of wildfire occurrences predict significant increases in respiratory conditions among young children.  Particulate Matter (PM) triggers a series of biological processes including inflammatory immune response, oxidative stress, which are associated with harmful changes in allergic respiratory diseases. Although some studies demonstrated no significant acute changes in lung function among people with asthma related to PM from wildfires, a possible explanation for these counterintuitive findings is the increased use of quick-relief medications, such as inhalers, in response to elevated levels of smoke among those already diagnosed with asthma. There is consistent evidence between wildfire smoke and the exacerbation of asthma. Asthma is one of the most common chronic disease among children in the United States, affecting an estimated 6.2 million children.  Research on asthma risk focuses specifically on the risk of air pollution during the gestational period. Several pathophysiology processes are involved in this. Considerable airway development occurs during the 2nd and 3rd trimesters and continues until 3 years of age.  It is hypothesized that exposure to these toxins during this period could have consequential effects, as the epithelium of the lungs during this time could have increased permeability to toxins. Exposure to air pollution during parental and pre-natal stage could induce epigenetic changes which are responsible for the development of asthma.  Studies have found significant association between PM2.5, NO2 and development of asthma during childhood despite heterogeneity among studies.  Furthermore, maternal exposure to chronic stressors is most likely present in distressed communities, and as this can be correlated with childhood asthma, it may further explain links between early childhood exposure to air pollution, neighborhood poverty, and childhood risk.|2023-09-25-02-19-54
Wildfire|Carbon monoxide danger| Carbon monoxide (CO) is a colorless, odorless gas that can be found at the highest concentration at close proximity to a smoldering fire. Thus, it is a serious threat to the health of wildfire firefighters. CO in smoke can be inhaled into the lungs where it is absorbed into the bloodstream and reduces oxygen delivery to the body's vital organs. At high concentrations, it can cause headaches, weakness, dizziness, confusion, nausea, disorientation, visual impairment, coma, and even death. Even at lower concentrations, such as those found at wildfires, individuals with cardiovascular disease may experience chest pain and cardiac arrhythmia.  A recent study tracking the number and cause of wildfire firefighter deaths from 1990 to 2006 found that 21.9% of the deaths occurred from heart attacks. Another important and somewhat less obvious health effect of wildfires is psychiatric diseases and disorders. Both adults and children from various countries who were directly and indirectly affected by wildfires were found to demonstrate different mental conditions linked to their experience with the wildfires. These include post-traumatic stress disorder (PTSD), depression, anxiety, and phobias.|2023-09-25-02-19-54
Wildfire|Epidemiology| The Western US has seen an increase in both the frequency and intensity of wildfires over the last several decades. This has been attributed to the arid climate of there and the effects of global warming. An estimated 46 million people were exposed to wildfire smoke from 2004 to 2009 in the Western US. Evidence has demonstrated that wildfire smoke can increase levels of airborne particulate. The EPA has defined acceptable concentrations of PM in the air, through the National Ambient Air Quality Standards and monitoring of ambient air quality has been mandated.  Due to these monitoring programs and the incidence of several large wildfires near populated areas, epidemiological studies have been conducted and demonstrate an association between human health effects and an increase in fine particulate matter due to wildfire smoke. An increase in PM smoke emitted from the Hayman fire in Colorado in June 2002, was associated with an increase in respiratory symptoms in patients with COPD.  Looking at the wildfires in Southern California in 2003, investigators have shown an increase in hospital admissions due to asthma symptoms while being exposed to peak concentrations of PM in smoke.  Another epidemiological study found a 7.2% (95% confidence interval: 0.25%, 15%) increase in risk of respiratory related hospital admissions during smoke wave days with high wildfire-specific particulate matter 2.5 compared to matched non-smoke-wave days. Children participating in the Children's Health Study were also found to have an increase in eye and respiratory symptoms, medication use and physician visits.  Mothers who were pregnant during the fires gave birth to babies with a slightly reduced average birth weight compared to those who were not exposed. Suggesting that pregnant women may also be at greater risk to adverse effects from wildfire.  Worldwide, it is estimated that 339,000 people die due to the effects of wildfire smoke each year. Besides the size of PM, their chemical composition should also be considered. Antecedent studies have demonstrated that the chemical composition of PM2.5 from wildfire smoke can yield different estimates of human health outcomes as compared to other sources of smoke such as solid fuels.|2023-09-25-02-19-54
Wildfire|History| The first evidence of wildfires is fossils of the giant fungi Prototaxites preserved as charcoal, discovered in South Wales and Poland, dating to the Silurian period (about 430 million years ago).  Smoldering surface fires started to occur sometime before the Early Devonian period 405 million years ago. Low atmospheric oxygen during the Middle and Late Devonian was accompanied by a decrease in charcoal abundance.   Additional charcoal evidence suggests that fires continued through the Carboniferous period. Later, the overall increase of atmospheric oxygen from 13% in the Late Devonian to 30–31% by the Late Permian was accompanied by a more widespread distribution of wildfires.  Later, a decrease in wildfire-related charcoal deposits from the late Permian to the Triassic periods is explained by a decrease in oxygen levels. Wildfires during the Paleozoic and Mesozoic periods followed patterns similar to fires that occur in modern times. Surface fires driven by dry seasons[clarification needed] are evident in Devonian and Carboniferous progymnosperm forests. Lepidodendron forests dating to the Carboniferous period have charred peaks, evidence of crown fires. In Jurassic gymnosperm forests, there is evidence of high frequency, light surface fires.  The increase of fire activity in the late Tertiary  is possibly due to the increase of C4-type grasses. As these grasses shifted to more mesic habitats, their high flammability increased fire frequency, promoting grasslands over woodlands.  However, fire-prone habitats may have contributed to the prominence of trees such as those of the genera Eucalyptus, Pinus and Sequoia, which have thick bark to withstand fires and employ pyriscence.|2023-09-25-02-19-54
Wildfire|Human involvement|" The human use of fire for agricultural and hunting purposes during the Paleolithic and Mesolithic ages altered pre-existing landscapes and fire regimes. Woodlands were gradually replaced by smaller vegetation that facilitated travel, hunting, seed-gathering and planting.  In recorded human history, minor allusions to wildfires were mentioned in the Bible and by classical writers such as Homer. However, while ancient Hebrew, Greek, and Roman writers were aware of fires, they were not very interested in the uncultivated lands where wildfires occurred.   Wildfires were used in battles throughout human history as early thermal weapons. From the Middle ages, accounts were written of occupational burning as well as customs and laws that governed the use of fire. In Germany, regular burning was documented in 1290 in the Odenwald and in 1344 in the Black Forest.  In the 14th century Sardinia, firebreaks were used for wildfire protection. In Spain during the 1550s, sheep husbandry was discouraged in certain provinces by Philip II due to the harmful effects of fires used in transhumance.   As early as the 17th century, Native Americans were observed using fire for many purposes including cultivation, signaling, and warfare. Scottish botanist David Douglas noted the native use of fire for tobacco cultivation, to encourage deer into smaller areas for hunting purposes, and to improve foraging for honey and grasshoppers. Charcoal found in sedimentary deposits off the Pacific coast of Central America suggests that more burning occurred in the 50 years before the Spanish colonization of the Americas than after the colonization.  In the post-World War II Baltic region, socio-economic changes led more stringent air quality standards and bans on fires that eliminated traditional burning practices.  In the mid-19th century, explorers from HMS Beagle observed Australian Aborigines using fire for ground clearing, hunting, and regeneration of plant food in a method later named fire-stick farming.  Such careful use of fire has been employed for centuries in lands protected by Kakadu National Park to encourage biodiversity. Wildfires typically occur during periods of increased temperature and drought. An increase in fire-related debris flow in alluvial fans of northeastern Yellowstone National Park was linked to the period between AD 1050 and 1200, coinciding with the Medieval Warm Period.  However, human influence caused an increase in fire frequency. Dendrochronological fire scar data and charcoal layer data in Finland suggests that, while many fires occurred during severe drought conditions, an increase in the number of fires during 850 BC and 1660 AD can be attributed to human influence.  Charcoal evidence from the Americas suggested a general decrease in wildfires between 1 AD and 1750 compared to previous years. However, a period of increased fire frequency between 1750 and 1870 was suggested by charcoal data from North America and Asia, attributed to human population growth and influences such as land clearing practices. This period was followed by an overall decrease in burning in the 20th century, linked to the expansion of agriculture, increased livestock grazing, and fire prevention efforts.  A meta-analysis found that 17 times more land burned annually in California before 1800 compared to recent decades (1,800,000 hectares/year compared to 102,000 hectares/year). According to a paper published in the journal Science, the number of natural and human-caused fires decreased by 24.3% between 1998 and 2015. Researchers explain this as a transition from nomadism to settled lifestyle and intensification of agriculture that lead to a drop in the use of fire for land clearing. Increases of certain tree species (i.e. conifers) over others (i.e. deciduous trees) can increase wildfire risk, especially if these trees are also planted in monocultures  
Some invasive species, moved in by humans (i.e., for the pulp and paper industry) have in some cases also increased the intensity of wildfires. Examples include species such as Eucalyptus in California   and gamba grass in Australia."|2023-09-25-02-19-54
Wildfire|Society and culture|" Wildfires have a place in many cultures. ""To spread like wildfire"" is a common idiom in English, meaning something that ""quickly affects or becomes known by more and more people"". Wildfire activity has been attributed as a major factor in the development of Ancient Greece. In modern Greece, as in many other regions, it is the most common natural disaster and figures prominently in the social and economic lives of its people. In 1937, U.S. President Franklin D. Roosevelt initiated a nationwide fire prevention campaign, highlighting the role of human carelessness in forest fires. Later posters of the program featured Uncle Sam, characters from the Disney movie Bambi, and the official mascot of the U.S. Forest Service, Smokey Bear.  The Smokey Bear fire prevention campaign has yielded one of the most popular characters in the United States; for many years there was a living Smokey Bear mascot, and it has been commemorated on postage stamps."|2023-09-25-02-19-54
Wildfire|Sources| Attribution|2023-09-25-02-19-54
Category:Air pollution|General| See Category:Pollution by country for air pollution issues specific to a geographical area; and air quality.|2020-08-29-20-37-46
