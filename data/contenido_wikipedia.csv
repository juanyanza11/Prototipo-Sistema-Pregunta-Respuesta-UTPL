Pagina|Topic|Contenido|Fecha_Modificacion
Programming language|| A programming language is a system of notation for writing computer programs. Most programming languages are text-based formal languages, but they may also be graphical. They are a kind of computer language. The description of a programming language is usually split into the two components of syntax (form) and semantics (meaning), which are usually defined by a formal language. Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard) while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common. Programming language theory is the subfield of computer science that studies the design, implementation, analysis, characterization, and classification of programming languages.|2023-09-25-11-33-19
Programming language|Definitions| There are many considerations when defining what constitutes a programming language.|2023-09-25-11-33-19
Programming language|Computer languages vs programming languages|" The term computer language is sometimes used interchangeably with programming language. However, the usage of both terms varies among authors, including the exact scope of each. One usage describes programming languages as a subset of computer languages. Similarly, languages used in computing that have a different goal than expressing computer programs are generically designated computer languages. For instance, markup languages are sometimes referred to as computer languages to emphasize that they are not meant to be used for programming.
One way of classifying computer languages is by the computations they are capable of expressing, as described by the theory of computation. The majority of practical programming languages are Turing complete, and all Turing complete languages can implement the same set of algorithms. ANSI/ISO SQL-92 and Charity are examples of languages that are not Turing complete, yet are often called programming languages. However, some authors restrict the term ""programming language"" to Turing complete languages. Another usage regards programming languages as theoretical constructs for programming abstract machines and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources. John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats."|2023-09-25-11-33-19
Programming language|Domain and target| In most practical contexts, a programming language involves a computer; consequently, programming languages are usually defined and studied this way. Programming languages differ from natural languages in that natural languages are only used for interaction between people, while programming languages also allow humans to communicate instructions to machines. The domain of the language is also worth consideration. Markup languages like XML, HTML, or troff, which define structured data, are not usually considered programming languages. Programming languages may, however, share the syntax with markup languages if a computational semantics is defined. XSLT, for example, is a Turing complete language entirely using XML syntax. Moreover, LaTeX, which is mostly used for structuring documents, also contains a Turing complete subset.|2023-09-25-11-33-19
Programming language|Abstractions| Programming languages usually contain abstractions for defining and manipulating data structures or controlling the flow of execution. The practical necessity that a programming language support adequate abstractions is expressed by the abstraction principle. This principle is sometimes formulated as a recommendation to the programmer to make proper use of such abstractions.|2023-09-25-11-33-19
Programming language|Early developments|" Very early computers, such as Colossus, were programmed without the help of a stored program, by modifying their circuitry or setting banks of physical controls. Slightly later, programs could be written in machine language, where the programmer writes each instruction in a numeric form the hardware can execute directly. For example, the instruction to add the value in two memory locations might consist of 3 numbers: an ""opcode"" that selects the ""add"" operation, and two memory locations. The programs, in decimal or binary form, were read in from punched cards, paper tape, magnetic tape or toggled in on switches on the front panel of the computer. Machine languages were later termed first-generation programming languages (1GL). The next step was the development of the so-called second-generation programming languages (2GL) or assembly languages, which were still closely tied to the instruction set architecture of the specific computer. These served to make the program much more human-readable and relieved the programmer of tedious and error-prone address calculations. The first high-level programming languages, or third-generation programming languages (3GL), were written in the 1950s. An early high-level programming language to be designed for a computer was Plankalk√ºl, developed for the German Z3 by Konrad Zuse between 1943 and 1945. However, it was not implemented until 1998 and 2000. John Mauchly's Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer. Unlike machine code, Short Code statements represented mathematical expressions in an understandable form. However, the program had to be translated into machine code every time it ran, making the process much slower than running the equivalent machine code. At the University of Manchester, Alick Glennie developed Autocode in the early 1950s. As a programming language, it used a compiler to automatically convert the language into machine code. The first code and compiler was developed in 1952 for the Mark 1 computer at the University of Manchester and is considered to be the first compiled high-level programming language. The second auto code was developed for the Mark 1 by R. A. Brooker in 1954 and was called the ""Mark 1 Autocode"". Brooker also developed an auto code for the Ferranti Mercury in the 1950s in conjunction with the University of Manchester. The version for the EDSAC 2 was devised by D. F. Hartley of University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances and was noted for its object code optimization and source-language diagnostics which were advanced for the time. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine. In 1954, FORTRAN was invented at IBM by John Backus. It was the first widely used high-level general-purpose programming language to have a functional implementation, as opposed to just a design on paper. It is still a popular language for high-performance computing and is used for programs that benchmark and rank the world's fastest supercomputers. Another early programming language was devised by Grace Hopper in the US, called FLOW-MATIC. It was developed for the UNIVAC I at Remington Rand during the period from 1955 until 1959. Hopper found that business data processing customers were uncomfortable with mathematical notation, and in early 1955, she and her team wrote a specification for an English programming language and implemented a prototype. The FLOW-MATIC compiler became publicly available in early 1958 and was substantially complete in 1959. FLOW-MATIC was a major influence in the design of COBOL, since only it and its direct descendant AIMACO were in actual use at the time."|2023-09-25-11-33-19
Programming language|Refinement|" The increased use of high-level languages introduced a requirement for low-level programming languages or system programming languages. These languages, to varying degrees, provide facilities between assembly languages and high-level languages. They can be used to perform tasks that require direct access to hardware facilities but still provide higher-level control structures and error-checking. The period from the 1960s to the late 1970s brought the development of the major language paradigms now in use: Each of these languages spawned descendants, and most modern programming languages count at least one of them in their ancestry. The 1960s and 1970s also saw considerable debate over the merits of structured programming, and whether programming languages should be designed to support it. Edsger Dijkstra, in a famous 1968 letter published in the Communications of the ACM, argued that Goto statements should be eliminated from all ""higher-level"" programming languages."|2023-09-25-11-33-19
Programming language|Consolidation and growth|" The 1980s were years of relative consolidation. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language derived from Pascal and intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating the so-called ""fifth-generation"" languages that incorporated logic programming constructs. The functional languages community moved to standardize ML and Lisp. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the previous decades. One important trend in language design for programming large-scale systems during the 1980s was an increased focus on the use of modules or large-scale organizational units of code. Modula-2, Ada, and ML all developed notable module systems in the 1980s, which were often wedded to generic programming constructs. The rapid growth of the Internet in the mid-1990s created opportunities for new languages. Perl, originally a Unix scripting tool first released in 1987, became common in dynamic websites. Java came to be used for server-side programming, and bytecode virtual machines became popular again in commercial settings with their promise of ""Write once, run anywhere"" (UCSD Pascal had been popular for a time in the early 1980s). These developments were not fundamentally novel; rather, they were refinements of many existing languages and paradigms (although their syntax was often based on the C family of programming languages). Programming language evolution continues, in both industry and research. Current directions include security and reliability verification, new kinds of modularity (mixins, delegates, aspects), and database integration such as Microsoft's LINQ. Fourth-generation programming languages (4GL) are computer programming languages that aim to provide a higher level of abstraction of the internal computer hardware details than 3GLs. Fifth-generation programming languages (5GL) are programming languages based on solving problems using constraints given to the program, rather than using an algorithm written by a programmer."|2023-09-25-11-33-19
Programming language|Elements| All programming languages have some primitive building blocks for the description of data and the processes or transformations applied to them (like the addition of two numbers or the selection of an item from a collection). These primitives are defined by syntactic and semantic rules which describe their structure and meaning respectively.|2023-09-25-11-33-19
Programming language|Syntax| A programming language's surface form is known as its syntax. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, some programming languages are more graphical in nature, using visual relationships between symbols to specify a program. The syntax of a language describes the possible combinations of symbols that form a syntactically correct program. The meaning given to a combination of symbols is handled by semantics (either formal or hard-coded in a reference implementation). Since most languages are textual, this article discusses textual syntax. The programming language syntax is usually defined using a combination of regular expressions (for lexical structure) and Backus‚ÄìNaur form (for grammatical structure). Below is a simple grammar, based on Lisp: This grammar specifies the following: The following are examples of well-formed token sequences in this grammar: 12345, () and (a b c232 (1)). Not all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language's rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it. Using natural language as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false: The following C language fragment is syntactically correct, but performs operations that are not semantically defined (the operation *p >> 4 has no meaning for a value having a complex type and p->im is not defined because the value of p is the null pointer): If the type declaration on the first line were omitted, the program would trigger an error on the undefined variable p during compilation. However, the program would still be syntactically correct since type declarations provide only semantic information. The grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars. Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem, and generally blur the distinction between parsing and execution. In contrast to Lisp's macro system and Perl's BEGIN blocks, which may contain general computations, C macros are merely string replacements and do not require code execution.|2023-09-25-11-33-19
Programming language|Semantics| The term semantics refers to the meaning of languages, as opposed to their form (syntax). A static semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms.[failed verification] For compiled languages, static semantics essentially include those semantic rules that can be checked at compile time. Examples include checking that every identifier is declared before it is used (in languages that require such declarations) or that the labels on the arms of a case statement are distinct. Many important restrictions of this type, like checking that identifiers are used in the appropriate context (e.g. not adding an integer to a function name), or that subroutine calls have the appropriate number and type of arguments, can be enforced by defining them as rules in a logic called a type system. Other forms of static analyses like data flow analysis may also be part of static semantics. Newer programming languages like Java and C# have definite assignment analysis, a form of data flow analysis, as part of their static semantics. Once data has been specified, the machine must be instructed to perform operations on the data. For example, the semantics may define the strategy by which expressions are evaluated to values, or the manner in which control structures conditionally execute statements. The dynamic semantics (also known as execution semantics) of a language defines how and when the various constructs of a language should produce a program behavior. There are many ways of defining execution semantics. Natural language is often used to specify the execution semantics of languages commonly used in practice. A significant amount of academic research went into formal semantics of programming languages, which allows execution semantics to be specified in a formal manner. Results from this field of research have seen limited application to programming language design and implementation outside academia.|2023-09-25-11-33-19
Programming language|Type system|" A type system defines how a programming language classifies values and expressions into types, how it can manipulate those types and how they interact. The goal of a type system is to verify and usually enforce a certain level of correctness in programs written in that language by detecting certain incorrect operations. Any decidable type system involves a trade-off: while it rejects many incorrect programs, it can also prohibit some correct, albeit unusual programs. In order to bypass this downside, a number of languages have type loopholes, usually unchecked casts that may be used by the programmer to explicitly allow a normally disallowed operation between different types. In most typed languages, the type system is used only to type check programs, but a number of languages, usually functional ones, infer types, relieving the programmer from the need to write type annotations. The formal design and study of type systems is known as type theory. A language is typed if the specification of every operation defines types of data to which the operation is applicable. For example, the data represented by ""this text between the quotes"" is a string, and in many programming languages dividing a number by a string has no meaning and will not be executed. The invalid operation may be detected when the program is compiled (""static"" type checking) and will be rejected by the compiler with a compilation error message, or it may be detected while the program is running (""dynamic"" type checking), resulting in a run-time exception. Many languages allow a function called an exception handler to handle this exception and, for example, always return ""-1"" as the result. A special case of typed languages is the single-typed languages. These are often scripting or markup languages, such as REXX or SGML, and have only one data type[dubious  ‚Äì discuss]‚Äì‚Äîmost commonly character strings which are used for both symbolic and numeric data. In contrast, an untyped language, such as most assembly languages, allows any operation to be performed on any data, generally sequences of bits of various lengths. High-level untyped languages include BCPL, Tcl, and some varieties of Forth. In practice, while few languages are considered typed from the type theory (verifying or rejecting all operations), most modern languages offer a degree of typing. Many production languages provide means to bypass or subvert the type system, trading type safety for finer control over the program's execution (see casting). In static typing, all expressions have their types determined before a program executes, typically at compile-time. For example, 1 and (2+2) are integer expressions; they cannot be passed to a function that expects a string or stored in a variable that is defined to hold dates. Statically-typed languages can be either manifestly typed or type-inferred. In the first case, the programmer must explicitly write types at certain textual positions (for example, at variable declarations). In the second case, the compiler infers the types of expressions and declarations based on context. Most mainstream statically-typed languages, such as C++, C#, and Java, are manifestly typed. Complete type inference has traditionally been associated with functional languages such as Haskell and ML. However, many manifestly-typed languages support partial type inference; for example, C++, Java, and C# all infer types in certain limited cases. Additionally, some programming languages allow for some types to be automatically converted to other types; for example, an int can be used where the program expects a float. Dynamic typing, also called latent typing, determines the type-safety of operations at run time; in other words, types are associated with run-time values rather than textual expressions. As with type-inferred languages, dynamically-typed languages do not require the programmer to write explicit type annotations on expressions. Among other things, this may permit a single variable to refer to values of different types at different points in the program execution. However, type errors cannot be automatically detected until a piece of code is actually executed, potentially making debugging more difficult. Lisp, Smalltalk, Perl, Python, JavaScript, and Ruby are all examples of dynamically-typed languages. Weak typing allows a value of one type to be treated as another, for example treating a string as a number. This can occasionally be useful, but it can also allow some kinds of program faults to go undetected at compile time and even at run time. Strong typing prevents these program faults. An attempt to perform an operation on the wrong type of value raises an error. Strongly-typed languages are often termed type-safe or safe. An alternative definition for ""weakly typed"" refers to languages, such as Perl and JavaScript, which permit a large number of implicit type conversions. In JavaScript, for example, the expression 2 * x implicitly converts x to a number, and this conversion succeeds even if x is null, undefined, an Array, or a string of letters. Such implicit conversions are often useful, but they can mask programming errors. Strong and static are now generally considered orthogonal concepts, but usage in the literature differs. Some use the term strongly typed to mean strongly, statically typed, or, even more confusingly, to mean simply statically typed. Thus C has been called both strongly typed and weakly, statically typed. It may seem odd to some professional programmers that C could be ""weakly, statically typed"". However, the use of the generic pointer, the void* pointer, does allow casting pointers to other pointers without needing to do an explicit cast. This is extremely similar to somehow casting an array of bytes to any kind of datatype in C without using an explicit cast, such as (int) or (char)."|2023-09-25-11-33-19
Programming language|Standard library and run-time system|" Most programming languages have an associated core library (sometimes known as the ""standard library"", especially if it is included as part of the published language standard), which is conventionally made available by all implementations of the language. Core libraries typically include definitions for commonly used algorithms, data structures, and mechanisms for input and output. The line between a language and its core library differs from language to language. In some cases, the language designers may treat the library as a separate entity from the language. However, a language's core library is often treated as part of the language by its users, and some language specifications even require that this library be made available in all implementations. Indeed, some languages are designed so that the meanings of certain syntactic constructs cannot even be described without referring to the core library. For example, in Java, a string literal is defined as an instance of the java.lang.String class; similarly, in Smalltalk, an anonymous function expression (a ""block"") constructs an instance of the library's BlockContext class. Conversely, Scheme contains multiple coherent subsets that suffice to construct the rest of the language as library macros, and so the language designers do not even bother to say which portions of the language must be implemented as language constructs, and which must be implemented as parts of a library."|2023-09-25-11-33-19
Programming language|Design and implementation|" Programming languages share properties with natural languages related to their purpose as vehicles for communication, having a syntactic form separate from its semantics, and showing language families of related languages branching one from another. But as artificial constructs, they also differ in fundamental ways from languages that have evolved through usage. A significant difference is that a programming language can be fully described and studied in its entirety since it has a precise and finite definition. By contrast, natural languages have changing meanings given by their users in different communities. While constructed languages are also artificial languages designed from the ground up with a specific purpose, they lack the precise and complete semantic definition that a programming language has. Many programming languages have been designed from scratch, altered to meet new needs, and combined with other languages. Many have eventually fallen into disuse. Although there have been attempts to design one ""universal"" programming language that serves all purposes, all of them have failed to be generally accepted as filling this role. The need for diverse programming languages arises from the diversity of contexts in which languages are used: One common trend in the development of programming languages has been to add more ability to solve problems using a higher level of abstraction. The earliest programming languages were tied very closely to the underlying hardware of the computer. As new programming languages have developed, features have been added that let programmers express ideas that are more remote from simple translation into underlying hardware instructions. Because programmers are less tied to the complexity of the computer, their programs can do more computing with less effort from the programmer. This lets them write more functionality per time unit. Natural-language programming has been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. Edsger W. Dijkstra took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs, and dismissed natural-language programming as ""foolish"". Alan Perlis was similarly dismissive of the idea. Hybrid approaches have been taken in Structured English and SQL. A language's designers and users must construct a number of artifacts that govern and enable the practice of programming. The most important of these artifacts are the language specification and implementation."|2023-09-25-11-33-19
Programming language|Specification| The specification of a programming language is an artifact that the language users and the implementors can use to agree upon whether a piece of source code is a valid program in that language, and if so what its behavior shall be. A programming language specification can take several forms, including the following:|2023-09-25-11-33-19
Programming language|Implementation| An implementation of a programming language provides a way to write programs in that language and execute them on one or more configurations of hardware and software. There are, broadly, two approaches to programming language implementation: compilation and interpretation. It is generally possible to implement a language using either technique. The output of a compiler may be executed by hardware or a program called an interpreter. In some implementations that make use of the interpreter approach, there is no distinct boundary between compiling and interpreting. For instance, some implementations of BASIC compile and then execute the source one line at a time. Programs that are executed directly on the hardware usually run much faster than those that are interpreted in software.[better¬†source¬†needed] One technique for improving the performance of interpreted programs is just-in-time compilation. Here the virtual machine, just before execution, translates the blocks of bytecode which are going to be used to machine code, for direct execution on the hardware.|2023-09-25-11-33-19
Programming language|Proprietary languages| Although most of the most commonly used programming languages have fully open specifications and implementations, many programming languages exist only as proprietary programming languages with the implementation available only from a single vendor, which may claim that such a proprietary language is their intellectual property. Proprietary programming languages are commonly domain-specific languages or internal scripting languages for a single product; some proprietary languages are used only internally within a vendor, while others are available to external users.[citation needed] Some programming languages exist on the border between proprietary and open; for example, Oracle Corporation asserts proprietary rights to some aspects of the Java programming language, and Microsoft's C# programming language, which has open implementations of most parts of the system, also has Common Language Runtime (CLR) as a closed environment. Many proprietary languages are widely used, in spite of their proprietary nature; examples include MATLAB, VBScript, and Wolfram Language. Some languages may make the transition from closed to open; for example, Erlang was originally Ericsson's internal programming language.|2023-09-25-11-33-19
Programming language|Use|" Thousands of different programming languages have been created, mainly in the computing field.
Individual software projects commonly use five programming languages or more. Programming languages differ from most other forms of human expression in that they require a greater degree of precision and completeness. When using a natural language to communicate with other people, human authors and speakers can be ambiguous and make small errors, and still expect their intent to be understood. However, figuratively speaking, computers ""do exactly what they are told to do"", and cannot ""understand"" what code the programmer intended to write. The combination of the language definition, a program, and the program's inputs must fully specify the external behavior that occurs when the program is executed, within the domain of control of that program. On the other hand, ideas about an algorithm can be communicated to humans without the precision required for execution by using pseudocode, which interleaves natural language with code written in a programming language. A programming language provides a structured mechanism for defining pieces of data, and the operations or transformations that may be carried out automatically on that data. A programmer uses the abstractions present in the language to represent the concepts involved in a computation. These concepts are represented as a collection of the simplest elements available (called primitives). Programming is the process by which programmers combine these primitives to compose new programs, or adapt existing ones to new uses or a changing environment. Programs for a computer might be executed in a batch process without human interaction, or a user might type commands in an interactive session of an interpreter. In this case the ""commands"" are simply programs, whose execution is chained together. When a language can run its commands through an interpreter (such as a Unix shell or other command-line interface), without compiling, it is called a scripting language."|2023-09-25-11-33-19
Programming language|Measuring language usage| Determining which is the most widely used programming language is difficult since the definition of usage varies by context. One language may occupy the greater number of programmer hours, a different one has more lines of code, and a third may consume the most CPU time. Some languages are very popular for particular kinds of applications. For example, COBOL is still strong in the corporate data center, often on large mainframes; Fortran in scientific and engineering applications; Ada in aerospace, transportation, military, real-time, and embedded applications; and C in embedded applications and operating systems. Other languages are regularly used to write many different kinds of applications. Various methods of measuring language popularity, each subject to a different bias over what is measured, have been proposed: Combining and averaging information from various internet sites, stackify.com reported the ten most popular programming languages (in descending order by overall popularity): Java, C, C++, Python, C#, JavaScript, VB .NET, R, PHP, and MATLAB.|2023-09-25-11-33-19
Programming language|Dialects, flavors and implementations| A dialect of a programming language or a data exchange language is a (relatively small) variation or extension of the language that does not change its intrinsic nature. With languages such as Scheme and Forth, standards may be considered insufficient, inadequate, or illegitimate by implementors, so often they will deviate from the standard, making a new dialect. In other cases, a dialect is created for use in a domain-specific language, often a subset. In the Lisp world, most languages that use basic S-expression syntax and Lisp-like semantics are considered Lisp dialects, although they vary wildly as do, say, Racket and Clojure. As it is common for one language to have several dialects, it can become quite difficult for an inexperienced programmer to find the right documentation. The BASIC language has many dialects.|2023-09-25-11-33-19
Programming language|Taxonomies| There is no overarching classification scheme for programming languages. A given programming language does not usually have a single ancestor language. Languages commonly arise by combining the elements of several predecessor languages with new ideas in circulation at the time. Ideas that originate in one language will diffuse throughout a family of related languages, and then leap suddenly across familial gaps to appear in an entirely different family. The task is further complicated by the fact that languages can be classified along multiple axes. For example, Java is both an object-oriented language (because it encourages object-oriented organization) and a concurrent language (because it contains built-in constructs for running multiple threads in parallel). Python is an object-oriented scripting language. In broad strokes, programming languages are classified by programming paradigm and intended domain of use, with general-purpose programming languages distinguished from domain-specific programming languages. Traditionally, programming languages have been regarded as describing computation in terms of imperative sentences, i.e. issuing commands. These are generally called imperative programming languages. A great deal of research in programming languages has been aimed at blurring the distinction between a program as a set of instructions and a program as an assertion about the desired answer, which is the main feature of declarative programming. More refined paradigms include procedural programming, object-oriented programming, functional programming, and logic programming; some languages are hybrids of paradigms or multi-paradigmatic. An assembly language is not so much a paradigm as a direct model of an underlying machine architecture. By purpose, programming languages might be considered general purpose, system programming languages, scripting languages, domain-specific languages, or concurrent/distributed languages (or a combination of these). Some general purpose languages were designed largely with educational goals. A programming language may also be classified by factors unrelated to the programming paradigm. For instance, most programming languages use English language keywords, while a minority do not. Other languages may be classified as being deliberately esoteric or not.|2023-09-25-11-33-19
ActionScript|| ActionScript is an object-oriented programming language originally developed by Macromedia Inc. (later acquired by Adobe). It is influenced by HyperTalk, the scripting language for HyperCard. It is now an implementation of ECMAScript (meaning it is a superset of the syntax and semantics of the language more widely known as JavaScript), though it originally arose as a sibling, both being influenced by HyperTalk. ActionScript code is usually converted to byte-code format by a compiler. ActionScript is used primarily for the development of websites and software targeting the Adobe Flash platform, originally finding use on web pages in the form of embedded SWF files. ActionScript 3 is also used with the Adobe AIR system for the development of desktop and mobile applications. The language itself is open-source in that its specification is offered free of charge and both an open-source compiler (as part of Apache Flex) and open-source virtual machine (Tamarin) are available. ActionScript was also used with Scaleform GFx for the development of three-dimensional video-game user interfaces and heads up displays.|2023-09-22-08-11-02
ActionScript|Overview| ActionScript was initially designed for controlling simple two-dimensional vector animations made in Adobe Flash (formerly Macromedia Flash). Initially focused on animation, early versions of Flash content offered few interactivity features, thus had very limited scripting ability. Later versions added functionality allowing for the creation of web-based games and rich web applications with streaming media (such as video and audio). Today, ActionScript is suitable for desktop and mobile development through Adobe AIR; it is used in some database applications and in basic robotics as in Make Controller Kit. Flash MX 2004 introduced ActionScript 2.0, a scripting language more suited to the development of Flash applications. Saving time is often possible by scripting something rather than animating it, which usually also enables a higher level of flexibility when editing. Since the arrival of the Flash Player 9 alpha (in 2006), a newer version of ActionScript has been released, ActionScript 3.0. This version of the language is intended to be compiled and run on a version of the Tamarin virtual machine, formely ActionScript Virtual Machine 2, that was also fully rewritten (dubbed AVM2). Because of this, code written in ActionScript 3.0 is generally targeted for Flash Player 9 and higher, and will not work in prior versions. At the same time, ActionScript 3.0 executes up to 10 times faster than legacy ActionScript code due to the just-in-time compiler enhancements. Flash libraries can be used with the XML abilities of the browser to render rich content in the browser. This technology is known as Asynchronous Flash and XML, much like AJAX. Adobe offers its Flex product line to meet the demand for rich web applications built on the Flash runtime, with behaviors and programming done in ActionScript. ActionScript 3.0 forms the foundation of the Flex 2 application programming interface (API).|2023-09-22-08-11-02
ActionScript|History|" ActionScript started as an object-oriented programming language for Macromedia's Flash authoring tool, later developed by Adobe Systems as Adobe Flash. The first three versions of the Flash authoring tool provided limited interactivity features. Early Flash developers could attach a simple command, called an ""action"", to a button or a frame. The set of actions was basic navigation controls, with commands such as ""play"", ""stop"", ""getURL"", and ""gotoAndPlay"". With the release of Flash 4 in 1999, this simple set of actions became a small scripting language. New capabilities introduced for Flash 4 included variables, expressions, operators, if statements, and loops. Although referred to internally as ""ActionScript"", the Flash 4 user manual and marketing documents continued to use the term ""actions"" to describe this set of commands."|2023-09-22-08-11-02
ActionScript|Timeline by ActionScript version|" With the release of Flash 5 in September 2000, the ""actions"" from Flash 4 were enhanced once more and named ""ActionScript"" for the first time. This was the first version of ActionScript with influences from JavaScript and the ECMA-262 (Third Edition) standard, supporting the said standard's object model and many of its core data types. Local variables may be declared with the var statement, and user-defined functions with parameter passing and return values can also be created. Notably, ActionScript could now also be typed with a text editor rather than being assembled by choosing actions from drop-down lists and dialog box controls. With the next release of its authoring tool, Flash MX, and its corresponding player, Flash Player 6, the language remained essentially unchanged; there were only minor changes, such as the addition of the switch statement and the ""strict equality"" (===) operator, which brought it closer to being ECMA-262-compliant. Two important features of ActionScript that distinguish it from later versions are its loose type system and its reliance on prototype-based inheritance. Loose typing refers to the ability of a variable to hold any type of data. This allows for rapid script development and is particularly well-suited for small-scale scripting projects. Prototype-based inheritance is the ActionScript 1.0 mechanism for code reuse and object-oriented programming. Instead of a class keyword that defines common characteristics of a class, ActionScript 1.0 uses a special object that serves as a ""prototype"" for a class of objects. All common characteristics of a class are defined in the class's prototype object and every instance of that class contains a link to that prototype object. The next major revision of the language, ActionScript 2.0, was introduced in September 2003 with the release of Flash MX 2004 and its corresponding player, Flash Player 7. In response to user demand for a language better equipped for larger and more complex applications, ActionScript 2.0 featured compile-time type checking and class-based syntax, such as the keywords class and extends.
(While this allowed for a more structured object-oriented programming approach, the code would still be compiled to ActionScript 1.0 bytecode, allowing it to be used on the preceding Flash Player 6 as well.
In other words, the class-based inheritance syntax was a layer on top of the existing prototype-based system.) With ActionScript 2.0, developers could constrain variables to a specific type by adding a type annotation so that type mismatch errors could be found at compile-time.
ActionScript 2.0 also introduced class-based inheritance syntax so that developers could create classes and interfaces, much as they would in class-based languages such as Java and C++. This version conformed partially to the ECMAScript Fourth Edition draft specification. In June 2006, ActionScript 3.0 debuted with Adobe Flex 2.0 and its corresponding player, Flash Player 9. ActionScript 3.0 was a fundamental restructuring of the language, so much so that it uses an entirely different virtual machine. Flash Player 9 contains two virtual machines, AVM1 for code written in ActionScript 1.0 and 2.0, and AVM2 for content written in ActionScript 3.0. ActionScript 3.0 added limited support for hardware acceleration (DirectX, OpenGL). The update to the language introduced several new features:"|2023-09-22-08-11-02
ActionScript|AIR| Adobe AIR supports ActionScript, in addition to some extended contents, such as the Stage3D engine Adobe has developed. The number of APIs (Application programming interfaces) available to ActionScript 3.0 has also risen dramatically.|2023-09-22-08-11-02
ActionScript|Syntax| ActionScript code is free form and thus may be created with whichever amount or style of whitespace that the author desires. The basic syntax is derived from ECMAScript.|2023-09-22-08-11-02
ActionScript|ActionScript 2.0|" The following code, which works in any compliant player, creates a text field at depth 0, at position (0, 0) on the screen (measured in pixels), that is 100 pixels wide and high. Then the text parameter is set to the ""Hello, world"" string, and it is automatically displayed in the player: When writing external ActionScript 2.0 class files the above example could be written in a file named Greeter.as as following."|2023-09-22-08-11-02
ActionScript|ActionScript 3.0| ActionScript 3.0 has a similar syntax to ActionScript 2.0, but a different set of APIs for creating objects. Compare the script below to the previous ActionScript 2.0 version: Minimal ActionScript 3.0 programs may be somewhat larger and more complicated due to the increased separation of the programming language and the Flash IDE. Presume the following file to be Greeter.as: ActionScript 2 can also be used in MXML files when using Apache's Flex framework:|2023-09-22-08-11-02
ActionScript|Data types|" ActionScript primarily consists of ""fundamental"" or ""simple"" data types that are used to create other data types. These data types are very similar to Java data types. Since ActionScript 3 was a complete rewrite of ActionScript 2, the data types and their inheritances have changed. ActionScript 2 top level data types ActionScript 2 complex data types There are additional ""complex"" data types. These are more processor and memory intensive and consist of many ""simple"" data types. For AS2, some of these data types are: ActionScript 3 primitive (prime) data types ActionScript 3 some complex data types"|2023-09-22-08-11-02
ActionScript|Using data types|" The basic syntax is: So in order to make an empty Object: Or, in an informal way: Some types are automatically put in place: Unlike some object-oriented languages, ActionScript makes no distinction between primitive types and reference types. In ActionScript, all variables are reference types. However, objects that belong to the primitive data types, which includes Boolean, Number, int, uint, and String, are immutable. So if a variable of a supposedly primitive type, e.g. an integer is passed to a function, altering that variable inside the function will not alter the original variable, as a new int Object is created when inside the function. If a variable of another (not primitive) datatype, e.g. XML is passed to a function, altering that variable inside the function will alter the original variable as well, as no new XML Object is created. Some data types can be assigned values with literals: A reference in ActionScript is a pointer to an instance of a class. A reference stores the memory address of an object ‚Äì operations against references will follow the value of the reference to the memory address of the object and carry out the operation on that object. All objects in ActionScript are accessed through references instead of being accessed directly. Only references to an object may be removed by using the ""delete"" keyword.  Removal of actual objects and data is done by the Flash Player garbage collector which checks for any existing references in the Flash memory space. If none are found (no other reference is made to the orphaned object), it is removed from memory.  For this reason, memory management in ActionScript requires careful application development planning."|2023-09-22-08-11-02
ActionScript|Code protection| Like most bytecode file formats, Flash SWF files can be decompiled into their source code and assets (similarly to how Microsoft .NET files can be decompiled). Some decompilers are capable of nearly full reconstruction of the original source file, down to the actual code that was used during creation (although results vary on a case-by-case basis). In opposition to the decompilers, ActionScript obfuscators have been introduced, which transform code into a form that breaks decompiler output while preserving the functionality and structure of the program. Higher-quality obfuscators implement lexical transformations such as identifier renaming, control flow transformation, and data abstraction transformation which collectively make it harder for decompilers to generate output likely to be useful to a human. Less robust obfuscators insert traps for decompilers. Such obfuscators either cause the decompiler software to crash unexpectedly or to generate unintelligible source code.[citation needed] The following is an example of ActionScript 3.0 code generated by a decompiler program, before and after obfuscation.[original research?] Code before obfuscation: Code after obfuscation:|2023-09-22-08-11-02
Ada (programming language)|| Ada is a structured, statically typed, imperative, and object-oriented high-level programming language, inspired by Pascal and other languages. It has built-in language support for design by contract (DbC), extremely strong typing, explicit concurrency, tasks, synchronous message passing, protected objects, and non-determinism. Ada improves code safety and maintainability by using the compiler to find errors in favor of runtime errors. Ada is an international technical standard, jointly defined by the International Organization for Standardization (ISO), and the International Electrotechnical Commission (IEC). As of 2020, the standard, called Ada 2012 informally, is ISO/IEC 8652:2012. Ada was originally designed by a team led by French computer scientist Jean Ichbiah of Honeywell under contract to the United States Department of Defense (DoD) from 1977 to 1983 to supersede over 450 programming languages used by the DoD at that time. Ada was named after Ada Lovelace (1815‚Äì1852), who has been credited as the first computer programmer.|2023-08-06-02-09-18
Ada (programming language)|Features|" Ada was originally designed for embedded and real-time systems. The Ada¬†95 revision, designed by S. Tucker Taft of Intermetrics between 1992 and 1995, improved support for systems, numerical, financial, and object-oriented programming (OOP). Features of Ada include: strong typing, modular programming mechanisms (packages), run-time checking, parallel processing (tasks, synchronous message passing, protected objects, and nondeterministic select statements), exception handling, and generics. Ada¬†95 added support for object-oriented programming, including dynamic dispatch. The syntax of Ada minimizes choices of ways to perform basic operations, and prefers English keywords (such as ""or else"" and ""and then"") to symbols (such as ""||"" and ""&&""). Ada uses the basic arithmetical operators ""+"", ""-"", ""*"", and ""/"", but avoids using other symbols. Code blocks are delimited by words such as ""declare"", ""begin"", and ""end"", where the ""end"" (in most cases) is followed by the identifier of the block it closes (e.g., if ... end if, loop ... end loop). In the case of conditional blocks this avoids a dangling else that could pair with the wrong nested if-expression in other languages like C or Java. Ada is designed for developing very large software systems. Ada packages can be compiled separately. Ada package specifications (the package interface) can also be compiled separately without the implementation to check for consistency. This makes it possible to detect problems early during the design phase, before implementation starts. A large number of compile-time checks are supported to help avoid bugs that would not be detectable until run-time in some other languages or would require explicit checks to be added to the source code.  For example, the syntax requires explicitly named closing of blocks to prevent errors due to mismatched end tokens. The adherence to strong typing allows detecting many common software errors (wrong parameters, range violations, invalid references, mismatched types, etc.) either during compile-time, or otherwise during run-time. As concurrency is part of the language specification, the compiler can in some cases detect potential deadlocks. Compilers also commonly check for misspelled identifiers, visibility of packages, redundant declarations, etc. and can provide warnings and useful suggestions on how to fix the error. Ada also supports run-time checks to protect against access to unallocated memory, buffer overflow errors, range violations, off-by-one errors, array access errors, and other detectable bugs. These checks can be disabled in the interest of runtime efficiency, but can often be compiled efficiently. It also includes facilities to help program verification. For these reasons, Ada is widely used in critical systems, where any anomaly might lead to very serious consequences, e.g., accidental death, injury or severe financial loss. Examples of systems where Ada is used include avionics, air traffic control, railways, banking, military and space technology. Ada's dynamic memory management is high-level and type-safe. Ada has no generic or untyped pointers; nor does it implicitly declare any pointer type. Instead, all dynamic memory allocation and deallocation must occur via explicitly declared access types. Each access type has an associated storage pool that handles the low-level details of memory management; the programmer can either use the default storage pool or define new ones (this is particularly relevant for Non-Uniform Memory Access). It is even possible to declare several different access types that all designate the same type but use different storage pools. Also, the language provides for accessibility checks, both at compile time and at run time, that ensures that an access value cannot outlive the type of the object it points to. Though the semantics of the language allow automatic garbage collection of inaccessible objects, most implementations do not support it by default, as it would cause unpredictable behaviour in real-time systems.  Ada does support a limited form of region-based memory management; also, creative use of storage pools can provide for a limited form of automatic garbage collection, since destroying a storage pool also destroys all the objects in the pool. A double-dash (""--""), resembling an em dash, denotes comment text.  Comments stop at end of line, to prevent unclosed comments from accidentally voiding whole sections of source code.  Disabling a whole block of code now requires the prefixing of each line (or column) individually with ""--"". While clearly denoting disabled code with a column of repeated ""--"" down the page, this renders the experimental dis/re-enablement of large blocks a more drawn out process. The semicolon ("";"") is a statement terminator, and the null or no-operation statement is null;. A single ; without a statement to terminate is not allowed. Unlike most ISO standards, the Ada language definition (known as the Ada Reference Manual or ARM, or sometimes the Language Reference Manual or LRM) is free content. Thus, it is a common reference for Ada programmers, not only programmers implementing Ada compilers. Apart from the reference manual, there is also an extensive rationale document which explains the language design and the use of various language constructs. This document is also widely used by programmers. When the language was revised, a new rationale document was written. One notable free software tool that is used by many Ada programmers to aid them in writing Ada source code is the GNAT Programming Studio, and GNAT which is part of the GNU Compiler Collection."|2023-08-06-02-09-18
Ada (programming language)|History|" In the 1970s the US Department of Defense (DoD) became concerned by the number of different programming languages being used for its embedded computer system projects, many of which were obsolete or hardware-dependent, and none of which supported safe modular programming. In 1975, a working group, the High Order Language Working Group (HOLWG), was formed with the intent to reduce this number by finding or creating a programming language generally suitable for the department's and the UK Ministry of Defence's requirements. After many iterations beginning with an original straw-man proposal the eventual programming language was named Ada. The total number of high-level programming languages in use for such projects fell from over 450 in 1983 to 37 by 1996. HOLWG crafted the Steelman language requirements, a series of documents stating the requirements they felt a programming language should satisfy. Many existing languages were formally reviewed, but the team concluded in 1977 that no existing language met the specifications. Requests for proposals for a new programming language were issued and four contractors were hired to develop their proposals under the names of Red (Intermetrics led by Benjamin Brosgol), Green (Honeywell, led by Jean Ichbiah), Blue (SofTech, led by John Goodenough) and Yellow (SRI International, led by Jay Spitzen). In April 1978, after public scrutiny, the Red and Green proposals passed to the next phase. In May 1979, the Green proposal, designed by Jean Ichbiah at Honeywell, was chosen and given the name Ada‚Äîafter Augusta Ada, Countess of Lovelace. This proposal was influenced by the language LIS that Ichbiah and his group had developed in the 1970s. The preliminary Ada reference manual was published in ACM SIGPLAN Notices in June 1979. The Military Standard reference manual was approved on December 10, 1980 (Ada Lovelace's birthday), and given the number MIL-STD-1815 in honor of Ada Lovelace's birth year. In 1981, C. A. R. Hoare took advantage of his Turing Award speech to criticize Ada for being overly complex and hence unreliable, but subsequently seemed to recant in the foreword he wrote for an Ada textbook. Ada attracted much attention from the programming community as a whole during its early days. Its backers and others predicted that it might become a dominant language for general purpose programming and not only defense-related work. Ichbiah publicly stated that within ten years, only two programming languages would remain: Ada and Lisp.  Early Ada compilers struggled to implement the large, complex language, and both compile-time and run-time performance tended to be slow and tools primitive.   Compiler vendors expended most of their efforts in passing the massive, language-conformance-testing, government-required Ada Compiler Validation Capability (ACVC) validation suite that was required in another novel feature of the Ada language effort. The Jargon File, a dictionary of computer hacker slang originating in 1975‚Äì1983, notes in an entry on Ada that ""it is precisely what one might expect given that kind of endorsement by fiat; designed by committee...difficult to use, and overall a disastrous, multi-billion-dollar boondoggle...Ada Lovelace...would almost certainly blanch at the use her name has been latterly put to; the kindest thing that has been said about it is that there is probably a good small language screaming to get out from inside its vast, elephantine bulk."" The first validated Ada implementation was the NYU Ada/Ed translator, certified on April 11, 1983. NYU Ada/Ed is implemented in the high-level set language SETL. Several commercial companies began offering Ada compilers and associated development tools, including Alsys, TeleSoft, DDC-I, Advanced Computer Techniques, Tartan Laboratories, Irvine Compiler, TLD Systems, and Verdix. Computer manufacturers who had a significant business in the defense, aerospace, or related industries, also offered Ada compilers and tools on their platforms; these included Concurrent Computer Corporation, Cray Research, Inc., Digital Equipment Corporation, Harris Computer Systems, and Siemens Nixdorf Informationssysteme AG. In 1991, the US Department of Defense began to require the use of Ada (the Ada mandate) for all software, though exceptions to this rule were often granted.  The Department of Defense Ada mandate was effectively removed in 1997, as the DoD began to embrace commercial off-the-shelf (COTS) technology. Similar requirements existed in other NATO countries: Ada was required for NATO systems involving command and control and other functions, and Ada was the mandated or preferred language for defense-related applications in countries such as Sweden, Germany, and Canada. By the late 1980s and early 1990s, Ada compilers had improved in performance, but there were still barriers to fully exploiting Ada's abilities, including a tasking model that was different from what most real-time programmers were used to. Because of Ada's safety-critical support features, it is now used not only for military applications, but also in commercial projects where a software bug can have severe consequences, e.g., avionics and air traffic control, commercial rockets such as the Ariane 4 and 5, satellites and other space systems, railway transport and banking.
For example, the Airplane Information Management System, the fly-by-wire system software in the Boeing 777, was written in Ada.  Developed by Honeywell Air Transport Systems in collaboration with consultants from DDC-I, it became arguably the best-known of any Ada project, civilian or military. The Canadian Automated Air Traffic System was written in 1 million lines of Ada (SLOC count). It featured advanced distributed processing, a distributed Ada database, and object-oriented design. Ada is also used in other air traffic systems, e.g., the UK's next-generation Interim Future Area Control Tools Support (iFACTS) air traffic control system is designed and implemented using SPARK Ada.
It is also used in the French TVM in-cab signalling system on the TGV high-speed rail system, and the metro suburban trains in Paris, London, Hong Kong and New York City."|2023-08-06-02-09-18
Ada (programming language)|Standardization| Preliminary Ada can be found in ACM Sigplan Notices Vol 14, No 6, June 1979. Ada was first published in 1980 as an ANSI standard ANSI/MIL-STD 1815. As this very first version held many errors and inconsistencies (see Summary of Ada Language Changes), the revised edition was published in 1983 as ANSI/MIL-STD 1815A. Without any further changes, it became an ISO standard in 1987, ISO 8652:1987. This version of the language is commonly known as Ada¬†83, from the date of its adoption by ANSI, but is sometimes referred to also as Ada¬†87, from the date of its adoption by ISO. This is the Ada 83 Reference Manual. There is also a French translation; DIN translated it into German as DIN 66268 in 1988. Ada¬†95, the joint ISO/IEC/ANSI standard ISO/IEC 8652:1995 (see Ada 95 RM) was published in February 1995, making it the first ISO standard object-oriented programming language. To help with the standard revision and future acceptance, the US Air Force funded the development of the GNAT Compiler. Presently, the GNAT Compiler is part of the GNU Compiler Collection. Work has continued on improving and updating the technical content of the Ada language. A Technical Corrigendum to Ada¬†95 was published in October 2001, ISO/IEC 8652:1995/Corr 1:2001 (see Ada 95 RM with TC 1), and a major Amendment, ISO/IEC 8652:1995/Amd 1:2007 (see Ada 2005 RM) was published on March 9, 2007, commonly known as Ada¬†2005 because work on the new standard was finished that year. At the Ada-Europe 2012 conference in Stockholm, the Ada Resource Association (ARA) and Ada-Europe announced the completion of the design of the latest version of the Ada language and the submission of the reference manual to the ISO/IEC JTC 1/SC 22/WG 9 of the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) for approval. ISO/IEC 8652:2012 (see Ada 2012 RM) was published in December 2012, known as Ada¬†2012. A technical corrigendum was published ISO/IEC 8652:2012/COR 1:2016 (see RM 2012 with TC 1). Despite the names Ada 83, 95 etc., legally there is only one Ada standard, the one of the last ISO/IEC standard: with the acceptance of a new standard version, the previous one becomes withdrawn. The other names are just informal ones referencing a certain edition. Other related standards include ISO/IEC 8651-3:1988 Information processing systems‚ÄîComputer graphics‚ÄîGraphical Kernel System (GKS) language bindings‚ÄîPart 3: Ada.|2023-08-06-02-09-18
Ada (programming language)|Language constructs| Ada is an ALGOL-like programming language featuring control structures with reserved words such as if, then, else, while, for, and so on. However, Ada also has many data structuring facilities and other abstractions which were not included in the original ALGOL 60, such as type definitions, records, pointers, enumerations. Such constructs were in part inherited from or inspired by Pascal.|2023-08-06-02-09-18
Ada (programming language)|"""Hello, world!"" in Ada"|" A common example of a language's syntax is the Hello world program:
(hello.adb) This program can be compiled by using the freely available open source compiler GNAT, by executing"|2023-08-06-02-09-18
Ada (programming language)|Data types|" Ada's type system is not based on a set of predefined primitive types but allows users to declare their own types. This declaration in turn is not based on the internal representation of the type but on describing the goal which should be achieved. This allows the compiler to determine a suitable memory size for the type, and to check for violations of the type definition at compile time and run time (i.e., range violations, buffer overruns, type consistency, etc.). Ada supports numerical types defined by a range, modulo types, aggregate types (records and arrays), and enumeration types. Access types define a reference to an instance of a specified type; untyped pointers are not permitted.
Special types provided by the language are task types and protected types. For example, a date might be represented as: Important to note: Day_type, Month_type, Year_type, Hours are incompatible types, meaning that for instance the following expression is illegal: The predefined plus-operator can only add values of the same type, so the expression is illegal. Types can be refined by declaring subtypes: Types can have modifiers such as limited, abstract, private etc. Private types do not show their inner structure; objects of limited types cannot be copied. Ada 95 adds further features for object-oriented extension of types."|2023-08-06-02-09-18
Ada (programming language)|Control structures|" Ada is a structured programming language, meaning that the flow of control is structured into standard statements. All standard constructs and deep-level early exit are supported, so the use of the also supported ""go to"" commands is seldom needed."|2023-08-06-02-09-18
Ada (programming language)|Packages, procedures and functions|" Among the parts of an Ada program are packages, procedures and functions. Example:
Package specification (example.ads) Package body (example.adb) This program can be compiled, e.g., by using the freely available open-source compiler GNAT, by executing Packages, procedures and functions can nest to any depth, and each can also be the logical outermost block. Each package, procedure or function can have its own declarations of constants, types, variables, and other procedures, functions and packages, which can be declared in any order."|2023-08-06-02-09-18
Ada (programming language)|Concurrency| Ada has language support for task-based concurrency. The fundamental concurrent unit in Ada is a task, which is a built-in limited type. Tasks are specified in two parts ‚Äì the task declaration defines the task interface (similar to a type declaration), the task body specifies the implementation of the task. Depending on the implementation, Ada tasks are either mapped to operating system threads or processes, or are scheduled internally by the Ada runtime. Tasks can have entries for synchronisation (a form of synchronous message passing). Task entries are declared in the task specification. Each task entry can have one or more accept statements within the task body. If the control flow of the task reaches an accept statement, the task is blocked until the corresponding entry is called by another task (similarly, a calling task is blocked until the called task reaches the corresponding accept statement). Task entries can have parameters similar to procedures, allowing tasks to synchronously exchange data. In conjunction with select statements it is possible to define guards on accept statements (similar to Dijkstra's guarded commands). Ada also offers protected objects for mutual exclusion. Protected objects are a monitor-like construct, but use guards instead of conditional variables for signaling (similar to conditional critical regions). Protected objects combine the data encapsulation and safe mutual exclusion from monitors, and entry guards from conditional critical regions.  The main advantage over classical monitors is that conditional variables are not required for signaling, avoiding potential deadlocks due to incorrect locking semantics. Like tasks, the protected object is a built-in limited type, and it also has a declaration part and a body. A protected object consists of encapsulated private data (which can only be accessed from within the protected object), and procedures, functions and entries which are guaranteed to be mutually exclusive (with the only exception of functions, which are required to be side effect free and can therefore run concurrently with other functions). A task calling a protected object is blocked if another task is currently executing inside the same protected object, and released when this other task leaves the protected object. Blocked tasks are queued on the protected object ordered by time of arrival. Protected object entries are similar to procedures, but additionally have guards. If a guard evaluates to false, a calling task is blocked and added to the queue of that entry; now another task can be admitted to the protected object, as no task is currently executing inside the protected object. Guards are re-evaluated whenever a task leaves the protected object, as this is the only time when the evaluation of guards can have changed. Calls to entries can be requeued to other entries with the same signature. A task that is requeued is blocked and added to the queue of the target entry; this means that the protected object is released and allows admission of another task. The select statement in Ada can be used to implement non-blocking entry calls and accepts, non-deterministic selection of entries (also with guards), time-outs and aborts. The following example illustrates some concepts of concurrent programming in Ada.|2023-08-06-02-09-18
Ada (programming language)|Pragmas| A pragma is a compiler directive that conveys information to the compiler to allow specific manipulating of compiled output.  Certain pragmas are built into the language, while others are implementation-specific. Examples of common usage of compiler pragmas would be to disable certain features, such as run-time type checking or array subscript boundary checking, or to instruct the compiler to insert object code instead of a function call (as C/C++ does with inline functions).|2023-08-06-02-09-18
Ada (programming language)|Generics| Ada has had generics since it was first designed in 1977‚Äì1980. The standard library uses generics to provide many services. Ada 2005 adds a comprehensive generic container library to the standard library, which was inspired by C++'s standard template library. A generic unit is a package or a subprogram that takes one or more generic formal parameters. A generic formal parameter is a value, a variable, a constant, a type, a subprogram, or even an instance of another, designated, generic unit. For generic formal types, the syntax distinguishes between discrete, floating-point, fixed-point, access (pointer) types, etc. Some formal parameters can have default values.|2023-08-06-02-09-18
Ada (programming language)|Rationale| These documents have been published in various forms, including print.|2023-08-06-02-09-18
Address programming language|| The Address programming language (Russian: –ê–¥—Ä–µ—Å–Ω—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è) is one of the world's first high-level programming languages. It was created in 1955 by Kateryna Yushchenko. In particular, the Address programming language made possible indirect addressing and addresses of the highest rank¬†‚Äì  analogous to pointers. Unlike Fortran and ALGOL 60, APL (Address programming language) supported indirect addressing and addressing of higher ranks. Indirect addressing is a mechanism that appeared in other programming languages much later (1964¬†‚Äì  in PL/1). The Address language was implemented on all the computers of the first and second generation produced in the Soviet Union. The Address language influenced the architecture of the Kyiv, M-20, Dnipro, Ural, Promin and Minsk computers. The Address programming language was used exclusively for the solution of economical problems, including aviation, space exploration, machine building, and military complex¬†‚Äì  in particular, to calculate the trajectories of ballistic missiles in flight¬†‚Äì  in the 1950‚Äì60s. Implementations of the Address programming language were used for nearly 20 years. A book about APL was published in Ukraine in 1963 and it was translated and published in France in 1974. The Address language affected not only the Soviet Union's and other socialist countries economical development, but information technology and programming of over the world. APL's proposed and implemented ideas and tools can be found in many programming-related fields, such as abstract data types, object-oriented programming, functional programming, logical programming, databases and artificial intelligence.|2023-09-19-11-45-52
Address programming language|Books| <https://files.infoua.net/yushchenko/Elements-de-programmation-sur-ordinateurs_BGnedenko-VKoroliouk-EIouchtchenko_1969_France_OCR.pdf>.|2023-09-19-11-45-52
Agda (programming language)||" Agda is a dependently typed functional programming language originally developed by Ulf Norell at Chalmers University of Technology with implementation described in his PhD thesis. The original Agda system was developed at Chalmers by Catarina Coquand in 1999. The current version, originally known as Agda 2, is a full rewrite, which should be considered a new language that shares a name and tradition. Agda is also a proof assistant based on the propositions-as-types paradigm, but unlike Coq, has no separate tactics language, and proofs are written in a functional programming style. The language has ordinary programming constructs such as data types, pattern matching, records, let expressions and modules, and a Haskell-like syntax. The system has Emacs, Atom, and VS Code interfaces but can also be run in batch mode from the command line. Agda is based on Zhaohui Luo's unified theory of dependent types (UTT), a type theory similar to Martin-L√∂f type theory. Agda is named after the Swedish song ""H√∂nan Agda"", written by Cornelis Vreeswijk, which is about a hen named Agda. This alludes to the name of the theorem prover Coq, which was named after Thierry Coquand, Catarina Coquand's husband."|2023-09-19-06-13-48
Agda (programming language)|Inductive types|" The main way of defining data types in Agda is via inductive data types which are similar to algebraic data types in non-dependently typed programming languages. Here is a definition of Peano numbers in Agda: Basically, it means that there are two ways to construct a value of type 




N



{\displaystyle \mathbb {N} }

, representing a natural number. To begin, zero is a natural number, and if n is a natural number, then suc n, standing for the successor of n, is a natural number too. Here is a definition of the ""less than or equal"" relation between two natural numbers: The first constructor, z‚â§n, corresponds to the axiom that zero is less than or equal to any natural number. The second constructor, s‚â§s, corresponds to an inference rule, allowing to turn a proof of n ‚â§ m into a proof of suc n ‚â§ suc m. So the value s‚â§s {zero} {suc zero} (z‚â§n {suc zero}) is a proof that one (the successor of zero), is less than or equal to two (the successor of one). The parameters provided in curly brackets may be omitted if they can be inferred."|2023-09-19-06-13-48
Agda (programming language)|Dependently typed pattern matching| In core type theory, induction and recursion principles are used to prove theorems about inductive types. In Agda, dependently typed pattern matching is used instead. For example, natural number addition can be defined like this: This way of writing recursive functions/inductive proofs is more natural than applying raw induction principles. In Agda, dependently typed pattern matching is a primitive of the language; the core language lacks the induction/recursion principles that pattern matching translates to.|2023-09-19-06-13-48
Agda (programming language)|Metavariables| One of the distinctive features of Agda, when compared with other similar systems such as Coq, is heavy reliance on metavariables for program construction. For example, one can write functions like this in Agda: ? here is a metavariable. When interacting with the system in emacs mode, it will show the user expected type and allow them to refine the metavariable, i.e., to replace it with more detailed code. This feature allows incremental program construction in a way similar to tactics-based proof assistants such as Coq.|2023-09-19-06-13-48
Agda (programming language)|Proof automation|" Programming in pure type theory involves a lot of tedious and repetitive proofs. Although Agda has no separate tactics language, it is possible to program useful tactics within Agda itself. Typically, this works by writing an Agda function that optionally returns a proof of some property of interest. A tactic is then constructed by running this function at type-checking time, for example using the following auxiliary definitions: Given a function check-even¬†: (n¬†: 




N



{\displaystyle \mathbb {N} }

) ‚Üí Maybe (Even n) that inputs a number and optionally returns a proof of its evenness, a tactic can then be constructed as follows: The actual proof of each lemma will be automatically constructed at type-checking time. If the tactic fails, type-checking will fail. Additionally, to write more complex tactics, Agda has support for automation via reflection. The reflection mechanism allows one to quote program fragments into ‚Äì or unquote them from ‚Äì the abstract syntax tree. The way reflection is used is similar to the way Template Haskell works. Another mechanism for proof automation is proof search action in emacs mode. It enumerates possible proof terms (limited to 5 seconds), and if one of the terms fits the specification, it will be put in the meta variable where the action is invoked. This action accepts hints, e.g., which theorems and from which modules can be used, whether the action can use pattern matching, etc."|2023-09-19-06-13-48
Agda (programming language)|Termination checking| Agda is a total language, i.e., each program in it must terminate and all possible patterns must be matched. Without this feature, the logic behind the language becomes inconsistent, and it becomes possible to prove arbitrary statements. For termination checking, Agda uses the approach of the Foetus termination checker.|2023-09-19-06-13-48
Agda (programming language)|Standard library| Agda has an extensive de facto standard library, which includes many useful definitions and theorems about basic data structures, such as natural numbers, lists, and vectors. The library is in beta, and is under active development.|2023-09-19-06-13-48
Agda (programming language)|Unicode| One of the more notable features of Agda is a heavy reliance on Unicode in program source code. The standard emacs mode uses shortcuts for input, such as \Sigma for Œ£.|2023-09-19-06-13-48
Agda (programming language)|Backends| There are two compiler backends, MAlonzo for Haskell and one for JavaScript.|2023-09-19-06-13-48
ALGOL 68||" ALGOL 68 (short for Algorithmic Language 1968) is an imperative programming language that was conceived as a successor to the ALGOL 60 programming language, designed with the goal of a much wider scope of application and more rigorously defined syntax and semantics. The complexity of the language's definition, which runs to several hundred pages filled with non-standard terminology, made compiler implementation difficult and it was said it had ""no implementations and no users"". This was only partly true; ALGOL 68 did find use in several niche markets, notably in the United Kingdom where it was popular on International Computers Limited (ICL) machines, and in teaching roles. Outside these fields, use was relatively limited. Nevertheless, the contributions of ALGOL 68 to the field of computer science have been deep, wide-ranging and enduring, although many of these contributions were only publicly identified when they had reappeared in subsequently developed programming languages. Many languages were developed specifically as a response to the perceived complexity of the language, the most notable being Pascal, or were reimplementations for specific roles, like Ada. Many languages of the 1970s trace their design specifically to ALGOL 68, selecting some features while abandoning others that were considered too complex or out-of-scope for given roles. Among these is the language C, which was directly influenced by ALGOL 68, especially by its strong typing and structures. Most modern languages trace at least some of their syntax to either C or Pascal, and thus directly or indirectly to ALGOL 68."|2023-08-25-20-26-29
ALGOL 68|Overview|" ALGOL 68 features include expression-based syntax, user-declared types and structures/tagged-unions, a reference model of variables and reference parameters, string, array and matrix slicing, and concurrency. ALGOL 68 was designed by the International Federation for Information Processing (IFIP) IFIP Working Group 2.1 on Algorithmic Languages and Calculi. On December 20, 1968, the language was formally adopted by the group, and then approved for publication by the General Assembly of IFIP. ALGOL 68 was defined using a formalism, a two-level formal grammar, invented by Adriaan van Wijngaarden. Van Wijngaarden grammars use a context-free grammar to generate an infinite set of productions that will recognize a particular ALGOL 68 program; notably, they are able to express the kind of requirements that in many other programming language technical standards are labelled semantics, and must be expressed in ambiguity-prone natural language prose, and then implemented in compilers as ad hoc code attached to the formal language parser. ALGOL 68 was the first (and possibly one of the last) major language for which a full formal definition was made before it was implemented. C. H. A. Koster The main aims and principles of design of ALGOL 68: ALGOL 68 has been criticized, most prominently by some members of its design committee such as C. A. R. Hoare and Edsger Dijkstra, for abandoning the simplicity of ALGOL 60, becoming a vehicle for complex or overly general ideas, and doing little to make the compiler writer's task easier, in contrast to deliberately simple contemporaries (and competitors) such as C, S-algol and Pascal. In 1970, ALGOL 68-R became the first working compiler for ALGOL 68. In the 1973 revision, certain features ‚Äî such as proceduring, gommas and formal bounds ‚Äî were omitted. C.f. The language of the unrevised report.r0 Though European defence agencies (in Britain Royal Signals and Radar Establishment (RSRE)) promoted the use of ALGOL 68 for its expected security advantages, the American side of the NATO alliance decided to develop a different project, the language Ada, making its use obligatory for US defense contracts. ALGOL 68 also had a notable influence in the Soviet Union, details of which can be found in Andrey Ershov's 2014 paper: ""ALGOL 68 and Its Impact on the USSR and Russian Programming"", and ""–ê–ª–≥–æ–ª 68 –∏ –µ–≥–æ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –°–°–°–† –∏ –†–æ—Å—Å–∏–∏"". Steve Bourne, who was on the ALGOL 68 revision committee, took some of its ideas to his Bourne shell (and thereby, to descendant Unix shells such as Bash) and to C (and thereby to descendants such as C++). The complete history of the project can be found in C. H. Lindsey's A History of ALGOL 68. For a full-length treatment of the language, see ""Programming ALGOL 68 Made Easy"" by Dr. Sian Mountbatten, or ""Learning ALGOL 68 Genie"" by Marcel van der Veer which includes the Revised Report."|2023-08-25-20-26-29
ALGOL 68|Origins| ALGOL 68, as the name implies, is a follow-on to the ALGOL language that was first formalized in 1960. That same year the International Federation for Information Processing (IFIP) formed and started the Working Group on ALGOL, or WG2.1. This group released an updated ALGOL 60 specification in Rome in April 1962. At a follow-up meeting in March 1964, it was agreed that the group should begin work on two follow-on standards, ALGOL X which would be a redefinition of the language with some additions, and an ALGOL Y, which would have the ability to modify its own programs in the style of the language LISP.|2023-08-25-20-26-29
ALGOL 68|Definition process|" The first meeting of the ALGOL X group was held in Princeton University in May 1965. A report of the meeting noted two broadly supported themes, the introduction of strong typing and interest in Euler's concepts of 'trees' or 'lists' for handling collections. At the second meeting in October in France, three formal proposals were presented, Niklaus Wirth's ALGOL W along with comments about record structures by C.A.R. (Tony) Hoare, a similar language by Gerhard Seegm√ºller, and a paper by Adriaan van Wijngaarden on ""Orthogonal design and description of a formal language"". The latter, written in almost indecipherable ""W-Grammar"", proved to be a decisive shift in the evolution of the language. The meeting closed with an agreement that van Wijngaarden would re-write the Wirth/Hoare submission using his W-Grammar. This seemingly simple task ultimately proved more difficult than expected, and the follow-up meeting had to be delayed six months. When it met in April 1966 in Kootwijk, van Wijngaarden's draft remained incomplete and Wirth and Hoare presented a version using more traditional descriptions. It was generally agreed that their paper was ""the right language in the wrong formalism"". As these approaches were explored, it became clear there was a difference in the way parameters were described that would have real-world effects, and while Wirth and Hoare protested that further delays might become endless, the committee decided to wait for van Wijngaarden's version. Wirth then implemented their current definition as ALGOL W. At the next meeting in Warsaw in October 1966, there was an initial report from the I/O Subcommittee who had met at the Oak Ridge National Laboratory and the University of Illinois but had not yet made much progress. The two proposals from the previous meeting were again explored, and this time a new debate emerged about the use of pointers; ALGOL W used them only to refer to records, while van Wijngaarden's version could point to any object. To add confusion, John McCarthy presented a new proposal for operator overloading and the ability to string together and or constructs, and Klaus Samelson wanted to allow anonymous functions. In the resulting confusion, there was some discussion of abandoning the entire effort. The confusion continued through what was supposed to be the ALGOL Y meeting in Zandvoort in May 1967."|2023-08-25-20-26-29
ALGOL 68|Publication|" A draft report was finally published in February 1968. This was met by ""shock, horror and dissent"", mostly due to the hundreds of pages of unreadable grammar and odd terminology. Charles H. Lindsey attempted to figure out what ""language was hidden inside of it"", a process that took six man-weeks of effort. The resulting paper, ""ALGOL 68 with fewer tears"", was widely circulated. At a wider information processing meeting in Zurich in May 1968, attendees complained that the language was being forced upon them and that IFIP was ""the true villain of this unreasonable situation"" as the meetings were mostly closed and there was no formal feedback mechanism. Wirth and Peter Naur formally resigned their authorship positions in WG2.1 at that time. The next WG2.1 meeting took place in Tirrenia in June 1968. It was supposed to discuss the release of compilers and other issues, but instead devolved into a discussion on the language. van Wijngaarden responded by saying (or threatening) that he would release only one more version of the report. By this point Naur, Hoare, and Wirth had left the effort, and several more were threatening to do so. Several more meetings followed, North Berwick in August 1968, Munich in December which produced the release of the official Report in January 1969 but also resulted in a contentious Minority Report being written. Finally, at Banff, Alberta in September 1969, the project was generally considered complete and the discussion was primarily on errata and a greatly expanded Introduction to the Report. The effort took five years, burned out many of the greatest names in computer science, and on several occasions became deadlocked over issues both in the definition and the group as a whole. Hoare released a ""Critique of ALGOL 68"" almost immediately, which has been widely referenced in many works. Wirth went on to further develop the ALGOL W concept and released this as Pascal in 1970."|2023-08-25-20-26-29
ALGOL 68|Implementations|" The first implementation of the standard, based on the late-1968 draft Report, was introduced by the Royal Radar Establishment in the UK as ALGOL 68-R in July 1970. This was, however, a subset of the full language, and Barry Mailloux, the final editor of the Report, joked that ""It is a question of morality.  We have a Bible and you are sinning!"" This version nevertheless became very popular on the ICL machines, and became a widely-used language in military coding, especially in the UK. Among the changes in 68-R was the requirement for all variables to be declared before their first use. This had a significant advantage that it allowed the compiler to be one-pass, as space for the variables in the activation record was set aside before it was used. However, this change also had the side-effect of demanding the PROCs be declared twice, once as a declaration of the types, and then again as the body of code. Another change was to eliminate the assumed VOID mode, an expression that returns no value (named a statement in other languages) and demanding the word VOID be added where it would have been assumed. Further, 68-R eliminated the explicit parallel processing commands based on PAR. The first full implementation of the language was introduced in 1974 by CDC Netherlands for the Control Data mainframe series. This saw limited use, mostly teaching in Germany and the Netherlands. A version similar to 68-R was introduced from Carnegie Mellon University in 1976 as 68S, and was again a one-pass compiler based on various simplifications of the original and intended for use on smaller machines like the DEC PDP-11. It too was used mostly for teaching purposes. A version for IBM mainframes did not become available until 1978, when one was released from Cambridge University. This was ""nearly complete"". Lindsey released a version for small machines including the IBM PC in 1984. Three open source Algol 68 implementations are known:"|2023-08-25-20-26-29
ALGOL 68|The Algorithmic Language ALGOL 68 Reports and Working Group members|" ""Van Wijngaarden once characterized the four authors, somewhat tongue-in-cheek, as: Koster: transputter, Peck: syntaxer, Mailloux: implementer, Van Wijngaarden: party ideologist."" ‚Äì Koster."|2023-08-25-20-26-29
ALGOL 68|Timeline of standardization|" 1968: On 20 December 1968, the ""Final Report"" (MR 101) was adopted by the Working Group, then subsequently approved by the General Assembly of UNESCO's IFIP for publication. Translations of the standard were made for Russian, German, French and Bulgarian, and then later Japanese and Chinese. The standard was also made available in Braille. 1984: TC 97 considered ALGOL 68 for standardisation as ""New Work Item"" TC97/N1642 [2][3]. West Germany, Belgium, Netherlands, USSR and Czechoslovakia willing to participate in preparing the standard but the USSR and Czechoslovakia ""were not the right kinds of member of the right ISO committees""[4] and Algol 68's ISO standardisation stalled.[5] 1988: Subsequently ALGOL 68 became one of the GOST standards in Russia."|2023-08-25-20-26-29
ALGOL 68|Bold symbols and reserved words|" The standard language contains about sixty reserved words, typically bolded in print, and some with ""brief symbol"" equivalents:"|2023-08-25-20-26-29
ALGOL 68|Units: Expressions| The basic language construct is the unit. A unit may be a formula, an enclosed clause, a routine text or one of several technically needed constructs (assignation, jump, skip, nihil). The technical term enclosed clause unifies some of the inherently bracketing constructs known as block, do statement, switch statement in other contemporary languages. When keywords are used, generally the reversed character sequence of the introducing keyword is used for terminating the enclosure, e.g. ( IF ~ THEN ~ ELSE ~ FI, CASE ~ IN ~ OUT ~ ESAC, FOR ~ WHILE ~ DO ~ OD ). This Guarded Command syntax was reused by Stephen Bourne in the common Unix Bourne shell. An expression may also yield a multiple value, which is constructed from other values by a collateral clause. This construct just looks like the parameter pack of a procedure call.|2023-08-25-20-26-29
ALGOL 68|mode: Declarations|" The basic data types (called modes in Algol 68 parlance) are real, int, compl (complex number), bool, char, bits and bytes. For example: However, the declaration REAL x; is just syntactic sugar for REF REAL x = LOC REAL;. That is, x is really the constant identifier for a reference to a newly generated local REAL variable. Furthermore, instead of defining both float and double, or int and long and short, etc., ALGOL 68 provides modifiers, so that the presently common double would be written as LONG REAL or LONG LONG REAL instead, for example. The prelude constants max real and min long int are provided to adapt programs to different implementations. All variables need to be declared, but declaration does not have to precede the first use. primitive-declarer: INT, REAL, COMPL, COMPLEXG, BOOL, CHAR, STRING, BITS, BYTES, FORMAT, FILE, PIPEG, CHANNEL, SEMA Complex types can be created from simpler ones using various type constructors: For some examples, see Comparison of ALGOL 68 and C++. Other declaration symbols include:  FLEX, HEAP, LOC, REF, LONG, SHORT, EVENTS A name for a mode (type) can be declared using a MODE declaration,
which is similar to TYPEDEF in C/C++ and TYPE in Pascal: This is similar to the following C code: For ALGOL 68, only the NEWMODE mode-indication appears to the left of the equals symbol, and most notably the construction is made, and can be read, from left to right without regard to priorities. Also, the lower bound of Algol 68 arrays is one by default, but can be any integer from -max int to max int. Mode declarations allow types to be recursive: defined directly or indirectly in terms of themselves.
This is subject to some restrictions ‚Äì for instance, these declarations are illegal: while these are valid:"|2023-08-25-20-26-29
ALGOL 68|Coercions: casting|" The coercions produce a coercee from a coercend according to three criteria: the a priori mode of the coercend before the application of any coercion, the a posteriori mode of the coercee required after those coercions, and the syntactic position or ""sort"" of the coercee. Coercions may be cascaded. The six possible coercions are termed deproceduring, dereferencing, uniting, widening, rowing, and voiding. Each coercion, except for uniting, prescribes a corresponding dynamic effect on the associated values. Hence, many primitive actions can be programmed implicitly by coercions. Context strength ‚Äì allowed coercions: ALGOL 68 has a hierarchy of contexts which determine the kind of coercions available at a particular point in the program. These contexts are: Also: Widening occurs if there is no loss of precision. For example: An INT will be coerced to a REAL, and a REAL will be coerced to a LONG REAL. But not vice versa.  Examples: A variable can also be coerced (rowed) to an array of length 1. For example: UNION(INT,REAL) var¬†:= 1 IF ~ THEN ... FI and 
FROM ~ BY ~ TO ~ WHILE ~ DO ... OD etc For more details about Primaries, Secondaries, Tertiary & Quaternaries refer to Operator precedence."|2023-08-25-20-26-29
ALGOL 68|pr & co: Pragmats and Comments|" Pragmats are directives in the program, typically hints to the compiler; in newer languages these are called ""pragmas"" (no 't'). e.g. Comments can be inserted in a variety of ways: Normally, comments cannot be nested in ALGOL 68. This restriction can be circumvented by using different comment delimiters (e.g. use hash only for temporary code deletions)."|2023-08-25-20-26-29
ALGOL 68|Expressions and compound statements|" ALGOL 68 being an expression-oriented programming language, the value returned by an assignment statement is a reference to the destination. Thus, the following is valid ALGOL 68 code: This notion is present in C and Perl, among others. Note that as in earlier languages such as Algol 60 and FORTRAN, spaces are allowed in identifiers, so that half pi is a single identifier (thus avoiding the underscores versus camel case versus all lower-case issues). As another example, to express the mathematical idea of a sum of f(i) from i=1 to n, the following ALGOL 68 integer expression suffices: Note that, being an integer expression, the former block of code can be used in any context where an integer value can be used. A block of code returns the value of the last expression it evaluated; this idea is present in Lisp, among other languages. Compound statements are all terminated by distinctive closing brackets: This scheme not only avoids the dangling else problem but also avoids having to use BEGIN and END in embedded statement sequences. Choice clause example with Brief symbols: Choice clause example with Bold symbols: Choice clause example mixing Bold and Brief symbols: Algol68 allowed the switch to be of either type INT or (uniquely) UNION. The latter allows the enforcing strong typing onto UNION variables. c.f. union below for example. This was considered the ""universal"" loop, the full syntax is: The construct have several unusual aspects: Subsequent ""extensions"" to the standard Algol68 allowed the TO syntactic element to be replaced with UPTO and DOWNTO to achieve a small optimisation.  The same compilers also incorporated: Further examples can be found in the code examples below."|2023-08-25-20-26-29
ALGOL 68|struct, union & [:]: Structures, unions and arrays| ALGOL 68 supports arrays with any number of dimensions, and it allows for the slicing of whole or partial rows or columns. Matrices can be sliced either way, e.g.: ALGOL 68 supports multiple field structures (STRUCT) and united modes. Reference variables may point to any MODE including array slices and structure fields. For an example of all this, here is the traditional linked list declaration: Usage example for UNION CASE of NODE:|2023-08-25-20-26-29
ALGOL 68|proc: Procedures|" Procedure (PROC) declarations require type specifications for both the parameters and the result (VOID if none): or, using the ""brief"" form of the conditional statement: The return value of a proc is the value of the last expression evaluated in the procedure. References to procedures (ref proc) are also permitted. Call-by-reference parameters are provided by specifying references (such as ref real) in the formal argument list. The following example defines a procedure that applies a function (specified as a parameter) to each element of an array: This simplicity of code was unachievable in ALGOL 68's predecessor ALGOL 60."|2023-08-25-20-26-29
ALGOL 68|op: Operators|" The programmer may define new operators and both those and the pre-defined ones may be overloaded and their priorities may be changed by the coder. The following example defines operator MAX with both dyadic and monadic versions (scanning across the elements of an array). These are technically not operators, rather they are considered ""units associated with names"" -, ABS, ARG, BIN, ENTIER, LENG, LEVEL, ODD, REPR, ROUND, SHORTEN -:=, +:=, *:=, /:=,¬†%:=,¬†%*:=, +=: Specific details: These are technically not operators, rather they are considered ""units associated with names"" Note: Quaternaries include names SKIP and ~. "":=:"" (alternatively ""IS"") tests if two pointers are equal; "":/=:"" (alternatively ""ISNT"") tests if they are unequal. Why¬†:=: and¬†:/=: are needed: Consider trying to compare two pointer values, such as the following variables, declared as pointers-to-integer: Now consider how to decide whether these two are pointing to the same location, or whether one of them is pointing to NIL. The following expression will dereference both pointers down to values of type INT, and compare those, since the ""="" operator is defined for INT, but not REF INT. It is not legal to define ""="" for operands of type REF INT and INT at the same time, because then calls become ambiguous, due to the implicit coercions that can be applied: should the operands be left as REF INT and that version of the operator called? Or should they be dereferenced further to INT and that version used instead? Therefore the following expression can never be made legal: Hence the need for separate constructs not subject to the normal coercion rules for operands to operators. But there is a gotcha. The following expressions: while legal, will probably not do what might be expected. They will always return FALSE, because they are comparing the actual addresses of the variables ip and jp, rather than what they point to. To achieve the right effect, one would have to write Most of Algol's ""special"" characters (‚äÇ, ‚â°, ‚ê£, √ó, √∑, ‚â§, ‚â•, ‚â†, ¬¨, ‚äÉ, ‚â°, ‚à®, ‚àß, ‚Üí, ‚Üì, ‚Üë, ‚åä, ‚åà, ‚é©, ‚éß, ‚ä•, ‚è®, ¬¢, ‚óã and ‚ñ°) can be found on the IBM 2741 keyboard with the APL ""golf-ball"" print head inserted; these became available in the mid-1960s while ALGOL 68 was being drafted. These characters are also part of the Unicode standard and most of them are available in several popular fonts."|2023-08-25-20-26-29
ALGOL 68|transput: Input and output|" Transput is the term used to refer to ALGOL 68's input and output facilities. It includes pre-defined procedures for unformatted, formatted and binary transput. Files and other transput devices are handled in a consistent and machine-independent manner. The following example prints out some unformatted output to the standard output device: Note the predefined procedures newpage and newline passed as arguments. The TRANSPUT is considered to be of BOOKS, CHANNELS and FILES: ""Formatted transput"" in ALGOL 68's transput has its own syntax and patterns (functions), with FORMATs embedded between two $ characters. Examples:"|2023-08-25-20-26-29
ALGOL 68|par: Parallel processing| ALGOL 68 supports programming of parallel processing. Using the keyword PAR, a collateral clause is converted to a parallel clause, where the synchronisation of actions is controlled using semaphores. In A68G the parallel actions are mapped to threads when available on the hosting operating system. In A68S a different paradigm of parallel processing was implemented (see below).|2023-08-25-20-26-29
ALGOL 68|Code sample| This sample program implements the Sieve of Eratosthenes to find all the prime numbers that are less than 100. NIL is the ALGOL 68 analogue of the null pointer in other languages. The notation x OF y accesses a member x of a STRUCT y.|2023-08-25-20-26-29
ALGOL 68|Operating systems written in ALGOL 68| Note: The Soviet Era computers –≠–ª—å–±—Ä—É—Å-1 (Elbrus-1) and –≠–ª—å–±—Ä—É—Å-2 were created using high-level language –≠–ª—å-76 (AL-76), rather than the traditional assembly. –≠–ª—å-76 resembles Algol-68, The main difference is the dynamic binding types in –≠–ª—å-76 supported at the hardware level. –≠–ª—å-76 is used for application, job control, system programming.|2023-08-25-20-26-29
ALGOL 68|Applications| Both ALGOL 68C and ALGOL 68-R are written in ALGOL 68, effectively making ALGOL 68 an application of itself. Other applications include:|2023-08-25-20-26-29
ALGOL 68|Program representation|" A feature of ALGOL 68, inherited from the ALGOL tradition, is its different representations. There is a representation language used to describe algorithms in printed work, a strict language (rigorously defined in the Report), and an official reference language intended to be used in compiler input. The examples contain BOLD typeface words, this is the STRICT language. ALGOL 68's reserved words are effectively in a different namespace from identifiers, and spaces are allowed in identifiers, so this next fragment is legal: The programmer who writes executable code does not always have an option of BOLD typeface or underlining in the code as this may depend on hardware and cultural issues. Different methods to denote these identifiers have been devised. This is called a stropping regime. For example, all or some of the following may be available programming representations: All implementations must recognize at least POINT, UPPER and RES inside PRAGMAT sections. Of these, POINT and UPPER stropping are quite common, while RES stropping is a contradiction to the specification (as there are no reserved words). QUOTE (single apostrophe quoting) was the original recommendation, while matched apostrophe quoting, common in ALGOL 60, is not used much in ALGOL 68. The following characters were recommended for portability, and termed ""worthy characters"" in the Report on the Standard Hardware Representation of Algol 68 Archived 2014-01-02 at the Wayback Machine: This reflected a problem in the 1960s where some hardware didn't support lower-case, nor some other non-ASCII characters, indeed in the 1973 report it was written: ""Four worthy characters ‚Äî ""|"", ""_"", ""["", and ""]"" ‚Äî are often coded differently, even at installations which nominally use the same character set."""|2023-08-25-20-26-29
ALGOL 68|Example of different program representations|" ALGOL 68 allows for every natural language to define its own set of keywords Algol-68. As a result, programmers are able to write programs using keywords from their native language. Below is an example of a simple procedure that calculates ""the day following"", the code is in two languages: English and German.[citation needed] Russian/Soviet example:
In English Algol68's case statement reads CASE ~ IN ~ OUT ~ ESAC, in Cyrillic this reads –≤—ã–± ~ –≤ ~ –ª–∏–±–æ ~ –±—ã–≤."|2023-08-25-20-26-29
ALGOL 68|Some Vanitas|" For its technical intricacies, ALGOL 68 needs a cornucopia of methods to deny the existence of something: c.f. below for other examples of ‚Ñµ. The term NIL IS var always evaluates to TRUE for any variable (but see above for correct use of IS¬†:/=:), whereas it is not known to which value a comparison x < SKIP evaluates for any integer x. ALGOL 68 leaves intentionally undefined what happens in case of integer overflow, the integer bit representation, and the degree of numerical accuracy for floating point. In contrast, the language Java has been criticized for over-specifying the latter. Both official reports included some advanced features that were not part of the standard language.  These were indicated with an ‚Ñµ and considered effectively private.  Examples include ""‚âÆ"" and ""‚âØ"" for templates, the  OUTTYPE/INTYPE for crude duck typing, and the STRAIGHTOUT and STRAIGHTIN operators for ""straightening"" nested arrays and structures. Extract from the 1973 report:"|2023-08-25-20-26-29
ALGOL 68|Revisions|" Except where noted (with a superscript), the language described above is that of the ""Revised Report(r1)""."|2023-08-25-20-26-29
ALGOL 68|The language of the unrevised report|" The original language (As per the ""Final Report""r0) differs in syntax of the mode cast, and it had the feature of proceduring, i.e. coercing the value of a term into a procedure which evaluates the term. Proceduring would be intended to make evaluations lazy. The most useful application could have been the short-circuited evaluation of boolean operators. In: b is only evaluated if a is true. As defined in ALGOL 68, it did not work as expected, for example in the code: against the programmers na√Øve expectations the print would be executed as it is only the value of the elaborated enclosed-clause after ANDF that was procedured. Textual insertion of the commented-out PROC BOOL: makes it work. Some implementations emulate the expected behaviour for this special case by extension of the language. Before revision, the programmer could decide to have the arguments of a procedure evaluated serially instead of collaterally by using semicolons instead of commas (gommas). For example in: The first argument to test is guaranteed to be evaluated before the second, but in the usual: then the compiler could evaluate the arguments in whatever order it felt like."|2023-08-25-20-26-29
ALGOL 68|Extension proposals from IFIP WG 2.1| After the revision of the report, some extensions to the language have been proposed to widen the applicability: So far, only partial parametrisation has been implemented, in Algol 68 Genie.|2023-08-25-20-26-29
ALGOL 68|True ALGOL 68s specification and implementation timeline| The S3 language that was used to write the ICL VME operating system and much other system software on the ICL 2900 Series was a direct derivative of Algol 68. However, it omitted many of the more complex features, and replaced the basic modes with a set of data types that mapped directly to the 2900 Series hardware architecture.|2023-08-25-20-26-29
ALGOL 68|Implementation specific extensions|" ALGOL 68R(R) from RRE was the first ALGOL 68 subset implementation, running on the ICL 1900. Based on the original language, the main subset restrictions were definition before use and no parallel processing. This compiler was popular in UK universities in the 1970s, where many computer science students learnt ALGOL 68 as their first programming language; the compiler was renowned for good error messages. ALGOL 68RS(RS) from RSRE was a portable compiler system written in ALGOL 68RS (bootstrapped from ALGOL 68R), and implemented on a variety of systems including the ICL 2900/Series 39, Multics and DEC VAX/VMS. The language was based on the Revised Report, but with similar subset restrictions to ALGOL 68R. This compiler survives in the form of an Algol68-to-C compiler. In ALGOL 68S(S) from Carnegie Mellon University the power of parallel processing was improved by adding an orthogonal extension, eventing. Any variable declaration containing keyword EVENT made assignments to this variable eligible for parallel evaluation, i.e. the right hand side was made into a procedure which was moved to one of the processors of the C.mmp multiprocessor system. Accesses to such variables were delayed after termination of the assignment. Cambridge ALGOL 68C(C) was a portable compiler that implemented a subset of ALGOL 68, restricting operator definitions and omitting garbage collection, flexible rows and formatted transput. Algol 68 Genie(G) by M. van der Veer is an ALGOL 68 implementation for today's computers and operating systems. ""Despite good intentions, a programmer may violate portability by inadvertently employing a local extension.  To guard against this, each implementation should provide a PORTCHECK pragmat option.  While this option is in force, the compiler prints a message for each construct that it recognizes as violating some portability constraint."""|2023-08-25-20-26-29
Apache Groovy|| Apache Groovy is a Java-syntax-compatible object-oriented programming language for the Java platform. It is both a static and dynamic language with features similar to those of Python, Ruby, and Smalltalk. It can be used as both a programming language and a scripting language for the Java Platform, is compiled to Java virtual machine (JVM) bytecode, and interoperates seamlessly with other Java code and libraries. Groovy uses a curly-bracket syntax similar to Java's. Groovy supports closures, multiline strings, and expressions embedded in strings. Much of Groovy's power lies in its AST transformations, triggered through annotations. Groovy 1.0 was released on January 2, 2007, and Groovy 2.0 in July, 2012. Since version 2, Groovy can be compiled statically, offering type inference and performance near that of Java. Groovy 2.4 was the last major release under Pivotal Software's sponsorship which ended in March 2015. Groovy has since changed its governance structure to a Project Management Committee in the Apache Software Foundation.|2022-11-30-00-04-29
Apache Groovy|History|" James Strachan first talked about the development of Groovy on his blog in August 2003. In March 2004, Groovy was submitted to the JCP as JSR 241 and accepted by ballot. Several versions were released between 2004 and 2006. After the Java Community Process (JCP) standardization effort began, the version numbering changed, and a version called ""1.0"" was released on January 2, 2007. After various betas and release candidates numbered 1.1, on December 7, 2007, Groovy 1.1 Final was released and immediately renumbered as Groovy 1.5 to reflect the many changes made. In 2007, Groovy won the first prize at JAX 2007 innovation award. In 2008, Grails, a Groovy web framework, won the second prize at JAX 2008 innovation award. In November 2008, SpringSource acquired the Groovy and Grails company (G2One). In August 2009 VMware acquired SpringSource. In April 2012, after eight years of inactivity, the Spec Lead changed the status of JSR 241 to dormant. Strachan had left the project silently a year before the Groovy 1.0 release in 2007.[citation needed] In Oct 2016, Strachan stated ""I still love groovy (jenkins pipelines are so groovy!), java, go, typescript and kotlin"". On July 2, 2012, Groovy 2.0 was released, which, among other new features, added static compiling and static type checking. When the Pivotal Software joint venture was spun-off by EMC Corporation (EMC) and VMware in April 2013, Groovy and Grails formed part of its product portfolio. Pivotal ceased sponsoring Groovy and Grails from April 2015.
That same month, Groovy changed its governance structure from a Codehaus repository to a Project Management Committee (PMC) in the Apache Software Foundation via its incubator.
Groovy graduated from Apache's incubator and became a top-level project in November 2015."|2022-11-30-00-04-29
Apache Groovy|Features| Most valid Java files are also valid Groovy files. Although the two languages are similar, Groovy code can be more compact, because it does not need all the elements that Java needs. This makes it possible for Java programmers to learn Groovy gradually by starting with familiar Java syntax before acquiring more Groovy programming idioms. Groovy features not available in Java include both static and dynamic typing (with the keyword def), operator overloading, native syntax for lists and associative arrays (maps), native support for regular expressions, polymorphic iteration, string interpolation, added helper methods, and the safe navigation operator ?. to check automatically for null pointers (for example, variable?.method(), or variable?.field). Since version 2 Groovy also supports modularity (being able to ship only the needed jars according to the project needs, thus reducing the size of Groovy's library), type checking, static compiling, Project Coin syntax enhancements, multicatch blocks and ongoing performance enhancements using the invokedynamic instruction introduced in Java 7. Groovy provides native support for various markup languages such as XML and HTML, accomplished via an inline Document Object Model (DOM) syntax. This feature enables the definition and manipulation of many types of heterogeneous data assets with a uniform and concise syntax and programming methodology.[citation needed] Unlike Java, a Groovy source code file can be executed as an (uncompiled) script, if it contains code outside any class definition, if it is a class with a main method, or if it is a Runnable or GroovyTestCase. A Groovy script is fully parsed, compiled, and generated before executing (similar to Python and Ruby). This occurs under the hood, and the compiled version is not saved as an artifact of the process.|2022-11-30-00-04-29
Apache Groovy|GroovyBeans, properties| GroovyBeans are Groovy's version of JavaBeans. Groovy implicitly generates getters and setters. In the following code, setColor(String color) and getColor() are implicitly generated. The last two lines, which appear to access color directly, are actually calling the implicitly generated methods. Groovy offers simple, consistent syntax for handling lists and maps, reminiscent of Java's array syntax.|2022-11-30-00-04-29
Apache Groovy|Prototype extension| Groovy offers support for prototype extension through ExpandoMetaClass, Extension Modules (only in Groovy 2), Objective-C-like Categories and DelegatingMetaClass. ExpandoMetaClass offers a domain-specific language (DSL) to express the changes in the class easily, similar to Ruby's open class concept: Groovy's changes in code through prototyping are not visible in Java, since each attribute/method invocation in Groovy goes through the metaclass registry. The changed code can only be accessed from Java by going to the metaclass registry. Groovy also allows overriding methods as getProperty(), propertyMissing() among others, enabling the developer to intercept calls to an object and specify an action for them, in a simplified aspect-oriented way. The following code enables the class java.lang.String to respond to the hex property: The Grails framework uses metaprogramming extensively to enable GORM dynamic finders, like User.findByName('Josh') and others.|2022-11-30-00-04-29
Apache Groovy|Dot and parentheses| Groovy's syntax permits omitting parentheses and dots in some situations. The following groovy code can be written as enabling the development of domain-specific languages (DSLs) that look like plain English.|2022-11-30-00-04-29
Apache Groovy|Functional programming|" Although Groovy is mostly an object-oriented language, it also offers functional programming features. According to Groovy's documentation: ""Closures in Groovy work similar to a 'method pointer', enabling code to be written and run in a later point in time"". Groovy's closures support free variables, i.e. variables that have not been explicitly passed as a parameter to it, but exist in its declaration context, partial application (that it terms 'currying'), delegation, implicit, typed and untyped parameters. When working on Collections of a determined type, the closure passed to an operation on the collection can be inferred: A group of expressions can be written in a closure block without reference to an implementation and the responding object can be assigned at a later point using delegation: Usually called partial application, this Groovy feature allows closures' parameters to be set to a default parameter in any of their arguments, creating a new closure with the bound value. Supplying one argument to the curry() method will fix argument one. Supplying N arguments will fix arguments 1 .. N. Curry can also be used in the reverse direction (fixing the last N arguments) using rcurry(). Groovy also supports lazy evaluation, reduce/fold, infinite structures and immutability, among others."|2022-11-30-00-04-29
Apache Groovy|JSON and XML processing| On JavaScript Object Notation (JSON) and XML processing, Groovy employs the Builder pattern, making the production of the data structure less verbose. For example, the following XML: can be generated via the following Groovy code: and also can be processed in a streaming way through StreamingMarkupBuilder. To change the implementation to JSON, the MarkupBuilder can be swapped to JsonBuilder. To parse it and search for a functional language, Groovy's findAll method can serve:|2022-11-30-00-04-29
Apache Groovy|String interpolation| In Groovy, strings can be interpolated with variables and expressions by using GStrings: GStrings containing variables and expressions must be declared using double quotes. A complex expression must be enclosed in curly brackets. This prevents parts of it from being interpreted as belonging to the surrounding string instead of to the expression: Expression evaluation can be deferred by employing arrow syntax:|2022-11-30-00-04-29
Apache Groovy|Abstract syntax tree transformation|" According to Groovy's own documentation, ""When the Groovy compiler compiles Groovy scripts and classes, at some point in the process, the source code will end up being represented in memory in the form of a Concrete Syntax Tree, then transformed into an Abstract Syntax Tree. The purpose of AST Transformations is to let developers hook into the compilation process to be able to modify the AST before it is turned into bytecode that will be run by the JVM. AST Transformations provides Groovy with improved compile-time metaprogramming capabilities allowing powerful flexibility at the language level, without a runtime performance penalty."" Examples of ASTs in Groovy are: among others."|2022-11-30-00-04-29
Apache Groovy|Traits|" According to Groovy's documentation, ""Traits are a structural construct of the language that allows: composition of behaviors, runtime implementation of interfaces, behavior overriding, and compatibility with static type checking/compilation."" Traits can be seen as interfaces carrying both default implementations and state. A trait is defined using the trait keyword: Then, it can be used like a normal interface using the keyword implements: Traits allow a wide range of abilities, from simple composition to testing."|2022-11-30-00-04-29
Apache Groovy|Adoption| Notable examples of Groovy adoption include:|2022-11-30-00-04-29
Apache Groovy|IDE support| Many integrated development environments (IDEs) and text editors support Groovy:|2022-11-30-00-04-29
Apache Groovy|Dialects| There is one alternative implementation of Groovy:|2022-11-30-00-04-29
APL (programming language)|| APL (named after the book A Programming Language) is a programming language developed in the 1960s by Kenneth E. Iverson. Its central datatype is the multidimensional array. It uses a large range of special graphic symbols to represent most functions and operators, leading to very concise code. It has been an important influence on the development of concept modeling, spreadsheets, functional programming, and computer math packages. It has also inspired several other programming languages.|2023-09-16-03-52-50
APL (programming language)|Mathematical notation| A mathematical notation for manipulating arrays was developed by Kenneth E. Iverson, starting in 1957 at Harvard University. In 1960, he began work for IBM where he developed this notation with Adin Falkoff and published it in his book A Programming Language in 1962. The preface states its premise: Applied mathematics is largely concerned with the design and analysis of explicit procedures for calculating the exact or approximate values of various functions. Such explicit procedures are called algorithms or programs. Because an effective notation for the description of programs exhibits considerable syntactic structure, it is called a programming language. This notation was used inside IBM for short research reports on computer systems, such as the Burroughs B5000 and its stack mechanism when stack machines versus register machines were being evaluated by IBM for upcoming computers. Iverson also used his notation in a draft of the chapter A Programming Language, written for a book he was writing with Fred Brooks, Automatic Data Processing, which would be published in 1963. In 1979, Iverson received the Turing Award for his work on APL.|2023-09-16-03-52-50
APL (programming language)|Development into a computer programming language|" As early as 1962, the first attempt to use the notation to describe a complete computer system happened after Falkoff discussed with William C. Carter his work to standardize the instruction set for the machines that later became the IBM System/360 family. In 1963, Herbert Hellerman, working at the IBM Systems Research Institute, implemented a part of the notation on an IBM 1620 computer, and it was used by students in a special high school course on calculating transcendental functions by series summation. Students tested their code in Hellerman's lab. This implementation of a part of the notation was called Personalized Array Translator (PAT). In 1963, Falkoff, Iverson, and Edward H. Sussenguth Jr., all working at IBM, used the notation for a formal description of the IBM System/360 series machine architecture and functionality, which resulted in a paper published in IBM Systems Journal in 1964. After this was published, the team turned their attention to an implementation of the notation on a computer system. One of the motivations for this focus of implementation was the interest of John L. Lawrence who had new duties with Science Research Associates, an educational company bought by IBM in 1964. Lawrence asked Iverson and his group to help use the language as a tool to develop and use computers in education. After Lawrence M. Breed and Philip S. Abrams of Stanford University joined the team at IBM Research, they continued their prior work on an implementation programmed in FORTRAN IV for a part of the notation which had been done for the IBM 7090 computer running on the IBSYS operating system. This work was finished in late 1965 and later named IVSYS (for Iverson system). The basis of this implementation was described in detail by Abrams in a Stanford University Technical Report, ""An Interpreter for Iverson Notation"" in 1966. The academic aspect of this was formally supervised by Niklaus Wirth. Like Hellerman's PAT system earlier, this implementation did not include the APL character set but used special English reserved words for functions and operators. The system was later adapted for a time-sharing system and, by November 1966, it had been reprogrammed for the IBM System/360 Model 50 computer running in a time-sharing mode and was used internally at IBM."|2023-09-16-03-52-50
APL (programming language)|Hardware| A key development in the ability to use APL effectively, before the wide use of cathode ray tube (CRT) terminals, was the development of a special IBM Selectric typewriter interchangeable typing element with all the special APL characters on it. This was used on paper printing terminal workstations using the Selectric typewriter and typing element mechanism, such as the IBM 1050 and IBM 2741 terminal. Keycaps could be placed over the normal keys to show which APL characters would be entered and typed when that key was struck. For the first time, a programmer could type in and see proper APL characters as used in Iverson's notation and not be forced to use awkward English keyword representations of them. Falkoff and Iverson had the special APL Selectric typing elements, 987 and 988, designed in late 1964, although no APL computer system was available to use them. Iverson cited Falkoff as the inspiration for the idea of using an IBM Selectric typing element for the APL character set. Many APL symbols, even with the APL characters on the Selectric typing element, still had to be typed in by over-striking two extant element characters. An example is the grade up character, which had to be made from a delta (shift-H) and a Sheffer stroke (shift-M). This was necessary because the APL character set was much larger than the 88 characters allowed on the typing element, even when letters were restricted to upper-case (capitals).|2023-09-16-03-52-50
APL (programming language)|Commercial availability|" The first APL interactive login and creation of an APL workspace was in 1966 by Larry Breed using an IBM 1050 terminal at the IBM Mohansic Labs near Thomas J. Watson Research Center, the home of APL, in Yorktown Heights, New York. IBM was chiefly responsible for introducing APL to the marketplace. The first publicly available version of APL was released in 1968 for the IBM 1130. IBM provided APL\1130 for free but without liability or support. It would run in as little as 8k 16-bit words of memory, and used a dedicated 1 megabyte hard disk. APL gained its foothold on mainframe timesharing systems from the late 1960s through the early 1980s, in part because it would support multiple users on lower-specification systems that had no dynamic address translation hardware. Additional improvements in performance for selected IBM System/370 mainframe systems included the APL Assist Microcode in which some support for APL execution was included in the processor's firmware, as distinct from being implemented entirely by higher-level software. Somewhat later, as suitably performing hardware was finally growing available in the mid- to late-1980s, many users migrated their applications to the personal computer environment. Early IBM APL interpreters for IBM 360 and IBM 370 hardware implemented their own multi-user management instead of relying on the host services, thus they were their own timesharing systems. First introduced for use at IBM in 1966, the APL\360 system was a multi-user interpreter. The ability to programmatically communicate with the operating system for information and setting interpreter system variables was done through special privileged ""I-beam"" functions, using both monadic and dyadic operations. In 1973, IBM released APL.SV, which was a continuation of the same product, but which offered shared variables as a means to access facilities outside of the APL system, such as operating system files. In the mid-1970s, the IBM mainframe interpreter was even adapted for use on the IBM 5100 desktop computer, which had a small CRT and an APL keyboard, when most other small computers of the time only offered BASIC. In the 1980s, the VSAPL program product enjoyed wide use with Conversational Monitor System (CMS), Time Sharing Option (TSO), VSPC, MUSIC/SP, and CICS users. In 1973‚Äì1974, Patrick E. Hagerty directed the implementation of the University of Maryland APL interpreter for the 1100 line of the Sperry UNIVAC 1100/2200 series mainframe computers.  In 1974, student Alan Stebbens was assigned the task of implementing an internal function. Xerox APL was available from June 1975 for Xerox 560 and Sigma 6, 7, and 9 mainframes running CP-V and for Honeywell CP-6. In the 1960s and 1970s, several timesharing firms arose that sold APL services using modified versions of the IBM APL\360 interpreter. In North America, the better-known ones were IP Sharp Associates, Scientific Time Sharing Corporation (STSC), Time Sharing Resources (TSR), and The Computer Company (TCC). CompuServe also entered the market in 1978 with an APL Interpreter based on a modified version of Digital Equipment Corp and Carnegie Mellon's, which ran on DEC's KI and KL 36-bit machines. CompuServe's APL was available both to its commercial market and the consumer information service. With the advent first of less expensive mainframes such as the IBM 4300, and later the personal computer, by the mid-1980s, the timesharing industry was all but gone. Sharp APL was available from IP Sharp Associates, first as a timesharing service in the 1960s, and later as a program product starting around 1979. Sharp APL was an advanced APL implementation with many language extensions, such as packages (the ability to put one or more objects into a single variable), a file system, nested arrays, and shared variables. APL interpreters were available from other mainframe and mini-computer manufacturers also, notably Burroughs, Control Data Corporation (CDC), Data General, Digital Equipment Corporation (DEC), Harris, Hewlett-Packard (HP), Siemens, Xerox and others. Garth Foster of Syracuse University sponsored regular meetings of the APL implementers' community at Syracuse's Minnowbrook Conference Center in Blue Mountain Lake, New York. In later years, Eugene McDonnell organized similar meetings at the Asilomar Conference Grounds near Monterey, California, and at Pajaro Dunes near Watsonville, California. The SIGAPL special interest group of the Association for Computing Machinery continues to support the APL community."|2023-09-16-03-52-50
APL (programming language)|Microcomputers|" On microcomputers, which became available from the mid-1970s onwards, BASIC became the dominant programming language. Nevertheless, some microcomputers provided APL instead ‚Äì the first being the Intel 8008-based MCM/70 which was released in 1974 and which was primarily used in education. Another machine of this time was the VideoBrain Family Computer, released in 1977, which was supplied with its dialect of APL called APL/S. The Commodore SuperPET, introduced in 1981, included an APL interpreter developed by the University of Waterloo. In 1976, Bill Gates claimed in his Open Letter to Hobbyists that Microsoft Corporation was implementing APL for the Intel 8080 and Motorola 6800 but had ""very little incentive to make [it] available to hobbyists"" because of software piracy. It was never released."|2023-09-16-03-52-50
APL (programming language)|APL2| Starting in the early 1980s, IBM APL development, under the leadership of Jim Brown, implemented a new version of the APL language that contained as its primary enhancement the concept of nested arrays, where an array can contain other arrays, and new language features which facilitated integrating nested arrays into program workflow. Ken Iverson, no longer in control of the development of the APL language, left IBM and joined I. P. Sharp Associates, where one of his major contributions was directing the evolution of Sharp APL to be more in accord with his vision. APL2 was first released for CMS and TSO in 1984. The APL2 Workstation edition (Windows, OS/2, AIX, Linux, and Solaris) followed later. As other vendors were busy developing APL interpreters for new hardware, notably Unix-based microcomputers, APL2 was almost always the standard chosen for new APL interpreter developments. Even today, most APL vendors or their users cite APL2 compatibility as a selling point for those products. IBM cites its use for problem solving, system design, prototyping, engineering and scientific computations, expert systems, for teaching mathematics and other subjects, visualization and database access.|2023-09-16-03-52-50
APL (programming language)|Modern implementations| Various implementations of APL by APLX, Dyalog, et al., include extensions for object-oriented programming, support for .NET, XML-array conversion primitives, graphing, operating system interfaces, and lambda calculus expressions. Freeware versions include GNU APL for Linux and NARS2000 for Windows (which runs on Linux under Wine). Both of these are fairly complete versions of APL2 with various language extensions.|2023-09-16-03-52-50
APL (programming language)|Derivative languages| APL has formed the basis of, or influenced, the following languages:[citation needed]|2023-09-16-03-52-50
APL (programming language)|Character set| APL has been criticized and praised for its choice of a unique, non-standard character set. Some who learn it become ardent adherents. In the 1960s and 1970s, few terminal devices or even displays could reproduce the APL character set. The most popular ones employed the IBM Selectric print mechanism used with a special APL type element. One of the early APL line terminals (line-mode operation only, not full screen) was the Texas Instruments TI Model 745 (c.‚Äâ1977) with the full APL character set which featured half and full duplex telecommunications modes, for interacting with an APL time-sharing service or remote mainframe to run a remote computer job, called an RJE. Over time, with the universal use of high-quality graphic displays, printing devices and Unicode support, the APL character font problem has largely been eliminated. However, entering APL characters requires the use of input method editors, keyboard mappings, virtual/on-screen APL symbol sets, or easy-reference printed keyboard cards which can frustrate beginners accustomed to other programming languages. With beginners who have no prior experience with other programming languages, a study involving high school students found that typing and using APL characters did not hinder the students in any measurable way. In defense of APL, it requires fewer characters to type, and keyboard mappings become memorized over time. Special APL keyboards are also made and in use today, as are freely downloadable fonts for operating systems such as Microsoft Windows. The reported productivity gains assume that one spends enough time working in the language to make it worthwhile to memorize the symbols, their semantics, and keyboard mappings, not to mention a substantial number of idioms for common tasks.[citation needed]|2023-09-16-03-52-50
APL (programming language)|Design|" Unlike traditionally structured programming languages, APL code is typically structured as chains of monadic or dyadic functions, and operators acting on arrays. APL has many nonstandard primitives (functions and operators) that are indicated by a single symbol or a combination of a few symbols. All primitives are defined to have the same precedence, and always associate to the right. Thus, APL is read or best understood from right-to-left. Early APL implementations (c.‚Äâ1970 or so) had no programming loop-flow control structures, such as do or while loops, and if-then-else constructs. Instead, they used array operations, and use of structured programming constructs was often not necessary, since an operation could be performed on a full array in one statement. For example, the iota function (Œπ) can replace for-loop iteration: ŒπN when applied to a scalar positive integer yields a one-dimensional array (vector), 1 2 3¬†... N. More recent implementations of APL generally include comprehensive control structures, so that data structure and program control flow can be clearly and cleanly separated. The APL environment is called a workspace. In a workspace the user can define programs and data, i.e., the data values exist also outside the programs, and the user can also manipulate the data without having to define a program. In the examples below, the APL interpreter first types six spaces before awaiting the user's input. Its own output starts in column one. In normal mathematical notation, it is equivalent to: 




m
=

‚àë

i
=
1


4


(
i
+
3
)



{\displaystyle \displaystyle m=\sum \limits _{i=1}^{4}(i+3)}

. Recall that mathematical expressions are not read or evaluated from right-to-left. The user can save the workspace with all values, programs, and execution status. APL uses a set of non-ASCII symbols, which are an extension of traditional arithmetic and algebraic notation. Having single character names for single instruction, multiple data (SIMD) vector functions is one way that APL enables compact formulation of algorithms for data transformation such as computing Conway's Game of Life in one line of code. In nearly all versions of APL, it is theoretically possible to express any computable function in one expression, that is, in one line of code.[citation needed] Because of the unusual character set, many programmers use special keyboards with APL keytops to write APL code. Although there are various ways to write APL code using only ASCII characters, in practice it is almost never done. (This may be thought to support Iverson's thesis about notation as a tool of thought.) Most if not all modern implementations use standard keyboard layouts, with special mappings or input method editors to access non-ASCII characters. Historically, the APL font has been distinctive, with uppercase italic alphabetic characters and upright numerals and symbols. Most vendors continue to display the APL character set in a custom font. Advocates of APL[who?] claim that the examples of so-called write-only code (badly written and almost incomprehensible code) are almost invariably examples of poor programming practice or novice mistakes, which can occur in any language. Advocates also claim that they are far more productive with APL than with more conventional computer languages, and that working software can be implemented in far less time and with far fewer programmers than using other technology.[citation needed] They also may claim that because it is compact and terse, APL lends itself well to larger-scale software development and complexity, because the number of lines of code can be reduced greatly. Many APL advocates and practitioners also view standard programming languages such as COBOL and Java as being comparatively tedious. APL is often found where time-to-market is important, such as with trading systems."|2023-09-16-03-52-50
APL (programming language)|Terminology| APL makes a clear distinction between functions and operators. Functions take arrays (variables or constants or expressions) as arguments, and return arrays as results. Operators (similar to higher-order functions) take functions or arrays as arguments, and derive related functions. For example, the sum function is derived by applying the reduction operator to the addition function. Applying the same reduction operator to the maximum function (which returns the larger of two numbers) derives a function which returns the largest of a group (vector) of numbers. In the J language, Iverson substituted the terms verb for function and adverb or conjunction for operator. APL also identifies those features built into the language, and represented by a symbol, or a fixed combination of symbols, as primitives. Most primitives are either functions or operators. Coding APL is largely a process of writing non-primitive functions and (in some versions of APL) operators. However a few primitives are considered to be neither functions nor operators, most noticeably assignment. Some words used in APL literature have meanings that differ from those in both mathematics and the generality of computer science.|2023-09-16-03-52-50
APL (programming language)|Syntax| APL has explicit representations of functions, operators, and syntax, thus providing a basis for the clear and explicit statement of extended facilities in the language, and tools to experiment on them.|2023-09-16-03-52-50
APL (programming language)|Examples|" This displays ""Hello, world"": A design theme in APL is to define default actions in some cases that would produce syntax errors in most other programming languages. The 'Hello, world' string constant above displays, because display is the default action on any expression for which no action is specified explicitly (e.g. assignment, function parameter). Another example of this theme is that exponentiation in APL is written as 2*3, which indicates raising 2 to the power 3 (this would be written as 2^3 or 2**3 in some languages, or relegated to a function call such as pow(2, 3); in others). Many languages use * to signify multiplication, as in 2*3, but APL chooses to use 2√ó3. However, if no base is specified (as with the statement *3 in APL, or ^3 in other languages), most programming languages one would see this as a syntax error. APL, however, assumes the missing base to be the natural logarithm constant e, and interprets *3 as 2.71828*3. Suppose that X is an array of numbers. Then (+/X)√∑‚ç¥X gives its average. Reading right-to-left, ‚ç¥X gives the number of elements in X, and since √∑ is a dyadic operator, the term to its left is required as well. It is surrounded by parentheses since otherwise X would be taken (so that the summation would be of X√∑‚ç¥X‚Äîeach element of X divided by the number of elements in X), and +/X gives the sum of the elements of X. Building on this, the following expression computes standard deviation: Naturally, one would define this expression as a function for repeated use rather than rewriting it each time. Further, since assignment is an operator, it can appear within an expression, so the following would place suitable values into T, AV and SD: This following immediate-mode expression generates a typical set of Pick 6 lottery numbers: six pseudo-random integers ranging from 1 to 40, guaranteed non-repeating, and displays them sorted in ascending order: The above does a lot, concisely, although it may seem complex to a new APLer. It combines the following APL functions (also called primitives and glyphs): Since there is no function to the left of the left-most x to tell APL what to do with the result, it simply outputs it to the display (on a single line, separated by spaces) without needing any explicit instruction to do that. ? also has a monadic equivalent called roll, which simply returns one random integer between 1 and its sole operand [to the right of it], inclusive. Thus, a role-playing game program might use the expression ?20 to roll a twenty-sided die. The following expression finds all prime numbers from 1 to R. In both time and space, the calculation complexity is 



O
(

R

2


)




{\displaystyle O(R^{2})\,\!}

 (in Big O notation). Executed from right to left, this means: (Note, this assumes the APL origin is 1, i.e., indices start with 1. APL can be set to use 0 as the origin, so that Œπ6 is 0 1 2 3 4 5, which is convenient for some calculations.) The following expression sorts a word list stored in matrix X according to word length: The following function ""life"", written in Dyalog APL, takes a boolean matrix and calculates the new generation according to Conway's Game of Life. It demonstrates the power of APL to implement a complex algorithm in very little code, but understanding it requires some advanced knowledge of APL (as the same program would in many languages). In the following example, also Dyalog, the first line assigns some HTML code to a variable txt and then uses an APL expression to remove all the HTML tags (explanation):"|2023-09-16-03-52-50
APL (programming language)|Naming|" APL derives its name from the initials of Iverson's book A Programming Language, even though the book describes Iverson's mathematical notation, rather than the implemented programming language described in this article. The name is used only for actual implementations, starting with APL\360. Adin Falkoff coined the name in 1966 during the implementation of APL\360 at IBM: As I walked by the office the three students shared, I could hear sounds of an argument going on. I poked my head in the door, and Eric asked me, ""Isn't it true that everyone knows the notation we're using is called APL?"" I was sorry to have to disappoint him by confessing that I had never heard it called that. Where had he got the idea it was well known? And who had decided to call it that? In fact, why did it have to be called anything? Quite a while later I heard how it was named. When the implementation effort started in June of 1966, the documentation effort started, too. I suppose when they had to write about ""it"", Falkoff and Iverson realized that they would have to give ""it"" a name. There were probably many suggestions made at the time, but I have heard of only two. A group in SRA in Chicago which was developing instructional materials using the notation was in favor of the name ""Mathlab"". This did not catch on. Another suggestion was to call it ""Iverson's Better Math"" and then let people coin the appropriate acronym. This was deemed facetious. Then one day Adin Falkoff walked into Ken's office and wrote ""A Programming Language"" on the board, and underneath it the acronym ""APL"". Thus it was born. It was just a week or so after this that Eric Iverson asked me his question, at a time when the name hadn't yet found its way the thirteen miles up the Taconic Parkway from IBM Research to IBM Mohansic. APL is occasionally re-interpreted as Array Programming Language or Array Processing Language, thereby making APL into a backronym."|2023-09-16-03-52-50
APL (programming language)|Logo|" There has always been cooperation between APL vendors, and joint conferences were held on a regular basis from 1969 until 2010. At such conferences, APL merchandise was often handed out, featuring APL motifs or collection of vendor logos. Common were apples (as a pun on the similarity in pronunciation of apple and APL) and the code snippet ‚ç∫*‚éï which are the symbols produced by the classic APL keyboard layout when holding the APL modifier key and typing ""APL"". Despite all these community efforts, no universal vendor-agnostic logo for the programming language emerged. As popular programming languages increasingly have established recognisable logos, Fortran getting one in 2020, British APL Association launched a campaign in the second half of 2021, to establish such a logo for APL, and after a community election and multiple rounds of feedback, a logo was chosen in May 2022."|2023-09-16-03-52-50
APL (programming language)|Use|" APL is used for many purposes including financial and insurance applications, artificial intelligence,
neural networks
and robotics. It has been argued that APL is a calculation tool and not a programming language; its symbolic nature and array capabilities have made it popular with domain experts and data scientists who do not have or require the skills of a computer programmer.[citation needed] APL is well suited to image manipulation and computer animation, where graphic transformations can be encoded as matrix multiplications. One of the first commercial computer graphics houses, Digital Effects, produced an APL graphics product named Visions, which was used to create television commercials and animation for the 1982 film Tron. Latterly, the Stormwind boating simulator uses APL to implement its core logic, its interfacing to the rendering pipeline middleware and a major part of its physics engine. Today, APL remains in use in a wide range of commercial and scientific applications, for example
investment management,
asset management,
health care,
and DNA profiling, 
and by hobbyists."|2023-09-16-03-52-50
APL (programming language)|APL\360|" The first implementation of APL using recognizable APL symbols was APL\360 which ran on the IBM System/360, and was completed in November 1966 though at that time remained in use only within IBM. In 1973 its implementors, Larry Breed, Dick Lathwell and Roger Moore, were awarded the Grace Murray Hopper Award from the Association for Computing Machinery (ACM). It was given ""for their work in the design and implementation of APL\360, setting new standards in simplicity, efficiency, reliability and response time for interactive systems."" In 1975, the IBM 5100 microcomputer offered APL\360 as one of two built-in ROM-based interpreted languages for the computer, complete with a keyboard and display that supported all the special symbols used in the language. Significant developments to APL\360 included CMS/APL, which made use of the virtual storage capabilities of CMS and APLSV, which introduced shared variables, system variables and system functions. It was subsequently ported to the IBM System/370 and VSPC platforms until its final release in 1983, after which it was replaced by APL2."|2023-09-16-03-52-50
APL (programming language)|APL\1130| In 1968, APL\1130 became the first publicly available APL system, created by IBM for the IBM 1130. It became the most popular IBM Type-III Library software that IBM released.|2023-09-16-03-52-50
APL (programming language)|APL*Plus and Sharp APL| APL*Plus and Sharp APL are versions of APL\360 with added business-oriented extensions such as data formatting and facilities to store APL arrays in external files. They were jointly developed by two companies, employing various members of the original IBM APL\360 development team. The two companies were I. P. Sharp Associates (IPSA), an APL\360 services company formed in 1964 by Ian Sharp, Roger Moore and others, and STSC, a time-sharing and consulting service company formed in 1969 by Lawrence Breed and others. Together the two developed APL*Plus and thereafter continued to work together but develop APL separately as APL*Plus and Sharp APL. STSC ported APL*Plus to many platforms with versions being made for the VAX 11, PC and UNIX, whereas IPSA took a different approach to the arrival of the personal computer and made Sharp APL available on this platform using additional PC-XT/360 hardware. In 1993, Soliton Incorporated was formed to support Sharp APL and it developed Sharp APL into SAX (Sharp APL for Unix). As of 2018, APL*Plus continues as APL2000 APL+Win. In 1985, Ian Sharp, and Dan Dyer of STSC, jointly received the Kenneth E. Iverson Award for Outstanding Contribution to APL.|2023-09-16-03-52-50
APL (programming language)|APL2| APL2 was a significant re-implementation of APL by IBM which was developed from 1971 and first released in 1984. It provides many additions to the language, of which the most notable is nested (non-rectangular) array support. The entire APL2 Products and Services Team was awarded the Iverson Award in 2007. In 2021, IBM sold APL2 to Log-On Software, who develop and sell the product as Log-On APL2.|2023-09-16-03-52-50
APL (programming language)|APLGOL| In 1972, APLGOL was released as an experimental version of APL that added structured programming language constructs to the language framework. New statements were added for interstatement control, conditional statement execution, and statement structuring, as well as statements to clarify the intent of the algorithm. It was implemented for Hewlett-Packard in 1977.|2023-09-16-03-52-50
APL (programming language)|Dyalog APL| Dyalog APL was first released by British company Dyalog Ltd. in 1983 and, as of 2018, is available for AIX, Linux (including on the Raspberry Pi), macOS and Microsoft Windows platforms. It is based on APL2, with extensions to support object-oriented programming, functional programming, and tacit programming. Licences are free for personal/non-commercial use. In 1995, two of the development team ‚Äì John Scholes and Peter Donnelly ‚Äì were awarded the Iverson Award for their work on the interpreter. Gitte Christensen and Morten Kromberg were joint recipients of the Iverson Award in 2016.|2023-09-16-03-52-50
APL (programming language)|NARS2000| NARS2000 is an open-source APL interpreter written by Bob Smith, a prominent APL developer and implementor from STSC in the 1970s and 1980s. NARS2000 contains advanced features and new datatypes and runs natively on Microsoft Windows, and other platforms under Wine. It is named after a development tool from the 1980s, NARS (Nested Arrays Research System).|2023-09-16-03-52-50
APL (programming language)|APLX| APLX is a cross-platform dialect of APL, based on APL2 and with several extensions, which was first released by British company MicroAPL in 2002. Although no longer in development or on commercial sale it is now available free of charge from Dyalog.|2023-09-16-03-52-50
APL (programming language)|York APL|" York APL was developed at the York University, Ontario around 1968, running on IBM 360 mainframes. One notable difference between it and APL\360 was that it defined the ""shape"" (œÅ) of a scalar as 1 whereas APL\360 defined it as the more mathematically correct 0 ‚Äî this made it easier to write functions that acted the same with scalars and vectors."|2023-09-16-03-52-50
APL (programming language)|GNU APL| GNU APL is a free implementation of Extended APL as specified in ISO/IEC 13751:2001 and is thus an implementation of APL2. It runs on Linux, macOS, several BSD dialects, and on Windows (either using Cygwin for full support of all its system functions or as a native 64-bit Windows binary with some of its system functions missing). GNU APL uses Unicode internally and can be scripted. It was written by J√ºrgen Sauermann. Richard Stallman, founder of the GNU Project, was an early adopter of APL, using it to write a text editor as a high school student in the summer of 1969.|2023-09-16-03-52-50
APL (programming language)|Interpretation and compilation of APL|" APL is traditionally an interpreted language, having language characteristics such as weak variable typing not well suited to compilation. However, with arrays as its core data structure it provides opportunities for performance gains through parallelism, parallel computing, massively parallel applications, and very-large-scale integration (VLSI), and from the outset APL has been regarded as a high-performance language ‚Äì for example, it was noted for the speed with which it could perform complicated matrix operations ""because it operates on arrays and performs operations like matrix inversion internally"". Nevertheless, APL is rarely purely interpreted and compilation or partial compilation techniques that are, or have been, used include the following:"|2023-09-16-03-52-50
APL (programming language)|Idiom recognition| Most APL interpreters support idiom recognition and evaluate common idioms as single operations. For example, by evaluating the idiom BV/‚ç≥‚ç¥A as a single operation (where BV is a Boolean vector and A is an array), the creation of two intermediate arrays is avoided.|2023-09-16-03-52-50
APL (programming language)|Optimised bytecode| Weak typing in APL means that a name may reference an array (of any datatype), a function or an operator. In general, the interpreter cannot know in advance which form it will be and must therefore perform analysis, syntax checking etc. at run-time. However, in certain circumstances, it is possible to deduce in advance what type a name is expected to reference and then generate bytecode which can be executed with reduced run-time overhead. This bytecode can also be optimised using compilation techniques such as constant folding or common subexpression elimination. The interpreter will execute the bytecode when present and when any assumptions which have been made are met. Dyalog APL includes support for optimised bytecode.|2023-09-16-03-52-50
APL (programming language)|Compilation| Compilation of APL has been the subject of research and experiment since the language first became available; the first compiler is considered to be the Burroughs APL-700 which was released around 1971. In order to be able to compile APL, language limitations have to be imposed. APEX is a research APL compiler which was written by Robert Bernecky and is available under the GNU Public License. The STSC APL Compiler is a hybrid of a bytecode optimiser and a compiler ‚Äì it enables compilation of functions to machine code provided that its sub-functions and globals are declared, but the interpreter is still used as a runtime library and to execute functions which do not meet the compilation requirements.|2023-09-16-03-52-50
APL (programming language)|Standards| APL has been standardized by the American National Standards Institute (ANSI) working group X3J10 and International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC), ISO/IEC Joint Technical Committee 1 Subcommittee 22 Working Group 3. The Core APL language is specified in ISO 8485:1989, and the Extended APL language is specified in ISO/IEC 13751:2001.|2023-09-16-03-52-50
AppleScript||" AppleScript is a scripting language created by Apple Inc. that facilitates automated control over scriptable Mac applications. First introduced in System 7, it is currently included in all versions of macOS as part of a package of system automation tools. The term ""AppleScript"" may refer to the language itself, to an individual script written in the language, or, informally, to the macOS Open Scripting Architecture that underlies the language."|2023-09-26-07-31-14
AppleScript|Overview|" AppleScript is primarily a scripting language developed by Apple to do inter-application communication (IAC) using Apple events. AppleScript is related to, but different from, Apple events. Apple events are designed to exchange data between and control other applications in order to automate repetitive tasks. AppleScript has some processing abilities of its own, in addition to sending and receiving Apple events to applications. AppleScript can do basic calculations and text processing, and is extensible, allowing the use of scripting additions that add new functions to the language. Mainly, however, AppleScript relies on the functionality of applications and processes to handle complex tasks. As a structured command language, AppleScript can be compared to Unix shells, the Microsoft Windows Script Host, or IBM REXX but it is distinct from all three. Essential to its functionality is the fact that Macintosh applications publish ""dictionaries"" of addressable objects and operations. AppleScript has some elements of procedural programming, object-oriented programming (particularly in the construction of script objects), and natural language programming tendencies in its syntax, but does not strictly conform to any of these programming paradigms."|2023-09-26-07-31-14
AppleScript|History|" In the late 1980s Apple considered using HyperCard's HyperTalk scripting language as the standard language for end-user development across the company and within its classic Mac OS operating system, and for interprocess communication between Apple and non-Apple products. HyperTalk could be used by novices to program a HyperCard stack. Apple engineers recognized that a similar, but more object-oriented scripting language could be designed to be used with any application, and the AppleScript project was born as a spin-off of a research effort to modernize the Macintosh as a whole and finally became part of System 7. AppleScript was released in October 1993 as part of System 7.1.1 (System 7 Pro, the first major upgrade to System 7). QuarkXPress (ver. 3.2) was one of the first major software applications that supported AppleScript. This in turn led to AppleScript being widely adopted within the publishing and prepress world, often tying together complex workflows. This was a key factor in retaining the Macintosh's dominant position in publishing and prepress, even after QuarkXpress and other publishing applications were ported to Microsoft Windows. After some uncertainty about the future of AppleScript on Apple's next generation OS, the move to Mac OS X (around 2002) and its Cocoa frameworks greatly increased the usefulness and flexibility of AppleScript. Cocoa applications allow application developers to implement basic scriptability for their apps with minimal effort, broadening the number of applications that are directly scriptable. At the same time, the shift to the Unix underpinnings and AppleScript's ability to run Unix commands directly, with the do shell script command, allowed AppleScripts much greater control over the operating system itself. AppleScript Studio, released with Mac OS X 10.2 as part of Xcode, and later AppleScriptObjC framework, released in Mac OS X 10.6, allowed users to build Cocoa applications using AppleScript. In a 2006 article, Macworld included AppleScript among its rankings of Apple's 30 most significant products to date, placing it at #17. In a 2013 article for Macworld, veteran Mac software developer and commentator John Gruber concluded his reflection on ""the unlikely persistence of AppleScript"" by noting: ""In theory, AppleScript could be much better; in practice, though, it's the best thing we have that works. It exemplifies the Mac's advantages over iOS for tinkerers and advanced users."" In October 2016, longtime AppleScript product manager and automation evangelist Sal Soghoian left Apple when his position was eliminated ""for business reasons"". Veterans in the Mac community such as John Gruber and Andy Ihnatko generally responded with concern, questioning Apple's commitment to the developer community and pro users. Apple senior vice president of software engineering Craig Federighi responded in an email saying that ""We have every intent to continue our support for the great automation technologies in macOS!"", though Jeff Gamet at The Mac Observer opined that it did little to assuage his doubt about the future of Apple automation in general and AppleScript in particular. For the time being, AppleScript remains one component of macOS automation technologies, along with Automator, Shortcuts, Services, and shell scripting."|2023-09-26-07-31-14
AppleScript|Basic concepts| AppleScript was designed to be used as an accessible end-user scripting language, offering users an intelligent mechanism to control applications, and to access and modify data and documents. AppleScript uses Apple events, a set of standardized data formats that the Macintosh operating system uses to send information to applications, roughly analogous to sending XPath queries over XML-RPC in the world of web services. Apple events allow a script to work with multiple applications simultaneously, passing data between them so that complex tasks can be accomplished without human interaction. For example, an AppleScript to create a simple web gallery might do the following: For the user, hundreds or thousands of steps in multiple applications have been reduced to the single act of running the script, and the task is accomplished in much less time and with no possibility of random human error. A large complex script could be developed to run only once, while other scripts are used again and again. An application's scriptable elements are visible in the application's Scripting Dictionary (distributed as part of the application), which can be viewed in any script editor. Elements are generally grouped into suites, according to loose functional relationships between them. There are two basic kinds of elements present in any suite: classes and commands. All scriptable applications share a few basic commands and objects, usually called the Standard Suite‚Äîcommands to open, close or save a file, to print something, to quit, to set data to variables‚Äîas well as a basic application object that gives the scriptable properties of the application itself. Many applications have numerous suites capable of performing any task the application itself can perform. In exceptional cases, applications may support plugins which include their own scripting dictionaries. AppleScript was designed with the ability to build scripts intuitively by recording user actions. Such AppleScript recordability has to be engineered into the app‚Äîthe app must support Apple events and AppleScript recording; as Finder supports AppleScript recording, it can be useful for reference. When AppleScript Editor (Script Editor) is open and the Record button clicked, user actions for recordable apps are converted to their equivalent AppleScript commands and output to the Script Editor window. The resulting script can be saved and re-run to duplicate the original actions, or modified to be more generally useful.|2023-09-26-07-31-14
AppleScript|Comments|" Comments can be made multiple ways. A one-line comment can begin with 2 hyphens (--). In AppleScript 2.0, first released in Mac OS X Leopard, it may also begin with a number sign (#). This permits a self-contained AppleScript script to be stored as an executable text file beginning with the shebang line #!/usr/bin/osascript
Example: For comments that take up multiple lines, AppleScript uses parentheses with asterisks inside.
Example:"|2023-09-26-07-31-14
AppleScript|Hello, world!|" In AppleScript, the traditional ""Hello, World!"" program could be written in many different forms, including: AppleScript has several user interface options, including dialogs, alerts, and list of choices. (The character ¬¨, produced by typing ‚å• Option+return in the Script Editor, denotes continuation of a single statement across multiple lines.) Each user interaction method can return the values of buttons clicked, items chosen or text entered for further processing. For example:"|2023-09-26-07-31-14
AppleScript|Natural language metaphor|" Whereas Apple events are a way to send messages into applications, AppleScript is a particular language designed to send Apple events. In keeping with the objective of ease-of-use for beginners, the AppleScript language is designed on the natural language metaphor, just as the graphical user interface is designed on the desktop metaphor. A well-written AppleScript should be clear enough to be read and understood by anyone, and easily edited. The language is based largely on HyperCard's HyperTalk language, extended to refer not only to the HyperCard world of cards and stacks, but also theoretically to any document. To this end, the AppleScript team introduced the AppleEvent Object Model (AEOM), which specifies the objects any particular application ""knows"". The heart of the AppleScript language is the use of terms that act as nouns and verbs that can be combined. For example, rather than a different verb to print a page, document or range of pages (such as printPage, printDocument, printRange), AppleScript uses a single ""print"" verb which can be combined with an object, such as a page, a document or a range of pages. Generally, AEOM defines a number of objects‚Äîlike ""document"" or ""paragraph""‚Äîand corresponding actions‚Äîlike ""cut"" and ""close"". The system also defines ways to refer to properties of objects, so one can refer to the ""third paragraph of the document 'Good Day'"", or the ""color of the last word of the front window"". AEOM uses an application dictionary to associate the Apple events with human-readable terms, allowing the translation back and forth between human-readable AppleScript and bytecode Apple events. To discover what elements of a program are scriptable, dictionaries for supported applications may be viewed. (In the Xcode and Script Editor applications, this is under File ‚Üí Open Dictionary.) To designate which application is meant to be the target of such a message, AppleScript uses a ""tell"" construct: Alternatively, the tell may be expressed in one line by using an infinitive: For events in the ""Core Suite"" (activate, open, reopen, close, print, and quit), the application may be supplied as the direct object to transitive commands: The concept of an object hierarchy can be expressed using nested blocks: The concept of an object hierarchy can also be expressed using either nested prepositional phrases or a series of possessives: which in another programming language might be expressed as sequential method calls, like in this pseudocode: AppleScript includes syntax for ordinal counting, ""the first paragraph"", as well as cardinal, ""paragraph one"". Likewise, the numbers themselves can be referred to as text or numerically, ""five"", ""fifth"" and ""5"" are all supported; they are synonyms in AppleScript. Also, the word ""the"" can legally be used anywhere in the script in order to enhance readability: it has no effect on the functionality of the script."|2023-09-26-07-31-14
AppleScript|Examples of scripts| A failsafe calculator: A simple username and password dialog box sequence. Here, the username is John and password is app123:|2023-09-26-07-31-14
AppleScript|Script editors| Script editors provide a unified programing environment for AppleScripts, including tools for composing, validating, compiling, running, and debugging scripts. They also provide mechanisms for opening and viewing AppleScript dictionaries from scriptable applications, saving scripts in a number of formats (compiled script files, application packages, script bundles, and plain text files), and usually provide features such as syntax highlighting and prewritten code snippets.|2023-09-26-07-31-14
AppleScript|Script launchers| AppleScripts can be run from a script editor, but it is usually more convenient to run scripts directly, without opening a script editor application. There are a number of options for doing so:|2023-09-26-07-31-14
AppleScript|AppleScript Libraries| Re-usable AppleScript modules (available since OS X Mavericks), written in AppleScript or AppleScriptObjC and saved as script files or bundles in certain locations, that can be called from other scripts. When saved as a bundle, a library can include an AppleScript dictionary (sdef) file, thus functioning like a scripting addition but written in AppleScript or AppleScriptObjC.|2023-09-26-07-31-14
AppleScript|AppleScript Studio| A framework for attaching Cocoa interfaces to AppleScript applications, part of the Xcode package in Mac OS X 10.4 and 10.5, now deprecated in favor of AppleScriptObjC.|2023-09-26-07-31-14
AppleScript|AppleScriptObjC| A Cocoa development software framework, also called AppleScript/Objective-C or ASOC, part of the Xcode package since Mac OS X Snow Leopard. AppleScriptObjC allows AppleScripts to use Cocoa classes and methods directly. The following table shows the availability of AppleScriptObjC in various versions of macOS: AppleScriptObjC can be used in all subsequent Mac OS X versions.|2023-09-26-07-31-14
AppleScript|Automator| A graphical, modular editing environment in which workflows are built up from actions. It is intended to duplicate many of the functions of AppleScript without the necessity for programming knowledge. Automator has an action specifically designed to contain and run AppleScripts, for tasks that are too complex for Automator's simplified framework.|2023-09-26-07-31-14
AppleScript|Scriptable core system applications| These background-only applications, packaged with macOS, are used to allow AppleScript to access features that would not normally be scriptable. As of Mac OS X 10.6.3 they include the scriptable applications for:|2023-09-26-07-31-14
AppleScript|Scripting Additions (OSAX)| Plug-ins for AppleScript developed by Apple or third parties. They are designed to extend the built-in command set, expanding AppleScript's features and making it somewhat less dependent on functionality provided by applications. macOS includes a collection of scripting additions referred to as Standard Additions (StandardAdditions.osax) that adds a set of commands and classes that are not part of AppleScript's core features, including user interaction dialogs, reading and writing files, file system commands, date functions, and text and mathematical operations; without this OSAX, AppleScript would have no capacity to perform many basic actions not directly provided by an application.|2023-09-26-07-31-14
AppleScript|Classes (data types)| While applications can define specialized classes (or data types), AppleScript also has a number of built-in classes. These basic data classes are directly supported by the language and tend to be universally recognized by scriptable applications. The most common ones are as follows:|2023-09-26-07-31-14
AppleScript|Language structures|" Many AppleScript processes are managed by blocks of code, where a block begins with a command command and ends with an end command statement. The most important structures are described below. AppleScript offers two kinds of conditionals. The repeat loop of AppleScript comes in several slightly different flavors. They all execute the block between repeat and end repeat lines a number of times. The looping can be prematurely stopped with command exit repeat. Repeat forever. Repeat a given number of times. Conditional loops. The block inside repeat while loop executes as long as the condition evaluates to true. The condition is re-evaluated after each execution of the block. The repeat until loop is otherwise identical, but the block is executed as long as the condition evaluates to false. Loop with a variable. When starting the loop, the variable is assigned to the start value. After each execution of the block, the optional step value is added to the variable. Step value defaults to 1. Enumerate a list. On each iteration set the loopVariable to a new item in the given list One important variation on this block structure is in the form of on ‚Äîend ... blocks that are used to define handlers (function-like subroutines). Handlers begin with on functionName() and ending with end functionName, and are not executed as part of the normal script flow unless called from somewhere in the script. Handlers can also be defined using ""to"" in place of ""on"" and can be written to accept labeled parameters, not enclosed in parens. There are four types of predefined handlers in AppleScript‚Äîrun, open, idle, and quit‚Äîeach of which is created in the same way as the run handler shown above. When a script containing an ""open handler' is saved as an applet, the applet becomes a droplet. A droplet can be identified in the Finder by its icon, which includes an arrow, indicating items can be dropped onto the icon. The droplet's open handler is executed when files or folders are dropped onto droplet's icon. References to the items dropped on the droplet's icon are passed to the droplet's script as the parameter of the open handler. A droplet can also be launched the same way as an ordinary applet, executing its run handler. An idle handler can be used in applets or droplets saved as stay-open applets, and is useful for scripts that watch for particular data or events. The length of the idle time is 30 seconds by default, but can be changed by including a 'return x' statement at the end of the subroutine, where x is the number of seconds the system should wait before running the handler again. Script objects may be defined explicitly using the syntax: Script objects can use the same 'tell' structures that are used for application objects, and can be loaded from and saved to files. Runtime execution time can be reduced in some cases by using script objects."|2023-09-26-07-31-14
AppleScript|Miscellaneous information| Using the same technique for scripting addition commands can reduce errors and improve performance.|2023-09-26-07-31-14
AppleScript|Open Scripting Architecture| An important aspect of the AppleScript implementation is the Open Scripting Architecture (OSA). Apple provides OSA for other scripting languages and third-party scripting/automation products such as QuicKeys and UserLand Frontier, to function on an equal status with AppleScript. AppleScript was implemented as a scripting component, and the basic specs for interfacing such components to the OSA were public, allowing other developers to add their own scripting components to the system. Public client APIs for loading, saving and compiling scripts would work the same for all such components, which also meant that applets and droplets could hold scripts in any of those scripting languages. One feature of the OSA is scripting additions, or OSAX for Open Scripting Architecture eXtension, which were inspired by HyperCard's External Commands. Scripting additions are libraries that allow programmers to extend the function of AppleScript. Commands included as scripting additions are available system-wide, and are not dependent on an application (see also ¬ß¬†AppleScript Libraries). The AppleScript Editor is also able to directly edit and run some of the OSA languages.|2023-09-26-07-31-14
AppleScript|JavaScript for Automation| Under OS X Yosemite and later versions of macOS, the JavaScript for Automation (JXA) component remains the only serious OSA language alternative to AppleScript, though the Macintosh versions of Perl, Python, Ruby, and Tcl all support native means of working with Apple events without being OSA components. JXA also provides an Objective-C (and C language) foreign language interface. Being an environment based on WebKit's JavaScriptCore engine, the JavaScript feature set is in sync with the system Safari browser engine. JXA provides a JavaScript module system and it is also possible to use CommonJS modules via browserify.|2023-09-26-07-31-14
AspectJ|| AspectJ is an aspect-oriented programming (AOP) extension created at PARC for the Java programming language. It is available in Eclipse Foundation open-source projects, both stand-alone and integrated into Eclipse. AspectJ has become a widely used de facto standard for AOP by emphasizing simplicity and usability for end users. It uses Java-like syntax, and included IDE integrations for displaying crosscutting structure since its initial public release in 2001.|2022-06-16-09-04-27
AspectJ|Simple language description| All valid Java programs are also valid AspectJ programs, but AspectJ lets programmers define special constructs called aspects. Aspects can contain several entities unavailable to standard classes. These are: AspectJ also supports limited forms of pointcut-based static checking and aspect reuse (by inheritance). See the AspectJ Programming Guide for a more detailed description of the language.|2022-06-16-09-04-27
AspectJ|AspectJ compatibility and implementations| AspectJ can be implemented in many ways, including source-weaving or bytecode-weaving, and directly in the virtual machine (VM). In all cases, the AspectJ program becomes a valid Java program that runs in a Java VM. Classes affected by aspects are binary-compatible with unaffected classes (to remain compatible with classes compiled with the unaffected originals). Supporting multiple implementations allows the language to grow as technology changes, and being Java-compatible ensures platform availability. Key to its success has been engineering and language decisions that make the language usable and programs deployable. The original Xerox AspectJ implementation used source weaving, which required access to source code. When Xerox contributed the code to Eclipse, AspectJ was reimplemented using the Eclipse Java compiler and a bytecode weaver based on BCEL, so developers could write aspects for code in binary (.class) form. At this time the AspectJ language was restricted to support a per-class model essential for incremental compilation and load-time weaving. This made IDE integrations as responsive as their Java counterparts, and it let developers deploy aspects without altering the build process. This led to increased adoption, as AspectJ became usable for impatient Java programmers and enterprise-level deployments. Since then, the Eclipse team has increased performance and correctness, upgraded the AspectJ language to support Java 5 language features like generics and annotations, and integrated annotation-style pure-java aspects from AspectWerkz. The Eclipse project supports both command-line and Ant interfaces. A related Eclipse project has steadily improved the Eclipse IDE support for AspectJ (called AspectJ Development Tools (AJDT)) and other providers of crosscutting structure. IDE support for emacs, NetBeans, and JBuilder foundered when Xerox put them into open source, but support for Oracle's JDeveloper did appear. IDE support has been key to Java programmers using AspectJ and understanding crosscutting concerns. BEA has offered limited VM support for aspect-oriented extensions, but for extensions supported in all Java VM's would require agreement through Sun's Java Community Process (see also the java.lang.instrument package available since Java SE 5 ‚Äî which is a common ground for JVM load-time instrumentation). Academic interest in the semantics and implementation of aspect-oriented languages has surrounded AspectJ since its release. The leading research implementation of AspectJ is the AspectBench Compiler, or abc; it supports extensions for changing the syntax and semantics of the language and forms the basis for many AOP experiments that the AspectJ team can no longer support, given its broad user base. Many programmers discover AspectJ as an enabling technology for other projects, most notably Spring AOP. A sister Spring project, Spring Roo, automatically maintains AspectJ inter-type declarations as its principal code generation output.|2022-06-16-09-04-27
AspectJ|History and contributors| Gregor Kiczales started and led the Xerox PARC team that eventually developed AspectJ. He coined the term crosscutting. Fourth on the team, Chris Maeda coined the term aspect-oriented programming. Jim Hugunin and Erik Hilsdale (Xerox PARC team members 12 and 13) were the original compiler and weaver engineers, Mik Kersten implemented the IDE integration and started the Eclipse AJDT project with Adrian Colyer (current lead of the AspectJ project) and Andrew Clement (current compiler engineer). The AspectBench Compiler was developed and is maintained as a joint effort of the Programming Tools Group at the Oxford University Computing Laboratory, the Sable Research Group at McGill University, and the Institute for Basic Research in Computer Science (BRICS).|2022-06-16-09-04-27
AspectJ|AspectWerkz| AspectWerkz is a dynamic, lightweight and high-performance AOP/AOSD framework for Java. It has been merged with the AspectJ project, which supports AspectWerkz functionality since AspectJ 5. Jonas Boner and Alex Vasseur engineered the AspectWerkz project, and later contributed to the AspectJ project when it merged in the AspectWerkz annotation style and load-time weaving support. Unlike AspectJ prior to version 5, AspectWerkz did not add any new language constructs to Java, but instead supported declaration of aspects within Java annotations. It utilizes bytecode modification to weave classes at project build-time, class load time, as well as runtime. It uses standardized JVM level APIs[clarify]. Aspects can be defined using either Java annotations (introduced with Java 5), Java 1.3/1.4 custom doclet or a simple XML definition file. AspectWerkz provides an API to use the very same aspects for proxies, hence providing a transparent experience, allowing a smooth transition for users familiar with proxies. AspectWerkz is free software. The LGPL-style license allows the use of AspectWerkz 2.0 in both commercial and open source projects.|2022-06-16-09-04-27
Auto-lead Data Format|| Auto-lead Data Format (ADF) is an open XML-based standard specifically for communicating consumer purchase requests to automotive dealerships. Thirteen leading automotive-related Internet companies developed the ADF standard, and now many vendors of Customer Retention Management systems serving the automotive industry support ADF.|2022-09-22-20-36-40
Auto-lead Data Format|Example XML File| The example found in the Official ADF Specifications (PDF) does not pass XML validation and files based on this example may be rejected by ADF lead software. Below is an example that passes XML validation and has been accepted and parsed by ADF lead software:|2022-09-22-20-36-40
Auto-lead Data Format|Mime Type| As this is not an IETF standard, the recommended mime type to use is application/x-adf+xml|2022-09-22-20-36-40
Auto-lead Data Format|External links| This programming-language-related article is a stub. You can help Wikipedia by expanding it.|2022-09-22-20-36-40
B (programming language)|| B is a programming language developed at Bell Labs circa 1969 by Ken Thompson and Dennis Ritchie. B was derived from BCPL, and its name may possibly be a contraction of BCPL.  Thompson's coworker Dennis Ritchie speculated that the name might be based on Bon, an earlier, but unrelated, programming language that Thompson designed for use on Multics. B was designed for recursive, non-numeric, machine-independent applications, such as system and language software. It was a typeless language, with the only data type being the underlying machine's natural memory word format, whatever that might be. Depending on the context, the word was treated either as an integer or a memory address. As machines with ASCII processing became common, notably the DEC PDP-11 that arrived at Bell, support for character data stuffed in memory words became important. The typeless nature of the language was seen as a disadvantage, which led Thompson and Ritchie to develop an expanded version of the language supporting new internal and user-defined types, which became the C programming language.|2023-08-13-20-24-53
B (programming language)|History|" BCPL semantics with a lot of SMALGOL syntax Circa 1969, Ken Thompson and later Dennis Ritchie developed B basing it mainly on the BCPL language Thompson used in the Multics project. B was essentially the BCPL system stripped of any component Thompson felt he could do without in order to make it fit within the memory capacity of the minicomputers of the time. The BCPL to B transition also included changes made to suit Thompson's preferences (mostly along the lines of reducing the number of non-whitespace characters in a typical program). Much of the typical ALGOL-like syntax of BCPL was rather heavily changed in this process. The assignment operator := reverted to the = of Rutishauser's Superplan, and the equality operator = was replaced by ==. Thompson added ""two-address assignment operators"" using x =+ y syntax to add y to x (in C the operator is written +=). This syntax came from Douglas McIlroy's implementation of TMG, in which B's compiler was first implemented (and it came to TMG from ALGOL 68's x +:= y syntax). Thompson went further by inventing the increment and decrement operators (++ and --). Their prefix or postfix position determines whether the value is taken before or after alteration of the operand. This innovation was not in the earliest versions of B. According to Dennis Ritchie, people often assumed that they were created for the auto-increment and auto-decrement address modes of the DEC PDP-11, but this is historically impossible as the machine didn't exist when B was first developed. The semicolon version of the for loop was borrowed by Ken Thompson from the work of Stephen Johnson. B is typeless, or more precisely has one data type: the computer word. Most operators (e.g. +, -, *, /) treated this as an integer, but others treated it as a memory address to be dereferenced. In many other ways it looked a lot like an early version of C. There are a few library functions, including some that vaguely resemble functions from the standard I/O library in C. 
In Thompson's words: ""B and the old old C were very very similar languages except for all the types [in C]"". Early implementations were for the DEC PDP-7 and PDP-11 minicomputers using early Unix, and Honeywell GE 645 36-bit mainframes running the operating system GCOS. The earliest PDP-7 implementations compiled to threaded code, and Ritchie wrote a compiler using TMG which produced machine code. In 1970 a PDP-11 was acquired and threaded code was used for the port; an assembler, dc, and the B language itself were written in B to bootstrap the computer. An early version of yacc was produced with this PDP-11 configuration. Ritchie took over maintenance during this period. The typeless nature of B made sense on the Honeywell, PDP-7 and many older computers, but was a problem on the PDP-11 because it was difficult to elegantly access the character data type that the PDP-11 and most modern computers fully support. Starting in 1971 Ritchie made changes to the language while converting its compiler to produce machine code, most notably adding data typing for variables. During 1971 and 1972 B evolved into ""New B"" (NB) and then C. B is almost extinct, having been superseded by the C language. However, it continues to see use on GCOS mainframes (as of 2014) 
and on certain embedded systems (as of 2000) for a variety of reasons: limited hardware in small systems, extensive libraries, tooling, licensing cost issues, and simply being good enough for the job. The highly influential AberMUD was originally written in B."|2023-08-13-20-24-53
B (programming language)|Examples| The following examples are from the Users' Reference to B by Ken Thompson:|2023-08-13-20-24-53
B (programming language)|See also| Computer programming portal|2023-08-13-20-24-53
BASIC||" BASIC (Beginners' All-purpose Symbolic Instruction Code) is a family of general-purpose, high-level programming languages designed for ease of use. The original version was created by John G. Kemeny and Thomas E. Kurtz at Dartmouth College in 1963. They wanted to enable students in non-scientific fields to use computers. At the time, nearly all computers required writing custom software, which only scientists and mathematicians tended to learn. In addition to the program language, Kemeny and Kurtz developed the Dartmouth Time Sharing System (DTSS), which allowed multiple users to edit and run BASIC programs simultaneously on remote terminals. This general model became popular on minicomputer systems like the PDP-11 and Data General Nova in the late 1960s and early 1970s. Hewlett-Packard produced an entire computer line for this method of operation, introducing the HP2000 series in the late 1960s and continuing sales into the 1980s. Many early video games trace their history to one of these versions of BASIC. The emergence of microcomputers in the mid-1970s led to the development of multiple BASIC dialects, including Microsoft BASIC in 1975. Due to the tiny main memory available on these machines, often 4¬†KB, a variety of Tiny BASIC dialects were also created. BASIC was available for almost any system of the era, and became the de facto programming language for home computer systems that emerged in the late 1970s. These PCs almost always had a BASIC interpreter installed by default, often in the machine's firmware or sometimes on a ROM cartridge. BASIC declined in popularity in the 1990s, as more powerful microcomputers came to market and programming languages with advanced features (such as Pascal and C) became tenable on such computers. In 1991, Microsoft released Visual Basic, combining an updated version of BASIC with a visual forms builder. This reignited use of the language and ""VB"" remains a major programming language in the form of VB.NET."|2023-09-24-05-20-45
BASIC|Origin|" John G. Kemeny was the math department chairman at Dartmouth College. Based largely on his reputation as an innovator in math teaching, in 1959 the school won an Alfred P. Sloan Foundation award for $500,000 to build a new department building. Thomas E. Kurtz had joined the department in 1956, and from the 1960s Kemeny and Kurtz agreed on the need for programming literacy among students outside the traditional STEM fields. Kemeny later noted that ""Our vision was that every student on campus should have access to a computer, and any faculty member should be able to use a computer in the classroom whenever appropriate. It was as simple as that."" Kemeny and Kurtz had made two previous experiments with simplified languages, DARSIMCO (Dartmouth Simplified Code) and DOPE (Dartmouth Oversimplified Programming Experiment). These did not progress past a single freshman class. New experiments using Fortran and ALGOL followed, but Kurtz concluded these languages were too tricky for what they desired. As Kurtz noted, Fortran had numerous oddly-formed commands, notably an ""almost impossible-to-memorize convention for specifying a loop: DO 100, I = 1, 10, 2. Is it '1, 10, 2' or '1, 2, 10', and is the comma after the line number required or not?"" Moreover, the lack of any sort of immediate feedback was a key problem; the machines of the era used batch processing and took a long time to complete a run of a program. While Kurtz was visiting MIT, John McCarthy suggested that time-sharing offered a solution; a single machine could divide up its processing time among many users, giving them the illusion of having a (slow) computer to themselves. Small programs would return results in a few seconds. This led to increasing interest in a system using time-sharing and a new language specifically for use by non-STEM students. Kemeny wrote the first version of BASIC. The acronym BASIC comes from the name of an unpublished paper by Thomas Kurtz. The new language was heavily patterned on FORTRAN II; statements were one-to-a-line, numbers were used to indicate the target of loops and branches, and many of the commands were similar or identical to Fortran. However, the syntax was changed wherever it could be improved. For instance, the difficult to remember DO loop was replaced by the much easier to remember FOR I = 1 TO 10 STEP 2, and the line number used in the DO was instead indicated by the NEXT I. Likewise, the cryptic IF statement of Fortran, whose syntax matched a particular instruction of the machine on which it was originally written, became the simpler IF I=5 THEN GOTO 100. These changes made the language much less idiosyncratic while still having an overall structure and feel similar to the original FORTRAN. The project received a $300,000 grant from the National Science Foundation, which was used to purchase a GE-225 computer for processing, and a Datanet-30 realtime processor to handle the Teletype Model 33 teleprinters used for input and output. A team of a dozen undergraduates worked on the project for about a year, writing both the DTSS system and the BASIC compiler. The first version BASIC language was released on 1 May 1964. Initially, BASIC concentrated on supporting straightforward mathematical work, with matrix arithmetic support from its initial implementation as a batch language, and character string functionality being added by 1965. Usage in the university rapidly expanded, requiring the main CPU to be replaced by a GE-235, and still later by a GE-635. By the early 1970s there were hundreds of terminals connected to the machines at Dartmouth, some of them remotely. Wanting use of the language to become widespread, its designers made the compiler available free of charge. In the 1960s, software became a chargeable commodity; until then, it was provided without charge as a service with expensive computers, usually available only to lease. They also made it available to high schools in the Hanover, New Hampshire, area and regionally throughout New England on Teletype Model 33 and Model 35 teleprinter terminals connected to Dartmouth via dial-up phone lines, and they put considerable effort into promoting the language. In the following years, as other dialects of BASIC appeared, Kemeny and Kurtz's original BASIC dialect became known as Dartmouth BASIC. New Hampshire recognized the accomplishment in 2019 when it erected a highway historical marker in Hanover describing the creation of ""the first user-friendly programming language""."|2023-09-24-05-20-45
BASIC|Spread on time-sharing services|" The emergence of BASIC took place as part of a wider movement towards time-sharing systems. First conceptualized during the late 1950s, the idea became so dominant in the computer industry by the early 1960s that its proponents were speaking of a future in which users would ""buy time on the computer much the same way that the average household buys power and water from utility companies"". General Electric, having worked on the Dartmouth project, wrote their own underlying operating system and launched an online time-sharing system known as Mark I. It featured BASIC as one of its primary selling points. Other companies in the emerging field quickly followed suit; Tymshare introduced SUPER BASIC in 1968, CompuServe had a version on the DEC-10 at their launch in 1969, and by the early 1970s BASIC was largely universal on general-purpose mainframe computers. Even IBM eventually joined the club with the introduction of VS-BASIC in 1973. Although time-sharing services with BASIC were successful for a time, the widespread success predicted earlier was not to be. The emergence of minicomputers during the same period, and especially low-cost microcomputers in the mid-1970s, allowed anyone to purchase and run their own systems rather than buy online time which was typically billed at dollars per minute."|2023-09-24-05-20-45
BASIC|Spread on minicomputers| BASIC, by its very nature of being small, was naturally suited to porting to the minicomputer market, which was emerging at the same time as the time-sharing services. These machines had small main memory, perhaps as little as 4¬†KB in modern terminology, and lacked high-performance storage like hard drives that make compilers practical. On these systems, BASIC was normally implemented as an interpreter rather than a compiler due to its lower requirement for working memory. A particularly important example was HP Time-Shared BASIC, which, like the original Dartmouth system, used two computers working together to implement a time-sharing system. The first, a low-end machine in the HP 2100 series, was used to control user input and save and load their programs to tape or disk. The other, a high-end version of the same underlying machine, ran the programs and generated output. For a cost of about $100,000, one could own a machine capable of running between 16 and 32 users at the same time. The system, bundled as the HP 2000, was the first mini platform to offer time-sharing and was an immediate runaway success, catapulting HP to become the third-largest vendor in the minicomputer space, behind DEC and Data General (DG). DEC, the leader in the minicomputer space since the mid-1960s, had initially ignored BASIC. This was due to their work with RAND Corporation, who had purchased a PDP-6 to run their JOSS language, which was conceptually very similar to BASIC. This led DEC to introduce a smaller, cleaned up version of JOSS known as FOCAL, which they heavily promoted in the late 1960s. However, with timesharing systems widely offering BASIC, and all of their competition in the minicomputer space doing the same, DEC's customers were clamoring for BASIC. After management repeatedly ignored their pleas, David H. Ahl took it upon himself to buy a BASIC for the PDP-8, which was a major success in the education market. By the early 1970s, FOCAL and JOSS had been forgotten and BASIC had become almost universal in the minicomputer market. DEC would go on to introduce their updated version, BASIC-PLUS, for use on the RSTS/E time-sharing operating system. During this period a number of simple text-based games were written in BASIC, most notably Mike Mayfield's Star Trek. David Ahl collected these, some ported from FOCAL, and published them in an educational newsletter he compiled. He later collected a number of these into book form, 101 BASIC Computer Games, published in 1973. During the same period, Ahl was involved in the creation of a small computer for education use, an early personal computer. When management refused to support the concept, Ahl left DEC in 1974 to found the seminal computer magazine, Creative Computing. The book remained popular, and was re-published on several occasions.|2023-09-24-05-20-45
BASIC|Explosive growth: the home computer era|" The introduction of the first microcomputers in the mid-1970s was the start of explosive growth for BASIC. It had the advantage that it was fairly well known to the young designers and computer hobbyists who took an interest in microcomputers, many of whom had seen BASIC on minis or mainframes. Despite Dijkstra's famous judgement in 1975, ""It is practically impossible to teach good programming to students that have had a prior exposure to BASIC: as potential programmers they are mentally mutilated beyond hope of regeneration"", BASIC was one of the few languages that was both high-level enough to be usable by those without training and small enough to fit into the microcomputers of the day, making it the de facto standard programming language on early microcomputers. The first microcomputer version of BASIC was co-written by Bill Gates, Paul Allen and Monte Davidoff for their newly formed company, Micro-Soft. This was released by MITS in punch tape format for the Altair 8800 shortly after the machine itself, immediately cementing BASIC as the primary language of early microcomputers. Members of the Homebrew Computer Club began circulating copies of the program, causing Gates to write his Open Letter to Hobbyists, complaining about this early example of software piracy. Partially in response to Gates's letter, and partially to make an even smaller BASIC that would run usefully on 4¬†KB machines, Bob Albrecht urged Dennis Allison to write their own variation of the language. How to design and implement a stripped-down version of an interpreter for the BASIC language was covered in articles by Allison in the first three quarterly issues of the People's Computer Company newsletter published in 1975 and implementations with source code published in Dr. Dobb's Journal of Tiny BASIC Calisthenics & Orthodontia: Running Light Without Overbyte. This led to a wide variety of Tiny BASICs with added features or other improvements, with versions from Tom Pittman and Li-Chen Wang becoming particularly well known. Micro-Soft, by this time Microsoft, ported their interpreter for the MOS 6502, which quickly become one of the most popular microprocessors of the 8-bit era. When new microcomputers began to appear, notably the ""1977 trinity"" of the TRS-80, Commodore PET and Apple II, they either included a version of the MS code, or quickly introduced new models with it. Ohio Scientific's personal computers also joined this trend at that time. By 1978, MS BASIC was a de facto standard and practically every home computer of the 1980s included it in ROM. Upon boot, a BASIC interpreter in direct mode was presented. Commodore Business Machines included Commodore BASIC, based on Microsoft BASIC. The Apple II and TRS-80 each had two versions of BASIC, a smaller introductory version introduced with the initial releases of the machines and an MS-based version introduced as interest in the platforms increased. As new companies entered the field, additional versions were added that subtly changed the BASIC family. The Atari 8-bit family had its own Atari BASIC that was modified in order to fit on an 8¬†KB ROM cartridge. Sinclair BASIC was introduced in 1980 with the Sinclair ZX80, and was later extended for the Sinclair ZX81 and the Sinclair ZX Spectrum. The BBC published BBC BASIC, developed by Acorn Computers Ltd, incorporating many extra structured programming keywords and advanced floating-point operation features. As the popularity of BASIC grew in this period, computer magazines published complete source code in BASIC for video games, utilities, and other programs. Given BASIC's straightforward nature, it was a simple matter to type in the code from the magazine and execute the program. Different magazines were published featuring programs for specific computers, though some BASIC programs were considered universal and could be used in machines running any variant of BASIC (sometimes with minor adaptations). Many books of type-in programs were also available, and in particular, Ahl published versions of the original 101 BASIC games converted into the Microsoft dialect and published it from Creative Computing as BASIC Computer Games. This book, and its sequels, provided hundreds of ready-to-go programs that could be easily converted to practically any BASIC-running platform. The book reached the stores in 1978, just as the home computer market was starting off, and it became the first million-selling computer book. Later packages, such as Learn to Program BASIC would also have gaming as an introductory focus. On the business-focused CP/M computers which soon became widespread in small business environments, Microsoft BASIC (MBASIC) was one of the leading applications. In 1978, David Lien published the first edition of The BASIC Handbook: An Encyclopedia of the BASIC Computer Language, documenting keywords across over 78 different computers. By 1981, the second edition documented keywords from over 250 different computers, showcasing the explosive growth of the microcomputer era."|2023-09-24-05-20-45
BASIC|IBM PC and compatibles|" When IBM was designing the IBM PC, they followed the paradigm of existing home computers in having a built-in BASIC interpreter. They sourced this from Microsoft ‚Äì IBM Cassette BASIC ‚Äì but Microsoft also produced several other versions of BASIC for MS-DOS/PC DOS including IBM Disk BASIC (BASIC D), IBM BASICA (BASIC A), GW-BASIC (a BASICA-compatible version that did not need IBM's ROM) and QBasic, all typically bundled with the machine. In addition they produced the Microsoft BASIC Compiler aimed at professional programmers. Turbo Pascal-publisher Borland published Turbo Basic 1.0 in 1985 (successor versions are still being marketed under the name PowerBASIC). These later variations introduced many extensions, such as improved string manipulation and graphics support, access to the file system and additional data types. More important were the facilities for structured programming, including additional control structures and proper subroutines supporting local variables. However, by the latter half of the 1980s, users were increasingly using pre-made applications written by others rather than learning programming themselves; while professional programmers now had a wide range of more advanced languages available on small computers. C and later C++ became the languages of choice for professional ""shrink wrap"" application development."|2023-09-24-05-20-45
BASIC|Visual Basic|" In 1991, Microsoft introduced Visual Basic, an evolutionary development of QuickBASIC. It included constructs from that language such as block-structured control statements, parameterized subroutines and optional static typing as well as object-oriented constructs from other languages such as ""With"" and ""For Each"". The language retained some compatibility with its predecessors, such as the Dim keyword for declarations, ""Gosub""/Return statements and optional line numbers which could be used to locate errors. An important driver for the development of Visual Basic was as the new macro language for Microsoft Excel, a spreadsheet program. To the surprise of many at Microsoft who still initially marketed it as a language for hobbyists, the language came into widespread use for small custom business applications shortly after the release of VB version 3.0, which is widely considered the first relatively stable version. While many advanced programmers still scoffed at its use, VB met the needs of small businesses efficiently as by that time, computers running Windows 3.1 had become fast enough that many business-related processes could be completed ""in the blink of an eye"" even using a ""slow"" language, as long as large amounts of data were not involved. Many small business owners found they could create their own small, yet useful applications in a few evenings to meet their own specialized needs. Eventually, during the lengthy lifetime of VB3, knowledge of Visual Basic had become a marketable job skill. Microsoft also produced VBScript in 1996 and Visual Basic .NET in 2001. The latter has essentially the same power as C# and Java but with syntax that reflects the original Basic language. The IDE, with its event-driven GUI builder, was also influential on other tools, most notably Borland Software's Delphi for Object Pascal and its own descendants such as Lazarus. Mainstream support for the final version 6.0 of the original Visual Basic ended on March 31, 2005, followed by extended support in March 2008. Owing to its persistent remaining popularity, third-party attempts to further support it, such as Rubberduck, exist. On February 2, 2017 Microsoft announced that development on VB.NET would no longer be in parallel with that of C#, and on March 11, 2020 it was announced that evolution of the VB.NET language had also concluded, although it was still supported and the third-party Mercury extension has since been produced. Meanwhile, competitors exist such as B4X, RAD Basic, twinBASIC, VisualFBEditor, Xojo, and Gambas."|2023-09-24-05-20-45
BASIC|Post-1990 versions and dialects| Many other BASIC dialects have also sprung up since 1990, including the open source QB64 and FreeBASIC, inspired by QBasic, and the Visual Basic-styled RapidQ, HBasic, Basic For Qt and Gambas. Modern commercial incarnations include PureBasic, PowerBASIC, Xojo, Monkey X and True BASIC (the direct successor to Dartmouth BASIC from a company controlled by Kurtz). Several web-based simple BASIC interpreters also now exist, including Microsoft's Small Basic. Many versions of BASIC are also now available for  smartphones and tablets via the Apple App Store, or Google Play store for Android. On game consoles, an application for the Nintendo 3DS and Nintendo DSi called Petit Computer allows for programming in a slightly modified version of BASIC with DS button support. A version has also been released for Nintendo Switch.|2023-09-24-05-20-45
BASIC|Calculators| Variants of BASIC are available on graphing and otherwise programmable calculators made by Texas Instruments, HP, Casio, and others.|2023-09-24-05-20-45
BASIC|Windows command-line| QBasic, a version of Microsoft QuickBASIC without the linker to make EXE files, is present in the Windows NT and DOS-Windows 95 streams of operating systems and can be obtained for more recent releases like Windows 7 which do not have them. Prior to DOS 5, the Basic interpreter was GW-Basic. QuickBasic is part of a series of three languages issued by Microsoft for the home and office power user and small-scale professional development; QuickC and QuickPascal are the other two. For Windows 95 and 98, which do not have QBasic installed by default, they can be copied from the installation disc, which will have a set of directories for old and optional software; other missing commands like Exe2Bin and others are in these same directories.|2023-09-24-05-20-45
BASIC|Other| The various Microsoft, Lotus, and Corel office suites and related products are programmable with Visual Basic in one form or another, including LotusScript, which is very similar to VBA 6. The Host Explorer terminal emulator uses WWB as a macro language; or more recently the programme and the suite in which it is contained is programmable in an in-house Basic variant known as Hummingbird Basic. The VBScript variant is used for programming web content, Outlook 97, Internet Explorer, and the Windows Script Host. WSH also has a Visual Basic for Applications (VBA) engine installed as the third of the default engines along with VBScript, JScript, and the numerous proprietary or open source engines which can be installed like PerlScript, a couple of Rexx-based engines, Python, Ruby, Tcl, Delphi, XLNT, PHP, and others; meaning that the two versions of Basic can be used along with the other mentioned languages, as well as LotusScript, in a WSF file, through the component object model, and other WSH and VBA constructions. VBScript is one of the languages that can be accessed by the 4Dos, 4NT, and Take Command enhanced shells. SaxBasic and WWB are also very similar to the Visual Basic line of Basic implementations. The pre-Office 97 macro language for Microsoft Word is known as WordBASIC. Excel 4 and 5 use Visual Basic itself as a macro language. Chipmunk Basic, an old-school interpreter similar to BASICs of the 1970s, is available for Linux, Microsoft Windows and macOS.|2023-09-24-05-20-45
BASIC|Legacy|" The ubiquity of BASIC interpreters on personal computers was such that textbooks once included simple ""Try It In BASIC"" exercises that encouraged students to experiment with mathematical and computational concepts on classroom or home computers. Popular computer magazines of the day typically included type-in programs. Futurist and sci-fi writer David Brin mourned the loss of ubiquitous BASIC in a 2006 Salon article as have others who first used computers during this era. In turn, the article prompted Microsoft to develop and release Small Basic; it also inspired similar projects like Basic-256. Dartmouth held a 50th anniversary celebration for BASIC on 1 May 2014, as did other organisations; at least one organisation of VBA programmers organised a 35th anniversary observance in 1999. Dartmouth College celebrated the 50th anniversary of the BASIC language with a day of events on April 30, 2014. A short documentary film was produced for the event."|2023-09-24-05-20-45
BASIC|Data types and variables|" Minimal versions of BASIC had only integer variables and one- or two-letter variable names, which minimized requirements of limited and expensive memory (RAM). More powerful versions had floating-point arithmetic, and variables could be labelled with names six or more characters long. There were some problems and restrictions in early implementations; for example, Applesoft BASIC allowed variable names to be several characters long, but only the first two were significant, thus it was possible to inadvertently write a program with variables ""LOSS"" and ""LOAN"", which would be treated as being the same; assigning a value to ""LOAN"" would silently overwrite the value intended as ""LOSS"". Keywords could not be used in variables in many early BASICs; ""SCORE"" would be interpreted as ""SC"" OR ""E"", where OR was a keyword. String variables are usually distinguished in many microcomputer dialects by having $ suffixed to their name as a sigil, and values are often identified as strings by being delimited by ""double quotation marks"". Arrays in BASIC could contain integers, floating point or string variables. Some dialects of BASIC supported matrices and matrix operations, which can be used to solve sets of simultaneous linear algebraic equations. These dialects would directly support matrix operations such as assignment, addition, multiplication (of compatible matrix types), and evaluation of a determinant. Many microcomputer BASICs did not support this data type; matrix operations were still possible, but had to be programmed explicitly on array elements."|2023-09-24-05-20-45
BASIC|Examples|" New BASIC programmers on a home computer might start with a simple program, perhaps using the language's PRINT statement to display a message on the screen; a well-known and often-replicated example is Kernighan and Ritchie's ""Hello, World!"" program: An infinite loop could be used to fill the display with the message: Note that the END statement is optional and has no action in most dialects of BASIC. It was not always included, as is the case in this example. This same program can be modified to print a fixed number of messages using the common FOR...NEXT statement: Most home computers BASIC versions, such as MSX BASIC and GW-BASIC, supported simple data types, loop cycles, and arrays. The following example is written for GW-BASIC, but will work in most versions of BASIC with minimal changes: The resulting dialog might resemble: The original Dartmouth Basic was unusual in having a matrix keyword, MAT. Although not implemented by most later microprocessor derivatives, it is used in this example from the 1968 manual which averages the numbers that are input: Second-generation BASICs (for example, VAX Basic, SuperBASIC, True BASIC, QuickBASIC, BBC BASIC, Pick BASIC, PowerBASIC, Liberty BASIC, QB64 and (arguably) COMAL) introduced a number of features into the language, primarily related to structured and procedure-oriented programming. Usually, line numbering is omitted from the language and replaced with labels (for GOTO) and procedures to encourage easier and more flexible design. In addition keywords and structures to support repetition, selection and procedures with local variables were introduced. The following example is in Microsoft QuickBASIC: Third-generation BASIC dialects such as Visual Basic, Xojo, Gambas, StarOffice Basic, BlitzMax and PureBasic introduced features to support object-oriented and event-driven programming paradigm. Most built-in procedures and functions are now represented as methods of standard objects rather than operators. Also, the operating system became increasingly accessible to the BASIC language. The following example is in Visual Basic .NET:"|2023-09-24-05-20-45
Behavioral Description Language|| Behavioral Description Language (BDL) is a programming language based on ANSI C with extensions for hardware description, developed to describe hardware at levels ranging from the algorithm level to the functional level.[citation needed] Although the term Behavioral Description Language is a generic term and can refer to multiple high-level description languages, NEC Corporation has developed a C-subset called BDL for High-Level Synthesis. This C-subset includes its own data types (called var-class), special constants for hardware design e.g. high impedance, timing descriptors and control statements. As BDL is meant for Hardware synthesis, the complete ANSI-C syntax is not supported. The principal unsupported operations are: (i) Floating point data types (ii) Sizeof operator (iii) unions and (iv) Recursive functions. BDL is sometimes also known as Cyber C because it is synthesized using NEC's High-Level Synthesis tool called CyberWorkBench [1].|2021-11-07-17-15-18
BLISS||" BLISS is a system programming language developed at Carnegie Mellon University (CMU) by W. A. Wulf, D. B. Russell, and A. N. Habermann around 1970. It was perhaps the best known system language until C debuted a few years later. Since then, C became popular and common, and BLISS faded into obscurity. When C was in its infancy, a few projects within Bell Labs debated the merits of BLISS vs. C.[citation needed] BLISS is a typeless block-structured programming language based on expressions rather than statements, and includes constructs for exception handling, coroutines, and macros. It does not include a goto statement. The name is variously said to be short for Basic Language for Implementation of System Software or System Software Implementation Language, Backwards. However, in his 2015 oral history for the Babbage Institute's Computer Security History Project, Wulf claimed that the acronym was originally based on the name ""Bill's Language for Implementing System Software."" The original Carnegie Mellon compiler was notable for its extensive use of optimizations, and formed the basis of the classic book The Design of an Optimizing Compiler. Digital Equipment Corporation (DEC) developed and maintained BLISS compilers for the PDP-10, PDP-11, VAX, DEC PRISM, MIPS, DEC Alpha, and Intel IA-32, The language did not become popular among customers and few had the compiler, but DEC used it heavily in-house into the 1980s; most of the utility programs for the OpenVMS operating system were written in BLISS-32. The DEC BLISS compiler has been ported to the IA-64 and x86-64 architectures as part of the ports of OpenVMS to these platforms. The x86-64 BLISS compiler uses LLVM as its backend code generator, replacing the proprietary GEM backend used for Alpha and IA-64."|2023-05-07-02-14-20
BLISS|Language description|" [excessive¬†quote] BLISS has many of the features of other modern high-level languages. It has block structure, an automatic stack, and mechanisms for defining and calling recursive routines¬†... provides a variety of predefined data structures and¬†... facilities for testing and iteration¬†... On the other hand, BLISS omits certain features of other high-level languages. It does not have built-in facilities for input/output, because a system-software project usually develops its own input/output or builds on basic monitor I/O or screen management services¬†... it permits access to machine-specific features, because system software often requires this. BLISS has characteristics that are unusual among high-level languages. A name¬†... is uniformly interpreted as the address of that segment rather than the value of the segment¬†... Also, BLISS is an ""expression language"" rather than a ""statement language"". This means that every construct of the language that is not a declaration is an expression. Expressions produce a value as well as possibly causing an action such as modification of storage, transfer of control, or execution of a program loop. For example, the counterpart of an assignment ""statement"" in BLISS is, strictly speaking, an expression that itself has a value. The value of an expression can be either used or discarded in BLISS¬†... Finally, BLISS includes a macro facility that provides a level of capability usually found only in macro-assemblers. The BLISS language has the following characteristics:"|2023-05-07-02-14-20
BLISS|Source example| The following example is taken verbatim from the Bliss Language Manual:|2023-05-07-02-14-20
Boo (programming language)|| Boo is an object-oriented, statically typed, general-purpose programming language that seeks to make use of the Common Language Infrastructure's support for Unicode, internationalization, and web applications, while using a Python-inspired syntax and a special focus on language and compiler extensibility. Some features of note include type inference, generators, multimethods, optional duck typing, macros, true closures, currying, and first-class functions. Boo was one of the three scripting languages for the Unity game engine (Unity Technologies employed De Oliveira, its designer), until official support was dropped in 2014 due to the small userbase. The Boo Compiler was removed from the engine in 2017. Boo has since been abandoned by De Oliveira, with development being taken over by Mason Wheeler. Boo is free software released under the BSD 3-Clause license. It is compatible with the Microsoft .NET and Mono frameworks.|2023-05-06-23-47-37
Boomerang (programming language)|| Boomerang is a programming language for writing lenses‚Äîwell-behaved bidirectional transformations ‚Äîthat operate on ad-hoc, textual data formats. Boomerang grew out of the Harmony generic data synchronizer, which grew out of the Unison file synchronization project.|2022-10-21-09-43-42
Boomerang (programming language)|External links| This computer-programming-related article is a stub. You can help Wikipedia by expanding it.|2022-10-21-09-43-42
Bs (programming language)||" bs is a programming language and a compiler/interpreter for modest-sized programs on UNIX systems. The bs command can be invoked either for interactive programming or with a file containing a program, optionally taking arguments, via a Unix shell, e.g., using a Shebang (Unix) #!/usr/bin/bs. An early man page states, ""[bs] is a remote descendant of Basic [sic] and SNOBOL4, with a little C thrown in."""|2021-07-25-12-16-45
Bs (programming language)|History|" The bs command appears in UNIX System III Release 3.0 (1980), first released outside of Bell Labs in 1982. It was written by Dick Haight (Richard C. Haight) circa 1978, who recounts it as follows: I wrote bs at the time Unix (V 3?) and all of the commands were being converted from assembler to C. So [ Ken Thompson‚Äôs ] bas became my bs ‚Äî sort of. The Release 3.0 manual mentions bs prominently on page 9 (emphasis added): Writing a program. To enter the text of a source program into a UNIX file, use ed(1). The four principal languages available under UNIX are C (see cc(1)), Fortran (see f77(1)), bs (a compiler/interpreter in the spirit of Basic, see bs(1)), and assembly language (see as(1)). While not released outside prior to System III, the bs command was present internally in UNIX/TS 1.0 (November 1978), PWB/UNIX 2.0 (June 1979), and CB UNIX editions 2.1 (November 1979) and 2.3 (1981).
The bs command does not appear in some earlier internal releases, e.g., the UNIX Support Group‚Äôs March 1977 release, nor the PWB/UNIX manual dated May, 1977, suggesting its creation circa 1978. It does not appear in any version of Research Unix nor the Berkeley Software Distribution. Subsequently and into the 1990s, bs was included in a variety of System III-derived or System V-derived commercial operating systems including, but not limited to: PC/IX; UNIX System V Releases 2 & 3: SVR2, SVR3, SVR3.2 (1986); HP-UX; AIX; and A/UX.
(The User's Manual for the AT&T UNIX PC (3B1) specifically mentions that the bs command is not available, but that it is available on SVR3.2.) Occasionally, bs was touted as one of the primary programming languages for development under UNIX. However, bs is not included in the POSIX.1 commands and utilities (the standard List of Unix commands) nor in the Single UNIX Specification and is not provided with most contemporary operating systems. For example in Linux, similar syntax and functionality is provided by bc, Perl, and POSIX shell. In the 21st century, bs is present in, at least, HP-UX Release 11i (2000), as well as AIX versions 6.1 (2007) and 7.2 (2018), likely due to their UNIX System V heritage."|2021-07-25-12-16-45
Bs (programming language)|Design and features|" The bs man page, ostensibly the programming language's only specification, characterizes it as follows: Bs is designed for programming tasks where program development time is as important as the resulting speed of execution. Formalities of data declaration and file/process manipulation are minimized. Line-at-a-time debugging, the trace and dump statements, and useful run-time  error messages all simplify program testing. Furthermore, incomplete programs can be debugged; inner functions can be tested before outer functions have been written and vice versa. A bs program is compiled and executed differently from programs written in the other principal Unix programming languages of the time: C, FORTRAN, and assembly language, whose respective commands compile program source code to executable assembler output (a.out). Instead, a bs program is, first, converted by the bs command to an internal reverse Polish (RPN) intermediate representation and then executed by the command's internal virtual stack machine. The bs language, thus, is a hybrid interpreter and compiler and a divergence in Unix programming from Ancient Unix. The bs language shares some features and syntax with BASIC, SNOBOL, and C, the two former presumably inspiring its name. Like BASIC, it can be used interactively, either executing statements immediately or collecting them into a program to be executed subsequently. Like in SNOBOL4, the assignment operator (=) is used for I/O and bs can execute code in strings, using its eval function. It also includes SNOBOL's interrogation operator (?) used to test whether an expression evaluation succeeds or not. The built-in format function, limited to one argument, supports a subset of C's printf format conversion specifiers, e.g., ""%f"". The language has some conspicuous elements. For instance, its program functions are defined using the fun ... nuf syntax and its functions can have local variables. Also, bs can operate in two modes, either interpreting (and executing) statements and programs or compiling them, and switching between the two using compile and stop. Otherwise, its functionality is unique only collectively (in one language), since individual features are redundant with those of coexisting tools, such as the Unix Shell, e.g., file I/O and loops, and AWK, e.g., associative arrays and Regular expression matching. The bs language was meant for convenient development and debugging of small, modular programs. It has a collection of syntax and features from prior, popular languages but it is internally compiled, unlike a Shell script. As such, in purpose, design, and function, bs is a largely unknown, modest predecessor of hybrid interpreted/compiled languages such as Perl and Python."|2021-07-25-12-16-45
Bs (programming language)|Syntax Examples| The following examples are derived from an A/UX bs(1) man page. This example uses bs as a calculator: This example is the outline of a typical bs program: This example demonstrates I/O:|2021-07-25-12-16-45
Bs (programming language)|Sample Program| The following is a sample bs program that emits the words to the song 99 Bottles of Beer using /usr/bin/bs.|2021-07-25-12-16-45
Cameleon (programming language)|| Cameleon is a free and open source graphical language for functional programming, released under an MIT License. Cameleon language is a graphical data flow language following a two-scale paradigm. It allows an easy up-scale, that is, the integration of any library writing in C++ into the data flow language. Cameleon language aims to democratize macro-programming by an intuitive interaction between the human and the computer where building an application based on a data-process and a GUI is a simple task to learn and to do. Cameleon language allows conditional execution and repetition to solve complex macro-problems. Cameleon is built on an extension of the petri net model for the description of how the Cameleon language executes a composition.|2023-03-14-21-48-15
Cameleon (programming language)|External links| This programming-language-related article is a stub. You can help Wikipedia by expanding it.|2023-03-14-21-48-15
C Sharp (programming language)|| C# (pronounced See sharp) is a general-purpose high-level programming language supporting multiple paradigms. C# encompasses static typing, strong typing, lexically scoped, imperative, declarative, functional, generic, object-oriented (class-based), and component-oriented programming disciplines. The C# programming language was designed by Anders Hejlsberg from Microsoft in 2000 and was later approved as an international standard by Ecma (ECMA-334) in 2002 and ISO/IEC (ISO/IEC 23270) in 2003. Microsoft introduced C# along with .NET Framework and Visual Studio, both of which were closed-source. At the time, Microsoft had no open-source products. Four years later, in 2004, a free and open-source project called Mono began, providing a cross-platform compiler and runtime environment for the C# programming language. A decade later, Microsoft released Visual Studio Code (code editor), Roslyn (compiler), and the unified .NET platform (software framework), all of which support C# and are free, open-source, and cross-platform. Mono also joined Microsoft but was not merged into .NET. As of November 2022, the most recent stable version of the language is C# 11.0, which was released in 2022 in .NET 7.0.|2023-09-26-01-52-24
C Sharp (programming language)|Design goals| The Ecma standard lists these design goals for C#:|2023-09-26-01-52-24
C Sharp (programming language)|History|" During the development of the .NET Framework, the class libraries were originally written using a managed code compiler system called ""Simple Managed C"" (SMC). In January 1999, Anders Hejlsberg formed a team to build a new language at the time called Cool, which stood for ""C-like Object Oriented Language"". Microsoft had considered keeping the name ""Cool"" as the final name of the language, but chose not to do so for trademark reasons. By the time the .NET project was publicly announced at the July 2000 Professional Developers Conference, the language had been renamed C#, and the class libraries and ASP.NET runtime had been ported to C#. Hejlsberg is C#'s principal designer and lead architect at Microsoft, and was previously involved with the design of Turbo Pascal, Embarcadero Delphi (formerly CodeGear Delphi, Inprise Delphi and Borland Delphi), and Visual J++. In interviews and technical papers he has stated that flaws in most major programming languages (e.g. C++, Java, Delphi, and Smalltalk) drove the fundamentals of the Common Language Runtime (CLR), which, in turn, drove the design of the C# language itself. James Gosling, who created the Java programming language in 1994, and Bill Joy, a co-founder of Sun Microsystems, the originator of Java, called C# an ""imitation"" of Java; Gosling further said that ""[C# is] sort of Java with reliability, productivity and security deleted."" Klaus Kreft and Angelika Langer (authors of a C++ streams book) stated in a blog post that ""Java and C# are almost identical programming languages. Boring repetition that lacks innovation"", ""Hardly anybody will claim that Java or C# are revolutionary programming languages that changed the way we write programs,"" and ""C# borrowed a lot from Java ‚Äì and vice versa. Now that C# supports boxing and unboxing, we'll have a very similar feature in Java."" In July 2000, Hejlsberg said that C# is ""not a Java clone"" and is ""much closer to C++"" in its design. Since the release of C# 2.0 in November 2005, the C# and Java languages have evolved on increasingly divergent trajectories, becoming two quite different languages. One of the first major departures came with the addition of generics to both languages, with vastly different implementations. C# makes use of reification to provide ""first-class"" generic objects that can be used like any other class, with code generation performed at class-load time.
Furthermore, C# has added several major features to accommodate functional-style programming, culminating in the LINQ extensions released with C# 3.0 and its supporting framework of lambda expressions, extension methods, and anonymous types.  These features enable C# programmers to use functional programming techniques, such as closures, when it is advantageous to their application. The LINQ extensions and the functional imports help developers reduce the amount of boilerplate code that is included in common tasks like querying a database, parsing an xml file, or searching through a data structure, shifting the emphasis onto the actual program logic to help improve readability and maintainability. C# used to have a mascot called Andy (named after Anders Hejlsberg). It was retired on January 29, 2004. C# was originally submitted to the ISO/IEC JTC 1 subcommittee SC 22 for review, under ISO/IEC 23270:2003, was withdrawn and was then approved under ISO/IEC 23270:2006. The 23270:2006 is withdrawn under 23270:2018 and approved with this version."|2023-09-26-01-52-24
C Sharp (programming language)|Name|" Microsoft first used the name C# in 1988 for a variant of the C language designed for incremental compilation. That project was not completed, and the name was later reused. The name ""C sharp"" was inspired by the musical notation whereby a sharp symbol indicates that the written note should be made a semitone higher in pitch.
This is similar to the language name of C++, where ""++"" indicates that a variable should be incremented by 1 after being evaluated. The sharp symbol also resembles a ligature of four ""+"" symbols (in a two-by-two grid), further implying that the language is an increment of C++. Due to technical limitations of display (standard fonts, browsers, etc.) and the fact that the sharp symbol (U+266F ‚ôØ MUSIC SHARP SIGN (&sharp;)) is not present on most keyboard layouts, the number sign (U+0023 # NUMBER SIGN (&num;)) was chosen to approximate the sharp symbol in the written name of the programming language.
This convention is reflected in the ECMA-334 C# Language Specification. The ""sharp"" suffix has been used by a number of other .NET languages that are variants of existing languages, including J# (a .NET language also designed by Microsoft that is derived from Java 1.1), A# (from Ada), and the functional programming language F#. The original implementation of Eiffel for .NET was called Eiffel#, a name retired since the full Eiffel language is now supported. The suffix has also been used for libraries, such as Gtk# (a .NET wrapper for GTK and other GNOME libraries) and Cocoa# (a wrapper for Cocoa)."|2023-09-26-01-52-24
C Sharp (programming language)|Syntax| The core syntax of the C# language is similar to that of other C-style languages such as C, C++ and Java, particularly:|2023-09-26-01-52-24
C Sharp (programming language)|Distinguishing features| Some notable features of C# that distinguish it from C, C++, and Java where noted, are:|2023-09-26-01-52-24
C Sharp (programming language)|Portability| By design, C# is the programming language that most directly reflects the underlying Common Language Infrastructure¬†(CLI).  Most of its intrinsic types correspond to value-types implemented by the CLI framework. However, the language specification does not state the code generation requirements of the compiler: that is, it does not state that a C# compiler must target a Common Language Runtime, or generate Common Intermediate Language (CIL), or generate any other specific format. Theoretically, a C# compiler could generate machine code like traditional compilers of C++ or Fortran.|2023-09-26-01-52-24
C Sharp (programming language)|Typing|" C# supports strongly, implicitly typed variable declarations with the keyword var, and implicitly typed arrays with the keyword new[] followed by a collection initializer. C# supports a strict Boolean data type, bool. Statements that take conditions, such as while and if, require an expression of a type that implements the true operator, such as the Boolean type. While C++ also has a Boolean type, it can be freely converted to and from integers, and expressions such as if (a) require only that a is convertible to bool, allowing a to be an int, or a pointer. C# disallows this ""integer meaning true or false"" approach, on the grounds that forcing programmers to use expressions that return exactly bool can prevent certain types of programming mistakes such as if (a = b) (use of assignment = instead of equality ==). C# is more type safe than C++. The only implicit conversions by default are those that are considered safe, such as widening of integers. This is enforced at compile-time, during JIT, and, in some cases, at runtime. No implicit conversions occur between Booleans and integers, nor between enumeration members and integers (except for literal 0, which can be implicitly converted to any enumerated type). Any user-defined conversion must be explicitly marked as explicit or implicit, unlike C++ copy constructors and conversion operators, which are both implicit by default. C# has explicit support for covariance and contravariance in generic types, unlike C++ which has some degree of support for contravariance simply through the semantics of return types on virtual methods. Enumeration members are placed in their own scope. The C# language does not allow for global variables or functions. All methods and members must be declared within classes. Static members of public classes can substitute for global variables and functions. Local variables cannot shadow variables of the enclosing block, unlike C and C++."|2023-09-26-01-52-24
C Sharp (programming language)|Metaprogramming| Metaprogramming can be achieved in several ways:|2023-09-26-01-52-24
C Sharp (programming language)|Methods and functions| A method in C# is a member of a class that can be invoked as a function (a sequence of instructions), rather than the mere value-holding capability of a class property. As in other syntactically similar languages, such as C++ and ANSI C, the signature of a method is a declaration comprising in order: any optional accessibility keywords (such as private), the explicit specification of its return type (such as int, or the keyword void if no value is returned), the name of the method, and finally, a parenthesized sequence of comma-separated parameter specifications, each consisting of a parameter's type, its formal name and optionally, a default value to be used whenever none is provided. Certain specific kinds of methods, such as those that simply get or set a class property by return value or assignment, do not require a full signature, but in the general case, the definition of a class includes the full signature declaration of its methods. Like C++, and unlike Java, C# programmers must use the scope modifier keyword virtual to allow methods to be overridden by subclasses. Extension methods in C# allow programmers to use static methods as if they were methods from a class's method table, allowing programmers to add methods to an object that they feel should exist on that object and its derivatives. The type dynamic allows for run-time method binding, allowing for JavaScript-like method calls and run-time object composition. C# has support for strongly-typed function pointers via the keyword delegate. Like the Qt framework's pseudo-C++ signal and slot, C# has semantics specifically surrounding publish-subscribe style events, though C# uses delegates to do so. C# offers Java-like synchronized method calls, via the attribute [MethodImpl(MethodImplOptions.Synchronized)], and has support for mutually-exclusive locks via the keyword lock.|2023-09-26-01-52-24
C Sharp (programming language)|Property| C# supports classes with properties. The properties can be simple accessor functions with a backing field, or implement getter and setter functions. Since C# 3.0 the syntactic sugar of auto-implemented properties is available, where the accessor (getter) and mutator (setter) encapsulate operations on a single attribute of a class.|2023-09-26-01-52-24
C Sharp (programming language)|Namespace|" A C# namespace provides the same level of code isolation as a Java package or a C++ namespace, with very similar rules and features to a package. Namespaces can be imported with the ""using"" syntax."|2023-09-26-01-52-24
C Sharp (programming language)|Memory access|" In C#, memory address pointers can only be used within blocks specifically marked as unsafe, and programs with unsafe code need appropriate permissions to run. Most object access is done through safe object references, which always either point to a ""live"" object or have the well-defined null value; it is impossible to obtain a reference to a ""dead"" object (one that has been garbage collected), or to a random block of memory. An unsafe pointer can point to an instance of an unmanaged value type that does not contain any references to objects subject to garbage collections such as class instances, arrays or strings. Code that is not marked as unsafe can still store and manipulate pointers through the System.IntPtr type, but it cannot dereference them. Managed memory cannot be explicitly freed; instead, it is automatically garbage collected. Garbage collection addresses the problem of memory leaks by freeing the programmer of responsibility for releasing memory that is no longer needed in most cases. Code that retains references to objects longer than is required can still experience higher memory usage than necessary, however once the final reference to an object is released the memory is available for garbage collection."|2023-09-26-01-52-24
C Sharp (programming language)|Exception| A range of standard exceptions are available to programmers. Methods in standard libraries regularly throw system exceptions in some circumstances and the range of exceptions thrown is normally documented. Custom exception classes can be defined for classes allowing handling to be put in place for particular circumstances as needed. Checked exceptions are not present in C# (in contrast to Java). This has been a conscious decision based on the issues of scalability and versionability.|2023-09-26-01-52-24
C Sharp (programming language)|Polymorphism|" Unlike C++, C# does not support multiple inheritance, although a class can implement any number of ""interfaces"" (fully abstract classes). This was a design decision by the language's lead architect to avoid complications and to simplify architectural requirements throughout CLI. When implementing multiple interfaces that contain a method with the same name and taking parameters of the same type in the same order (i.e. the same signature), similar to Java, C# allows both a single method to cover all interfaces and if necessary specific methods for each interface. However, unlike Java, C# supports operator overloading."|2023-09-26-01-52-24
C Sharp (programming language)|Language Integrated Query (LINQ)| C# has the ability to utilize LINQ through the .NET Framework. A developer can query a variety of data sources, provided IEnumerable<T> interface is implemented on the object. This includes XML documents, an ADO.NET dataset, and SQL databases. Using LINQ in C# brings advantages like Intellisense support, strong filtering capabilities, type safety with compile error checking ability, and consistency for querying data over a variety of sources.  There are several different language structures that can be utilized with C# and LINQ and they are query expressions, lambda expressions, anonymous types, implicitly typed variables, extension methods, and object initializers. LINQ has two syntaxes: query syntax and method syntax. However, the compiler always converts the query syntax to method syntax at compile time.|2023-09-26-01-52-24
C Sharp (programming language)|Functional programming| Though primarily an imperative language, C# always adds functional features over time, for example:|2023-09-26-01-52-24
C Sharp (programming language)|Common type system| C# has a unified type system. This unified type system is called Common Type System (CTS). A unified type system implies that all types, including primitives such as integers, are subclasses of the System.Object class. For example, every type inherits a ToString() method.|2023-09-26-01-52-24
C Sharp (programming language)|Categories of data types| CTS separates data types into two categories: Instances of value types neither have referential identity nor referential comparison semantics. Equality and inequality comparisons for value types compare the actual data values within the instances, unless the corresponding operators are overloaded. Value types are derived from System.ValueType, always have a default value, and can always be created and copied. Some other limitations on value types are that they cannot derive from each other (but can implement interfaces) and cannot have an explicit default (parameterless) constructor because they already have an implicit one which initializes all contained data to the type-dependent default value (0, null, or alike). Examples of value types are all primitive types, such as int (a signed 32-bit integer), float (a 32-bit IEEE floating-point number), char (a 16-bit Unicode code unit), and System.DateTime (identifies a specific point in time with nanosecond precision). Other examples are enum (enumerations) and struct (user defined structures). In contrast, reference types have the notion of referential identity, meaning that each instance of a reference type is inherently distinct from every other instance, even if the data within both instances is the same. This is reflected in default equality and inequality comparisons for reference types, which test for referential rather than structural equality, unless the corresponding operators are overloaded (such as the case for System.String). Some operations are not always possible, such as creating an instance of a reference type, copying an existing instance, or performing a value comparison on two existing instances. Though specific reference types can provide such services by exposing a public constructor or implementing a corresponding interface (such as ICloneable or IComparable). Examples of reference types are object (the ultimate base class for all other C# classes), System.String (a string of Unicode characters), and System.Array (a base class for all C# arrays). Both type categories are extensible with user-defined types.|2023-09-26-01-52-24
C Sharp (programming language)|Boxing and unboxing| Boxing is the operation of converting a value-type object into a value of a corresponding reference type. Boxing in C# is implicit. Unboxing is the operation of converting a value of a reference type (previously boxed) into a value of a value type. Unboxing in C# requires an explicit type cast. A boxed object of type T can only be unboxed to a T (or a nullable T). Example:|2023-09-26-01-52-24
C Sharp (programming language)|Libraries| The C# specification details a minimum set of types and class libraries that the compiler expects to have available. In practice, C# is most often used with some implementation of the Common Language Infrastructure (CLI), which is standardized as ECMA-335 Common Language Infrastructure (CLI). In addition to the standard CLI specifications, there are many commercial and community class libraries that build on top of the .NET framework libraries to provide additional functionality. C# can make calls to any library included in the List of .NET libraries and frameworks.|2023-09-26-01-52-24
C Sharp (programming language)|Hello World|" The following is a very simple C# program, a version of the classic ""Hello world"" example using the top-level statements feature introduced in C# 9: For code written as C# 8 or lower, the entry point logic of a program must be written in a Main method inside a type: This code will display this text in the console window: Each line has a purpose: The above line imports all types in the System namespace. For example, the Console class used later in the source code is defined in the System namespace, meaning it can be used without supplying the full name of the type (which includes the namespace). This line is a comment; it describes and documents the code for the programmer(s). Above is a class definition for the Program class. Everything that follows between the pair of braces describes that class. The curly brackets demarcate the boundaries of a code block. In this first instance, they are marking the start and end of the Program class. This declares the class member method where the program begins execution. The .NET runtime calls the Main method. Unlike in Java, the Main method does not need the public keyword, which tells the compiler that the method can be called from anywhere by any class. Writing static void Main(string[] args) is equivalent to writing private static void Main(string[] args). The static keyword makes the method accessible without an instance of Program. Each console application's Main entry point must be declared static otherwise the program would require an instance of Program, but any instance would require a program. To avoid that irresolvable circular dependency, C# compilers processing console applications (like that above) report an error if there is no static Main method. The void keyword declares that Main has no return value. This line writes the output. Console is a static class in the System namespace. It provides an interface to the standard input, output, and error streams for console applications. The program calls the Console method WriteLine, which displays on the console a line with the argument, the string ""Hello, world!""."|2023-09-26-01-52-24
C Sharp (programming language)|GUI|" A Windows GUI example: This example is similar to the previous example, except that it generates a dialog box that contains the message ""Hello, World!"" instead of writing it to the console."|2023-09-26-01-52-24
C Sharp (programming language)|Images|" Another useful library is the System.Drawing library, which is used to programmatically draw images. For example: This will create an image that is identical to that stored in ""Image.png""."|2023-09-26-01-52-24
C Sharp (programming language)|Standardization and licensing| In August 2001, Microsoft, Hewlett-Packard and Intel co-sponsored the submission of specifications for C# as well as the Common Language Infrastructure (CLI) to the standards organization Ecma International. In December 2001, ECMA released ECMA-334 C# Language Specification. C# became an ISO/IEC standard in 2003 (ISO/IEC 23270:2003 - Information technology ‚Äî Programming languages ‚Äî C#). ECMA had previously adopted equivalent specifications as the 2nd edition of C#, in December 2002. In June 2005, ECMA approved edition 3 of the C# specification, and updated ECMA-334. Additions included partial classes, anonymous methods, nullable types, and generics (somewhat similar to C++ templates). In July 2005, ECMA submitted to ISO/IEC JTC 1/SC 22, via the latter's Fast-Track process, the standards and related TRs. This process usually takes 6‚Äì9 months. The C# language definition and the CLI are standardized under ISO/IEC and Ecma standards that provide reasonable and non-discriminatory licensing protection from patent claims. Microsoft initially agreed not to sue open-source developers for violating patents in non-profit projects for the part of the framework that is covered by the Open Specification Promise. Microsoft has also agreed not to enforce patents relating to Novell products against Novell's paying customers with the exception of a list of products that do not explicitly mention C#, .NET or Novell's implementation of .NET (The Mono Project). However, Novell maintained that Mono does not infringe any Microsoft patents. Microsoft also made a specific agreement not to enforce patent rights related to the Moonlight browser plugin, which depends on Mono, provided it is obtained through Novell. A decade later, Microsoft began developing free, open-source, and cross-platform tooling for C#, namely Visual Studio Code, .NET Core, and Roslyn. Mono joined Microsoft as a project of Xamarin, a Microsoft subsidiary.|2023-09-26-01-52-24
C Sharp (programming language)|Implementations| Microsoft is leading the development of the open-source reference C# compilers and set of tools. The first compiler, Roslyn, compiles into intermediate language (IL), and the second one, RyuJIT, is a JIT (just-in-time) compiler, which is dynamic and does on-the-fly optimization and compiles the IL into native code for the front-end of the CPU. RyuJIT is open source and written in C++. Roslyn is entirely written in managed code (C#), has been opened up and functionality surfaced as APIs.  It is thus enabling developers to create refactoring and diagnostics tools. Two branches of official implementation are .NET Framework (closed-source, Windows-only) and .NET Core (open-source, cross-platform); they eventually converged into one open-source implementation: .NET 5.0. At .NET Framework 4.6, a new JIT compiler replaced the former. Other C# compilers (some of which include an implementation of the Common Language Infrastructure and .NET class libraries): The Unity game engine uses C# as its primary scripting language. The Godot game engine has implemented an optional C# module thanks to a donation of $24,000 from Microsoft.|2023-09-26-01-52-24
Caml|| Caml (originally an acronym for Categorical Abstract Machine Language) is a multi-paradigm, general-purpose programming language which is a dialect of the ML programming language family.  Caml was developed in France at INRIA and ENS. Caml is statically typed, strictly evaluated, and uses automatic memory management. OCaml, the main descendant of Caml, adds many features to the language, including an object layer.|2023-08-18-07-03-48
Caml|Examples| In the following, # represents the Caml prompt.|2023-08-18-07-03-48
Caml|Factorial function (recursion and purely functional programming)| Many mathematical functions, such as factorial, are most naturally represented in a purely functional form. The following recursive, purely functional Caml function implements factorial: The function can be written equivalently using pattern matching: This latter form is the mathematical definition of factorial as a recurrence relation. Note that the compiler inferred the type of this function to be int -> int, meaning that this function maps ints onto ints. For example, 12! is:|2023-08-18-07-03-48
Caml|Numerical derivative (higher-order functions)|" Since Caml is a functional programming language, it is easy to create and pass around functions in Caml programs. This capability has an enormous number of applications. Calculating the numerical derivative of a function is one such application. The following Caml function d computes the numerical derivative of a given function f at a given point x: This function requires a small value delta. A good choice for delta is the cube root of the machine epsilon[citation needed]. The type of the function d indicates that it maps a float onto another function with the type (float -> float) -> float -> float. This allows us to partially apply arguments. This functional style is known as currying. In this case, it is useful to partially apply the first argument delta to d, to obtain a more specialised function: Note that the inferred type indicates that the replacement d is expecting a function with the type float -> float as its first argument. We can compute a numerical approximation to the derivative of 




x

3


‚àí
x
‚àí
1


{\displaystyle x^{3}-x-1}

 at 



x
=
3


{\displaystyle x=3}

 with: The correct answer is 




f
‚Ä≤

(
x
)
=
3

x

2


‚àí
1
‚Üí

f
‚Ä≤

(
3
)
=
27
‚àí
1
=
26


{\displaystyle f'(x)=3x^{2}-1\rightarrow f'(3)=27-1=26}

. The function d is called a ""higher-order function"" because it accepts another function (f) as an argument.
We can go further and create the (approximate) derivative of f, by applying d while omitting the x argument: The concepts of curried and higher-order functions are clearly useful in mathematical programs. In fact, these concepts are equally applicable to most other forms of programming and can be used to factor code much more aggressively, resulting in shorter programs and fewer bugs."|2023-08-18-07-03-48
Caml|Discrete wavelet transform (pattern matching)| The 1D Haar wavelet transform of an integer-power-of-two-length list of numbers can be implemented very succinctly in Caml and is an excellent example of the use of pattern matching over lists, taking pairs of elements (h1 and h2) off the front and storing their sums and differences on the lists s and d, respectively: For example: Pattern matching allows complicated transformations to be represented clearly and succinctly. Moreover, the Caml compiler turns pattern matches into very efficient code, at times resulting in programs that are shorter and faster than equivalent code written with a case statement (Cardelli 1984, p.¬†210.).|2023-08-18-07-03-48
Caml|History|" The first Caml implementation was written in Lisp by Asc√°nder Su√°rez in 1987 at the French Institute for Research in Computer Science and Automation (INRIA). Its successor, Caml Light, was implemented in C by Xavier Leroy and Damien Doligez, and the original was nicknamed ""Heavy Caml"" because of its higher memory and CPU requirements. Caml Special Light was a further complete rewrite that added a powerful module system to the core language. It was augmented with an object layer to become Objective Caml, eventually renamed OCaml."|2023-08-18-07-03-48
Carbon (programming language)||" Carbon is an experimental programming language designed for interoperability with C++. The project is open-source and was started at Google. Google engineer Chandler Carruth first introduced Carbon at the CppNorth conference in Toronto in July 2022. He stated that Carbon was created to be a C++ successor. The language is expected to have a 1.0 release in 2024 or 2025. The language intends to fix several perceived shortcomings of C++ but otherwise provides a similar feature set.
The main goals of the language are readability and ""bi-directional interoperability"" (which allows the user to include C++ code in the Carbon file), as opposed to using a new language like Rust, that, while being influenced by C++, is not two-way compatible with C++ programs. Changes to the language will be decided by the Carbon leads. Carbon's documents, design, implementation, and related tools are hosted on GitHub under the Apache-2.0 license with LLVM Exceptions."|2023-09-24-16-05-28
Carbon (programming language)|Example|" The following shows how a ""Hello, World!"" program is written in Carbon: The following is the equivalent ""Hello, World!"" program written in C++:"|2023-09-24-16-05-28
Carbon (programming language)|External links| This Google-related article is a stub. You can help Wikipedia by expanding it.|2023-09-24-16-05-28
Cedar (programming language)|| Redirect to:|2017-06-20-02-42-28
C√©u (programming language)||" C√©u is ""Structured Synchronous Reactive Programming"" 
According to its web page, C√©u supports synchronous concurrency with shared memory and deterministic execution and has a small memory footprint."|2023-08-06-14-45-24
C√©u (programming language)|Sources| This programming-language-related article is a stub. You can help Wikipedia by expanding it.|2023-08-06-14-45-24
Charm (programming language)|| Charm is a computer programming language devised in the early 1990s with similarities to the RTL/2, Pascal and C languages in addition to containing some unique features of its own. The Charm language is defined by a context-free grammar amenable to being processed by recursive descent parser as described in seminal books on compiler design. A set of Charm tools including a compiler, assembler and linker was made available for Acorn's RISC OS platform. Charm reworked for RISC OS platforms has subsequently been reviewed in Archive magazine. Charm is further described in the e-book Programming in Charm on the Raspberry Pi.|2022-10-21-20-00-42
Charm (programming language)|Grammar| The definition of the Charm grammar in Backus‚ÄìNaur form along with descriptive examples of Charm constructs is defined on the Charm language page. The language is block structured, with each block being introduced by a language keyword that is descriptive of the operation being performed in the block e.g. for, while, repeat (iteration), case, if (selection). Each block is enclosed by { and } delimiters.  Additionally language lines within a block are normally indented for clarity, though this not required as white space is ignored. Each grammatically conforming text represents a collection of executable code and associated data which can be used by a Charm tool set as a component when assembling a program that can be run under an operating system utilising the services it provides to do useful work such as data processing or interacting with users through a graphical user interface (GUI).|2022-10-21-20-00-42
Charm (programming language)|Data types| Charm is a strongly typed language, but does allow some implicit conversions between numeric and floating point types. The following basic variable types are supported: Data aggregates of the same type may be declared and statically initialised using the array keyword, and these may be multidimensional. Aggregates of different types may be declared using the record keyword, and it is allowable for such a declaration to define a union of record fields that overlay each other in terms of storage allocation. Modules may also aggregate a mixture of static and dynamic data members. Instances of both records and modules (dynamic content only) can be instantiated on the stack, or on the heap via the new operator. Modules may also define a constructor ~new procedure to initialise dynamic data and corresponding ~delete deconstructor procedure to release resources in a similar manner to the C++ language.|2022-10-21-20-00-42
Charm (programming language)|Referencing| Data or procedures within the scope of a module may be made global to the final application by using the export keyword. If a module wishes to reference a procedure or data from another Charm module, it does so using the import keyword. Modules may contain instance based member variables which are accessible through procedures declared with the dynamic keyword through the implicit first parameter this pointer. References to data constructs and procedures may be made using the ref keyword. These can be dereferenced using the val keyword. When using reference variables, comparison operators are available to check whether two reference variables refer to the same item of data (¬†:=: ) or whether the data they point to is the same ( = ).|2022-10-21-20-00-42
Charm (programming language)|Example| The original classic Hello world program written in Charm is: and the equivalent latest version following evolutionary syntactic language changes is:|2022-10-21-20-00-42
Charm (programming language)|Tool set| Tool set implementations are expected to provide a compiler and an assembler to generate object files from Charm source code and assembler source code, which can then be linked together along with library and run time support files to generate an executable program. At the time of writing only one Charm tool set installation is available (free of charge) for download. The tools are themselves written in the Charm language, and the source code is available under the terms of the GNU General Public License. They run on RISC OS PCs and platforms with ARM CPUs (such as the Raspberry Pi) and on emulators for RISC OS which are hosted on Windows or Linux platforms (such as RPCEmu). Code generation for hardware assisted double precision floating point operations is supported for platforms based on ARM chips that support the VFP version 2 coprocessor architecture.|2022-10-21-20-00-42
Charm (programming language)|Compiler|" The Charm compiler is a recursive descent single pass compiler which parses Charm source code to generate quadruples of the form result¬†:= lhs op rhs in an intermediate language that supports arithmetic, logical and flow of control operations. Data is stored in temporaries which are assigned to registers and memory locations in the back end of the compiler. Two back ends are currently in existence, one generating Motorola 68000 assembly language, and the other generating ARM architecture. The quadruple output from the hello world example is: and the assembler output is: In more recent releases of Charm, the I/O procedures have been split into their own modules In and Out. Other standard library procedures are organised into a set of records with procedure references as fields. As part of this reorganisation, the write_string method is now invoked through the run time library module Out via static member reference .vdu as procedure str i.e. in the hello world example above write_string (""Hello world"") becomes Out.vdu.str (""Hello world"")."|2022-10-21-20-00-42
Charm (programming language)|Assembler| The assembler accepts instruction mnemonics, data declarations and directives and constructs an object file containing information readily understandable by the CPU of the target processor, in particular code instructions coded in binary.|2022-10-21-20-00-42
Charm (programming language)|Linker| One and only one of the Charm modules linked to form an executable program must contain a procedure matching one of the signatures: This is analogous to the main function in the C and Java languages. Here argc contains the number of parameters passed on the command line and argv contains a reference to an array of argc + 1 strings (one string per positional parameter in order and a terminating nil). In addition, modules may optional contain static startup and shutdown procedures invoked during program startup and shutdown that match the signatures: The linker adds any necessary header information required by the operating system in order to execute the program, and ensures the run time library assembler support code is run which sets up the run time environment (data and stack pointers) and passes control to the start procedure of the application. A map file showing the names of all modules linked to form the program along with global data and code references is optionally produced which can be used by debuggers and profilers.|2022-10-21-20-00-42
